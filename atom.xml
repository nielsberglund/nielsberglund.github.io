<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Niels Berglund</title>
  <link href="https://nielsberglund.com/atom.xml" rel="self"/>
  <link href="https://nielsberglund.com"/>
  <updated>2019-08-04T11:15:18+02:00</updated>
  <id>https://nielsberglund.com/</id>
  <generator uri="http://gohugo.io/">Hugo</generator>

  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 31, 2019]]></title>
    <link href="https://nielsberglund.com/2019/08/04/interesting-stuff---week-31-2019/" rel="alternate" type="text/html"/>
    <updated>2019-08-04T11:15:18+02:00</updated>
    <id>https://nielsberglund.com/2019/08/04/interesting-stuff---week-31-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://medium.com/@aptxkid/paxos-at-its-heart-is-very-simple-b6a0eafbeb50">Paxos at its heart is very simple</a>. This is a post where the author tries to explain <a href="https://en.wikipedia.org/wiki/Paxos_(computer_science)">Paxos</a> in simple terms. In my opinion, the author has made an excellent job of it.</li>
</ul>

<h2 id="data-science-machine-learning">Data Science / Machine Learning</h2>

<ul>
<li><a href="https://towardsdatascience.com/the-5-basic-statistics-concepts-data-scientists-need-to-know-2c96740377ae">The 5 Basic Statistics Concepts Data Scientists Need to Know</a>. Machine Learning is not <a href="https://towardsdatascience.com/no-machine-learning-is-not-just-glorified-statistics-26d3952234e3">glorified statistics</a>, but by understanding statistics, we can get a better understanding of what our Machine Learning / Data Science algorithms show us.</li>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2019/07/30/microsoft-machine-learning-server-9-4-is-now-available/">Microsoft Machine Learning Server 9.4 is now available</a>. The title says it all, Microsoft has now released Machine Learning Server 9.4, and it is based on Microsoft R Open 3.5.2 and Python 3.7.1. This means that we should be able to update R and Python in SQL Server Machine Learning Services.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/linkedin-streams-brooklin/">A Dive Into Streams @LinkedIn With Brooklin</a>. This is an <a href="https://www.infoq.com/">InfoQ</a> presentation where the presenter talks about Brooklin â€“ Linked In&rsquo;s managed data streaming service that supports multiple pluggable sources and destinations, which can be data stores or messaging systems. The presenter also dives deeper into Brooklin&rsquo;s architecture and use cases, as well as Linked In&rsquo;s future plans for Brooklin.</li>
<li><a href="https://www.confluent.io/blog/introducing-confluent-platform-5-3">Introducing Confluent Platform 5.3</a>. This is another post where the title says it all; Confluent has released Confluent PLatform 5.3. There are quite a few new features in this release. The ones that excite me the most are: Confluent Operator, Role Based Access Control, and Secret Protection - encrypting secrets within configuration files.<br /></li>
<li><a href="https://www.confluent.io/blog/building-shared-state-microservices-for-distributed-systems-using-kafka-streams">Building Shared State Microservices for Distributed Systems Using Kafka Streams</a>. This is an excellent article discussing how to use Kafka Streams to build shared state microservices that serve as fault-tolerant, highly available single sources of truth about the state of objects in a system. This article came at the right time as we are looking at that in <a href="/derivco">Derivco</a> at the moment!</li>
</ul>

<h2 id="sql-server-2019">SQL Server 2019</h2>

<ul>
<li><a href="/2019/08/03/sql-server-2019-ctp3.2--java/">SQL Server 2019 CTP3.2 &amp; Java</a>. Each new SQL Server CTP release seems to introduce new &ldquo;stuff&rdquo; for the Java language extension. In this post, I look at the inclusion of the Azul OpenJDK in SQL Server 2019 CTP 3.2, and what it introduces.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SQL Server 2019 CTP3.2 &amp; Java]]></title>
    <link href="https://nielsberglund.com/2019/08/03/sql-server-2019-ctp3.2--java/" rel="alternate" type="text/html"/>
    <updated>2019-08-03T06:55:33+02:00</updated>
    <id>https://nielsberglund.com/2019/08/03/sql-server-2019-ctp3.2--java/</id>
    <content type="html"><![CDATA[<p>It seems that for each new SQL Server 2019 CTP release, there are changes to the Java extensions, and CTP 3.2 is no exception. Well, that is not exactly true as in CTP 3.2 release the changes are not about the extension and how we write code, but Java itself.</p>

<p>One of the <a href="https://cloudblogs.microsoft.com/sqlserver/2019/07/24/free-supported-java-in-sql-server-2019-is-now-available/">announcements</a> of what is new in CTP 3.2 was that SQL Server now includes <a href="https://www.azul.com/">Azul System&rsquo;s</a> Zulu Embedded right out of the box for all scenarios where we use Java in SQL Server, including Java extensibility.</p>

<p>So, in this post, we look at the impact, (if any), this has to how we use the Java extensibility framework in SQL Server 2019.</p>

<p></p>

<p>First of all, I find Microsoft&rsquo;s change of stance related to Java very interesting as I remember back in the day the <a href="https://en.wikipedia.org/wiki/Microsoft_Java_Virtual_Machine">battle</a> between Sun and Microsoft regarding Java and the subsequent Java vs .NET &ldquo;war&rdquo;.</p>

<blockquote>
<p><strong>NOTE:</strong> Around <sup>1995</sup>&frasl;<sub>96</sub> I attended a Microsoft event in London where Microsoft introduced what was to be Visual J++. It went under the code name &ldquo;Jakarta&rdquo;. The MS people presenting at the conference explained why they chose that name: <em>Jakarta is the capital of Java</em>. That was not entirely true as Jakarta is the capital of Indonesia, but Java is the largest island, so&hellip;</p>
</blockquote>

<p>Anyway, let us get on with it, and see what happens when we install SQL Server 2019 CTP 3.2.</p>

<h2 id="installation">Installation</h2>

<p>You may remember from <a href="/2018/12/02/sql-server-2019-extensibility-framework--java---hello-world/">SQL Server 2019 Extensibility Framework &amp; Java - Hello World</a> that to enable Java in SQL Server 2019; you have to - during installation - choose the <em>Machine Learning Services</em> feature:</p>

<p><img src="/images/posts/sql_2k19_java_intro_install1.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Enable In-Database Machine Learning</em></p>

<p>In <em>Figure 1</em> we have chosen <em>Machine Learning Services</em> which ensures that the required components, (<em>Launchpad</em>, and so on), gets installed. In pre-CTP 3.2, we had to - in addition to choosing <em>Machine Learning Services</em> - also ensure that Java is installed on the box where we install the SQL Server instance.</p>

<blockquote>
<p><strong>NOTE:</strong> We also need to do other &ldquo;stuff&rdquo; if we want to enable Java, and I cover that below.</p>
</blockquote>

<p>When we install SQL Server CTP 3.2, we see something like so:</p>

<p><img src="/images/posts/s2k19_ctp32_java.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Java Feature Selection</em></p>

<p>What we see in <em>Figure 2</em> is:</p>

<ul>
<li>The <em>Machine Learning Services</em> feature is now named <em>Machine Learning Services and Language Extensions</em>.</li>
<li>Java appears as a feature in the same way as R/Python.</li>
</ul>

<p>We choose Java as a feature, as in <em>Figure 2</em>, and after feature rules and instance configuration, we come to a new dialog; <em>Java Install Location</em>:</p>

<p><img src="/images/posts/s2k19_ctp32_java2.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Java Install Location</em></p>

<p>In the dialog, we see in <em>Figure 3</em>, how we have the ability to choose to install the Azule Zulu OpenJDK which is part of the SQL Server install. We choose the OpenJDK and continue with the installation.</p>

<p>Eventually the installation finish, and if we look in the installation path of the instance we just installed, we see:</p>

<p><img src="/images/posts/s2k19_ctp32_openjdk.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Azule Zulu OpenJDK</em></p>

<p>If we drill down in the <code>AZUL-OpenJDK-JRE</code> directory, we see it is the directory for the Java JRE. So we now have Java installed, and theoretically, we should be able to execute Java code, exactly as we execute R/Python?</p>

<h2 id="post-install">Post Install</h2>

<p>Well, to execute Java code is not as straightforward as in the previous paragraph even though Java is now installed. We still have to do some things:</p>

<ul>
<li>Enable external scripts: <code>EXEC sp_configure 'external scripts enabled', 1</code></li>
<li>Register Java as an external language in the database from where you want to execute your code: <code>CREATE EXTERNAL LANGUAGE ...</code>. Read more about it in <a href="/2019/06/06/sql-server-2019-extensibility-framework--external-languages/">SQL Server 2019 Extensibility Framework &amp; External Languages</a>.</li>
<li>Deploy the Java external language extension SDK to the database above: <code>CREATE EXTERNAL LIBRARY ...</code>. Refer to <a href="/2019/05/26/java--sql-server-2019-extensibility-framework-the-sequel/">Java &amp; SQL Server 2019 Extensibility Framework: The Sequel</a> for more about that.</li>
</ul>

<p>When the above is done, we deploy our code either via <code>CREATE EXTERNAL LIBRARY ...</code>, or have our code available in the <code>CLASSPATH</code>. Having done that we should be able to execute:</p>

<pre><code class="language-sql">DECLARE @p1 int = 21;
DECLARE @p2 int = 21;
EXEC sp_execute_external_script
  @language = N'Java'
, @script = N'sql.JavaTest1'
, @params = N'@x int, @y int'
, @x = @p1
, @y = @p2   
GO
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Execute Java Code</em></p>

<p>In <em>Code Snippet 1</em> we see that we want to <code>EXEC sp_execute_external_script</code> against some Java code deployed as a <code>.jar</code>, (this is the same code as in the <a href="/2019/06/06/sql-server-2019-extensibility-framework--external-languages/">external languages</a> post). However, when we execute the result is like so:</p>

<p><img src="/images/posts/s2k19_ctp32_exec1.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Missing Environment Variable</em></p>

<p>So even though we explicitly said during the SQL installation that we want to use the included Java and it got installed, we still need to set the <code>JRE_HOME</code> variable. The installation does not do that for us.</p>

<p>After we have created the <code>JRE_HOME</code> environment variable and set it to point to the <code>AZUL-OpenJDK-JRE</code> directory we see in <em>Figure 4</em>, we need to restart the <em>Launchpad</em> service, and then we can execute the code in <em>Code Snippet 1</em> again:</p>

<p><img src="/images/posts/s2k19_ctp32_exec2.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>Success</em></p>

<p>Now everything works as we see in <em>Figure 6</em>.</p>

<h2 id="scenarios">Scenarios</h2>

<p>Even though we explicitly choose the included Java in the installation, we still need to set up Java as in pre-CTP 3.2, so what does this give us?</p>

<p>It gives us peace of mind! Remember that Oracle is phasing out free support for Java, and I wonder how interested they are in solving issues with Java on SQL Server. If we use open source OpenJDK, we rely on the community for support, which may not be ideal in an enterprise production environment. So by using the included Java, we are guaranteed enterprise support - peace of mind!</p>

<p>Does this then mean that we need to use the included Java? Not at all! Below follows some scenarios/options. All require what we did above; create external language, deploy the SDK, deploy your code:</p>

<ul>
<li>Enable <em>Machine Learning Services and Language Extensions</em> only, (no Java): Ensure that your <code>JRE_HOME</code> environment variable points to whatever Java JRE you want to use.</li>
<li>Enable <em>Machine Learning Services and Language Extensions</em> including Java, (as per above): The <code>JRE_HOME</code> variable can point to some other Java JRE than Azul, even though the Azul JDK gets installed.</li>
</ul>

<p>What happens if I enable Java, but instead of choosing the included JRE as in <em>Figure 3</em>, I choose the second option: I provide the location of a different JRE? For Java in <em>Machine Learning Services and Language Extensions</em> that has no effect. You need to, as in all other scenarios, point the <code>JRE_HOME</code> variable to a valid JRE directory.</p>

<blockquote>
<p><strong>NOTE:</strong> It is good practice to whenever you change, (or first set), the <code>JRE_HOME</code> variable to restart the <em>Launchpad</em> service.</p>
</blockquote>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 30, 2019]]></title>
    <link href="https://nielsberglund.com/2019/07/28/interesting-stuff---week-30-2019/" rel="alternate" type="text/html"/>
    <updated>2019-07-28T09:40:56+02:00</updated>
    <id>https://nielsberglund.com/2019/07/28/interesting-stuff---week-30-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="http://muratbuffalo.blogspot.com/2019/07/dissecting-performance-bottlenecks-of.html">Dissecting performance bottlenecks of strongly-consistent replication protocols</a>. This is a post by <a href="https://twitter.com/muratdemirbas">Murat</a> about one of his white papers. The post dissects the white paper which discusses the performance of Paxos protocols. If you are into distributed computing and consensus protocol, this is for you.</li>
<li><a href="https://goto.docker.com/virtual-event-docker-enterprise-3.0.html">Drive High-Velocity Innovation with Docker Enterprise 3.0</a>. Earlier this week <a href="https://blog.docker.com/2019/07/announcing-docker-enterprise-3-0-ga/">Docker announced general availability</a> of Docker Enterprise 3.0. The post I link to is registration for a 5-part virtual event to get a deep dive on new features and enhancements in Docker Enterprise 3.0.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/kafka-distributed-deployments">Streaming Log Analytics with Kafka</a>. This is an <a href="https://www.infoq.com/">InfoQ</a> presentation where the presenter discusses how and why they use Kafka internally and demos how they utilize it as a straightforward event-sourcing model for distributed deployments.</li>
<li><a href="https://www.confluent.io/blog/fault-tolerance-distributed-systems-tracing-with-apache-kafka-jaeger">Fault Tolerance in Distributed Systems: Tracing with Apache Kafka and Jaeger</a>. This is the second post about distributed tracing with Kafka and Jaeger. This post looks at how to make Jaeger fault-tolerant, i.e. how to safeguard against Jaeger going down.</li>
</ul>

<h2 id="sql-server-2019">SQL Server 2019</h2>

<ul>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2019/07/24/sql-server-2019-community-technology-preview-3-2-is-now-available/">SQL Server 2019 community technology preview 3.2 is now available</a>. I guess the title of the post says it all; CTP 3.2 is released. One of the big &ldquo;things&rdquo; in this release is that the SQL Server 2019 Big Data Cluster (BDC) is now in public preview. What this means is that you no longer need to sign up for the preview, but you can deploy directly. Look out for an upcoming blog post from me regarding this.</li>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2019/07/24/free-supported-java-in-sql-server-2019-is-now-available/">Free supported Java in SQL Server 2019 is now available</a>. Wow, &ldquo;the times they are a changin&rsquo;&rdquo;, as <a href="https://www.youtube.com/watch?v=e7qQ6_RV4VQ">Bob Dylan sang</a>! I remember years and years ago, the battle between Microsoft and Sun regarding <a href="https://en.wikipedia.org/wiki/Microsoft_Java_Virtual_Machine">Java</a>. Who would then have thought that come 2019, Microsoft would include Java in one of Microsoft&rsquo;s flagship products: SQL Server 2019. Very cool!</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 29, 2019]]></title>
    <link href="https://nielsberglund.com/2019/07/21/interesting-stuff---week-29-2019/" rel="alternate" type="text/html"/>
    <updated>2019-07-21T06:16:02+02:00</updated>
    <id>https://nielsberglund.com/2019/07/21/interesting-stuff---week-29-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="data-science-machine-learning">Data Science / Machine Learning</h2>

<ul>
<li><a href="https://devblogs.microsoft.com/dotnet/announcing-ml-net-1-2-and-model-builder-updates-machine-learning-for-net/">Announcing ML.NET 1.2 and Model Builder updates (Machine Learning for .NET)</a>. So, <a href="https://dotnet.microsoft.com/apps/machinelearning-ai/ml-dotnet">ML.NET</a> has come a long way since its inception, and I have covered it on and off in these roundups during the last year or so. In the linked blog post, Microsoft introduces ML.NET 1.2, with some cool new features. I particularly like the time series support and the updates to Model Builder.</li>
</ul>

<h2 id="misc">Misc.</h2>

<ul>
<li><a href="https://medium.com/it-dead-inside/implementing-new-tech-in-your-company-is-tougher-than-you-think-e5ff7f887bff">Implementing new tech in your company is tougher than you think</a>. The topic of this blog post is close to my heart; getting new technologies into a company. The post talks about some of the &ldquo;barriers to entry&rdquo; when it comes to new tech. Well worth a read!</li>
<li><a href="https://medium.com/@rusty.alderson/enterprise-data-architecture-c5c579b54abe">Enterprise Data Architecture</a>. This post is a white paper about data architecture and data architects. It tries to define what data architecture is and what it means to be a data architect. Very interesting!</li>
</ul>

<h2 id="sql-server-2019">SQL Server 2019</h2>

<ul>
<li><a href="/2019/07/16/set-your-sql-server-data-free-with-kafka-extensibility-framework/">Set Your SQL Server Data Free with Kafka: Extensibility Framework</a>. In this post by yours truly I look at how we can push data from SQL Server 2019 to Apache Kafka by the use of SQL Server Extensibility Framework, and the Java language extension.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Set Your SQL Server Data Free with Kafka: Extensibility Framework]]></title>
    <link href="https://nielsberglund.com/2019/07/16/set-your-sql-server-data-free-with-kafka-extensibility-framework/" rel="alternate" type="text/html"/>
    <updated>2019-07-16T05:10:24+02:00</updated>
    <id>https://nielsberglund.com/2019/07/16/set-your-sql-server-data-free-with-kafka-extensibility-framework/</id>
    <content type="html"><![CDATA[<p>As many of you may know, (or not), is that my background is SQL Server. Ever since I started programming, SQL Server has been my &ldquo;trusty companion&rdquo;, and my belief is that if you don&rsquo;t have SQL Server as a backend, then there is something wrong. At work, (<a href="/derivco">Derivco</a>), it is the same thing, and we are jokingly saying that we do not have business logic in the database, we have full-blown applications!</p>

<p>However, both me personally and at work, we do realise the value of streaming data; for real-time processing as well as to distribute data without having to rely on replication. In the ideal world, we would change the applications/systems that are the source of the data to both publish the data as event streams as well as persisting the data to the database. However, it may not be possible to change those applications/systems - at least not in the time frame we would like. So what we want to do is to use the database as the source of the data, but treat the data, not as rows in a database but, as streaming events.</p>

<p>This is the first post in a &ldquo;mini&rdquo; series where we look at how we can do what is outlined above. In this post, we look at how to use the <strong>SQL Server Extensibility Framework</strong>, and more specifically the Java language extension to solve the issue.</p>

<p></p>

<blockquote>
<p><strong>NOTE:</strong> What we do in this post requires SQL Server 2019 CTP 3.0+.</p>
</blockquote>

<h2 id="scenario-code">Scenario &amp; Code</h2>

<p>In this post, we look at a scenario somewhat like what we have at <a href="/derivco">Derivco</a>. We have online Casino gameplay where the user, (player), plays Casino games, (slots, table games, etc.), on his PC, tablet or mobile. The game play is persisted to a SQL Server database:</p>

<pre><code class="language-sql">USE master;
GO

DROP DATABASE IF EXISTS GamePlayDB;
GO

CREATE DATABASE GamePlayDB;
GO

USE GamePlayDB;
GO

CREATE TABLE dbo.tb_GamePlay
(
  RowID bigint identity PRIMARY KEY,
  UserID int NOT NULL,
  GameID int NOT NULL,
  WagerAmount decimal(10, 2) NOT NULL,
  PayoutAmount decimal(10, 2) NOT NULL,
  EventTime datetime2 NOT NULL
);
GO

</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Game Play Database</em></p>

<p>In <em>Code Snippet 1</em> we see some SQL code which creates:</p>

<ul>
<li>The database <code>GamePlayDB</code>.</li>
<li>A table, <code>dbo.tb_GamePlay</code>, to persist each outcome of a spin, hand, etc.</li>
</ul>

<blockquote>
<p><strong>NOTE:</strong> The code is as simple it can be, this to concentrate on the important parts.</p>
</blockquote>

<p>When the player spins, etc., the gameplay goes via various services, and finally, the last service in the call-chain calls a stored procedure which persists the outcome:</p>

<pre><code class="language-sql">CREATE PROCEDURE dbo.pr_LogWager @UserID INT,
                                 @GameID INT,
                                 @WagerAmount decimal(10, 2),
                                 @PayoutAmount decimal(10, 2),
                                 @EventTime datetime2
AS
BEGIN
  IF(@EventTime IS NULL)
  BEGIN
    SET @EventTime = SYSUTCDATETIME();
  END
  BEGIN TRY
    BEGIN TRAN
      INSERT INTO dbo.tb_GamePlay(UserID, GameID, WagerAmount, 
                                  EventTime, PayoutAmount)
      VALUES(@UserID, @GameID, @WagerAmount, @PayoutAmount, EventTime);
      --do more tx &quot;stuff&quot; here
    COMMIT TRAN;

  END TRY
  BEGIN CATCH
    ROLLBACK TRAN;
  END CATCH

END  
GO
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Game Play Procedure</em></p>

<p>We see how the procedure in <em>Code Snippet 2</em> takes relevant gameplay details and inserts them into the <code>dbo.tb_GamePlay</code> table.</p>

<p>In our scenario, we want to stream the individual gameplay events, but we cannot alter the services which generate the gameplay. We instead decide to generate the event from the database using, as we mentioned above, the SQL Server Extensibility Framework.</p>

<h2 id="sql-server-extensibility-frameworkback">SQL Server Extensibility FrameworkBack</h2>

<p>Together with the release of SQL Server 2016, Microsoft introduced the feature to be able to execute R script code from inside SQL Server against an external R engine. SQL Server 2017 added the ability to execute Python code against an external Python environment.</p>

<p>A SQL Server framework enables the ability to call R/Python: the <strong>SQL Server Extensibility Framework</strong>, and you can read more about it in my blog post <a href="/2019/06/06/sql-server-2019-extensibility-framework--external-languages/">SQL Server 2019 Extensibility Framework &amp; External Languages</a>. For SQL Server 2019, Java is available as an external language, and that is what we use in this post.</p>

<p>If you are unsure about what I talk about here are some blog posts that may help:</p>

<ul>
<li><a href="/2018/03/07/microsoft-sql-server-r-services---sp_execute_external_script---i/">Microsoft SQL Server R Services: sp_execute_external_script - I</a> - When calling out to R/Python/Java you do it from a specific procedure: <code>sp_execute_external_script</code>. This is the first post of three looking at <code>sp_execute_external_script</code>.</li>
<li><a href="/2018/03/11/microsoft-sql-server-r-services---sp_execute_external_script---ii/">Microsoft SQL Server R Services - sp_execute_external_script - II</a> - The second post about <code>sp_execute_external_script</code>.</li>
<li><a href="/2018/03/21/microsoft-sql-server-r-services---sp_execute_external_script---iii/">Microsoft SQL Server R Services: sp_execute_external_script - III</a>  - The third post about <code>sp_execute_external_script</code>.</li>
<li><a href="/s2k19_ext_framework_java">SQL Server 2019 Extensibility Framework &amp; Java</a> - A series of posts discussing how to write Java code so we can call it from SQL server.</li>
</ul>

<h2 id="solution">Solution</h2>

<p>As we mentioned above, we want to generate the wager &ldquo;event&rdquo; from inside the database, and there are a couple of ways we can do that:</p>

<ul>
<li><a href="https://docs.microsoft.com/en-us/sql/relational-databases/track-changes/about-change-data-capture-sql-server?view=sql-server-2017">Change Data Capture</a> - CDC captures insert, update, and delete activity that is applied to a SQL Server table, and makes the details of the changes available to consumers.</li>
<li><a href="https://docs.microsoft.com/en-us/sql/relational-databases/track-changes/about-change-tracking-sql-server?view=sql-server-2017">Change Tracking</a> - CT tracks and makes available rows that change in SQL Server tables. The difference
between CT and CDC is that CT does not capture the changed values.</li>
</ul>

<p>Both CDC, and CT can be used to get data from SQL Server to Kafka, and we will cover that in future posts. In this post however, we look at doing the event generation in another way: hook-points in stored procedures.</p>

<p>The idea with stored procedure hook-points is that at a place in a stored procedure, a code block is inserted, (hooked in), and this code block executes some code. In this case, it publishes a message to Kafka.</p>

<blockquote>
<p><strong>NOTE:</strong> With the definition of a hook-point above, you may ask what the difference is between a hook-point and an ordinary procedure call? There is no real difference; the way I see it is that the hook-point executes code which is, to a large degree, un-related to what the procedure does. Oh, and the name &ldquo;hook-point&rdquo; sounds cool.</p>
</blockquote>

<p>So, where do we insert the hook-point in the procedure we see in <em>Code Snippet 2</em>? As the <code>pr_LogWager</code> is transactional, we insert the hook-point after the commit, so we do not publish any messages for rolled back wagers. The implication of publishing the event/message after the last commit is that if you have several stored procedures calling each other, the hook-point may be in the beginning of the call-chain.</p>

<p>Now we know where the hook-point should be, but what should it do? Obviously, it should publish to Kafka, but what should it publish? In this case, it should publish an event, so the hook-point also needs to generate the event. What the event should look like is very much up to you, for now, let us assume the event looks something like so:</p>

<pre><code class="language-json">{
   &quot;$schema&quot;: &quot;http://json-schema.org/draft-04/schema#&quot;,
   &quot;title&quot;: &quot;Wager&quot;,
   &quot;description&quot;: &quot;A placed wager&quot;,
   &quot;type&quot;: &quot;object&quot;,
  
   &quot;properties&quot;: {
  
      &quot;eventTypeId&quot;: {
         &quot;description&quot;: &quot;Unique identifier for the event type&quot;,
         &quot;type&quot;: &quot;integer&quot;
      },

      &quot;userId&quot;: {
         &quot;description&quot;: &quot;The unique identifier for a user&quot;,
         &quot;type&quot;: &quot;integer&quot;
      },

      &quot;userId&quot;: {
         &quot;description&quot;: &quot;The unique identifier for a game&quot;,
         &quot;type&quot;: &quot;integer&quot;
      },

      &quot;wagerAmount&quot;: {
         &quot;description&quot;: &quot;Amount wagered&quot;,
         &quot;type&quot;: &quot;number&quot;
      },

      &quot;payoutAmount&quot;: {
         &quot;description&quot;: &quot;Amount paid out (win)&quot;,
         &quot;type&quot;: &quot;number&quot;
      },
      
      &quot;eventTime&quot;: {
         &quot;description&quot;: &quot;Time when the wager happened&quot;,
         &quot;type&quot;: &quot;string&quot;,
         &quot;format&quot;: &quot;date-time&quot;
      },
    
     
   },
  
   &quot;required&quot;: [&quot;eventTypeId&quot;, &quot;userId&quot;, &quot;userId&quot;, 
                &quot;wagerAmount&quot;, &quot;payoutAmount&quot;]
}
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Wager Schema</em></p>

<p>We see in <em>Code Snippet 3</em> how the schema for the event looks very much like what we persist to the table in our stored procedure. The only difference is that we also define an <code>eventTypeId</code>, which we can use when we do stream processing to filter out various types of events.</p>

<p>Even though the hook-point can be just a few lines of T-SQL code, best practice is to define an event-specific stored procedure:</p>

<pre><code class="language-sql">CREATE PROCEDURE dbo.pr_GenerateAndPublishWagerEvent @UserID INT,
                                          @GameID INT,
                                          @WagerAmount decimal(10, 2),
                                          @PayoutAmount decimal(10, 2),
                                          @EventTime datetime2
AS
BEGIN
  DECLARE @msg nvarchar(max);

  --generate the event
  SET @msg =  (SELECT 1500 AS eventTypeId,
                 @UserId AS userId,
                 @GameId AS gameId,
                 @WagerAmount AS wagerAmount,
                 @PayoutAmount AS payoutAmount,
                 @EventTime AS eventTime
     FOR JSON PATH, WITHOUT_ARRAY_WRAPPER);
  -- call &quot;something&quot; to publish to Kafka
END
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Hook Point Procedure</em></p>

<p>The code we see in <em>Code Snippet 4</em> is the start of the hook-point procedure. We see how the procedure generates the wager event by using the T-SQL <code>FOR JSON</code> syntax. A placeholder for the publish call follows the creation of the event.</p>

<blockquote>
<p><strong>NOTE:</strong> when you look at the hook-point procedure it may seem like overengineering as the call is basically a pass-through from the <code>dbo.pr_LogWager</code> procedure. The reason we have a specific procedure is that in the &ldquo;real world&rdquo; you most likely do more things inside the procedure.</p>
</blockquote>

<p>So, the placeholder for the publish call; that is where we call into some Java code that publishes to Kafka. Before we look at the Java code, let us see what the Kafka setup should look like.</p>

<h2 id="kafka">Kafka</h2>

<p>I have assumed in this post that we have Kafka installed &ldquo;somewhere&rdquo;, and that we can connect to it. If that is not the case, have a look at the <a href="/2019/06/18/confluent-platform--kafka-for-a-.net-developer-on-windows/">Confluent Platform &amp; Kafka for a .NET Developer on Windows</a> post to see how you install Kafka in a Docker container.</p>

<p>Now, when we have Kafka installed, let us create two topics:</p>

<ul>
<li><code>testTopic</code> - as the name implies, it is a topic for test. We use it initially just to make sure our Java code works. Create it with 1 partition.</li>
<li><code>wagers</code> - this is the topic to where we publish wagers. We create it with 4 partitions.</li>
</ul>

<blockquote>
<p><strong>NOTE:</strong> In a future post we will talk more about partitions, and what role they play.</p>
</blockquote>

<p>As I did in the <a href="/2019/06/18/confluent-platform--kafka-for-a-.net-developer-on-windows/">post</a> mentioned above, I create the topics using <em>Control Center</em>:</p>

<p><img src="/images/posts/sql_kafka_extlang_topics.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Create Wagers Topic</em></p>

<p>In <em>Figure 1</em> we see how I create the <code>wagers</code> topic in <em>Control Center</em>&rsquo;s <em>New topic</em> screen.</p>

<blockquote>
<p><strong>NOTE:</strong> If you don&rsquo;t have <em>Control Center</em> you can create topics via the command line, using the <code>kafka-topics --create ...</code> statement.</p>
</blockquote>

<p>When we have the two topics, let us write some Java code.</p>

<h2 id="java">Java</h2>

<blockquote>
<p><strong>NOTE:</strong> I am by no means a Java developer, so I apologize in advance for simplistic and naive code. Furthermore, the code is definitely not production code; no error handling, etc., so use it on your own risk.</p>
</blockquote>

<p>For this blog post, I use <em>VS Code</em> together with the <em>Maven</em> extension. You can read more about that in the <a href="/2019/01/17/sql-server-2019--java-with-visual-studio-code/">SQL Server 2019 &amp; Java with Visual Studio Code</a> post.</p>

<p>To begin with, let us:</p>

<ul>
<li>Create an empty folder where you want your Java project.</li>
<li>Open <em>VS Code</em> and open the folder you created above.</li>
<li>Create a new <em>Maven</em> project, using the archetype <code>maven-archetype-quickstart</code>.</li>
</ul>

<p>During the creation of the project, you are asked for some properties of the project: <code>groupId</code>,  <code>artifactId</code>, <code>version</code>, and <code>packageId</code>. In my project, I set them to:</p>

<ul>
<li><code>groupId</code>: <code>com.nielsberglund.sqlserver</code>.</li>
<li><code>artifactId</code>: <code>SqlToKafka</code>.</li>
<li><code>version</code>: <code>1.0</code>.</li>
<li><code>package</code>: <code>kafkapublish</code>.</li>
</ul>

<p>To be able to publish to Kafka, we need a Java Kafka client, and we use the native client from <code>org.apache.kafka</code>: <code>kafka-clients</code>. To use the client, we need to add it as a dependency in the projects <code>pom.xml</code> file:</p>

<pre><code class="language-xml">&lt;dependencies&gt;
  &lt;dependency&gt;
    &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;
    &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;
    &lt;version&gt;2.3.0&lt;/version&gt;
  &lt;/dependency&gt;
  ...
&lt;/dependencies&gt;
</code></pre>

<p><strong>Code Snippet 5:</strong> <em>Kafka Clients Dependencies</em></p>

<p>We see in <em>Code Snippet 5</em> the dependency we added to the <code>pom.xml</code> file. We are now ready to start to write some code.</p>

<p>To begin with, we create a very basic method which publishes to Kafka:</p>

<pre><code class="language-java">package kafkapublish;
import java.util.Properties;

//import necessary Kafka packages
import org.apache.kafka.clients.producer.Producer;
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerRecord;

public class SqlKafka 
{
  public static void main( String[] args )
  {
      SqlKafka sq = new SqlKafka();
      sq.publishToKafka();
  }

  public void publishToKafka() {
      String topicName = &quot;testTopic&quot;;

      Properties config = new Properties();
      config.put(&quot;client.id&quot;, &quot;1&quot;);
      config.put(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;);
      config.put(&quot;acks&quot;, &quot;all&quot;);

      config.put(&quot;key.serializer&quot;, \
      &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);
      config.put(&quot;value.serializer&quot;, \
      &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);

      Producer&lt;String, String&gt; producer = \
           new KafkaProducer&lt;String, String&gt;(config);

      for (int i = 0; i &lt; 10; i++) {
          producer.send(new ProducerRecord&lt;String, String&gt; \
          (topicName, null, String.format(&quot;Hello number: %s&quot;, i)));
      }
      producer.close();
  }
}
</code></pre>

<p><strong>Code Snippet 6:</strong> <em>Simple Publish</em></p>

<p>The code we see in <em>Code Snippet 6</em> is in the project just for us to make sure that we can publish to the broker, and has nothing to do with SQL Server. In the <code>publishToKafka</code> method, we see how we:</p>

<ul>
<li>Set some properties, amongst them are the brokers, (<code>bootstrap.servers</code>), we connect to.</li>
<li>Set the serializers we use.</li>
<li>Create a <code>Producer</code> instance.</li>
<li>Call <code>send</code> on the <code>producer</code> instance.</li>
</ul>

<p>To check that everything works we &ldquo;spin up&rdquo; a <code>bash</code> shell in the Kafka broker instance. If you run this in Docker you:</p>

<ul>
<li>Do a <code>Docker exec -it &lt;kafka_instance_name&gt; bash</code>.</li>
<li>From the command prompt in the container you <code>cd</code> into the <code>/usr/bin/</code> directory.</li>
</ul>

<p>When you are in the <code>/usr/bin/</code> directory you start up a Kafka console consumer like so:</p>

<pre><code class="language-bash">$ ./kafka-console-consumer --bootstrap-server broker:29092 \
                           --topic testTopic
</code></pre>

<p><strong>Code Snippet 7:</strong> <em>Kafka Console Consumer</em></p>

<p>You may ask yourself why, in the code in <em>Code Snippet 6</em>, <code>bootstrap.servers</code> is <code>localhost:9092</code>, whereas in <em>Code Snippet 7</em> it is <code>broker:29092</code>; different host-name as well as port number? That is because we run in a Docker container where we have different listeners for internal network connections compared to external (from the host machine or other machines).</p>

<blockquote>
<p><strong>NOTE:</strong> <a href="https://twitter.com/rmoff">Robin Moffat</a>, who is a Kafka guru, has written a blog post about port addresses and listeners: <a href="https://rmoff.net/2018/08/02/kafka-listeners-explained/">Kafka Listeners - Explained</a>. If you are interested in Kafka, you should read that post, and whatever else Robin publishes. He knows his stuff!</p>
</blockquote>

<p>Anyway, you execute the code in <em>Code Snippet 7</em>. When you subsequently run the Java code in <em>Code Snippet 6</em>, the output in the terminal window where you run <code>kafka-console-consumer</code> looks like so:</p>

<p><img src="/images/posts/sql_kafka_extlang_output1.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Consume Output</em></p>

<p>We see in <em>Figure 2</em> the commands we used to run the containers <code>bash</code> shell, together with the <code>kafka-console-consumer</code> command, and the output, (&ldquo;Hello number: <em>n</em>&rdquo;), which we receive after executing the Java code. That&rsquo;s cool - our simple code works, but - once again - the code is very simple, and we definitely cannot use it, as is in SQL Server. Let us see what we need to do to make this work from SQL.</p>

<h2 id="java-code-sql-server">Java Code &amp; SQL Server</h2>

<p>This post is not about how to write Java code so it can be used from SQL Server, read my <a href="/s2k19_ext_framework_java">SQL Server 2019 Extensibility Framework &amp; Java</a> series for that. There is one thing however that is important that I want to re-iterate, and that is the <a href="https://docs.microsoft.com/en-us/sql/language-extensions/how-to/extensibility-sdk-java-sql-server">Microsoft Extensibility SDK for Java</a>, which is required if we want to write Java code for SQL Server. The SDK was introduced together with SQL Server 2019 CTP 2.5, (I am now on CTP 3.1). I wrote about the SDK in <a href="/2019/05/26/java--sql-server-2019-extensibility-framework-the-sequel/">Java &amp; SQL Server 2019 Extensibility Framework: The Sequel</a>. One difference between CTP 2.5 and CTP 3.1 is that the SDK is now part of the SQL Server distribution, so you do not need to download it. You find it at: <code>\&lt;path_to_instance_install&gt;\MSSQL\Binn\mssql-java-lang-extension.jar</code>.</p>

<p>What we need to do is to add the SDK as a dependency for our project. If you use <em>VS Code</em> and <em>Maven</em> I covered how to do it in the <a href="/2019/05/26/java--sql-server-2019-extensibility-framework-the-sequel/">Java &amp; SQL Server 2019 Extensibility Framework: The Sequel</a>. A short recap:</p>

<ul>
<li>Create a new directory in <em>Maven</em>&rsquo;s local repository directory. For me, on Windows, it is <code>%USERPROFILE%\.m2\repository</code>.</li>
<li>Create a subdirectory of the new directory, named <code>mssql-java-lang-extension</code>.</li>
<li>Create a subdirectory of <code>mssql-java-lang-extension</code>, and name it as a version number. (<code>1.0</code> for example).</li>
<li>Copy <code>mssql-java-lang-extension.jar</code> to the &ldquo;version&rdquo; directory and add the &ldquo;version&rdquo; number to the <code>.jar</code> file like so: <code>mssql-java-lang-extension-1.0.jar</code>:</li>
</ul>

<p><img src="/images/posts/sql_2k19_java_sdk_dep_hierarch.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Folder Hierarchy Dependency</em></p>

<p>In <em>Figure 3</em> we see the &ldquo;layout&rdquo; of the local <em>Maven</em> repository after I have set it up for the SDK dependency, and we see how I named the top-level directory <code>nielsb</code>. Outlined in blue we see the different folders below<code>..\m2\repository</code>, and the outline in red shows the renamed SDK file. We can now add the dependency to the <code>pom.xml</code> file:</p>

<pre><code class="language-xml">&lt;dependencies&gt;
  &lt;dependency&gt;
    &lt;groupId&gt;nielsb&lt;/groupId&gt;
    &lt;artifactId&gt;mssql-java-lang-extension&lt;/artifactId&gt;
    &lt;version&gt;1.0&lt;/version&gt;
  &lt;/dependency&gt;
  ...
&lt;/dependencies&gt;
</code></pre>

<p><strong>Code Snippet 8:</strong> <em>Java SDK Dependency</em></p>

<p>By having added the dependency to the <code>pom.xml</code> file, we can now reference the SDK in our code:</p>

<pre><code class="language-java">package kafkapublish;

...

import com.microsoft.sqlserver.javalangextension.*;
import java.util.LinkedHashMap;

public class SqlKafka  extends AbstractSqlServerExtensionExecutor
{
  ...
}
</code></pre>

<p><strong>Code Snippet 9:</strong> <em>Extending the SqlKafka Class</em></p>

<p>What we see in <em>Code Snippet 9</em> is how I <code>import</code> all classes in <code>com.microsoft.sqlserver.javalangextension</code>, and how I subsequently extend the <code>SqlKafka</code> class with <code>AbstractSqlServerExtensionExecutor</code>. Oh, I also <code>import</code> <code>java.util.LinkedHashMap</code>, which I use later.</p>

<p>We know from <a href="/2019/05/26/java--sql-server-2019-extensibility-framework-the-sequel/">Java &amp; SQL Server 2019 Extensibility Framework: The Sequel</a> that when we call Java code from SQL Server, our code needs to implement the <code>execute</code> method from the <code>AbstractSqlServerExtensionExecutor</code> class, and that the method looks like so:</p>

<pre><code class="language-java">public AbstractSqlServerExtensionDataset execute(
               AbstractSqlServerExtensionDataset input, 
               LinkedHashMap&lt;String, Object&gt; params) {...}
</code></pre>

<p><strong>Code Snippet 10:</strong> <em>Execute Method</em></p>

<p>In <em>Code Snippet 10</em> we see how the method expects two parameters: a dataset, and a map of strings and objects. That is quite useful for us, as, even though right now the publish method in our code has hardcoded values for broker, topic, message, and so on; in a &ldquo;real world&rdquo; scenario we do do not publish to just one topic, and we may publish the same event to different brokers.</p>

<p>So, the way we code it is that we expect the dataset to contain a broker address, (including port-number), topic, and partition value. We pass in the event, (message), as a parameter, part of the <code>LinkedHashMap</code>. That way, we do not duplicate the message if we publish to multiple brokers:</p>

<pre><code class="language-java">public PrimitiveDataset execute(PrimitiveDataset input, 
                               LinkedHashMap&lt;String, 
                               Object&gt; params) 
{  
    String[] brokers = input.getStringColumn(0);
    String[] partitions = input.getStringColumn(1);
    String[] topics = input.getStringColumn(2);
    String message = (String)params.get(&quot;msg&quot;);

    int rowCount = brokers.length;

    for(int i= 0; i &lt; rowCount; i++)
    {
      //grab the column values  
      String broker = (String)brokers[i];
      String partition = (String)partitions[i];
      String topic = (String)topics[i];
      
      sqlPublishToKafka(topic, partition, broker, message);
    }
    return null;
}

public void sqlPublishToKafka(String topic, String partition, String broker, String message) 
{
    
    Properties config = new Properties();
    config.put(&quot;client.id&quot;, &quot;1&quot;);
    config.put(&quot;bootstrap.servers&quot;, broker);
    config.put(&quot;acks&quot;, &quot;all&quot;);

    config.put(&quot;key.serializer&quot;, \
    &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);
    config.put(&quot;value.serializer&quot;, \
    &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);

    Producer&lt;String, String&gt; producer = new KafkaProducer&lt;String, String&gt;(config);
    producer.send(new ProducerRecord&lt;String, String&gt;(topic, partition, message));
    producer.close();
}

</code></pre>

<p><strong>Code Snippet 11:</strong> <em>The Execute Method</em></p>

<p>We see in <em>Code Snippet 11</em> how I have implemented the <code>execute</code> method, and how I, in that method, handle the incoming parameters and subsequently call into the new <code>sqlPublishToKafka</code> method.</p>

<p>The Java language extension has some requirements on the code, which means we have to set some member variables in the class constructor like so:</p>

<pre><code class="language-java">public class SqlKafka  extends AbstractSqlServerExtensionExecutor
{

  public SqlKafka() 
  {
    executorExtensionVersion = SQLSERVER_JAVA_LANG_EXTENSION_V1;
    executorInputDatasetClassName = PrimitiveDataset.class.getName();
    executorOutputDatasetClassName = PrimitiveDataset.class.getName();
  }

  public PrimitiveDataset execute(PrimitiveDataset input, 
                                  LinkedHashMap&lt;String, Object&gt; params) 
  { ... }

  public void sqlPublishToKafka(String topic, String partition, 
                                String broker, String message) 
  { ... }
}
</code></pre>

<p><strong>Code Snippet 12:</strong> <em>Variables Required by Language Extension</em></p>

<p>You can read more about the required variables we see in <em>Code Snippet 12</em> in the <a href="/2019/05/26/java--sql-server-2019-extensibility-framework-the-sequel/">Java &amp; SQL Server 2019 Extensibility Framework: The Sequel</a> post.</p>

<p>Normally we can now build the application and create a <code>.jar</code> file out of it, to use later. If you use <em>VS Code</em> together with <em>Maven</em> see my post <a href="/2019/01/17/sql-server-2019--java-with-visual-studio-code/">SQL Server 2019 &amp; Java with Visual Studio Code</a> if you are unsure how to create a <code>.jar</code> file.</p>

<p>In the previous paragraph, I wrote &ldquo;normally&rdquo;, because when we want to use our code from SQL Server, it is not as straightforward as to just create a <code>.jar</code> file. As the <code>.jar</code> file will be deployed to SQL Server we need to ensure that all dependencies also are included, i.e., we need an &ldquo;uber jar&rdquo;.</p>

<p>To create this &ldquo;uber jar&rdquo;, (at least in <em>VS Code</em> and <em>Maven</em>), we use a <em>Maven</em> plugin <code>maven-shade-plugin</code> which is part of the <code>org.apache.maven.plugins</code> group, and we add the plugin to the <code>pom.xml</code> file. Let us see how and where to place it.</p>

<p>When we create a <em>Maven</em> project, the <code>pom.xml</code> file looks something like so:</p>

<p><img src="/images/posts/ql_kafka_extlang_maven_plugins1.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Build Section</em></p>

<p>We see in <em>Figure 4</em> that there is a <code>&lt;build&gt;</code> section in the <code>pom.xml</code> file, and it is in that section we add the plugin. In fact, for this project, we do not need anything else in the <code>&lt;build&gt;</code> section apart from our plugin. So we replace the whole existing <code>&lt;build&gt;</code> section with the following:</p>

<pre><code class="language-xml">&lt;build&gt;
  &lt;plugins&gt;
    &lt;plugin&gt;
      &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
      &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;
      &lt;version&gt;3.2.1&lt;/version&gt;
      &lt;configuration&gt;
        &lt;createDependencyReducedPom&gt;false&lt;/createDependencyReducedPom&gt;
        &lt;artifactSet&gt;
          &lt;excludes&gt;
            &lt;exclude&gt;nielsb:*&lt;/exclude&gt;  
          &lt;/excludes&gt;
        &lt;/artifactSet&gt;
      &lt;/configuration&gt;
      &lt;executions&gt;
        &lt;execution&gt;
          &lt;phase&gt;package&lt;/phase&gt;
          &lt;goals&gt;
            &lt;goal&gt;shade&lt;/goal&gt;
          &lt;/goals&gt;
        &lt;/execution&gt;
      &lt;/executions&gt;
    &lt;/plugin&gt;
  &lt;/plugins&gt;
&lt;/build&gt;
</code></pre>

<p><strong>Code Snippet 13:</strong> <em>New Build Section</em></p>

<p>Notice in <em>Code Snippet 13</em> how there is an <code>&lt;excludes&gt; section containing</code><exclude>nielsb:*</exclude><code>. That</code>nielsb` refers to the directory we added for the Java SDK dependency, (<em>Figure 3</em>). What we say here is that we do not want to include the Java SDK as a dependency in our &ldquo;uber jar&rdquo;, as we later deploy the SDK standalone, and having two SDK&rsquo;s deployed to the same database cause bad things to happen.</p>

<p>Having done this, we:</p>

<ul>
<li>Save the <code>pom.xml</code> file.</li>
<li>Compile the project.</li>
<li>Create the <code>.jar</code> via the <code>package</code> command.</li>
</ul>

<p>The created <code>.jar</code> file is in the project&rsquo;s <code>target</code> directory, and &ldquo;weighs&rdquo; in at around 9Mb. Later we see how we use this <code>.jar</code> file, but let us now go back to the database.</p>

<h2 id="sql">SQL</h2>

<p>Having a <code>.jar</code> file for our application means that we can deploy it to the database and test and see what happens. However, there are a couple of things we need to do before that.</p>

<p>Remember from the code in <em>Code Snippet 11</em> how we send in a dataset with the relevant information about where to publish a message to. That information comes from tables in the database. Let us create the tables:</p>

<pre><code class="language-sql">CREATE TABLE dbo.tb_KafkaCluster
(
  ClusterID int,
  BootstrapServers nvarchar(4000) NOT NULL,
  [Description] nvarchar(4000) NOT NULL,
  CONSTRAINT [pk_KafkaCluster] PRIMARY KEY
  (ClusterID)
);
GO

CREATE TABLE dbo.tb_KafkaEventSubscriber
(
  SubscriberID int identity,
  EventID int NOT NULL,
  ClusterID int NOT NULL,
  Topic nvarchar(256) NOT NULL,
  CONSTRAINT [pk_KafkaEventSubscriber] PRIMARY KEY
  (EventID, ClusterID, Topic),
  CONSTRAINT [fk_KafkaEventSubscriber_ClusterID] 
  FOREIGN KEY (ClusterID)
  REFERENCES dbo.tb_KafkaCluster(ClusterID)
);
GO
</code></pre>

<p><strong>Code Snippet 14:</strong> <em>Routing Tables</em></p>

<p>The code in <em>Code Snippet 14</em> creates two tables which will contain information about who wants the different events, and where to send it to. Notice the <code>BootstrapServers</code> column in the <code>dbo.tb_KafkaCluster</code> table. That column contains the bootstrap servers as a comma-delimited string.</p>

<blockquote>
<p><strong>NOTE:</strong> The setup above is very simplistic. We should have a tables for different type of events, and with foreign key references to that table. Potentially also a table for topics. The <code>BootstrapServers</code> column is also a shortcut. We should have a separate table for the brokers with foreign key reference to the cluster table. However, for this blog-post, this is enough.</p>
</blockquote>

<p>When we have created the tables in <em>Code Snippet 14</em> we insert some data:</p>

<pre><code class="language-sql">INSERT INTO dbo.tb_KafkaCluster(ClusterID, BootstapServers, 
                               [Description])
VALUES(1, 'localhost:9092', 'First cluster');

INSERT INTO dbo.tb_KafkaEventSubscriber(EventID, ClusterID, 
                                        Topic)
VALUES (1, 1, 'testTopic'),
       (1500, 1, 'wagers');
</code></pre>

<p><strong>Code Snippet 15:</strong> <em>Insert Data</em></p>

<p>As I run both SQL Server and Kafka on my local machine, I set the <code>BootstapServers</code> value to <code>localhost:9092</code> which we see in <em>Code Snippet 15</em>.</p>

<p>In the hook-point procedure, (<em>Code Snippet 4</em>), we can publish straight to Kafka, but that means we need the same publishing code in each event type&rsquo;s hook-point procedure. So let us instead create a procedure which does the actual publish to Kafka:</p>

<pre><code class="language-sql">CREATE PROCEDURE dbo.pr_PublishToKafka @EventID int,
                                       @PartitionValue nvarchar(50),
                                       @EventMessage nvarchar(max)
AS
BEGIN

  IF(@PartitionValue = '')
  BEGIN
    SET @PartitionValue = NULL
  END

  EXEC sp_execute_external_script 
    @language = N'Java',
    @script = N'kafkapublish.SqlKafka',
    @input_data_1 = N'SELECT kc.BootstrapServers, @partVal, 
                             kes.Topic
                      FROM dbo.tb_KafkaCluster kc
                      JOIN dbo.tb_KafkaEventSubscriber kes
                        ON kc.ClusterID = kes.ClusterID
                      WHERE kes.EventID =  @eventID ',
    @params = N'@msg nvarchar(max), @eventID int, 
                @partVal nvarchar(50)',
    @eventID = @EventID,
    @partVal = @PartitionValue,
    @msg = @EventMessage;
END
GO
</code></pre>

<p><strong>Code Snippet 16:</strong> <em>Procedure to Publish</em></p>

<p>In <em>Code Snippet 16</em> we see how we pass in parameters for what type of event it is, the partition value, and the event message. To publish to Kafka we use the procedure <code>sp_execute_external_script</code>, and in the procedure we:</p>

<ul>
<li>Set the language to Java.</li>
<li>Set the <code>@script</code> parameter to our package and class name.</li>
<li>Retrieve the broker and topic information for the event type, together with the partition value. This the <code>SELECT</code> statement in the <code>@input_data_1</code> oarameter.</li>
<li>Define parameters that we use in the <code>SELECT</code>, and also by our Java code, (the <code>@msg</code> parameter).</li>
</ul>

<p>Now we are almost done, and we alter the procedures to call into our hook-point procedure, and the publish procedure:</p>

<pre><code class="language-sql">ALTER PROCEDURE dbo.pr_LogWager ...
AS
BEGIN
  ...

  BEGIN TRY
    BEGIN TRAN
      ...
    COMMIT TRAN;

    EXEC dbo.pr_GenerateAndPublishWagerEvent 
                              @UserID = @UserID,
                              @GameID = @GameID,
                              @WagerAmount = @WagerAmount,
                              @PayoutAmount = @PayoutAmount,
                              @EventTime = @EventTime;
  ...

END  
GO

ALTER PROCEDURE dbo.pr_GenerateAndPublishWagerEvent ... 
                                
AS
BEGIN
  DECLARE @msg nvarchar(max);

  --generate the event
  SET @msg =  ...;
  
  EXEC dbo.pr_PublishToKafka @EventID = 1500,
                             @PartitionValue = @UserID,
                             @EventMessage = @msg; 
END
GO
</code></pre>

<p><strong>Code Snippet 17:</strong> <em>Altering Procedures</em></p>

<p>In the procedures in <em>Code Snippet 17</em> I have taken out the parameter definitions, and also left out some of the code for brevity. We see how <code>dbo.pr_LogWager</code> calls into <code>dbo.pr_GenerateAndPublishWagerEvent</code>, which calls into <code>dbo.pr_PublishToKafka</code>.</p>

<h2 id="deploy">Deploy</h2>

<p>We are now ready to deploy our Java code into the database. However, if this is the first time we deploy Java code to the database, we need to create Java as an external language in the database. I covered this in my post: <a href="/2019/06/06/sql-server-2019-extensibility-framework--external-languages/">SQL Server 2019 Extensibility Framework &amp; External Languages</a>. From reading that post, we see we need to:</p>

<ul>
<li>Create an archive file (<code>.zip</code>) of the Java language extension file <code>javaextension.dll</code>, which is located at <code>..\&lt;path_to_sql_instance&gt;\MSSQL\Binn\javaextension.dll</code>.</li>
<li>Deploy the zip file to the database using the <code>CREATE EXTERNAL LANGUAGE</code> syntax:</li>
</ul>

<pre><code class="language-sql">CREATE EXTERNAL LANGUAGE Java
FROM (CONTENT = 'W:\javaextension.zip'
      , FILE_NAME = 'javaextension.dll');
GO
</code></pre>

<p><strong>Code Snippet 18:</strong> <em>Creating External Language</em></p>

<p>I zipped the extension dll and placed it in the root of my <code>W:\</code> drive, and then called <code>CREATE EXTERNAL LANGUAGE</code>. We can check that it worked by calling <code>SELECT * FROM sys.external_languages</code>. If all is well we can go ahead.</p>

<blockquote>
<p><strong>NOTE:</strong> The name we assign to the language is as such of no importance, except that we use it in <code>sp_execute_external_script</code>, and as well as when we create external libraries, which we see below.</p>
</blockquote>

<p>Once again, if this is the first time we deploy Java to a database we also need to deploy the Java language SDK (<code>mssql-java-lang-extension.jar</code>), the one we used in our Java code above. For this, we use the <code>CREATE EXTERNAL LIBRARY</code>:</p>

<pre><code class="language-sql">CREATE EXTERNAL LIBRARY javaSDK 
FROM (CONTENT = 'W:\mssql-java-lang-extension.jar')
WITH (LANGUAGE = 'Java');
GO 
</code></pre>

<p><strong>Code Snippet 19:</strong> <em>Creating Java SDK Library</em></p>

<p>As in <em>Code Snippet 18</em>, I copied the SDK file to <code>W:\</code>, and then ran what we see in <em>Code Snippet 19</em>. The name we give the library does not matter, but it is a good idea to keep it somewhat descriptive. Use <code>SELECT * FROM sys.external_libraries</code> to ensure it worked.</p>

<p>Right, so finally we can deploy our application, and what we deploy is the <code>.jar</code> file we created just after <em>Code Snippet 13</em>:</p>

<pre><code class="language-sql">CREATE EXTERNAL LIBRARY javePublishToKafka 
FROM (CONTENT = 'W:\SqlToKafka-1.0.jar')
WITH (LANGUAGE = 'Java');
GO 
</code></pre>

<p><strong>Code Snippet 20:</strong> <em>Deploying the Application</em></p>

<p>We deploy the application by creating an external library, as we see in <em>Code Snippet 20</em>, and yes, I copied the <code>.jar</code> file to <code>W:\</code> this time as well.</p>

<h2 id="run-the-application">Run the Application</h2>

<p>So finally, it is time to see what we have done works. Start with &ldquo;spinning up&rdquo; a Kafka consumer against the <code>wagers</code> topic. Use similar code to what we see in <em>Code Snippet 7</em>, but change <code>--topics testTopic</code> to <code>--topics wagers</code>. When the Kafka consumer is up and listening, it is time to see if it works:</p>

<pre><code class="language-sql">dbo.pr_EmulateGamePlay @Loops = 5,
                       @MinDelay = 50,
                       @MaxDelay = 500;
</code></pre>

<p><strong>Code Snippet 21:</strong> <em>Emulate Game Play</em></p>

<p>In <em>Code Snippet 21</em> we see a procedure which emulates gameplay, and it expects some parameters:</p>

<ul>
<li><code>@Loops</code> - In the procedure, we loop around <code>dbo.pr_LogWager</code> and passes in random values for the required parameters in <code>dbo.pr_LogWager</code>.</li>
<li><code>@MinDelay</code>, <code>@MaxDelay</code> - in the procedure we do a random wait between each loop and these two parameters define min, and max values, (milliseconds).</li>
</ul>

<p>Let us see if we get any output from the Kafka consumer when we execute the code in <em>Code Snippet 21</em>:</p>

<p><img src="/images/posts/sql_kafka_extlang_publish_output.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Kafka Output</em></p>

<p>Yes! From what we see in <em>Figure 4</em> it works, as we see 5 events as output. We have succeeded in streaming data out from the database into Kafka by using SQL Server Extensibility Framework and the Java external language.</p>

<blockquote>
<p><strong>NOTE:</strong> You may ask what the <code>dbo.pr_EmulateGamePlay</code> looks like. I have included the source for that procedure at the very end of this post as an Appendix.</p>
</blockquote>

<h2 id="summary">Summary</h2>

<p>There are various ways one can get data out of SQL Server and into the streaming world. In this blog post, we looked at using SQL Server&rsquo;s Extensibility framework and Java as an external language.</p>

<p>We looked at:</p>

<ul>
<li>Creating hook-point procedures and injecting them into the procedures whose data we want to capture and create events from.</li>
<li>Using the Java Kafka client to publish data.</li>
<li>Adding a dependency in the Java project against the Microsoft Java SDK.</li>
<li>Creating an &ldquo;uber jar&rdquo; containing all Java dependencies, except for the Java SDK. The actual application should also be in that &ldquo;uber jar&rdquo;.</li>
<li>Creating the Java language in the database.</li>
<li>Deploying the Java SDK to the database.</li>
<li>Deploying the &ldquo;uber jar&rdquo; to the database.</li>
<li>Using <code>sp_execute_external_script</code> to call into the Java code from SQL Server.</li>
</ul>

<p>As I mentioned above, there are various ways to &ldquo;free&rdquo; your data, and we look at other ways in future posts.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>

<h2 id="appendix-i">Appendix - I</h2>

<p>Below is the code for the <code>dbo.pr_EmulateGamePlay</code> procedure:</p>

<pre><code class="language-sql">CREATE PROCEDURE dbo.pr_EmulateGamePlay @Loops int = 10,
                                        @MinDelay int = 50,
                                        @MaxDelay int = 500
AS
BEGIN                                        
  DECLARE @delay int;
  DECLARE @waitfor varchar(25);
  DECLARE @winMultiplier int;
  DECLARE @wager decimal(10, 2);
  DECLARE @userId int;
  DECLARE @gameId int;
  DECLARE @payout decimal(10, 2) = 0.00;
  DECLARE @minUserId int = 100;
  DECLARE @maxUserId int = 125;
  DECLARE @minwager int = 25;
  DECLARE @maxwager int = 5000;
  DECLARE @minWinMult int = 1;
  DECLARE @maxWinMult int = 10;
  DECLARE @minGameId int = 1000;
  DECLARE @maxGameId int = 1050;
  DECLARE @minWinIndicator int = 1;
  DECLARE @maxWinIndicator int = 12;
  DECLARE @noLoops int = 0;
  DECLARE @eventTime datetime2;
  
  WHILE(@noLoops &lt; @Loops)
  BEGIN

    SET @eventTime = SYSUTCDATETIME();
  -- get random values for delay between games, 
  -- wager size, user id, game id,
  -- win multiplier
    SELECT @delay = @MinDelay + ROUND(RAND() * 
                   (@MaxDelay + 1 - @MinDelay), 0),
            @wager = (@minwager + ROUND(RAND() * 
                     (@maxwager + 1 - @minwager), 0)) / 100,
            @userId = @minUserId + ROUND(RAND() * 
                      (@maxUserId + 1 - @minUserId), 0),
            @gameId = @minGameId + ROUND(RAND() * 
                      (@maxGameId + 1 - @minGameId), 0),
            @winMultiplier = @minWinMult + ROUND(RAND() * 
                            (@maxWinMult + 1 - @minWinMult), 0)
    -- set up the waitfor variable
    SELECT @waitfor = FORMATMESSAGE('00:00:00.%i', @delay);
    --check if win
    IF(CAST((@minWinIndicator + ROUND(RAND() * 
             (@maxWinIndicator + 1 - @minWinIndicator), 
                 0)) AS int) % 3) = 0
    BEGIN
        SET @payout =   @wager * @winMultiplier;
    END

    EXEC dbo.pr_LogWager    @UserID = @userId,
                            @GameID = @gameId,
                            @WagerAmount = @wager,
                            @PayoutAmount = @payout,
                            @EventTime = @eventTime;
  
    SELECT @noLoops += 1, @delay = null, @wager = null, 
           @userId = null, @gameId = null, @winMultiplier = null,  
         @waitfor = '', @payout = 0;

    WAITFOR DELAY @waitfor;

  END
END  
GO
</code></pre>

<p><strong>Code Snippet 22:</strong> <em>Procedure to Generate Game Play</em></p>

<p>As we see in <em>Code Snippet 22</em>, the procedure is looping, and in each loop, it generates some random values based on min and max setting variables.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 28, 2019]]></title>
    <link href="https://nielsberglund.com/2019/07/14/interesting-stuff---week-28-2019/" rel="alternate" type="text/html"/>
    <updated>2019-07-14T19:32:15+02:00</updated>
    <id>https://nielsberglund.com/2019/07/14/interesting-stuff---week-28-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="sql-server">SQL Server</h2>

<ul>
<li><a href="https://techcommunity.microsoft.com/t5/Azure-SQL-Database/Check-out-what-s-new-in-June-2019-version-of-Azure-SQL-Database/ba-p/742054">Check out whatâ€™s new in June 2019 version of Azure SQL Database managed instance</a>. A blog post from Microsoft, looking at new features and functionality in Azure Managed SQL Server. What is especially cool is that it is available in the South African data-centers, yay!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/deploying-kafka-streams-and-ksql-with-gradle-part-3-ksql-user-defined-functions-and-kafka-streams">Deploying Kafka Streams and KSQL with Gradle â€“ Part 3: KSQL User-Defined Functions and Kafka Streams</a>. This blog post is the third part in a series about using Gradle to deploy Kafka Streams and KSQL queries. This post covers using Gradle to build and deploy KSQL user-defined functions (UDFs) and Kafka Streams microservices.</li>
<li><a href="https://www.confluent.io/blog/ksql-training-for-hands-on-learning">KSQL Training for Hands-On Learning</a>. This is a &ldquo;short and sweet&rdquo; post discussing a new Udemy KSQL course. The post also contains a link to a special offer for the course: only USD 9.99! I had a quick look and signed up on the spot!</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>Stay tuned for the first post in a series looking at how to get data out of SQL Server, and into a streaming world. I am doing the last &ldquo;bits and pieces&rdquo; on it, and I should be able to publish in a day or two. In the meantime, here are some posts that give you background to the new post:</p>

<ul>
<li><a href="/2018/03/07/microsoft-sql-server-r-services---sp_execute_external_script---i/">Microsoft SQL Server R Services: sp_execute_external_script - I</a> - When calling out to R/Python/Java you do it from a specific procedure: <code>sp_execute_external_script</code>. This is the first post of three looking at <code>sp_execute_external_script</code>.</li>
<li><a href="/2018/03/11/microsoft-sql-server-r-services---sp_execute_external_script---ii/">Microsoft SQL Server R Services - sp_execute_external_script - II</a> - The second post about <code>sp_execute_external_script</code>.</li>
<li><a href="/2018/03/21/microsoft-sql-server-r-services---sp_execute_external_script---iii/">Microsoft SQL Server R Services: sp_execute_external_script - III</a>  - The third post about <code>sp_execute_external_script</code>.</li>
<li><a href="/s2k19_ext_framework_java">SQL Server 2019 Extensibility Framework &amp; Java</a> - A series of posts discussing how to write Java code so we can call it from SQL server.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 27, 2019]]></title>
    <link href="https://nielsberglund.com/2019/07/07/interesting-stuff---week-27-2019/" rel="alternate" type="text/html"/>
    <updated>2019-07-07T07:32:53+02:00</updated>
    <id>https://nielsberglund.com/2019/07/07/interesting-stuff---week-27-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="misc">Misc.</h2>

<ul>
<li><a href="https://www.infoq.com/news/2019/06/wsl-2-linux">Windows Subsystem for Linux 2 Has Linux Kernel Shipping in Windows</a>. This is an <a href="https://www.infoq.com/">InfoQ</a> article about how  Windows Subsystem for Linux (WSL) 2 is now available through the Windows Insiders program. WSL 2 presents a new architecture that aims to increase file system performance and provide full system call compatibility.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/ksql-in-football-fifa-womens-world-cup-data-analysis">KSQL in Football: FIFA Womenâ€™s World Cup Data Analysis</a>. In this blog post, the author looks at how we can use Kafka, KSQL and Kafka Connectors together to join disparate streaming data sources. He also looks at how to push the data through Google&rsquo;s Natural Language API to determine sentiment. Very interesting! I had no idea there were pre-built Kafka connectors for Twitter and RSS, cool!</li>
<li><a href="https://blog.acolyer.org/2019/07/03/one-sql-to-rule-them-all/">One SQL to rule them all: an efficient and syntactically idiomatic approach to management of streams and tables</a>. In this post by <a href="https://twitter.com/adriancolyer">Adrian</a>, he dissects a white paper about building SQL interfaces for streaming data. The paper comes up with a set of proposed extensions to the SQL standard itself. Interesting!<br /></li>
<li><a href="https://www.confluent.io/blog/kafka-listeners-explained">Kafka Listeners â€“ Explained</a>. This post by <a href="https://twitter.com/rmoff">Robin</a> discusses Kafka listeners, what they are good for, and what you need to do to get your Kafka clusters to work correctly with internal as well as external components.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>Yeah, what am I doing:</p>

<ul>
<li>As you may know, I am a Microsoft Data Platform MVP, and the annual renewal for the MVP program happens July 1. Every MVP is on &ldquo;tenterhooks&rdquo; this date; &ldquo;will I get renewed or not&rdquo;. Well, I am happy to say that I was renewed for another year. I want to thank all of you that read my blog posts, comes to my presentations at conferences, etc.: this is all down to you - <strong>THANK YOU</strong>!</li>
<li>A week or two ago I started on the first post in a series looking at how to get data out of SQL Server into a streaming world. I had hoped to have that post published by now, but I am still working on it. Hopefully, I can get it out in a week or so.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 26, 2019]]></title>
    <link href="https://nielsberglund.com/2019/06/30/interesting-stuff---week-26-2019/" rel="alternate" type="text/html"/>
    <updated>2019-06-30T08:40:49+02:00</updated>
    <id>https://nielsberglund.com/2019/06/30/interesting-stuff---week-26-2019/</id>
    <content type="html"><![CDATA[<p>I cannot believe we have reached half year mark already - week 26!</p>

<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="sql-server-2019">SQL Server 2019</h2>

<ul>
<li><a href="http://sql-sasquatch.blogspot.com/2019/06/sql-server-2019-ctp-30-plus-p-startup.html">SQL Server 2019 CTP 3.0 plus P startup option (for experimentation only)</a>. This is a very interesting post by <a href="https://twitter.com/sqL_handLe">Lonny</a> where he discusses some new startup options in SQL Server. More specifically how you can, at startup time, have SQL Server create an arbitrary number of schedulers. Very cool!</li>
</ul>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://www.confluent.io/blog/designing-the-net-api-for-apache-kafka">Designing the .NET API for Apache Kafka</a>. I debated with myself whether this post belonged under .NET or Streaming, as you see .NET won. Anyway, this post is about the Confluent .NET Kafka client and the work that has gone into it. Now, if we could get a .NET implementation of the Kafka Streams API as well, #justsaying!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://rmoff.net/2019/06/23/manually-delete-a-connector-from-kafka-connect/">Manually delete a connector from Kafka Connect</a>. This is a handy post by <a href="https://twitter.com/rmoff">Robin</a>, looking at how one can manually delete Kafka connectors. I have said it before, and I say it again - if you are interested in Kafka, <a href="https://twitter.com/rmoff">Robin</a>&rsquo;s <a href="https://rmoff.net/">blog</a> <strong>MUST</strong> be in your reading list.</li>
<li><a href="https://www.confluent.io/blog/microservices-apache-kafka-domain-driven-design">Microservices, Apache Kafka, and Domain-Driven Design</a>. I must say that this post came at the exact right time for me, as what it covers: Kafka as a backbone for microservices architectures, is what I am looking at right now! Excellent!</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 25, 2019]]></title>
    <link href="https://nielsberglund.com/2019/06/23/interesting-stuff---week-25-2019/" rel="alternate" type="text/html"/>
    <updated>2019-06-23T16:19:22+02:00</updated>
    <id>https://nielsberglund.com/2019/06/23/interesting-stuff---week-25-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://azure.microsoft.com/mediahandler/files/resourcefiles/kubernetes-learning-path/Kubernetes%20Learning%20Path%20version1.0.pdf">50 days from zero to hero with Kubernetes</a>. If you want to get to know Kubernetes this guide is for you! It will help you to get to know the basics of Kubernetes, and you also get hands-on experience with its various components, capabilities, and solutions.</li>
</ul>

<h2 id="misc">Misc.</h2>

<ul>
<li><a href="https://www.tbray.org/ongoing/When/201x/2019/06/12/Go-Creeping-In">Go Creeping In</a>. In this post, <a href="https://twitter.com/timbray">Tim Bray</a>, discusses the <a href="https://golang.org/">Go</a> language, and some of its features. Very interesting!</li>
</ul>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://devblogs.microsoft.com/dotnet/the-evolving-infrastructure-of-net-core/">The Evolving Infrastructure of .NET Core</a>. This blog post gives an interesting insight into the infrastructure history behind .NET Core.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.lightbend.com/blog/managing-streaming-and-queryable-state-in-spark-akka-streams-kafka-streams-flink">Managing Streaming And Queryable State In Spark, Akka Streams, Kafka Streams, And Flink</a>. This post takes a look at the built-in support for state management and queryable state, when available, or how they can be implemented in Apache Spark, Apache Flink, Kafka Streams, and Akka Streams.</li>
<li><a href="https://medium.com/thousandeyes-engineering/kafka-topics-pitfalls-and-insights-38bafc791a83">Kafka Topics: Pitfalls and Insights</a>. In this post, there are quite a few useful insights about Kafka topics. If you use Kafka in production, you should read this post.</li>
<li><a href="https://databricks.com/blog/2019/06/18/simplifying-streaming-stock-analysis-using-delta-lake-and-apache-spark-on-demand-webinar-and-faq-now-available.html">Simplifying Streaming Stock Analysis using Delta Lake and Apache Spark</a>. A blog post further expanding on a webinar on how to build streaming systems to analyze stock data in real-time, by using Databricks Delta Lake and Apache Spark.</li>
<li><a href="/2019/06/18/confluent-platform--kafka-for-a-.net-developer-on-windows/">Confluent Platform &amp; Kafka for a .NET Developer on Windows</a>. This is a blog post by yours truly, where I look at how to run Confluent Kafka in Docker on a Windows machine and how we write .NET Code against it.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Confluent Platform &amp; Kafka for a .NET Developer on Windows]]></title>
    <link href="https://nielsberglund.com/2019/06/18/confluent-platform--kafka-for-a-.net-developer-on-windows/" rel="alternate" type="text/html"/>
    <updated>2019-06-18T04:49:36+02:00</updated>
    <id>https://nielsberglund.com/2019/06/18/confluent-platform--kafka-for-a-.net-developer-on-windows/</id>
    <content type="html"><![CDATA[<p>As many of you know, I am an un-ashamed .NET developer on the Windows platform. Seeing that I do a lot of SQL Server development my development and OS platforms of choice is sufficient for my work, as I on my development box (Windows of course) install SQL Server Developer edition and whatever .NET framework I use.</p>

<p>That has been sufficient up until now when I want to develop against Kafka. At <a href="/derivco">Derivco</a> we are getting more and more interested in Kafka, and obviously, I want to install it so I can <del>play around</del> investigate it. However, to install it on my development machine as I would typically do with technologies I am interested in, SQL Server, RabbitMQ, etc., is difficult, if not impossible.</p>

<blockquote>
<p><strong>NOTE:</strong> Yes I know, there are articles on the web discussing how to run Kafka on Windows, but it is a hit and miss whether it works.</p>
</blockquote>

<p>A while ago I wrote a <a href="/2018/07/10/install-confluent-platform-kafka-on-windows/">post</a> about how to run Kafka under <strong>Windows Subsystem for Linux</strong> (WSL), and yes it works, but I have had issues with it, and to me, it is still a hack. So the options then (if we rule out WSL) are:</p>

<ul>
<li>Running it on a Linux in a virtualized environment, (Virtual Box, VMWare, Hyper-V, etc.).</li>
<li>Docker.</li>
</ul>

<p>The rest of this post goes through how to set up the <em>Confluent Platform</em> in a Docker environment and use it from .NET.</p>

<p></p>

<h2 id="pre-reqs">Pre-reqs</h2>

<p>The assumption is that you use a Windows box as your development machine, and you want to run Kafka on your box. Based on that assumption, these are the pre-reqs:</p>

<ul>
<li><a href="https://docs.docker.com/docker-for-windows/">Docker Desktop for Windows</a>. The install instructions and download link are <a href="https://docs.docker.com/docker-for-windows/install/">here</a>.</li>
<li>.NET Framework or .NET Core.</li>
<li>Your IDE of choice. I kind of like <a href="https://code.visualstudio.com/"><strong>VS Code</strong></a>.</li>
</ul>

<p>As I in this post use <em>Confluent Platform</em>, (more about that below), I need to allocate at least 8Gb of memory to Docker.</p>

<h2 id="confluent-platform"><em>Confluent Platform</em></h2>

<p>I mentioned above how in this post we install and use <em>Confluent Platform</em>. Some of you may ask why I use the enterprise edition, (which <em>Confluent Platform</em> is), which you need a license for and not the Community Edition which is open source and license free.</p>

<p>There are two reasons really for this:</p>

<ul>
<li>With <em>Confluent Platform</em> I get ALL the goodies, including <strong>Control Center</strong>.</li>
<li>With the introduction of <em>Confluent Platform</em> 5.2. Confluent <a href="https://www.confluent.io/blog/introducing-confluent-platform-5-2">announced</a> that <em>Confluent Platform</em> is &ldquo;free forever&rdquo; on a single Kafka broker! In other words, it is like a &ldquo;Developer Edition&rdquo; of <em>Confluent Platform</em>. That to me, is excellent, as I can now build awesome streaming and event-driven applications on Apache Kafka using the powerful capabilities of <em>Confluent Platform</em>.</li>
</ul>

<blockquote>
<p><strong>NOTE:</strong> I confess; I &ldquo;lifted&rdquo; parts of the last bullet point from the previously mentioned <a href="https://www.confluent.io/blog/introducing-confluent-platform-5-2">announcement</a>.</p>
</blockquote>

<h2 id="install-confluent-platform-using-docker">Install <em>Confluent Platform</em> Using Docker</h2>

<p>It is time to install the Docker images we need for <em>Confluent Platform</em>. All the images for the individual components of <em>Confluent Platform</em> are on <a href="https://hub.docker.com/u/confluentinc/">Docker Hub</a>, and we could get them from there. However, if we did that, then we would need to &ldquo;compose&rdquo; them together, and I am way too lazy for that.</p>

<blockquote>
<p><strong>NOTE:</strong> Docker Hub is a cloud-based repository for container images, to which organizations upload their container images.</p>
</blockquote>

<p>Instead of us grabbing the individual containers, we use a Docker Compose file that Confluent have been kind enough to create. For those of you who don&rsquo;t know what Docker Compose is, you can read more about it <a href="https://docs.docker.com/compose/overview/">here</a>.</p>

<p>We get the file by:</p>

<ul>
<li>Cloning the <a href="https://github.com/confluentinc/cp-docker-images"><em>Confluent Platform</em> Docker Images GitHub Repository</a>. That gives us a directory <code>cp-docker-images</code>.</li>
<li><code>cd</code> into the directory and check out the branch <code>5.2.1-post</code>: <code>git checkout 5.2.1-post</code>.</li>
</ul>

<p>After the check out the <code>cp-docker-images</code> directory looks like so:</p>

<p><img src="/images/posts/confluent_kafka_checkout1.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Directory Structure After Checkout</em></p>

<p>Notice in <em>Figure 1</em> how we have a directory named <code>examples</code>. Underneath this directory are directories for different examples, (who&rsquo;d &ldquo;thunk&rdquo;), of Kafka setups. We are interested in an example, (and directory), named <code>cp-all-in-one</code>:</p>

<p><img src="/images/posts/confluent_kafka_cp-all-in-one.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Example Directory</em></p>

<p>When we navigate to the <code>cp-all-in-one</code> we see something like what we see in <em>Figure 2</em>. Among the files in the directory, there is a <code>docker-compose.yml</code> file, which includes all of the <em>Confluent Platform</em> components. After opening the <code>docker-compose.yml</code> file in a text editor, we see what components make up the <em>Confluent Platform</em>:</p>

<ul>
<li>Zookeeper.</li>
<li>Kafka broker.</li>
<li>Schema registry.</li>
<li>Kafka connect.</li>
<li>Control center.</li>
<li>KSQL server.</li>
<li>KSQL CLI.</li>
<li>KSQL datagen.</li>
<li>Rest proxy.</li>
</ul>

<p>When we look at the list above, there is one component that we should not use in production: the <code>ksql-datagen</code> component. It is included for development and test purposes, and we can use it to generate data loads, and you can read more about it <a href="https://docs.confluent.io/current/ksql/docs/tutorials/generate-custom-test-data.html">here</a>.</p>

<blockquote>
<p><strong>NOTE:</strong> This post does not cover in any greater detail the various components. Stay tuned for future posts for that.</p>
</blockquote>

<p>While we look at the <code>docker-compose.yml</code> file, let us look a bit closer at the <code>broker</code> section, which describes the Kafka broker:</p>

<p><img src="/images/posts/confluent_kafka_docker_yml.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Kafka Broker</em></p>

<p>In <em>Figure 3</em> we see two areas outlined in red, and those two areas have to do with what ports the Kafka broker listens on. In a non-Docker Kafka installation, the port Kafka exposes is typically 9092, and clients, as well as internal components, can connect without any issues. However, in a Docker environment, things are somewhat different, as you have both a Docker internal network, as well as an external network (host machine to Docker containers, for example). That&rsquo;s why we define two <code>ports</code>, (29092 and 9092), and set up two listeners. The <code>broker:29092</code> is for the internal Docker network, and the <code>localhost:9092</code> is for external connections.</p>

<blockquote>
<p><strong>NOTE:</strong> <a href="https://twitter.com/rmoff">Robin Moffat</a>, who is a Kafka guru, has written a blog post about port addresses and listeners: <a href="https://rmoff.net/2018/08/02/kafka-listeners-explained/">Kafka Listeners - Explained</a>. If you are interested in Kafka, you should read that post, and whatever else Robin publishes. He knows his stuff!</p>
</blockquote>

<p>A final word before we install the Docker containers; please do not forget to increase memory for Docker to at least 8Gb:</p>

<p><img src="/images/posts/confluent_kafka_docker_memory.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Docker Memory</em></p>

<p>In <em>Figure 4</em> we see how we have increased the memory for Docker to 8192MB. After you do this, you need to restart Docker.</p>

<p>We are now ready for installation so <code>cd</code> into the <code>cp-all-in-one</code> directory. In there, from the command line, execute:</p>

<pre><code class="language-bash">$ docker-compose up -d --build
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Execute Docker Command</em></p>

<p>The code in <em>Code Snippet 1</em>:</p>

<ul>
<li>Pulls the different images for the <em>Confluent Platform</em> (if they are not on the machine already).</li>
<li>* The <code>--build</code> flag builds the Kafka Connect image together with the <code>datagen</code> connector.</li>
<li>Creates the containers and starts up the <em>Confluent Platform</em>.</li>
</ul>

<p>When you execute the command in <em>Code Snippet 1</em>, you see something like so:</p>

<p><img src="/images/posts/confluent_kafka_pull_images.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Pull Images</em></p>

<p>Unless the images for <em>Confluent Platform</em> exists on the machine, the <code>docker-compose</code> command starts to pull them from the registry, and in <em>Figure 5</em> we see how we initially pull ZooKeeper. Eventually, all images have been pulled and the containers created.</p>

<p><img src="/images/posts/confluent_kafka_create_images.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>Create Images</em></p>

<p>As we see in <em>Figure 6</em> we have now created the containers for <em>Confluent Platform</em>, and everything should be up and running. Let us first ensure that that is the case (containers running). We use the <code>docker-compose ps</code> command, which lists containers related to images declared in the <code>docker-compose.yml</code> file:</p>

<p><img src="/images/posts/confluent_kafka_container_status.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>Container Status</em></p>

<p>From what we see in <em>Figure 7</em> all containers have a <code>State</code> of <code>Up</code>, so everything should be good. That we see that the <code>State</code> is <code>Up</code> do not necessarily indicate that the individual components are up and running, (they most likely are), so to be on the safe side we can check that the Kafka broker is up.</p>

<p>We can confirm this in various ways, and here we use the command line and have a look at logs. For this, we use the <code>docker logs</code> command, which shows information logged by a running container: <code>docker logs &lt;container_name&gt;</code>. So the question is then what the <code>container_name</code> is for the Kafka broker? Well, if we look in the <code>docker-compose.yml</code> file, we see that for the Kafka broker we have a <code>container-name</code> of <code>broker</code>, and as we see in <em>Figure 6</em> we have a corresponding <code>broker</code> container. With this in mind, the <code>logs</code> command looks like so:</p>

<pre><code class="language-bash">$ docker logs broker | Select-String -Pattern 'Started'
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>View Logs</em></p>

<p>I use PowerShell for this, and I also do a <code>grep</code> like selection by the <code>Select-String</code> command-let, which we see in <em>Code Snippet 2</em>. The reason for this is to filter out what the <code>logs</code> command returns. When I run it, I see:</p>

<p><img src="/images/posts/confluent_kafka_broker_started.png" alt="" /></p>

<p><strong>Figure 8:</strong> <em>Kafka Broker Started</em></p>

<p>Cool, from what we see in <em>Figure 8</em> it looks like the Kafka broker is up and running! Now if we want to, we can go ahead and create topics etc. Once again, there are various ways we can do this. One way is to do it from the command line; we spin up the <code>bash</code> shell in the Kafka broker container and use: <code>kafka-topics --create ...</code>. However, since I said one big reason for me to use <em>Confluent Platform</em> is <em>Control Center</em>, let us create topics via <em>Control Center</em>.</p>

<h2 id="control-center">Control Center</h2>

<p><em>Control Center</em> is a web UI for managing and monitoring Kafka. It does a lot more though than just managing/monitoring the Kafka broker. With <em>Control Center</em> you can manage and monitor:</p>

<ul>
<li>Data Streams</li>
<li>System Health</li>
<li>Configuration of Kafka Connect</li>
<li>and more &hellip;</li>
</ul>

<p>In a Docker installation, you find where to load the UI from by looking at the <code>Ports</code> column for <code>control-center</code> after you run <code>docker-compose ps</code>. In <em>Figure 6</em> we see <code>control-center</code> exposed by port 9021. So let us, in our favourite browser, browse to: <code>localhost:9021</code>:</p>

<p><img src="/images/posts/confluent_kafka_control_center_ui.png" alt="" /></p>

<p><strong>Figure 9:</strong> <em>Control Center</em></p>

<p>In <em>Figure 9</em> we see the user interface for <em>Control Center</em>. We see charts showing information about our data pipelines, and on the left a menu where we can choose between different functions of <em>Control Center</em>. For now, as we want to create a topic, what we are interested in, is the <strong>Topics</strong> menu, outlined in red. We click on it, and we see a screen like so:</p>

<p><img src="/images/posts/confluent_kafka_control_center_topics1.png" alt="" /></p>

<p><strong>Figure 10:</strong> <em>Control Center Topics</em></p>

<p>We see in <em>Figure 10</em> the <strong>Topics</strong> screen, and some pre-configured topics (this is a new Kafka installation). To create a topic we click on the <em>+ Create topic</em> button, (outlined in red), on the right-hand side of the screen:</p>

<p><img src="/images/posts/confluent_kafka_control_center_create_topic.png" alt="" /></p>

<p><strong>Figure 11:</strong> <em>Control Center New Topic</em></p>

<p>In <em>Figure 11</em> we see the <strong>New topic</strong> screen, and we see - outlined in blue - the area where we define the topic name and how many partitions we want for the topic. I am about to create a new topic: <code>testTopic</code> with default settings and a partition count of 1. I then click on the <em>Create with defaults</em> button, (outlined in red), and I go back to the <em>Topics</em> screen:</p>

<p><img src="/images/posts/confluent_kafka_control_center_testTopic.png" alt="" /></p>

<p><strong>Figure 12:</strong> <em>New Topic</em></p>

<p>We see in <em>Figure 12</em> our new <code>testTopic</code>, and we can now start to use it.</p>

<p>Before we look at how to publish and consume messages with .NET, let us make sure the topic actually works. We do that by using a couple of Kafka command line tools that ship with any Kafka installation. In fact Kafka ships with quite a few command line tools, (we spoke above of one of them: <code>kafka-topics</code>), and the two we use here are:</p>

<ul>
<li><code>kafka-console-consumer</code>: reads data from a Kafka topic and writes the data to standard output.</li>
<li><code>kafka-console-producer</code>: the opposite of <code>kafka-console-consumer</code>, it reads data from standard output and writes it to a Kafka topic.</li>
</ul>

<p>To use the two tools we:</p>

<ul>
<li>Open two command prompt windows.</li>
<li>In both windows, <code>docker exec</code> into the <code>bash</code> shell of the Kafka broker container:</li>
</ul>

<pre><code class="language-bash">$ docker exec -it broker bash
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Execute into Bash Shell</em></p>

<p>We see in <em>Code Snippet 3</em> the code to get into the Docker container&rsquo;s shell, and when we execute the code in both command windows we see:</p>

<p><img src="/images/posts/confluent_kafka_command_windows1.png" alt="" /></p>

<p><strong>Figure 13:</strong> <em>Docker Container Shell</em></p>

<p>As we see in <em>Figure 13</em> we are now in the shell as <code>root</code>. The reason we have &ldquo;spun&rdquo; up two command windows is so we can use one for publishing and the other for consuming messages.</p>

<p>Let us start with the command window for consumption:</p>

<pre><code class="language-bash">$ cd /usr/bin
$ ./kafka-console-consumer --bootstrap-server broker:29092 --topic testTopic
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Console Consumer</em></p>

<p>In <em>Code Snippet 4</em> we first <code>cd</code> into the <code>/usr/bin</code> directory where the Kafka command line tools are. We then execute the <code>kafka-console-consumer</code> command, where we say what broker and topic we want to connect to. Notice how we define the internal Docker connection here, (<code>broker:29092</code>), as this is on the internal network.</p>

<p>When we execute the code in <em>Code Snippet 4</em>, we see how the command window now waits for data to read from the broker. Now we set up the producer in the same way (in the second command window):</p>

<pre><code class="language-bash">$ cd /usr/bin
$ ./kafka-console-producer --broker-list broker:29092 --topic testTopic
</code></pre>

<p><strong>Code Snippet 5:</strong> <em>Console Producer</em></p>

<p>When we execute the code in <em>Code Snippet 5</em> we see how the command window waits for input. We enter some strings, and we see how the &ldquo;consume&rdquo; window reads the data from the broker topic and writes it to standard output.</p>

<p><img src="/images/posts/confluent_kafka_publish_consume.png" alt="" /></p>

<p><strong>Figure 14:</strong> <em>Publish Consume</em></p>

<p>The upper window we see in <em>Figure 14</em> is the publish window, and we see how I entered three strings: <code>Hello</code>, <code>World</code>, and <code>Again</code>, and how they appear in the second window. This simple test shows that our topic works, and we can now move on to .NET.</p>

<h2 id="net">.NET</h2>

<p>For the .NET part I use <em>VS Code</em> and .NET Core (2.2). As I am a &ldquo;newbie&rdquo; when it comes to <em>VS Code</em> and .NET Core, I list the steps I to get up and running so I have something to come back to.</p>

<blockquote>
<p><strong>NOTE:</strong> Actually, I am pretty much a newbie in all topics in this post.</p>
</blockquote>

<p>I start with the publisher, and I:</p>

<ul>
<li>Create a new folder for my publisher project.</li>
<li>In <em>VS Code</em> I open that folder.</li>
</ul>

<p>From the integrated terminal in <em>VS Code</em>, (<strong>View &gt; Integrated Terminal</strong>), I create a new console project: <code>dotnet new console</code>:</p>

<p><img src="/images/posts/confluent_kafka_vscode_new_console.png" alt="" /></p>

<p><strong>Figure 15:</strong> <em>New Console Project</em></p>

<p>When I execute the outlined statement in <em>Figure 15</em>, some files get created in the chosen directory:</p>

<p><img src="/images/posts/confluent_kafka_vscode_created_project.png" alt="" /></p>

<p><strong>Figure 16:</strong> <em>Newly Created Project</em></p>

<p>What we see in <em>Figure 16</em> are the various project related files, including the source file <code>Program.cs</code>. What is missing now is a Kafka client. For .NET there exists a couple of clients, and theoretically, you can use any one of them. However, in practice, there is only one, and that is the <a href="https://github.com/confluentinc/confluent-kafka-dotnet"><strong>Confluent Kafka DotNet</strong></a> client. The reason I say this is because it has the best parity with the original Java client. The client has NuGet packages, and you install it via <em>VS Code</em>&rsquo;s integrated terminal: <code>dotnet add package Confluent.Kafka --version 1.0.1.1</code>:</p>

<p><img src="/images/posts/confluent_kafka_vscode_install_nuget.png" alt="" /></p>

<p><strong>Figure 17:</strong> <em>Install NuGet Package</em></p>

<p>When you execute the <code>dotnet add package</code> the result is as we see in <em>Figure 17</em>; <em>VS Code</em> downloads necessary files and then installs the package, (outlined in blue). We can now &ldquo;code up&rdquo; our publisher.</p>

<blockquote>
<p><strong>NOTE:</strong> The code here is purely for demonstration purposes, no error handling, etc.</p>
</blockquote>

<h4 id="publish">Publish</h4>

<p>When publishing messages to Kafka with the Confluent .NET client, you need an instance of a <code>Publisher</code> class. When creating a <code>Publisher</code>, you need a <code>PublisherConfig</code> class which - as the name implies - configures the <code>Publisher</code>. In the configuration, you set up things like:</p>

<ul>
<li>Bootstrap servers - a list of brokers for the client to connect to.</li>
<li>Retries.</li>
<li>Max message sizes.</li>
<li>etc., etc.</li>
</ul>

<p>To create the <code>Publisher</code>, you use the <code>ProducerBuilder</code> class which expects a <code>PublisherConfig</code> in the constructor. The code to create a <code>Publisher</code> looks something like so:</p>

<pre><code class="language-csharp">using System;
using System.Threading;
using Confluent.Kafka;

namespace publisher
{
  class KafkaPublishTest
  {
    static readonly AutoResetEvent _closing = new AutoResetEvent(false);

    static IProducer&lt;string, string&gt; producer = null;
    static ProducerConfig producerConfig = null;

    static void Main(string[] args){}
          
    static void CreateConfig() {
      producerConfig = new ProducerConfig
      {
        BootstrapServers = &quot;localhost:9092&quot;
      };
    }

    static void CreateProducer() {
      var pb = new ProducerBuilder&lt;string, string&gt;(producerConfig);
      producer = pb.Build();
    }
  }
}

</code></pre>

<p><strong>Code Snippet 6:</strong> <em>PublisherConfig and ProducerBuilder</em></p>

<p>The code we see in <em>Code Snippet 6</em> is the beginning of the publish application. The <code>Main</code> method is not &ldquo;hooked&rdquo; up yet; we do that later. We see how we have a <code>using</code> statement for the <code>Confluent.Kafka</code> namespace, and how we declare two class variables of the types <code>IProducer</code>, and <code>ProducerConfig</code>. In the method <code>CreateConfig</code> we instantiate <code>ProducerConfig</code> and set the <code>BootstrapServer</code> property to our Kafka broker. Notice how we use the external listener port, as we now connect into the Docker container from outside the Docker internal network. Oh, the <code>AutoResetEvent</code> class variable is used to react on <code>CTRL-C</code> key press to exit the application.</p>

<p>The <code>producerConfig</code> is then used in the <code>CreateProducer</code> method where we see how we use the <code>Build</code> method on <code>ProducerBuilder</code> to get an <code>IProducer</code> instance.</p>

<p>Having a producer, we now code the method to publish messages:</p>

<pre><code class="language-csharp">using System;
using System.Threading;
using Confluent.Kafka;

namespace publisher
{
  class KafkaPublishTest
  {
    static readonly AutoResetEvent _closing = new AutoResetEvent(false);

    static IProducer&lt;string, string&gt; producer = null;
    static ProducerConfig producerConfig = null;

    static void Main(string[] args){}
          
    static void CreateConfig() {...}

    static void CreateProducer() {...}

    static async void SendMessage(string topic, string message) {
      var msg = new Message&lt;string, string&gt; {
          Key = null,
          Value = message
      };

      var delRep = await producer.ProduceAsync(topic, msg);
      var topicOffset = delRep.TopicPartitionOffset;

      Console.WriteLine($&quot;Delivered '{delRep.Value}' to: {topicOffset}&quot;);
    }

  }
}

</code></pre>

<p><strong>Code Snippet 7:</strong> <em>Publishing a Message</em></p>

<p>The method to publish messages is <code>SendMessage</code> as we see in <em>Code Snippet 7</em>. The method takes two parameters; the topic we want to send to, and the actual message we want to send.</p>

<p>In the method, we create an instance of the <code>Message</code> class. That class has two properties:</p>

<ul>
<li><code>Key</code> - this is for if/when our topic has multiple partitions. It refers to the value we want to use for Kafka to decide what partition to target. In our case, we have not defined partitions, so we set the value to <code>null</code>.</li>
<li><code>Value</code> - the message we want to send. In the code, we set it to the incoming <code>message</code> parameter.</li>
</ul>

<p>To publish the message, we call the <code>ProduceAsync</code> method, which expects a topic name, and an instance of the <code>Message</code> class. The method returns an instance of the <code>DeliveryReport</code> class. This class contains information about the delivery of the message, and we are interested to see the original message and partition and offset it was sent to. This, we then write out to the console.</p>

<p>The final thing to do is to &ldquo;hook up&rdquo; everything in the <code>Main</code> method:</p>

<pre><code class="language-csharp">using System;
using System.Threading;
using Confluent.Kafka;

namespace publisher
{
  class KafkaPublishTest
  {
    static readonly AutoResetEvent _closing = new AutoResetEvent(false);

    static IProducer&lt;string, string&gt; producer = null;
    static ProducerConfig producerConfig = null;

    static void Main(string[] args)
    {
      CreateConfig();
      CreateProducer();
      SendMessage(&quot;testTopic&quot;, &quot;This is a test42&quot;);
      Console.WriteLine(&quot;Press Ctrl+C to exit&quot;);
      
      Console.CancelKeyPress += new ConsoleCancelEventHandler(OnExit);
      _closing.WaitOne();
    }
    
    static void OnExit(object sender, ConsoleCancelEventArgs args)
    {
      Console.WriteLine(&quot;Exit&quot;);
      _closing.Set();
    }
          
    static void CreateConfig() {...}

    static void CreateProducer() {...}

    static async void SendMessage(string topic, string message) {...}

  }
}

</code></pre>

<p><strong>Code Snippet 8:</strong> <em>Main Method</em></p>

<p>In <em>Code Snippet 8</em> we see how we, in <code>Main</code>, call the different methods. We also have some code to capture <code>CTRL-C</code> to exit the application. We should now be able to publish messages to the broker. We can test that this works without having a consumer, and ensure that we get an offset back from the delivery report.</p>

<blockquote>
<p><strong>NOTE:</strong> If you want to you can also use <code>kafka-console-consumer</code> and the code in <em>Code Snippet 4</em>.</p>
</blockquote>

<p>To make sure everything works, we use the integrated terminal in <em>VS Code</em> and execute <code>dotnet build</code>, followed by <code>dotnet run</code>:</p>

<p><img src="/images/posts/confluent_kafka_vscode_build_run_publish.png" alt="" /></p>

<p><strong>Figure 18:</strong> <em>Build and Run</em></p>

<p>What we see in <em>Figure 18</em> is how the <code>build</code> command succeeded, (sections outlined in yellow), and <code>run</code> also succeeded, (sections outlined in red). The delivery report says that the message was delivered to the topic <code>testTopic</code> on partition 0, and the offset for the message is 12, (@12).</p>

<p>Cool, let us now create an application to consume messages.</p>

<h4 id="consume">Consume</h4>

<p>To consume messages, we create a new application much along the lines of what we did for the publishing app:</p>

<ul>
<li>Create a new folder for the consumer project.</li>
<li>Open that folder in <em>VS Code</em>.</li>
<li>In <em>VS Code</em>&rsquo;s integrated terminal create a new console application: <code>dotnet new console</code>.</li>
<li>After the project has been created, add the <a href="https://github.com/confluentinc/confluent-kafka-dotnet"><strong>Confluent Kafka DotNet</strong></a> client to the project as we did above.</li>
</ul>

<p>The way we create a Kafka consumer is more or less the same way we did with the publisher; we have a configuration, <code>ConsumerConfig</code>, and a builder: <code>ConsumerBuilder</code>:</p>

<pre><code class="language-csharp">using System;
using System.Threading;
using Confluent.Kafka;

namespace consumer
{
  class KafkaConsumer
  {
    static CancellationTokenSource cts = new CancellationTokenSource();
    static ConsumerConfig consumerConfig = null;
    static void Main(string[] args)
    {
       CreateConfig();
       CreateConsumerAndConsume();
    }

    static void CreateConfig() {
        consumerConfig = new ConsumerConfig {
            BootstrapServers = &quot;localhost:9092&quot;,
            GroupId = &quot;test-group&quot;,
            AutoOffsetReset = AutoOffsetReset.Earliest
        };
    }

    static void CreateConsumerAndConsume() {

        var cb = new ConsumerBuilder&lt;string, string&gt;(consumerConfig);
                  
        Console.WriteLine(&quot;Press Ctrl+C to exit&quot;);
                   
        Console.CancelKeyPress += new ConsoleCancelEventHandler(OnExit);
                
        using(var consumer = cb.Build() ) {
          consumer.Subscribe(&quot;testTopic&quot;);

          try {
            while(!cts.IsCancellationRequested) {
              var cr = consumer.Consume(cts.Token);
              var offset = cr.TopicPartitionOffset
              Console.WriteLine($&quot;Message '{cr.Value}' at: '{offset}'.&quot;);
            }
          }
          catch (Exception e) {
            Console.WriteLine(e.Message);
            consumer.Close();
          }
        }
    }

    static void OnExit(object sender, ConsoleCancelEventArgs args)
    {
      args.Cancel = true;
      Console.WriteLine(&quot;In OnExit&quot;);
      cts.Cancel();
        
    }
  }
}
</code></pre>

<p><strong>Code Snippet 9:</strong> <em>Consumer Code</em></p>

<p>We see in <em>Code Snippet 9</em> the two interesting methods <code>CreateConfig</code> and <code>CreateConsumerAndConsume</code>. In <code>CreateConfig</code> we set three properties:</p>

<ul>
<li><code>BootstrapServers</code> - as for the publisher, this is a list of brokers to connect to.</li>
<li><code>GroupId</code> - the <code>GroupId</code> is the name of the consumer group you connect as to the broker. The <a href="https://www.oreilly.com/library/view/kafka-the-definitive/9781491936153/ch04.html">article here</a> explains consumer groups fairly well.</li>
<li><code>AutoOffsetReset</code> - this tells Kafka where to start reading offsets from.</li>
</ul>

<p>In <code>CreateConsumerAndConsume</code> we:</p>

<ul>
<li>Create a <code>ConsumerBuilder</code> based on the <code>ConsumerConfig</code> instance we created above.</li>
<li>Subscribe to the topic(s) we are interested in.</li>
<li>Consume in a <code>while</code> loop.</li>
</ul>

<p>The <code>Consume</code> method returns a <code>ConsumeResult</code> instance which we use to print information from to the console. What is left now is to build and run from the integrated terminal as we did with the publisher:</p>

<p><img src="/images/posts/confluent_kafka_vscode_build_run_consumer.png" alt="" /></p>

<p><strong>Figure 19:</strong> <em>Build and Run Consumer</em></p>

<p>Dependent on the <code>AutoOffsetReset</code> value you now either see a list of messages (if there are any), or the consumer &ldquo;sits&rdquo; and wait for messages to arrive (as in <em>Figure 19</em>). Let us finish this post with sending some messages from the publish application above. In my code, I changed the <code>Main</code> method to instead of just sending one message, send messages in a <code>while</code> loop:</p>

<pre><code class="language-csharp">while(x &lt; 100) {
  SendMessage(&quot;testTopic&quot;, $&quot;This test: {x}&quot;);
  x++;
  Thread.Sleep(200);
}
</code></pre>

<p><strong>Code Snippet 10:</strong> <em>Loop and Send Messages</em></p>

<p>In <em>Code Snippet 10</em> we see the <code>while</code> loop, and how we send 100 messages, pausing 200ms between each message. The reason I pause is that it makes it clearer to see what happens when I run the code. I build the project, and when I run the code I see in the publisher terminal:</p>

<p><img src="/images/posts/confluent_kafka_publish_messages.png" alt="" /></p>

<p><strong>Figure 20:</strong> <em>Publishing Messages</em></p>

<p>At the same time, I see in the consumer terminal:</p>

<p><img src="/images/posts/confluent_kafka_consume_messages.png" alt="" /></p>

<p><strong>Figure 21:</strong> <em>Consuming Messages</em></p>

<p>So from what we see in *Figure 20, and <em>Figure 21</em> both the publishing application, as well as the consuming application, works! Awesome!</p>

<h2 id="summary">Summary</h2>

<p>In this blog post, we saw how we can install Docker containers for the <em>Confluent Platform</em> on a Windows development machine. Thanks to the announcement of one broker &ldquo;free forever&rdquo; I have the ability to write streaming applications fully utilizing the <em>Confluent Platform</em>.</p>

<p>In the second part of the post we saw how we can use <em>VS Code</em> together with the <em>Confluent Kafka DotNet</em> client.</p>

<p>In future posts, I will &ldquo;dig&rdquo; deeper into both Kafka as well as the .NET client.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 24, 2019]]></title>
    <link href="https://nielsberglund.com/2019/06/16/interesting-stuff---week-24-2019/" rel="alternate" type="text/html"/>
    <updated>2019-06-16T11:12:38+02:00</updated>
    <id>https://nielsberglund.com/2019/06/16/interesting-stuff---week-24-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="databases">Databases</h2>

<ul>
<li><a href="https://medium.com/yugabyte/why-distributed-sql-beats-polyglot-persistence-for-building-microservices-c19dc76b16d0">Why Distributed SQL Beats Polyglot Persistence for Building Microservices?</a>. It is a common belief, (misconception?), that in the microservices world, each microservice should have its own persistence store - polyglot persistence. The blog post I link to here highlights the loss of agility that microservices development and operations suffer when adopting polyglot persistence. The post reviews how distributed SQL serves as an alternative approach that doesn&rsquo;t compromise this agility.</li>
</ul>

<h2 id="misc">Misc.</h2>

<ul>
<li><a href="https://www.paraesthesia.com/archive/2019/06/12/windows-and-linux-docker-containers-side-by-side/">Windows and Linux Docker Containers: Side by Side!</a>. Docker is awesome! One thing though, it is near impossible to run Windows and Linux containers side by side. Wouldn&rsquo;t it be great if you could? That is what this blog post discusses! Good stuff!</li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/petastorm-ml-pipelines/">Petastorm: A Light-Weight Approach to Building ML Pipelines</a>. This is an <a href="https://www.infoq.com/">InfoQ</a> presentation describing how <a href="https://eng.uber.com/petastorm/">Petastorm</a> facilitates tighter integration between Big Data and Deep Learning worlds; simplifies data management and data pipelines; and speeds up model experimentation.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>Right now, I am busy finishing off a blog post looking at the Confluent Platform, (Kafka), from the perspective of a .NET developer on the Windows platform. I hope to publish the post sometime this coming week.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 23, 2019]]></title>
    <link href="https://nielsberglund.com/2019/06/09/interesting-stuff---week-23-2019/" rel="alternate" type="text/html"/>
    <updated>2019-06-09T06:38:03+02:00</updated>
    <id>https://nielsberglund.com/2019/06/09/interesting-stuff---week-23-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="sql-server">SQL Server</h2>

<ul>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2019/06/06/the-june-release-of-azure-data-studio-is-now-available/">The June release of Azure Data Studio is now available</a>. What the title says. You can now download the June release of Azure Data Studio, or, if you already have it installed, it should upgrade automatically. There are quite a few highlights in this release. The one that interests me the most is the improvements in SQL Notebooks</li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://blog.acolyer.org/2019/06/03/ease-ml-ci/">Continuous integration of machine learning models with ease.ml/ci</a>. The white paper <a href="https://twitter.com/adriancolyer">Adrian</a> dissects here is about what does a continuous integration testing environment look like for a machine learning model? The paper presents <strong>ease.ml/ci</strong> which is a continuous integration system for machine learning.</li>
<li><a href="https://azure.microsoft.com/en-us/blog/build-more-accurate-forecasts-with-new-capabilities-in-automated-machine-learning/">Build more accurate forecasts with new capabilities in automated machine learning</a>. This blog post discusses new capabilities in Azure Machine Learning service related to time-series forecasting. Very interesting!</li>
<li><a href="https://blog.acolyer.org/2019/06/05/data-validation-for-machine-learning/">Data validation for machine learning</a>. Machine learning is good and all, and you can achieve a lot with ML. However, unless the data passed into ML is correct, you only get garbage. The paper <a href="https://twitter.com/adriancolyer">Adrian</a> dissects in this post focuses on the problem of validation the input data fed to ML pipelines.</li>
</ul>

<h2 id="sql-server-2019">SQL Server 2019</h2>

<ul>
<li><a href="/2019/06/06/sql-server-2019-extensibility-framework--external-languages/">SQL Server 2019 Extensibility Framework &amp; External Languages</a>. A couple of weeks ago, I wrote a <a href="/2019/05/26/java--sql-server-2019-extensibility-framework-the-sequel/">blog post</a> about changes in SQL Server 2019 CTP 2.5 related to how you write Java code for use by SQL Server. Well, a bit later, Microsoft released SQL Server 2019 CTP 3.0 with more changes around this topic (Java code in SQL Server), and in this blog post, I discuss these changes.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SQL Server 2019 Extensibility Framework &amp; External Languages]]></title>
    <link href="https://nielsberglund.com/2019/06/06/sql-server-2019-extensibility-framework--external-languages/" rel="alternate" type="text/html"/>
    <updated>2019-06-06T05:37:52+02:00</updated>
    <id>https://nielsberglund.com/2019/06/06/sql-server-2019-extensibility-framework--external-languages/</id>
    <content type="html"><![CDATA[<p>A little while ago I wrote a blog post, <a href="/2019/05/26/java--sql-server-2019-extensibility-framework-the-sequel/"><strong>Java &amp; SQL Server 2019 Extensibility Framework: The Sequel</strong></a>, about changes in SQL Server 2019 CTP 2.5 impacting how we write Java code for use from SQL Server. While I wrote that post, Microsoft released SQL Server 2019 CTP 3.0, and, (surprise, surprise), that release contains more changes affecting Java code in SQL Server.</p>

<p>This post covers those changes as well as discusses what SQL Server Extensibility Framework and Language Extensions are.</p>

<p></p>

<p>Before we &ldquo;dive&rdquo; into the &ldquo;nitty-gritty&rdquo; let look at the data we use in this post.</p>

<h2 id="demo-data">Demo Data</h2>

<p>The data we see here is for you who want to &ldquo;code along&rdquo;. It is lifted from the <a href="/2019/05/26/java--sql-server-2019-extensibility-framework-the-sequel/">post</a> mentioned above:</p>

<pre><code class="language-sql">USE master;
GO
SET NOCOUNT ON;
GO
DROP DATABASE IF EXISTS JavaTestDB;
GO
CREATE DATABASE JavaTestDB;
GO
USE JavaTestDB;
GO

GO
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Create Database</em></p>

<p>We see from <em>Code Snippet 1</em> how we create a database where we want to run some Java code from.</p>

<h2 id="background">Background</h2>

<p>In SQL Server 2016, Microsoft introduced SQL Server R Services. That allowed you to, from inside SQL Server, call to the R engine via a special procedure, <code>sp_execute_external_script</code>, and execute R scripts. The R engine was, (and is), part of the SQL server installation but it runs as an external process, (not in SQL Server&rsquo;s process), and subsequently, R is seen as an external language.</p>

<p>In SQL Server 2017, Microsoft added Python as an external language and renamed SQL Server R Services to SQL Server Machine Learning Services. The way Python works in SQL Server is the same as R:</p>

<ul>
<li>The Python engine is included in the SQL Server installation.</li>
<li>You execute Python code using the <code>sp_execute_external_script</code>.</li>
<li>Python runs in an external process.</li>
</ul>

<p>The communication between SQL Server and the external engine goes over the <em>Launchpad</em> service:</p>

<p><img src="/images/posts/sql_r_services_ect_script1.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>External Script and Language</em></p>

<p>We see in <em>Figure 1</em> how:</p>

<ul>
<li>We execute the procedure <code>sp_execute_external_script</code>.</li>
<li>That calls into the <em>Launchpad</em> service.</li>
<li>The <em>Launchpad</em> service passes the script into the relevant launcher based on the <code>@language</code> parameter in <code>sp_execute_external_script</code>. The knowledge of what launcher to call lives inside of the <em>Launchpad</em> service.</li>
<li>The launcher dll loads the relevant external engine, passes the script to the engine and executes.</li>
</ul>

<p>The above is a very high-level overview of how it works, and you can read more about the inner workings of it in <a href="/sql_server_2k16_r_services"><strong>SQL Server R Services</strong></a>.</p>

<p>So, a launcher dll is a piece of code, typically written in C++, who knows how to interact with the external engine.</p>

<p>After the introduction of Python in SQL Server 2017, the documentation started to mention how R and Python code runs in an extensibility framework, which is isolated from the core engine processes. Around this time, Microsoft started to mention the possibility of other languages becoming part of the extensibility framework.</p>

<h2 id="sql-server-2019-java">SQL Server 2019 &amp; Java</h2>

<p>At the time of SQL Server 2017 and the inclusion of Python, the extensibility framework was more just a name or - at least - it was purely some internal Microsoft SQL Server code. It was nothing that you and I could use directly. Then came SQL Server 2019.</p>

<p>In CTP 2.0 of SQL Server 2019, Microsoft made Java publicly available as an external language. Having Java as an external language may not seem that much different from R/Python, but there are some differences:</p>

<ul>
<li>Java is a compiled language, where we call into a specific method. R/Python are scripting languages where we send a script to the engine.</li>
<li>R/Python are part of the SQL Server install, together with launcher dll&rsquo;s and so forth. For Java, there is an equivalent of a launcher dll, (<code>javaextension.dll</code>), which calls into the JVM. The difference here between R/Python and Java is that the JVM is not part of the SQL Server install but must be installed separately.</li>
</ul>

<p>What Microsoft could have done with the Java integration in SQL Server 2019 was to just treat it as R/Python, and &ldquo;hardcode&rdquo; Java as a language in the <em>Launchpad</em> service and let the <em>Launchpad</em> service call the <code>javaextension.dll</code>.</p>

<blockquote>
<p><strong>NOTE:</strong> There are most likely quite substantial differences between the <code>javaextension.dll</code> and the R/Python launcher dll&rsquo;s, but in his post, I treat them as being more or less equivalent.</p>
</blockquote>

<p>However, Microsoft did not &ldquo;hack&rdquo; the <em>Launchpad</em> service, but what they did was, with the view to &ldquo;properly&rdquo; expose an extensibility framework with multiple external languages, that they introduced some new components (hosts). The <em>Launchpad</em> service calls these hosts for all languages except R/Python. Yes, yes, I do know that for now (we are now at CTP 3.0), it is only Java, but&hellip;</p>

<blockquote>
<p><strong>NOTE:</strong> In future posts I will talk more about these new components.</p>
</blockquote>

<p>Having read this far in the post you may say: <em>Hey Niels, this is all interesting and all, but you have not said anything we don&rsquo;t already know</em>.</p>

<h2 id="external-language">External Language</h2>

<p>Ok, so let us see what this is all about. Remember how we, in the <a href="/2019/05/26/java--sql-server-2019-extensibility-framework-the-sequel/"><strong>Java &amp; SQL Server 2019 Extensibility Framework: The Sequel</strong></a> post, discussed how SQL Server CTP 2.5 introduced a Java SDK, <code>mssql-java-lang-extension.jar</code>, that we as developers need to develop against when we write Java code we want to execute from SQL Server. That is a requirement in CTP 3.0 as well, but the way you get the <code>.jar</code> file is different. In CTP 2.5 you downloaded the file, whereas in CTP 3.0 the file is part of the SQL Server distribution, and you find it at: <code>..\&lt;path_to_sql_instance&gt;\MSSQL\Binn\mssql-java-lang-extension.jar</code>.</p>

<blockquote>
<p><strong>NOTE:</strong> The file name of the SDK is somewhat misleading as it is not the Java language extension itself, it is the SDK for the Java language extension.</p>
</blockquote>

<p>We know from the <a href="/2019/05/26/java--sql-server-2019-extensibility-framework-the-sequel/">post mentioned above</a> that we need to create an external library based on the <code>.jar</code> file, so I copy the file to a more accessible location and then:</p>

<pre><code class="language-sql">USE JavaTestDB;
GO

CREATE EXTERNAL LIBRARY javaSDK 
FROM (CONTENT = 'W:\mssql-java-lang-extension.jar')
WITH (LANGUAGE = 'Java');
GO
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Create External SDK Library</em></p>

<p>In CTP 2.5 the code in <em>Code Snippet 2</em> runs just fine, but when we run it in CTP 3.0 we get an exception:</p>

<p><img src="/images/posts/sql_2k19_ext_lang_error1.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Exception when Creating External Library</em></p>

<p>The exception we see in <em>Figure 2</em> is due to one of the changes in CTP 3.0: an external language needs to be &ldquo;registered&rdquo; with SQL Server before we can reference it. Registering a language with SQL Server allows Microsoft and/or 3rd parties to expose arbitrary languages to be used from SQL Server.</p>

<p>What we register is the actual language extension file for that particular language, together with a name for the language.</p>

<h2 id="create-external-language">Create External Language</h2>

<p>The way we register/create an external language is similar to how we create an external library; we use a <code>CREATE EXTERNAL ...</code> DDL statement: <code>CREATE EXTERNAL LANGUAGE</code>:</p>

<pre><code class="language-sql">CREATE EXTERNAL LANGUAGE language_name  
[ AUTHORIZATION owner_name ]  
FROM (CONTENT = { &lt;file_spec&gt; }
     FILE_NAME = &lt;file_name&gt;
    [ , PLATFORM = &lt;platform&gt; ]
    [ , PARAMETERS = &lt;parameters&gt; ]
    [ , ENVIRONMENT_VARIABLES = &lt;env_variables&gt; )
[ ; ] 
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Signature CREATE EXTERNAL LIBARY</em></p>

<p>The arguments we see in <em>Code Snippet 3</em> are:</p>

<ul>
<li><code>language_name</code>: A unique name for the language.</li>
<li><code>owner_name</code>: This optional parameter specifies the name of the user or role that owns the language.</li>
<li><code>file_spec</code>: The <code>file_spec</code> specifies the content of the language extension file for a specific platform, and it can either be in the form of a file location (local path/network path) or a hex literal. If we install the package from a file location, the file needs to be in the form of an archive file (<code>zip on Windows,</code>tar.gz` on Linux).</li>
<li><code>file_name</code>: Name of the language extension <code>dll</code> or <code>so</code> file.</li>
<li><code>platform</code>: The <code>PLATFORM</code> parameter, which defines the platform for the content of the library. The <code>PLATFORM</code> can be Windows or Linux, and it defaults to Windows.</li>
<li><code>parameters</code>: Optional parameters for the external language runtime. Not supported in CTP 3.0.</li>
<li><code>env_variables</code>: Optional parameter to set environment variables for the external language runtime. Not supported in CTP 3.0.</li>
</ul>

<p>The above is a somewhat simplified explanation of the arguments, but it should be enough for us to get started. You find a more in-depth description <a href="https://docs.microsoft.com/en-us/sql/t-sql/statements/create-external-language-transact-sql?view=sqlallproducts-allversions">here</a>.</p>

<h2 id="using-create-external-language">Using CREATE EXTERNAL LANGUAGE</h2>

<p>Before we write code to create an external language, let us think back to what I wrote in <a href="/2019/03/10/sql-server-2019-java--external-libraries---i/">SQL Server 2019, Java &amp; External Libraries - I</a>, and <a href="/2019/04/10/installing-r-packages-in-sql-server-machine-learning-services---iii/">Installing R Packages in SQL Server Machine Learning Services - III</a> about <code>CREATE EXTERNAL LIBRARY</code> and how there were some new system catalog views introduced together with <code>CREATE EXTERNAL LIBRARY</code>: more specifically <code>sys.external_libraries</code> and friends. The same is true for <code>CREATE EXTERNAL LANGUAGE</code>:</p>

<ul>
<li><code>sys.external_languages</code> - contains a row for each external language in the database.</li>
<li><code>sys.external_language_files</code> - contains a row for each external language extension file in the database.</li>
</ul>

<p>Let us look at what we see if we run a <code>SELECT</code> against those two catalog views. I do this on a freshly installed CTP 3.0 where I have not created any external languages. I have only enabled <em>Machine Learning Services</em> together with R and Python. When I execute my <code>SELECT</code>&rsquo;s, I see:</p>

<p><img src="/images/posts/sql_2k19_ext_lang_cat_views1.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>External Languages</em></p>

<p>What I see in <em>Figure 3</em> surprises me somewhat; even though I have not created any external languages myself, the mere fact that I have enabled <em>Machine Learning Services</em> bootstraps two languages: R and Python, as we see in the upper result grid, (<code>SELECT * FROM sys.external_languages</code>). Notice also how in the lower result grid, (<code>SELECT * FROM sys.external_language_files</code>), I see files for both the Windows as well as the Linux platforms.</p>

<p>So let us create Java as an external language. We know from above that the Java language extension file is the <code>javaextension.dll</code>, which is part of the SQL Server distribution and you find it in the same directory as the SDK <code>.jar</code> mentioned above: <code>..\&lt;path_to_sql_instance&gt;\MSSQL\Binn\javaextension.dll</code>. However, you cannot use it directly in the <code>CREATE EXTERNAL LIBRARY</code> call; you need to archive it into a <code>.zip</code> file first - as mentioned above.</p>

<p>I zipped the dll and placed it in the same location as the <code>.jar</code> file in <em>Code Snippet 2</em> and I am now ready to create the external language:</p>

<pre><code class="language-sql">CREATE EXTERNAL LANGUAGE Java
FROM (CONTENT = 'W:\javaextension.zip'
      , FILE_NAME = 'javaextension.dll');
GO
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Creating External Language</em></p>

<p>The reason we, in <em>Code Snippet 4</em>, set the file name in the <code>FILE_NAME</code> parameter is that the zip file may contain multiple files and the file name defines the language extension. After we execute the code in <em>Code Snippet 4</em>, we run the <code>SELECT</code> statements we used above against the external language catalog views, and we get:</p>

<p><img src="/images/posts/sql_2k19_ext_lang_cat_views2.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Java External Language</em></p>

<p>WWe see in <em>Figure 4</em> how we have added Java as an external language to the <code>JavaTestDB</code> database. In the lower result grid, we see how the binary representation of the zip file is persisted as well, the same as it is for external libraries. Speaking of external libraries, in the posts I did about those, we discussed how the external libraries, when resolved, were copied to file directories: <code>..\&lt;path_to_sql_instance&gt;\MSSQL\ExternalLibraries</code>. I wonder if it is the same for external languages?</p>

<p>Sure enough, when looking at <code>..\&lt;path_to_sql_instance&gt;\MSSQL</code> I see an <code>ExternalLanguages</code> directory, and as with external libraries, it is empty. Remember from the posts mentioned above, how the <code>ExternalLibraries</code> directory got populated when we resolved an external library. Let us see if it is the same for external languages.</p>

<p>As we have created the external language, we can now do what we tried to do earlier; create the external SDK library. So, we run the code in <em>Code Snippet 2</em> again, and now it succeeds. We verify that by executing: <code>SELECT * FROM sys.external_libraries</code>. When we have deployed the SDK, we can deploy our Java code that we want to call from inside SQL Server. In this post, I use the same Java code as in the first example in <a href="/2019/05/26/java--sql-server-2019-extensibility-framework-the-sequel/"><strong>Java &amp; SQL Server 2019 Extensibility Framework: The Sequel</strong></a>:</p>

<pre><code class="language-java">package sql;

import com.microsoft.sqlserver.javalangextension.PrimitiveDataset;
import java.util.LinkedHashMap;
import com.microsoft.sqlserver.javalangextension.\
            AbstractSqlServerExtensionExecutor;
public class JavaTest1 extends AbstractSqlServerExtensionExecutor {

  public PrimitiveDataset execute(PrimitiveDataset input, 
                                  LinkedHashMap&lt;String, 
                                  Object&gt; params) {

      int x = (int)params.get(&quot;x&quot;);
      int y = (int)params.get(&quot;y&quot;);

      System.out.printf(&quot;The result of adding %d and %d = %d&quot;, 
                         x, y, x + y);  
      return null;

  }
}
</code></pre>

<p><strong>Code Snippet 5:</strong> <em>JavaTest1 Class and Execute Method</em></p>

<p>I compile the code in <em>Code Snippet 5</em> into a <code>.jar</code> file which I then deploy:</p>

<pre><code class="language-sql">CREATE EXTERNAL LIBRARY mySqlJar 
FROM (CONTENT = 'W:\sql-1.0.jar')
WITH (LANGUAGE = 'Java');
GO
</code></pre>

<p><strong>Code Snippet 6:</strong> <em>Deploy Java Code</em></p>

<p>We see after we ran the code in <em>Code Snippet 6</em> that nothing changed in the <code>ExternalLanguage</code> directory, and nothing changed for that matter in <code>ExternalLibraries</code> either. Hopefully, we see some changes when we execute the code calling into our class:</p>

<pre><code class="language-sql">DECLARE @p1 int = 21;
DECLARE @p2 int = 21;
EXEC sp_execute_external_script
  @language = N'Java'
, @script = N'sql.JavaTest1'
, @params = N'@x int, @y int'
, @x = @p1
, @y = @p2   
GO
</code></pre>

<p><strong>Code Snippet 7:</strong> <em>Execute Java Code</em></p>

<p>The result after running the code in <em>Code Snippet 7</em>:</p>

<p><img src="/images/posts/sql_2k19_ext_lang_exec1.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Success</em></p>

<p>As we see in <em>Figure 5</em>, the code we ran in <em>Code Snippet 7</em> executed successfully. So, what has happened in the file system:</p>

<p><img src="/images/posts/sql_2k19_ext_lang_filesystem1.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>File System External Languages</em></p>

<p>In <em>Figure 6</em> we see how, when executing the code in <em>Code Snippet 7</em>, the language extension file gets copied to a directory with the structure <code>ExternalLanguage | Database Id | External Language Id | File Name</code>. As with external libraries, SQL Server loads the extension from that directory.</p>

<p>What is interesting is that even though R/Python shows as external languages in the catalog views, when you execute some R/Python code, the launcher dll&rsquo;s do not get copied to the external languages directory.</p>

<h2 id="summary">Summary</h2>

<p>In this post, we discussed the requirement in SQL Server CTP 3.0 to register any external language other than R/Python which you want to use from inside SQL Server.</p>

<blockquote>
<p><strong>NOTE:</strong> At the time of writing this post the only external language besides R/Python is Java, but other languages will most definitely become available.</p>
</blockquote>

<p>So, what did we say:</p>

<ul>
<li>Before you can use Java as an external language, you need to register it with SQL Server in the database you want to call Java code from.</li>
<li>You register not only the language name but also the language extension: the bridge between SQL Server and the external runtime.</li>
<li>To register you call <code>CREATE EXTERNAL LANGUAGE</code>, where you can either use a file path or a binary representation of the archive file containing the language extension.</li>
<li>In future releases you can send in parameters as well as environment variables in the <code>CREATE EXTERNAL LANGUAGE</code> call.</li>
</ul>

<p>Something we didn&rsquo;t touch upon in this post was that the security model for executing against an external language has changed somewhat. We cover that in a future post.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 22, 2019]]></title>
    <link href="https://nielsberglund.com/2019/06/03/interesting-stuff---week-22-2019/" rel="alternate" type="text/html"/>
    <updated>2019-06-03T18:29:57+02:00</updated>
    <id>https://nielsberglund.com/2019/06/03/interesting-stuff---week-22-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://devblogs.microsoft.com/dotnet/porting-desktop-apps-to-net-core/">Porting desktop apps to .NET Core</a>. This is a very informative post discussing porting of .NET Framework applications to .NET Core. Seeing that the apps to port can be of different grades of complexity, this first post covers simple use cases, and a follow-up post covers the more complex scenarios.</li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://towardsdatascience.com/apache-spark-mllib-tutorial-ec6f1cb336a9">Apache Spark MLlib Tutorial: Linear Regression</a>. This post is the first part of a tutorial on how to work with MLLib in Apache Spark. For me, this is interesting as I can see us at <a href="/derivco">Derivco</a> start to use Spark.</li>
</ul>

<h2 id="vs-code">VS Code</h2>

<ul>
<li><a href="https://blog.usejournal.com/visual-studio-code-for-java-the-ultimate-guide-2019-8de7d2b59902">Visual Studio Code for Java: The Ultimate Guide 2019</a>. I have written a couple of posts where I have used <em>VS Code</em> and Java. However, since I am a .NET guy &ldquo;at heart&rdquo;, the Java ecosystem is a mystery to me, and I have &ldquo;fumbled&rdquo; my way through. I wish I had come across the post I link to here, as it gives you awesome information about how to work with <em>VS Code</em> and Java.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://azure.microsoft.com/en-us/blog/visual-data-ops-for-apache-kafka-on-azure-hdinsight-powered-by-lenses/">Visual data ops for Apache Kafka on Azure HDInsight, powered by Lenses</a>. This post looks at how you can manage your streaming data operations, from visibility to monitoring, by the use of <a href="https://lenses.io/">Lenses</a>. The post looks at how to do it in Azure, but it is as applicable on-prem as well. Once again, this is interesting to me as, at <a href="/derivco">Derivco</a>, we are great fans of Kafka.</li>
<li><a href="https://www.confluent.io/blog/deploying-kafka-streams-and-ksql-with-gradle-part-2-managing-ksql-implementations">Deploying Kafka Streams and KSQL with Gradle â€“ Part 2: Managing KSQL Implementations</a>. The second part in a series about how to develop, and deploy Kafka Streams and KSQL parts of streaming applications using <a href="https://gradle.org/">Gradle</a>.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 21, 2019]]></title>
    <link href="https://nielsberglund.com/2019/05/26/interesting-stuff---week-21-2019/" rel="alternate" type="text/html"/>
    <updated>2019-05-26T10:02:45+02:00</updated>
    <id>https://nielsberglund.com/2019/05/26/interesting-stuff---week-21-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/schemas-contracts-compatibility">Schemas, Contracts, and Compatibility</a>. This blog post looks at how Kafka together with its schema registry can be used in a microservices environment, potentially as a replacement for REST. Very interesting!</li>
<li><a href="https://www.confluent.io/blog/kafka-summit-london-2019-session-videos">Kafka Summit London 2019 Session Videos</a>. In last weeks <a href="/2019/05/19/interesting-stuff---week-20-2019/">roundup</a> I mentioned that I had attended the Kafka Summit in London. The organizers have now made all session videos and slides available. So go to the post I link to and look at the sessions that interest you!</li>
</ul>

<h2 id="sql-server-2019">SQL Server 2019</h2>

<ul>
<li><a href="/2019/05/26/java--sql-server-2019-extensibility-framework-the-sequel/">Java &amp; SQL Server 2019 Extensibility Framework: The Sequel</a>. In SQL Server 2019 CTP 2.5, Microsoft made some changes to requirements for writing Java code to be used by SQL Server. In this post, I look at what those changes are, and what our Java code should look like going forward.</li>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2019/05/22/sql-server-2019-community-technology-preview-3-0-is-now-available/">SQL Server 2019 community technology preview 3.0 is now available</a>. The title says it all. Microsoft just released SQL Server 2019 CTP 3.0. Go and get it while it is hot! Oh, above I mentioned about changes in CTP 2.5 impacting how we write Java code for SQL Server. The 3.0 release has some additional changes, so expect a follow-up blog post about that.</li>
</ul>

<h2 id="next-weeks-roundup">Next Weeks Roundup</h2>

<p>I am away the whole of next week, and not back until Tuesday, June 4. Due to this, the roundup for next week may be delayed for a couple of days.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Java &amp; SQL Server 2019 Extensibility Framework: The Sequel]]></title>
    <link href="https://nielsberglund.com/2019/05/26/java--sql-server-2019-extensibility-framework-the-sequel/" rel="alternate" type="text/html"/>
    <updated>2019-05-26T07:20:09+02:00</updated>
    <id>https://nielsberglund.com/2019/05/26/java--sql-server-2019-extensibility-framework-the-sequel/</id>
    <content type="html"><![CDATA[<p>As you may know, a while back I wrote some posts about the support for Java in SQL Server 2019: <a href="/s2k19_ext_framework_java"><strong>SQL Server 2019 Extensibility Framework &amp; Java</strong></a>. The posts covered in some detail how Java in SQL Server worked, and how to write Java code for use in SQL Server. However, a week or two ago &ldquo;the sky came tumbling down&rdquo; when Microsoft released SQL Server 2019 CTP 2.5.</p>

<blockquote>
<p><strong>NOTE:</strong> CTP stands for Community Technology Preview and is like a beta release.</p>
</blockquote>

<p>What Microsoft did in CTP 2.5 was to introduce Java Language Extension SDK, and your Java code now needs to inherit an abstract base class from the SDK. This requirement makes a large part of my previous posts &ldquo;null and void&rdquo;, so in this post, we look at what to do going forward.</p>

<p></p>

<p>What happened here, (functionality introduced that negates previous functionality), is the danger when writing about beta releases. I should know, as it has happened before. Back in 2003 some colleagues and I wrote a book about the upcoming SQL Server 2005 release: <a href="https://www.amazon.com/First-Look-Server-2005-Developers/dp/0321180593/ref=sr_1_fkmrnull_1">A First Look at SQL Server 2005 for Developers</a>, and we wrote the book based on beta releases. When Microsoft eventually released SQL Server 2005, at least a couple of chapters in the book covered functionality that no longer existed. Well, what can you do?</p>

<p>Anyway, let us go back to SQL Server 2019 and Java.</p>

<h2 id="recap-pre-ctp-2-5">Recap (pre CTP 2.5)</h2>

<p>When I started this post, my idea was to do a brief recap of what the Java implementation looked like in the previous CTP&rsquo;s, to show what it used to be, and refer to that in this post. After I had written 90% of the <em>Recap</em> I realized it had become way too long, so I decided to skip it.</p>

<p>If you are interested in what it used to be, you can go back and read the posts in the <a href="/s2k19_ext_framework_java"><strong>SQL Server 2019 Extensibility Framework &amp; Java</strong></a> series. The most relevant posts are:</p>

<ul>
<li><a href="/2018/12/02/sql-server-2019-extensibility-framework--java---hello-world/">SQL Server 2019 Extensibility Framework &amp; Java - Hello World</a>: We looked at installing and enabling the Java extension, as well as some very basic Java code.</li>
<li><a href="/2018/12/08/sql-server-2019-extensibility-framework--java---passing-data/">SQL Server 2019 Extensibility Framework &amp; Java - Passing Data</a>: In this post, we discussed what is required to pass data back and forth between SQL Server and Java.</li>
<li><a href="/2018/12/19/sql-server-2019-extensibility-framework--java---null-values/">SQL Server 2019 Extensibility Framework &amp; Java - Null Values</a>: This, the <a href="/2018/12/19/sql-server-2019-extensibility-framework--java---null-values/">Null Values</a>, post is a follow up to the <a href="/2018/12/08/sql-server-2019-extensibility-framework--java---passing-data/">Passing Data</a> post, and we look at how to handle <code>null</code> values in data passed to Java.</li>
</ul>

<h2 id="demo-data">Demo Data</h2>

<p>In this post, we use some data from the database, so let us set up the necessary database, tables, and load data into the tables:</p>

<pre><code class="language-sql">USE master;
GO
SET NOCOUNT ON;
GO
DROP DATABASE IF EXISTS JavaTestDB;
GO
CREATE DATABASE JavaTestDB;
GO
USE JavaTestDB;
GO

DROP TABLE IF EXISTS dbo.tb_Rand10
CREATE TABLE dbo.tb_Rand10(RowID int identity primary key, x int, 
                          y int;

INSERT INTO dbo.tb_Rand10(x, y)
SELECT TOP(10) CAST(ABS(CHECKSUM(NEWID())) % 14 AS int) 
  , CAST(ABS(CHECKSUM(NEWID())) % 20 AS int)
FROM sys.objects o1
CROSS JOIN sys.objects o2
GO
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Create Database Objects</em></p>

<p>We see from <em>Code Snippet 1</em> how we:</p>

<ul>
<li>Create a database: <code>JavaTestDB</code>.</li>
<li>Create a table: <code>dbo.tb_Rand10</code>.</li>
<li>Insert some data into the table.</li>
</ul>

<p>The data we insert is entirely random, but it gives us something to &ldquo;play around&rdquo; with. Now, when we have a database and some data let us get started.</p>

<h2 id="microsoft-extensibility-sdk-for-java">Microsoft Extensibility SDK for Java</h2>

<p>As mentioned above, in CTP 2.5, Microsoft changes the way we implement Java code in SQL Server, and they do it to create a better developer experience when writing Java code for SQL Server.</p>

<blockquote>
<p><strong>NOTE:</strong> I am not totally sure this change gives us a better developer experience, I guess time will tell.</p>
</blockquote>

<p>In CTP 2.5 and onwards when you write Java code for SQL Server you implement your code using the <strong>Microsoft Extensibility SDK for Java</strong>, (SDK). The SDK acts sort of like an interface as it exposes abstract classes that your code need to extend/target, (more about that later).</p>

<p>The SDK comes in the form of a <code>.jar</code> file, and you download the SDK from <a href="http://aka.ms/mssql-java-lang-extension">here</a>. Since a <code>.jar</code> file is essentially an archive file you can open the SDK <code>.jar</code> with your favorite file archiver utility and when you do, you see:</p>

<p><img src="/images/posts/sql_2k19_java_sdk_jar1.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>SDK Jar - I</em></p>

<p>We see in <em>Figure 1</em> how I have extracted the SDK <code>.jar</code> to a folder, and how the <code>.jar</code> file contains at the top level two folders: <code>com</code> and <code>META-INF</code>. The <code>com</code> folder is the top level folder for the Java SDK package, and below we look a bit more into it. The <code>META-INF</code> folder contains metadata information about the <code>.jar</code> package, and in this post we do not care about it.</p>

<p>Coming back to the <code>com</code> folder I mentioned it was the top level folder for the package, and if we drill down into it, it looks something like so:</p>

<p><img src="/images/posts/sql_2k19_java_sdk_jar2.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>SDK Jar - II</em></p>

<p>In <em>Figure 2</em>, outlined in blue, we see how the package name follows the standard of a hierarchical naming pattern: <code>com.microsoft.sqlserver.javalengextension</code>. Below <code>javalangextension</code> we have three classes outlined in red - these are the classes mentioned above:</p>

<ul>
<li><strong><code>AbstractSqlServerExtensionExecutor</code></strong></li>
<li><strong><code>AbstractSqlServerExtensionDataset</code></strong></li>
<li><strong><code>PrimitiveDataset</code></strong></li>
</ul>

<p>Let us look at what these classes do.</p>

<h2 id="abstractsqlserverextensionexecutor">AbstractSqlServerExtensionExecutor</h2>

<p>The <code>AbstractSqlServerExtensionExecutor</code> abstract class is the class you need to inherit from/extend in the classes that SQL Server calls. The source code looks like so (I have copied the code from <a href="https://docs.microsoft.com/en-us/sql/advanced-analytics/java/java-sdk?view=sqlallproducts-allversions#abstractsqlserverextensionexecutor-source-code">here</a>):</p>

<pre><code class="language-java"> package com.microsoft.sqlserver.javalangextension;

import com.microsoft.sqlserver.javalangextension.AbstractSqlServerExtensionDataset;
import java.lang.UnsupportedOperationException;
import java.util.LinkedHashMap;

/**
 * Abstract class containing interface used by the Java extension
 */
public abstract class AbstractSqlServerExtensionExecutor {
  /* Supported versions of the Java extension */
  public final int SQLSERVER_JAVA_LANG_EXTENSION_V1 = 1;

  /* Members used by the extension to determine application specifics */
  protected int executorExtensionVersion;
  protected String executorInputDatasetClassName;
  protected String executorOutputDatasetClassName;

  public AbstractSqlServerExtensionExecutor() { }

  public void init(String sessionId, int taskId, int numTasks) {
    /* Default implementation of init() is no-op */
  }

  public AbstractSqlServerExtensionDataset execute(
               AbstractSqlServerExtensionDataset input, 
               LinkedHashMap&lt;String, Object&gt; params) {
    throw new UnsupportedOperationException(
       &quot;AbstractSqlServerExtensionExecutor execute() is not implemented&quot;);
  }

  public void cleanup() {
    /* Default implementation of cleanup() is no-op */
  }
}
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>AbstractSqlServerExtensionExecutor</em></p>

<p>When looking at the code in <em>Code Snippet 2</em> we see how the class:</p>

<ul>
<li>Has three class members that according to the comments have something to do with application specifics.</li>
<li>Has three methods: <code>init</code>, <code>execute</code>, and <code>cleanup</code>.</li>
</ul>

<p>Later in the post, I come back to the class members, but now I want to look at the three methods. More specifically, I want to look at the <code>execute</code> method since <code>init</code>, and <code>cleanup</code> are fairly self-explanatory: <code>init</code> if any initialization needs to be done, and <code>cleanup</code> for any, well, clean up after usage.</p>

<h4 id="execute">execute</h4>

<p>That leaves us <code>execute</code>. Notice in <em>Code Snippet 2</em> how both <code>init</code> and <code>cleanup</code> are no-ops, whereas <code>execute</code> is not. Furthermore, if someone calls <code>execute</code> in a class which extends <code>AbstractSqlServerExtensionExecutor</code>, and there is no implementation of <code>execute</code> the method throws an <code>UnsupportedOperationException</code> error. So who would call <code>execute</code>?</p>

<p>To answer the question about who is calling <code>execute</code>, let us remind ourselves what happens when we call <code>sp_execute_external_script</code>. We do that by looking at what happens when we execute R/Python code. In my <a href="/sql_server_2k16_r_services">SQL Server R Services</a> series we  talked about the components which make up <strong>SQL Server Machine Learning Services</strong>, and we saw how the flow when we execute an external script, looks something like so:</p>

<p><img src="/images/posts/sql_2k19_java_intro_flow1.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Components &amp; Flow</em></p>

<p>The flow is similar when we execute Java code; e.g. when we execute <code>sp_execute_external_script</code> SQL Server calls into the <em>Launchpad</em> service which then &ldquo;spins&rdquo; up the external engine and your code runs. In this case the call goes into the Java extension library (a <code>.dll</code>), and the extension library calls into the JVM. So it is the extension library that calls the <code>execute</code> method. This is different to pre CTP 2.5 where the extension called a method specified in the <code>@script</code> parameter: <code>@script = N'packagename.classname.method'</code>, and now it is: <code>@script = N'packagename.classname'</code>.</p>

<p>The implication of this is that in pre CTP 2.5 you could have multiple &ldquo;entry&rdquo; points, (methods), to call into, whereas now the entry point is the <code>execute</code> method.</p>

<p>Above I mentioned that one of the reasons for introducing the SDK was to create a better developer experience, and the signature of <code>execute</code> gives some hints about this:</p>

<pre><code class="language-java">public AbstractSqlServerExtensionDataset execute(
               AbstractSqlServerExtensionDataset input, 
               LinkedHashMap&lt;String, Object&gt; params) {...}
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Execute Method</em></p>

<p>From the signature in <em>Code Snippet 3</em> we see how the <code>execute</code> method takes two parameters and has a return type. This in itself is interesting as pre CTP 2.5 the methods you called into did not allow parameters, and had to be <code>void</code>.</p>

<p>When we look at the parameters, the <code>execute</code> method expects we see they are:</p>

<ul>
<li><code>AbstractSqlServerExtensionDataset input</code></li>
<li><code>LinkedHashMap&lt;String, Object&gt; params</code></li>
</ul>

<p>The <code>input</code> parameter references any dataset you pass in the class, (from the <code>@input_data_1</code> parameter in <code>sp_execute_external_script</code>). We talk more about <code>AbstractSqlServerExtensionDataset</code> below.</p>

<p>What about the <code>params</code> parameter? As the name implies, it has to do with passing in parameters to the <code>execute</code> method. Remember that in pre CTP 2.5 a method could not have parameters and if you wanted to send in parameters you first defined them in the <code>@params</code> parameter in <code>sp_execute_external_script</code> and declared them like so:</p>

<pre><code class="language-sql">DECLARE @p1 int = 21;
DECLARE @p2 int = 21;
EXEC sp_execute_external_script
  @language = N'Java'
, @script = N'JavaTest1.adder'
, @params = N'@x int, @y int'
, @x = @p1
, @y = @p2   
GO
</code></pre>

<p><strong>Code Snippet 4</strong> <em>Call from T-SQL to Java with Parameters</em></p>

<p>What we see in <em>Code Snippet 4</em> how we add two parameter definitions, (<code>@x</code> and <code>@y</code>), to the <code>@params</code> parameter, and how we then declare and assign values to them: <code>@x = @p1</code>, and <code>@y = @p2</code>. In the Java code, as the methods had to be parameterless, we added the parameters as class members and named them the same as in the SQL code, but without the <code>@</code> sign. In the methods, we then used those class members.</p>

<p>In CTP 2.5 and onwards, we still declare the parameters as before in <code>sp_execute_external_script</code>, but we no longer need to define the parameters as class members in the Java code. The Java extension dll takes the parameters and populates the <code>LinkedHasMap</code>, with the parameters defined in <code>sp_execute_external_script</code>. The extension adds them as key-value pairs, with the key being the parameter name, (without the <code>@</code>), and the value is the value of the parameter. In the <code>execute</code> method, you retrieve them from the <code>params</code> parameter and use them.</p>

<p>So far I have not mentioned anything about the return type of the <code>execute</code> method, other than it being an <code>AbstractSqlServerExtensionDataset</code>, (as is the first input parameter in <code>execute</code>). So, let us discuss <code>AbstractSqlServerExtensionDataset</code>.</p>

<h2 id="abstractsqlserverextensiondataset">AbstractSqlServerExtensionDataset</h2>

<p>As the name implies, the <code>AbstractSqlServerExtensionDataset</code> &ldquo;deals&rdquo; with datasets. In pre CTP 2.5 if you wanted to send in a dataset like: <code>SELECT col1, col2 FROM someTable</code>, you had to - in your class - define arrays as class members representing the columns in the dataset. For return datasets, you had to do the same. Both for input datasets as well as return datasets the class members had to have well-known names: <code>inputDataCol*N*</code>, and <code>outputDataCol*N*</code>, where <em>N</em> is the column number (1 based). For input datasets, the Java extension populated the <code>inputDataCol</code> class members, and in your code, you looped through them. When returning a dataset from your code, you populated the <code>outputDataCol</code> class members, and the Java extension converted it to a result set when returning.</p>

<p>Many developers found the above complex and convoluted, so the Java SDK introduces the <code>AbstractSqlServerExtensionDataset</code>. The class contains methods for handling input and output data, and you see the source code for it <a href="https://docs.microsoft.com/en-us/sql/advanced-analytics/java/java-sdk?view=sqlallproducts-allversions#abstractsqlserverextensiondataset">here</a>. As a developer, you - instead of defining all the various input and output column arrays - create an implementation of the <code>AbstractSqlServerExtensionDataset</code> and uses that in the code. Unless you have specific requirements, you do not even have to create the implementation of <code>AbstractSqlServerExtensionDataset</code>; an implementation already exists in the SDK, the <code>PrimitiveDataset</code>.</p>

<h2 id="primitivedataset">PrimitiveDataSet</h2>

<p>The <code>PrimitiveDataset</code> is a concrete implementation of the <code>AbstractSqlServerExtensionDataset</code>, and it is similar to how we handled datasets pre CTP 2.5 in that it stores simple types as primitives arrays. You find the source of the class <a href="https://docs.microsoft.com/en-us/sql/advanced-analytics/java/java-sdk?view=sqlallproducts-allversions#primitivedataset">here</a>, and below, we see how we use it.</p>

<h2 id="java-code">Java Code</h2>

<p>It is time for some code, but before we do that, ensure you have downloaded the SDK from <a href="http://aka.ms/mssql-java-lang-extension">here</a>. For the code I write here I use <em>VS Code</em> together with the <em>Maven</em> extension. I wrote a blog post about <em>VS Code</em>, Java and <em>Maven</em> <a href="/2019/01/17/sql-server-2019--java-with-visual-studio-code/">here</a> if you want to refresh your memory.</p>

<p>I start with creating a <em>Maven</em> project based on the <em>Maven</em> archetype <code>maven-archetype-quickstart</code>. This gives me a &ldquo;starter&rdquo; class <code>App</code> containing a <code>public static void main()</code> entry point. I add to the project a class <code>JavatTest1</code> in the source file <code>JavaTest1.java</code>, and this is the class that I want to inherit from <code>AbstractSqlServerExtensionExecutor</code>. So I write some code like so:</p>

<pre><code class="language-java">public class JavaTest1 extends AbstractSqlServerExtensionExecutor {

}
</code></pre>

<p><strong>Code Snippet 5:</strong> <em>Extending AbstractSqlServerExtensionExecutor</em></p>

<p>As we see in <em>Code Snippet 5</em> I extend the <code>AbstractSqlServerExtensionExecutor</code> class, but when I do it I immediately see red &ldquo;squiggles&rdquo; under <code>AbstractSqlServerExtensionExecutor</code>, and when I mouse over I get a dialog like so:</p>

<p><img src="/images/posts/sql_2k19_java_sdk_dep1.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Inheritance Error</em></p>

<h4 id="maven-dependencies">Maven Dependencies</h4>

<p>As we see in <em>Figure 2</em>, it looks like <em>Maven</em>/<em>VS Code</em> cannot resolve the name <code>AbstractSqlServerExtensionExecutor</code>. That is not that strange as we do not have any dependency on the <code>.jar</code> file. So how do we set a dependency on the downloaded SDK? Well, we add a dependency in the <code>pom.xml</code> file, and, (for <em>Maven</em>), it needs to be in the form of:</p>

<pre><code class="language-xml">&lt;dependencies&gt;
  &lt;dependency&gt;
    &lt;groupId&gt;some_groupId&lt;/groupId&gt;
    &lt;artifactId&gt;the_artifactId&lt;/artifactId&gt;
    &lt;version&gt;some_version&lt;/version&gt;
  &lt;/dependency&gt;
  ...
&lt;/dependencies&gt;
</code></pre>

<p><strong>Code Snippet 6:</strong> <em>Dependency</em></p>

<p>We see in <em>Code Snippet 6</em> how a dependency consists of a <code>groupId</code>, <code>artifactId</code>, and <code>version</code>. Usually, you follow the <em>Maven</em> <a href="https://maven.apache.org/guides/mini/guide-naming-conventions.html">naming standards</a>, but in our case, where we have downloaded the SDK <code>jar</code> directly, we do not have to do that. Regardless of that, the <code>artifactId</code> needs to match the filename, sans extension, and a version number is required.</p>

<p>The dependency points out where to find the dependent file in the local <em>Maven</em> repository, or to be downloaded to from a remote repository. On Windows, we find the local <em>Maven</em> repository at <code>%USERPROFILE%\.m2\repository</code>. Coming back to <em>Code Snippet 6</em>, the <code>groupId</code>\<code>artifactId</code>\<code>version</code> defines the folder hierarchy in the local <em>Maven</em> repository:</p>

<pre><code class="language-xml">&lt;dependencies&gt;
  &lt;dependency&gt;
    &lt;groupId&gt;nielsb&lt;/groupId&gt;
    &lt;artifactId&gt;mssql-java-lang-extension&lt;/artifactId&gt;
    &lt;version&gt;1.0&lt;/version&gt;
  &lt;/dependency&gt;
  ...
&lt;/dependencies&gt;
</code></pre>

<p><strong>Code Snippet 7:</strong> <em>Java SDK Dependency</em></p>

<p>The <code>dependency</code> in <em>Code Snippet 7</em> sets the expectation that the <code>.jar</code> file is located at: <code>%USERPROFILE%\.m2\repository\nielsb\mssql-java-lang-extension\1.0</code>. The <code>nielsb</code> directory is just a random directory, and it could be anything. The one thing to think about is that when you copy the actual file to the directory, the file-name needs to include the version. So as per <em>Code Snippet 7</em>, the file name is: <code>mssql-java-lang-extension-1.0.jar</code>:</p>

<p><img src="/images/posts/sql_2k19_java_sdk_dep_hierarch.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Folder Hierarchy Dependency</em></p>

<p>In <em>Figure 3</em> we see the &ldquo;layout&rdquo; of the local <em>Maven</em> repository after I have set it up for the SDK dependency. Outlined in blue we see the different folders below<code>..\m2\repository</code>, and the outline in red shows the renamed SDK file. Having done this <em>VS Code</em> now &ldquo;picks up&rdquo; the dependency and we can start using it in our code.</p>

<h4 id="use-the-sdk">Use the SDK</h4>

<p>Our project should now compile OK, so let us add some logic to the <code>JavaTest1</code> class. We start with writing similar code to what we saw in the [<strong>SQL Server 2019 Extensibility Framework &amp; Java - Hello World</strong>] post; the <code>adder</code> method where we took two variables and added them together.</p>

<p>However, now when we use the SDK, we do not have to declare the variables as global class members, they are instead  part of the <code>params</code> parameter in the <code>execute</code> method:</p>

<pre><code class="language-java">package sql;

import com.microsoft.sqlserver.javalangextension.PrimitiveDataset;
import java.util.LinkedHashMap;
import com.microsoft.sqlserver.javalangextension.\
            AbstractSqlServerExtensionExecutor;
public class JavaTest1 extends AbstractSqlServerExtensionExecutor {

  public PrimitiveDataset execute(PrimitiveDataset input, 
                                  LinkedHashMap&lt;String, 
                                  Object&gt; params) {

      int x = (int)params.get(&quot;x&quot;);
      int y = (int)params.get(&quot;y&quot;);

      System.out.printf(&quot;The result of adding %d and %d = %d&quot;, 
                         x, y, x + y);  
      return null;

  }
}
</code></pre>

<p><strong>Code Snippet 8:</strong> <em>JavaTest1 Class and Execute Method</em></p>

<p>In <em>Code Snippet 8</em> we see the complete <code>JavaTest1</code> source code. We see how we do some <code>import</code> of classes we use, and in the <code>execute</code> method, we <code>get</code> the two parameters we want from the <code>params</code> parameter. We return <code>null</code> since we do not have any resultset to pass back. Oh, the Java language extension does, still, not support output parameters.</p>

<p>In the <em>VS Code</em> project we have an <code>App.java</code> source file with a <code>main</code> method, by which we can test that our code works:</p>

<pre><code class="language-java">public static void main( String[] args )
{
    JavaTest1 j1 = new JavaTest1();
    LinkedHashMap&lt;String, Object&gt; lh = 
            new LinkedHashMap&lt;String, Object&gt;();
    lh.put(&quot;x&quot;, 21);
    lh.put(&quot;y&quot;, 21);

    j1.execute(null, lh);

}
</code></pre>

<p><strong>Code Snippet 9:</strong> <em>Main Method</em></p>

<p>In <em>Code Snippet 9</em> we see a big difference between pre CTP 2.5 and now, in that now the method (<code>execute</code>) is not required to be <code>static</code> any more. The Java language extension now &ldquo;news up&rdquo; an instance of the class that we call into.</p>

<p>Let us create a <code>.jar</code> file out of our project so we can deploy to SQL Server. Since I am using <em>Maven</em>, in the <em>VS Code</em>&rsquo;s&rsquo; <em>Maven</em> extension I click on <code>package</code>, (read more about it in the <a href="/2019/01/17/sql-server-2019--java-with-visual-studio-code/">Java with Visual Studio Code</a> post). What happens is that <em>Maven</em> recompiles, (if any changes have taken place), and then builds the <code>.jar</code> file, and places it in the <code>..\target</code> directory.</p>

<p>Theoretically when we have the <code>.jar</code> file we can deploy it to the database where we want to execute the code from, by using the <code>CREATE EXTERNAL LIBRARY</code> statement we discussed in the <strong>SQL Server 2019, Java &amp; External Libraries</strong> - <a href="/2019/03/10/sql-server-2019-java--external-libraries---i/">I</a>, and <a href="/2019/03/17/sql-server-2019-java--external-libraries---ii/">II</a> posts. The issue with that is if we try to do that in a database where we have not deployed any Java code to, exceptions happen when we execute against the code, as the SDK is not present in the database (the <code>.jar</code> does not contain the SDK). So we first need to deploy the SDK, and as we do it on the local machine, we can deploy it based on file location:</p>

<pre><code class="language-sql">CREATE EXTERNAL LIBRARY javaSDK 
FROM (CONTENT = 'W:\mssql-java-lang-extension.jar')
WITH (LANGUAGE = 'Java');
GO 
</code></pre>

<p><strong>Code Snippet 10:</strong> <em>Create SDK External Library</em></p>

<p>When you have run the code in <em>Code Snippet 10</em> you can check that everything worked by executing: <code>SELECT * FROM sys.external_libraries</code>, and you see an entry named <code>javaSDK</code>. Oh, the name we give the library is of no importance. Having done this, we deploy our <code>.jar</code> to the database, also using <code>CREATE EXTERNAL LIBRARY</code>:</p>

<pre><code class="language-sql">CREATE EXTERNAL LIBRARY mySqlJar 
FROM (CONTENT = 'W:\sql-1.0.jar')
WITH (LANGUAGE = 'Java');
GO 
</code></pre>

<p><strong>Code Snippet 11:</strong> <em>Create External Library from Java Project</em></p>

<p>After executing the code in <em>Code Snippet 11</em> we try and execute the Java code:</p>

<pre><code class="language-sql">DECLARE @p1 int = 21;
DECLARE @p2 int = 21;
EXEC sp_execute_external_script
  @language = N'Java'
, @script = N'sql.JavaTest1'
, @params = N'@x int, @y int'
, @x = @p1
, @y = @p2   
GO
</code></pre>

<p><strong>Code Snippet 12</strong> <em>Call from T-SQL to Java with Parameters</em></p>

<p>The code in <em>Code Snippet 12</em> is almost identical to what we see in <em>Code Snippet 4</em>, apart from that we no longer call into a method, (<code>adder</code>), but instead a class: <code>JavaTest1</code>. Unfortunately, when we run the code in <em>Code Snippet 12</em> we get an exception:</p>

<pre><code class="language-sql">Started executing query at Line 17
Msg 39004, Level 16, State 20, Line 0

A 'Java' script error occurred during execution of 
  'sp_execute_external_script' with HRESULT 0x80004004.

STDOUT message(s) from external script: 
2019-05-25 08:24:25.01  Error: 
         Unsupported executor version encountered

Total execution time: 00:00:01.230
</code></pre>

<p><strong>Code Snippet 13:</strong> <em>Exception</em></p>

<p>The exception is, as we see in <em>Code Snippet 13</em>: <code>Unsupported executor version encountered</code>, hmm what is that? Go back and look at <em>Code Snippet 2</em>, and the beginning of the <code>AbstractSqlServerExtensionExecutor</code> class:</p>

<pre><code class="language-java">public abstract class AbstractSqlServerExtensionExecutor {
  /* Supported versions of the Java extension */
  public final int SQLSERVER_JAVA_LANG_EXTENSION_V1 = 1;

  /* Members used by the extension to determine application specifics */
  protected int executorExtensionVersion;
  protected String executorInputDatasetClassName;
  protected String executorOutputDatasetClassName;

  ...

}
</code></pre>

<p><strong>Code Snippet 14:</strong> <em>AbstractSqlServerExtensionExecutor</em></p>

<p>Notice in <em>Code Snippet 14</em> the four members:</p>

<ul>
<li><code>public final int SQLSERVER_JAVA_LANG_EXTENSION_V1 = 1;</code></li>
<li><code>protected int executorExtensionVersion;</code></li>
<li><code>protected String executorInputDatasetClassName;</code></li>
<li><code>protected String executorOutputDatasetClassName;</code></li>
</ul>

<p>The four members above are there for the Java language extension to use. They indicate what version of the extension it is and what class to use for input and output dataset. These are required, and we set them like so:</p>

<pre><code class="language-java">...
public class JavaTest1 extends AbstractSqlServerExtensionExecutor {
    
    public JavaTest1() {
        executorExtensionVersion = SQLSERVER_JAVA_LANG_EXTENSION_V1;
        executorInputDatasetClassName = PrimitiveDataset.class.getName();
        executorOutputDatasetClassName = PrimitiveDataset.class.getName();
    }
  
  public PrimitiveDataset execute(PrimitiveDataset input, 
                                  LinkedHashMap&lt;String, 
                                  Object&gt; params) {...}
}
</code></pre>

<p><strong>Code Snippet 15:</strong> <em>Executor Version and Data Set Class Names</em></p>

<p>As we see in <em>Code Snippet 15</em> we set the members in the class <code>ctor</code>, and when we have done it we:</p>

<ul>
<li>Re-build the <code>.jar</code>.</li>
<li>Drop the external library.</li>
<li>Re-create the external library as in <em>Code Snippet 11</em>.</li>
</ul>

<p>When we now execute the code in <em>Code Snippet 12</em>:</p>

<p><img src="/images/posts/sql_2k19_java_sdk_success_1.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Success</em></p>

<p>So, after we have set the various member values, all works OK. It is worth noticing that even though we do not pass any datasets, we still need to set the values for <code>executorInputDatasetClassName</code> and <code>executorOutputDatasetClassName</code>. Having said that, let us look at how we use datasets.</p>

<p>To look at datasets we want to pass in data from the table <code>dbo.tb_Rand10</code>, in fact, we want to pass in the <code>RowID</code>, <code>x</code>, and <code>y</code> columns: <code>SELECT * FROM dbo.tb_Rand10</code>. In our Java code, we then add the value of the <code>x</code>, and <code>y</code> columns together and return a dataset containing the <code>RowID</code> and the result. So we create a new class, (and source file), <code>JavaTest2</code>. In the <code>execute</code> method, we do as follows:</p>

<pre><code class="language-java">public PrimitiveDataset execute(PrimitiveDataset input, LinkedHashMap&lt;String, Object&gt; params) {

    /*
      grab the RowID, x and y columns
      and convert them into arrays
    */
    int[] rowIds = input.getIntColumn(0);
    int[] xCol = input.getIntColumn(1);
    int[] yCol = input.getIntColumn(2);
    int rowCount = rowIds.length;

    //arrays for output data
    int[] outIds = new int[rowCount];
    int[] outRes = new int[rowCount];

    for(int i = 0; i &lt; rowCount; i++) {
        int x = xCol[i];
        int y = yCol[i];
        outIds[i] = rowIds[i];
        outRes[i] = x + y;
    }

    //Create the return dataset
    PrimitiveDataset outData = new PrimitiveDataset();
    //set up metadata
    outData.addColumnMetadata(0, &quot;RowID&quot;, java.sql.Types.INTEGER, 0, 0);
    outData.addColumnMetadata(1, &quot;Result&quot;, java.sql.Types.INTEGER, 0, 0);
    
    //add the arrays to the dataset
    outData.addIntColumn(0, outIds, null);
    outData.addIntColumn(1, outRes, null);
            
    return outData;

}

</code></pre>

<p><strong>Code Snippet 16:</strong> <em>Input and Output Datasets</em></p>

<p>In the <code>execute</code> method in <em>Code Snippet 16</em> we see how we expect the Java language extension to pass in an instance of a <code>PrimitiveDataset</code> as the <code>input</code> parameter. In our code, we then:</p>

<ul>
<li>Take the individual columns and convert them to arrays.</li>
<li>Create two output arrays, one for the <code>RowID</code>, and one for the result.</li>
</ul>

<p>When we have the output arrays, we loop the input arrays, and:</p>

<ul>
<li>Assign the <code>RowID</code> to the array for <code>RowID</code>.</li>
<li>Get the values for the <code>x</code> and <code>y</code> column arrays.</li>
<li>Add them together and assign the value to the output result array, (<code>outRes</code>).</li>
</ul>

<p>We then create an instance of the <code>PrimitiveDataset</code> class, and:</p>

<ul>
<li>Add column meta data for the columns we want to return.</li>
<li>Assign the output arrays to the output columns.</li>
<li>Finally we return the <code>PrimitiveDataset</code> instance.</li>
</ul>

<p>We can now compile the code and create a <code>.jar</code> file, and deploy to the database as we did after <em>Code Snippet 15</em>. The code to call into the class looks like so:</p>

<pre><code class="language-sql">EXEC sp_execute_external_script
  @language = N'Java'
, @script = N'sql.JavaTest2'
, @input_data_1 = N'SELECT * FROM dbo.tb_Rand10'
WITH RESULT SETS ((RowID int, Result int))
GO
</code></pre>

<p><strong>Code Snippet 17:</strong> <em>SQL Code to Pass in Data</em></p>

<p>In <em>Code Snippet 17</em> we pass in data via the <code>@input_data_1</code> parameter, and we use the <code>WITH RESULT SETS</code> to format the output. The result when we execute looks like so:</p>

<p><img src="/images/posts/sql_2k19_java_sdk_success_2.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Data Passing</em></p>

<p>We see in <em>Figure 5</em> that our code is working, and we get back the result of adding the <code>x</code>, and <code>y</code> columns. Happy Days!</p>

<h2 id="summary">Summary</h2>

<p>In this post, we set out to look at how the introduction of the Java Language Extension SDK changes the programming model when writing Java code that should be called from SQL Server. However, before we started to look into the programming model, we looked at how we can add dependencies to <em>VS Code</em> and the <em>Maven</em> extension. We saw that:</p>

<ul>
<li>We add a <code>&lt;dependency&gt;</code> to the <code>&lt;dependencies&gt;</code> section.</li>
<li>The <code>&lt;dependency&gt;</code> consists (at least) of <code>groupId</code>, <code>artifactId</code>, and <code>version</code>.</li>
<li>The <code>groupId</code>, <code>artifactId</code>, and <code>version</code> should match the directory structure of the local <em>Maven</em> repository.</li>
<li><code>artifactId</code> corresponds to the dependency file, sans extension.</li>
<li>The name of the dependency file we copy to the local repository must include the <code>&lt;version&gt;</code> number.</li>
</ul>

<p>So what about the SDK programming model? We saw that:</p>

<ul>
<li>Our classes which we want to call into need to inherit from <code>AbstractSqlServerExtensionExecutor</code>.</li>
<li>We have to implement an &ldquo;entry-point&rdquo; method: <code>execute</code>, which is what the Java language extension calls.</li>
<li>We no longer need to create class member variables for parameters, as the <code>execute</code> method accepts a <code>LinkedHashMap</code> corresponding to the parameters we want to pass in.</li>
<li>We no longer need to create class member variables for input dataset , as the <code>execute</code> method accepts a concrete implementation of <code>AbstractSqlServerExtensionDataset</code>.</li>
<li>The SDK contains a concrete implementation of <code>AbstractSqlServerExtensionDataset</code>: <code>PrimitiveDataset</code>.</li>
<li>For return datasets we use a concrete implementation of <code>AbstractSqlServerExtensionDataset</code>, for example <code>PrimitiveDataset</code>.</li>
<li>The class we call into needs to expose certain members indicating version of the language extension and class name of the <code>AbstractSqlServerExtensionDataset</code> implementation.</li>
</ul>

<p>This post was a high level overview of the new programming model using the SDK, and I have only &ldquo;scraped the surface&rdquo; on certain parts of it. Expect follow-up posts going deeper into the programming model, for example how to handle <code>null</code> values within the <code>AbstractSqlServerExtensionDataset</code>.</p>

<p><strong>STOP THE PRESSES</strong></p>

<p>While I wrote this blog post Microsoft released SQL Server CTP 3.0, which introduces further changes to the Extension Language programming model. Instead of delaying this post, I cover that in future posts.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 20, 2019]]></title>
    <link href="https://nielsberglund.com/2019/05/19/interesting-stuff---week-20-2019/" rel="alternate" type="text/html"/>
    <updated>2019-05-19T16:28:12+02:00</updated>
    <id>https://nielsberglund.com/2019/05/19/interesting-stuff---week-20-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://devblogs.microsoft.com/dotnet/default-implementations-in-interfaces/">Default implementations in interfaces</a>. A blog post introducing a new feature in C# 8.0: default method implementations in interfaces. This comes in real handy if you, for example, want to add new methods to an existing interface.</li>
<li><a href="https://devblogs.microsoft.com/dotnet/performance-improvements-in-net-core-3-0/">Performance Improvements in .NET Core 3.0</a>. A blog post which takes a tour through some of the many improvements, big and small, that have gone into the .NET Core 3.0 runtime and core libraries in order to make applications and services leaner and faster.</li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/h2o-driverless-ai">H2O&rsquo;s Driverless AI: An AI that Creates AI</a>.  An <a href="https://www.infoq.com/">InfoQ</a> presentation where the presenter shares an approach on automating ML using H2Oâ€™s Driverless AI. Driverless AI employs the techniques of expert data scientists in an easy-to-use application that helps scale data science efforts; empowers data scientists to work on projects faster using automation and state-of-the-art computing power from GPUs to accomplish tasks in minutes that used to take months. Very cool!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<p>I attended the <a href="https://kafka-summit.org/events/kafka-summit-london-2019/">Kafka Summit</a>, in London the week of May 13. The conference was very well organized, and I came away impressed by that. However, I am not that impressed by the speakers at the conference. Don&rsquo;t get me wrong, the Confluent speakers were all top-notch, engaging and presenting interesting concepts. What not impressed me was the 3rd party speakers. In my opinion, the topics were not centred around Kafka, and the speakers were in general, not that engaging.</p>

<p>Anyway, below is a couple of links related to announcements during the conference.</p>

<ul>
<li><a href="https://www.confluent.io/blog/announcing-confluent-community-catalyst-program">Announcing the Confluent Community Catalyst Program</a>. The conference started with <a href="https://twitter.com/tlberglund">Tim Berglund</a> announcing the Confluent Community Catalyst Program, an MVP like program for Kafka.</li>
<li><a href="https://www.confluent.io/blog/introducing-cloud-native-experience-for-apache-kafka-in-confluent-cloud">Introducing a Cloud-Native Experience for Apache Kafka in Confluent Cloud</a>. As part of the keynote, Neha Narkhede, (co-founder of Confluent), announced the availability of Apache Kafka as a service in the cloud. She demonstrated the ease of setting up Kafka in the cloud - 5 seconds to fully functional Kafka! Kafka as a service is initially available on AWS and Google Cloud, let us hope it comes to Azure soon!</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>The CTP 2.5 release of SQL Server 2019 changed a lot regarding the Java language extension and how to write Java code to be executed from <code>sp_execute_external_script</code>. I am at the moment writing a blog post, where I look at the changes to the programming model. I plan to try and publish it in about a week.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 19, 2019]]></title>
    <link href="https://nielsberglund.com/2019/05/11/interesting-stuff---week-19-2019/" rel="alternate" type="text/html"/>
    <updated>2019-05-11T06:39:50+02:00</updated>
    <id>https://nielsberglund.com/2019/05/11/interesting-stuff---week-19-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://devblogs.microsoft.com/dotnet/net-core-is-the-future-of-net/">.NET Core is the Future of .NET</a>. .NET is Dead! Long Live .NET! Somewhat melodramatic, but anyway. So, in this blog post Microsoft announces what almost everyone already knew - that .NET Framework 4.8 is the last major version of .NET Framework. Going forward Microsoft&rsquo;s efforts will be on .NET Core.<br /></li>
<li><a href="https://devblogs.microsoft.com/dotnet/introducing-net-5/">Introducing .NET 5</a>. Hot on the heels of the blog post above, announcing the death of .NET Framework, comes this post, laying out the future of .NET Core, .NET 5.</li>
<li><a href="https://devblogs.microsoft.com/dotnet/introducing-diagnostics-improvements-in-net-core-3-0/">Introducing diagnostics improvements in .NET Core 3.0</a>. Yet another .NET Core post. This post discusses a suite of tools that utilize new features in the .NET runtime that makes it easier to diagnose and solve performance problems.</li>
</ul>

<h2 id="data-science-machine-learning">Data Science / Machine Learning</h2>

<ul>
<li><a href="https://devblogs.microsoft.com/dotnet/announcing-ml-net-1-0/">Announcing ML.NET 1.0</a>. I have during the last months now and then posted in these &ldquo;roundups&rdquo; about new releases of ML.NET. Microsoft has now released version 1.0 with a lot of interesting new features, and that is what this post is about.</li>
<li><a href="https://www.infoq.com/presentations/h2o-model-spark">Productionizing H2O Models with Apache Spark</a>. This is an <a href="https://www.infoq.com/">InfoQ</a> presentation demonstrating the creation of pipelines integrating H2O machine learning models and their deployments using Scala or Python.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.infoq.com/news/2019/05/kafka-zeebe-streams-workflows">Event Streams and Workflow Engines â€“ Kafka and Zeebe</a>. An <a href="https://www.infoq.com/">InfoQ</a> discussing how Kafka fits in an Event-Driven Architecture, and how workflow engines can handle complex business processes. The article also mentioned how Zeebe, a new highly scalable workflow engine, can be used with Kafka.</li>
<li><a href="https://www.confluent.io/blog/apache-kafka-data-access-semantics-consumers-and-membership">Apache Kafka Data Access Semantics: Consumers and Membership</a>. This is an article discussing in detail how the Kafka consumer works. It also talks about consumer groups, how their state is saved, and consistency is ensured. It discusses how consumer groups are managed in a distributed way, and finally, the article looks at the rebalance protocol.</li>
<li><a href="https://www.confluent.io/blog/journey-to-event-driven-part-4-four-pillars-of-event-streaming-microservices">Journey to Event Driven â€“ Part 4: Four Pillars of Event Streaming Microservices</a>. This is the fourth &ldquo;episode&rdquo; in the &ldquo;Journey to Event Driven&rdquo;. This time the discussion is around the four individual parts that make up event streaming. I cannot wait to hear more about it next week at the Kafka Summit in London!</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 18, 2019]]></title>
    <link href="https://nielsberglund.com/2019/05/05/interesting-stuff---week-18-2019/" rel="alternate" type="text/html"/>
    <updated>2019-05-05T07:13:49+02:00</updated>
    <id>https://nielsberglund.com/2019/05/05/interesting-stuff---week-18-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li>[Designing Distributed Systems with TLA+][]. An <a href="https://www.infoq.com/">InfoQ</a> presentation where the presenter discusses the ideas behind TLA+, which is a specification language that describes a system, its properties, and how it works.</li>
</ul>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://devblogs.microsoft.com/dotnet/using-net-and-docker-together-dockercon-2019-update/">Using .NET and Docker Together â€“ DockerCon 2019 Update</a>. This post is about the improvements and new features in .NET Core 3.0 related to Docker and running your code in Docker.</li>
</ul>

<h2 id="databases">Databases</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/yugabytedb">YugaByte DB - A Planet-scale Database for Low Latency Transactional Apps</a>. This is an <a href="https://www.infoq.com/">InfoQ</a> presentation introducing and demoing YugaByte DB, a large scale DB, highlighting distributed transactions with global consistency.</li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://databricks.com/blog/2019/05/02/detecting-financial-fraud-at-scale-with-decision-trees-and-mlflow-on-databricks.html">Detecting Financial Fraud at Scale with Decision Trees and MLflow on Databricks</a>. An excellent post about how to use Databricks to detect fraud. Why I like this article is because of the sample code, it makes it easy to follow along.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/optimizing-kafka-streams-applications">Optimizing Kafka Streams Applications</a>. This article discusses how one can optimize Kafka Stream applications based on the new processor topology optimization framework which Kafka Streams 2.1 introduced.</li>
<li><a href="https://www.confluent.io/blog/pipelinedb-team-joins-confluent">The PipelineDB Team Joins Confluent</a>. I had no idea that <a href="https://www.pipelinedb.com/">PipelineDB</a> existed before this blog post. In my mind, PipelineDB joining Confluent can be huge, and I cannot wait to see what they dream up.</li>
<li><a href="https://www.buzzsprout.com/186154/1073627-load-balanced-apache-kafka-derivco-s-globally-distributed-gaming-business">Load-Balanced Apache Kafka: Derivco&rsquo;s Globally Distributed Gaming Business</a>. My colleague <a href="https://twitter.com/charllamprecht">Charl Lamprecht</a> and I had the pleasure of being interviewed by Tim Berglund (no relations) for a Kafka podcast. We, or rather Charl as I had audio issues, spoke about Kafka, load balancing via F5&rsquo;s and the journey we have had to get Kafka implemented in <a href="/derivco">Derivco</a>.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 17, 2019]]></title>
    <link href="https://nielsberglund.com/2019/04/28/interesting-stuff---week-17-2019/" rel="alternate" type="text/html"/>
    <updated>2019-04-28T08:12:38+02:00</updated>
    <id>https://nielsberglund.com/2019/04/28/interesting-stuff---week-17-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://devblogs.microsoft.com/dotnet/introducing-net-for-apache-spark/">Introducing .NET for ApacheÂ® Sparkâ„¢ Preview</a>. What the title says! Microsoft has released a preview of .NET that you can use together with Apache Spark. It is built on the Spark interop layer, designed to provide high-performance bindings to multiple languages. Being able to write C# code for Spark is so awesome; hopefully, we can soon use it in Notebooks as well.</li>
<li><a href="https://towardsdatascience.com/why-kubernetes-is-a-great-choice-for-data-scientists-e130603b9b2d">Why Kubernetes is a Great Choice for Data Scientists</a>. This is an interesting post discussing how Kubernetes can be used in a data science world.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://debezium.io/blog/2019/02/19/reliable-microservices-data-exchange-with-the-outbox-pattern/">Reliable Microservices Data Exchange With the Outbox Pattern</a>. At <a href="/derivco">work</a> we have started looking at <a href="https://twitter.com/debezium">Debezium</a> as a way to get data from the database into other systems, and while I was investigating this, I came across the linked blog-post. If you are interested in how to turn your databases into event stream sources, then this post is a must read!</li>
</ul>

<h2 id="sql-server-2019">SQL Server 2019</h2>

<ul>
<li><a href="https://docs.microsoft.com/en-us/sql/sql-server/sql-server-ver15-release-notes?view=sqlallproducts-allversions">SQL Server 2019 preview release notes</a>. Earlier this week, Microsoft released SQL Server 2019 preview CTP 2.5. Some very cool new features! Go and get it!</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
</feed>

