<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Niels Berglund</title>
  <link href="https://nielsberglund.com/atom.xml" rel="self"/>
  <link href="https://nielsberglund.com"/>
  <updated>2021-10-17T08:52:05+02:00</updated>
  <id>https://nielsberglund.com/</id>
  <generator uri="http://gohugo.io/">Hugo</generator>

  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 42, 2021]]></title>
    <link href="https://nielsberglund.com/2021/10/17/interesting-stuff---week-42-2021/" rel="alternate" type="text/html"/>
    <updated>2021-10-17T08:52:05+02:00</updated>
    <id>https://nielsberglund.com/2021/10/17/interesting-stuff---week-42-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="azure-data-explorer">Azure Data Explorer</h2>

<ul>
<li><a href="https://youtu.be/52dPYC1V5a0">Azure Data Explorer Shorts: Managed Ingestion</a>. An excellent short (~9 minutes) video explaining the ins and outs of data ingestion into <strong>Azure Data Explorer</strong>.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/how-baader-built-a-predictive-analytics-machine-learning-system-with-kafka-and-rstudio/">Apache Kafka and R: Real-Time Prediction and Model (Re)training</a>. This blog post looks at how KStreams, ksqlDB, and R can be used to create a data pipeline in which a machine learning model is applied to streaming data. The post also looks at how the model can be automatically retrained once the prediction results exceed a certain threshold. Very Cool!</li>
<li><a href="https://databricks.com/blog/2021/10/12/native-support-of-session-window-in-spark-structured-streaming.html">Native Support of Session Window in Spark Structured Streaming</a>. The post linked to, looks at a new window type in the upcoming Apache Spark 3.2 version. Before Spark 3.2, Spark supported tumbling and sliding windows. In the 3.2 version, the session window is introduced. The interesting thing with a session window is that it has a dynamic size of window length depending on the input.</li>
<li><a href="https://www.confluent.io/blog/new-confluent-cloud-connector-features-and-single-message-transforms/">Introducing Single Message Transforms and New Connector Features on Confluent Cloud</a>. Part of Confluent cloud is managed Kafka Connect connectors, and this post announces new features for most of the managed connectors. I am quite &ldquo;chuffed&rdquo; about seeing Single Message Transforms as one such new feature.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 41, 2021]]></title>
    <link href="https://nielsberglund.com/2021/10/10/interesting-stuff---week-41-2021/" rel="alternate" type="text/html"/>
    <updated>2021-10-10T10:15:26+02:00</updated>
    <id>https://nielsberglund.com/2021/10/10/interesting-stuff---week-41-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="misc">Misc.</h2>

<ul>
<li><a href="https://devblogs.microsoft.com/python/python-in-visual-studio-code-october-2021-release/">Python in Visual Studio Code – October 2021 Release</a>. In early October, Python 3.10 stable was released. Hot on the heels of that release comes what is mentioned in this blog post: a new release of the <strong>VS Code</strong> Python extension. The post looks at some of the significant new features and fixes. One of the new features is an improved &ldquo;getting started experience&rdquo; for Python in VS Code. Since I have had issues in the past getting Python up and running, I think I will uninstall my existing Python extension and try this improved extension from &ldquo;scratch&rdquo;.</li>
</ul>

<h2 id="sql-server-big-data-cluster">SQL Server Big Data Cluster</h2>

<ul>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2021/10/06/whats-new-with-sql-server-big-data-clusters-cu13-release/">What&rsquo;s new with SQL Server Big Data Clusters—CU13 Release</a>. I guess the title of the blog post says it all. The post looks at new &ldquo;stuff&rdquo; in the CU13 release of <strong>SQL Server 2019 Big Data Cluster</strong>. The big one for me in this release is the switch from Apache Spark 2.4 to Apache Spark 3.1.2.</li>
</ul>

<h2 id="azure-data-explorer">Azure Data Explorer</h2>

<ul>
<li><a href="https://techcommunity.microsoft.com/t5/azure-data-explorer/aligning-timeseries-application-requirements-into-azure-data/ba-p/2758959">Aligning Timeseries Application Requirements into Azure Data Explorer (ADX)</a>. Time series analysis has become critical in almost any analytical application. This blog post looks at the support for time series analysis in Azure, more specifically in <strong>Azure Data Explorer</strong>. After reading the post, I think it is safe to say that the support in ADX is &ldquo;pretty darn good&rdquo;.</li>
<li><a href="https://davemccollough.com/2021/02/01/kusto-query-language-101/">Kusto Query Language 101</a>. Above, we spoke about time series analysis in <strong>Azure Data Explorer</strong>. It is all good and well that we have that functionality, but how do we query ADX? Well, if you read the post linked to, you will get a &ldquo;crash course&rdquo; in the query language for ADX: <strong>Kusto Query Language</strong> (KQL).</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/resources/demo/stream-governance-discover-understand-and-trust-your-data-in-motion/">Stream Governance: Discover, understand, and trust your data in motion</a>. A month or so ago, Confluent <a href="https://www.confluent.io/blog/governing-data-with-confluent-stream-governance/">announced the release</a> of their platform for stream governance: the <a href="https://www.confluent.io/product/stream-governance/"><strong>Stream Governance</strong> suite</a>. Since the release, Confluent has been busy creating learning resources etc., and the post linked to is the registration page for a Stream Governance webinar. If you are working with streaming data, I do suggest you sign up!</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 40, 2021]]></title>
    <link href="https://nielsberglund.com/2021/10/03/interesting-stuff---week-40-2021/" rel="alternate" type="text/html"/>
    <updated>2021-10-03T09:34:52+02:00</updated>
    <id>https://nielsberglund.com/2021/10/03/interesting-stuff---week-40-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://azure.microsoft.com/en-us/blog/govern-your-data-wherever-it-resides-with-azure-purview/">Govern your data wherever it resides with Azure Purview</a>. This post looks at <strong>Azure Purview</strong>. Azure Purview is a unified data governance solution that helps you achieve a complete understanding of your data. This is regardless of whether it&rsquo;s housed on-premises in services like SQL Server and Oracle, in different clouds like Amazon Web Services (AWS) S3, or SaaS applications like Salesforce. This is something we dearly need at <a href="/derivco">Derivco</a>!</li>
</ul>

<h2 id="azure-data-explorer">Azure Data Explorer</h2>

<ul>
<li><a href="https://techcommunity.microsoft.com/t5/azure-data-explorer/monitor-batching-ingestion-with-adx-insights/ba-p/2673509">Monitor batching ingestion with ADX Insights</a>. <strong>ADX Insights</strong> is a system providing comprehensive monitoring of your ADX clusters. This post talks about the new Ingestion monitoring feature that allows you to monitor the status of batching ingestion operations to ADX. Very cool!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/serverless-kafka-streaming-with-confluent-cloud-and-aws-lambda/">Trigger AWS Lambda Functions Directly from an Apache Kafka Topic</a>. This post looks at how you can stream data from Confluent Cloud Kafka topics into Amazon DynamoDB tables by triggering an AWS Lambda function - providing a completely serverless architecture. I need to test this out on Azure using <strong>Azure Functions</strong>!</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 39, 2021]]></title>
    <link href="https://nielsberglund.com/2021/09/26/interesting-stuff---week-39-2021/" rel="alternate" type="text/html"/>
    <updated>2021-09-26T10:49:18+02:00</updated>
    <id>https://nielsberglund.com/2021/09/26/interesting-stuff---week-39-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://medium.com/event-driven-utopia/understanding-materialized-views-3-stream-table-joins-with-cdc-77591d2d6fa0">Understanding Materialized Views — 3 : Stream-Table Joins with CDC</a>. In a roundup a couple of weeks ago, I linked to a post about materialized views, and I wrote how I couldn&rsquo;t wait for a follow-up post. Well, here it is. In this post, the author looks at joining streams with lookup tables to create materialized views. Very cool!</li>
<li><a href="https://www.confluent.io/blog/apache-kafka-3-0-major-improvements-and-new-features/">What&rsquo;s New in Apache Kafka 3.0.0</a>. I guess the title says it all. Apache Kafka version 3.0 has just been released, and this blog post looks at some of the new features, fixes, and improvements.</li>
<li><a href="https://k6.io/blog/load-test-your-kafka-producers-and-consumers-using-k6/">How to Load Test Your Kafka Producers and Consumers using k6</a>. A couple of weeks ago, I came across <a href="https://k6.io/">k6</a>, a modern load testing framework for both developers as testers. I thought it would be cool if I somehow could load-test Kafka producers and consumers in the framework. Well, I can now do it, and the post I have linked to discusses the newly developed Kafka k6 extension: xk6-kafka. I cannot wait to put it through its paces.</li>
<li><a href="https://www.confluent.io/blog/announcing-ksqldb-0-21-new-features-major-upgrades/">Announcing ksqlDB 0.21.0</a>. Above I linked to the announcement of Kafka 3.0. This post discusses the new ksqlDB 0.21.0 release and looks at some of the new features.</li>
<li><a href="https://www.kai-waehner.de/blog/2021/09/23/real-time-kappa-architecture-mainstream-replacing-batch-lambda/">Kappa Architecture is Mainstream Replacing Lambda</a>. In this post, the author looks at the benefits the Kappa architecture provides over the Lambda architecture. One of the major, major benefits is a much simpler infrastructure.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>Right now, I am &ldquo;prepping&rdquo; for two conference talks this coming week:</p>

<ul>
<li><a href="https://datadrivencommunity.com/speaker-NielsBerglund-session.html"><strong>Analyze Billions of Rows of Data in Real-Time Using Azure Data Explorer</strong></a>: On Wednesday (Sept 29), I deliver this presentation which is an overview of <strong>Azure Data Explorer</strong>, and how it is ideal for near-real-time analytics of huge volumes of data.</li>
<li><a href="https://azurebootcamp.co.za/"><strong>Improve Customer Lifetime Value using Azure Databricks &amp; Delta Lake</strong></a>. Then on Thursday (Sept 30), I present how you can calculate and improve Customer Lifetime Value (CLV) using Azure Databricks.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 38, 2021]]></title>
    <link href="https://nielsberglund.com/2021/09/19/interesting-stuff---week-38-2021/" rel="alternate" type="text/html"/>
    <updated>2021-09-19T10:37:19+02:00</updated>
    <id>https://nielsberglund.com/2021/09/19/interesting-stuff---week-38-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://www.startree.ai/blogs/launching-at-linkedin-the-story-of-apache-pinot/">Launching at LinkedIn: The Story of Apache Pinot</a>. This is the story of how Apache Pinot started from a &ldquo;simple&rdquo; beginning at LinkedIn, how it grew over time to being adopted at Uber. Very interesting!</li>
<li><a href="https://databricks.com/blog/2021/09/17/timeliness-and-reliability-in-the-transmission-of-regulatory-reports.html">Timeliness and Reliability in the Transmission of Regulatory Reports</a>. Regulations impact more and more companies. Part of most regulations is the requirement to create reports for the regulators. God knows that we at <a href="/derivco">Derivco</a> feel the pain around this subject. The post linked to here demonstrates the benefits of the Databricks Lakehouse architecture in the ingestion, processing, validation and transmission of regulatory data.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/governing-data-with-confluent-stream-governance/">Confluent Unlocks the Full Power of Event Streams with Stream Governance</a>. Data governance has become a requirement for most organizations, and the organizations have adopted the governance tools needed to manage their data. However, most of the tools are built for data at rest. What about streaming data? In this blog post, Confluent announces the release of their Stream Governance Suite, a set of tools allowing you to govern your streaming data. This is something we have wished for here at <a href="/derivco">Derivco</a>!</li>
<li><a href="https://www.confluent.io/blog/kafka-summit-americas-2021-recap/">Kafka Summit Americas 2021 Recap</a>. This year&rsquo;s final Kafka Summit (Kafka Summit Americas) was a wrap earlier this week. The blog post lists some of the highlights of the summit. Have a look and see what catches your interest!</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 37, 2021]]></title>
    <link href="https://nielsberglund.com/2021/09/12/interesting-stuff---week-37-2021/" rel="alternate" type="text/html"/>
    <updated>2021-09-12T09:43:08+02:00</updated>
    <id>https://nielsberglund.com/2021/09/12/interesting-stuff---week-37-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="http://babkin-cep.blogspot.com/2021/09/the-practice-of-parallel-programming.htm">The Practice of Parallel Programming</a>. A link to a free downloadable pdf version of the book <strong>The Practice of Parallel Programming</strong>. The pdf provides an advanced guide to the issues of parallel and multithreaded programming. It goes beyond the high-level design of the applications into the details that are often overlooked but vital to make the programs work. It is an excellent read!</li>
</ul>

<h2 id="sql-server-2019-language-extensions">SQL Server 2019 Language Extensions</h2>

<ul>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2021/09/08/open-sourcing-the-net-5-c-language-extension-for-sql-server/">Open sourcing the .NET 5 C# Language Extension for SQL Server</a>. If you follow my blog, you may know that I have been writing quite a lot about the <strong>SQL Server Language Extensions</strong> and how extensions for Python, R, and Java exist and are open source. Well, the time has now come for C#. The post linked to announces the open-source release of SQL Server Language Extensions for C#! That.Is.So.Awesome! Expect some blog posts from yours truly looking at this.</li>
</ul>

<h2 id="azure-data-explorer">Azure Data Explorer</h2>

<ul>
<li><a href="https://youtu.be/lN3HkAZ3oGA">Azure Data Explorer ADX Overview</a>. In this YouTube video, the presenter dives into Azure Data Explorer – from data ingestion to dashboards – and looks at how Azure Data Explorer allows us to focus on discovering insights in the data while simplifying infrastructure and managing cost.</li>
<li><a href="/2021/09/06/run-self-managed-kusto-kafka-connector-serverless-in-azure-container-instances/">Run Self-Managed Kusto Kafka Connector Serverless in Azure Container Instances</a>. This blog post by me looks at how to run the Kafka Connector for Azure Data Explorer server-less in Azure. We look at creating a Docker image for the connector and deploying it to Azure Container Instances.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://medium.com/event-driven-utopia/understanding-materialized-views-part-2-ae957d40a403">Understanding Materialized Views — Part 2</a>. This post is the second in a series about <strong>Materialized Views</strong>. In this post, the author looks at stream processing and explores two essential concepts in stateful stream processing; streams and tables. Based on streams and tables, he looks at how streams turn into tables that make materialized views. He concludes the post by learning how these materialized views can be scaled and recovered from failures. The first part of the series is [here], and I - for one - cannot wait for the next instalment!</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Run Self-Managed Kusto Kafka Connector Serverless in Azure Container Instances]]></title>
    <link href="https://nielsberglund.com/2021/09/06/run-self-managed-kusto-kafka-connector-serverless-in-azure-container-instances/" rel="alternate" type="text/html"/>
    <updated>2021-09-06T06:11:51+02:00</updated>
    <id>https://nielsberglund.com/2021/09/06/run-self-managed-kusto-kafka-connector-serverless-in-azure-container-instances/</id>
    <content type="html"><![CDATA[<p>A couple of weeks ago, I <a href="/2021/08/14/run-confluent-cloud--serverless-apache-kafka-on-azure/">posted</a> how I set up Kafka to run serverless in Azure by deploying Confluent Cloud.</p>

<p>If you have followed my blog lately, you have probably seen that I am interested in <strong>Azure Data Explorer</strong> and that I have a couple of conference talks coming. One being:</p>

<ul>
<li><strong>How to do Real-Time Analytics Using Apache Kafka and Azure Data Explorer</strong>: We are looking at how to stream events from Apache Kafka to Azure Data Explorer (ADX) and perform user-facing analytics in near real-time.</li>
</ul>

<p>The question is how to connect Kafka with ADX? You normally connect Kafka with another system using a Kafka Connect connector, and fortunately a connector exists for connecting Kafka with ADX: the <a href="https://github.com/Azure/kafka-sink-azure-kusto"><strong>Kafka Connect Kusto Sink Connector</strong></a>.</p>

<p>However, since I am running managed Kafka (Confluent Cloud, remember), I need a managed connector to run it in Confluent Cloud&rsquo;s Kafka Connect. In the previous paragraph, I mentioned I was fortunate as we had a Kafka connector for ADX. Unfortunately, it is not a managed connector, so I cannot run it in Confluent Cloud - bummer!</p>

<p>So, this post looks at the various options we have if we want to use the Kafka Connect Kusto Sink Connector connecting Confluent Cloud in Azure with Azure Data Explorer. However, if you are not interested in neither Kafka nor ADX, the post may still be of use for you. The reason being it also covers running Docker images in Azure Container Instances (ACI).</p>

<p></p>

<blockquote>
<p><strong>NOTE:</strong> Even though this post comes out of me wanting to use the Kafka Connect Kusto Sink Connector, the post is <strong>NOT</strong> about the usage or the configuration of the connector. That is covered in a future post.</p>
</blockquote>

<h2 id="credits">Credits</h2>

<p>Usually, the credits &ldquo;roll&rdquo; at the end of the <del>movie</del> post, but I feel I should start with the credits as this post would not have happened if it wasn&rsquo;t for this blog post:</p>

<ul>
<li><a href="https://rmoff.net/2021/01/11/running-a-self-managed-kafka-connect-worker-for-confluent-cloud/">Running a self-managed Kafka Connect worker for Confluent Cloud</a>. I came across this post by Kafka guru extraordinaire <a href="https://twitter.com/rmoff">Robin Moffat</a> when I looked into running a self-managed connector when you use Confluent Cloud. His post made me look into what it would take to run the connector in Azure Container Instances (ACI).</li>
</ul>

<p>As I said, <a href="https://twitter.com/rmoff">Robin Moffat</a> is a Kafka Guru, and if you are into Kafka, then you <strong>MUST</strong> read his <a href="https://rmoff.net/">blog</a>.</p>

<h2 id="pre-reqs">Pre-Reqs</h2>

<p>This section is here if you want to follow along. I am listing what you need if you&rsquo;re going to deploy and run a container in ACI; not all required components for Kafka and ADX:</p>

<ul>
<li>An Azure account. If you don&rsquo;t have an Azure subscription, sign up for a <a href="https://azure.microsoft.com/en-us/free/">free account</a>.</li>
<li>Azure CLI. The Azure CLI is a cross-platform command-line tool to connect to Azure and execute administrative commands on Azure resources. You find downloads and install instructions <a href="https://docs.microsoft.com/en-us/cli/azure/install-azure-cli">here</a>.</li>
<li>Docker Desktop: We will build a Docker image, so we need Docker Desktop.</li>
<li>Something to build the image from. The image I build for this post includes the Kusto Sink Connector, which I download from <a href="https://github.com/Azure/kafka-sink-azure-kusto/releases/download/v2.0.0/kafka-sink-azure-kusto-2.0.0-jar-with-dependencies.jar">here</a>.</li>
</ul>

<p>The version (2.0) of the Kusto Sink Connector I downloaded is not the latest you find on the <a href="https://github.com/Azure/kafka-sink-azure-kusto/releases">release page</a>, but I could not get the 2.1 version to work.</p>

<p>In addition to the above, I have Confluent Cloud deployed as per my <a href="/2021/08/14/run-confluent-cloud--serverless-apache-kafka-on-azure/">post</a> mentioned in the beginning. As I want to connect Kafka to Azure Data Explorer, I also have Azure Data Explorer installed.</p>

<h2 id="background">Background</h2>

<p>Before we get down into it, let us get an understanding of some of the components in this post:</p>

<ul>
<li>Azure Data Explorer</li>
<li>Confluent Cloud</li>
<li>Kafka Connect</li>
<li>Kafka Connect Kusto Sink Connector</li>
</ul>

<h4 id="azure-data-explorer">Azure Data Explorer</h4>

<p>Azure Data Explorer is a fully-managed big data analytics cloud platform and data-exploration service that ingests structured, semi-structured (like JSON) and unstructured data. The service then stores this data and answers analytic ad hoc queries on it with seconds of latency. It is a full-text indexing and retrieval database, including time series analysis capabilities, machine learning, regular expression evaluation, and text parsing.</p>

<p>It is ideal for analyzing large volumes of diverse data from any data source, such as websites, applications, IoT devices, and more. Essentially it is a distributed database running on a cluster of compute nodes in Azure.</p>

<h4 id="confluent-cloud">Confluent Cloud</h4>

<p>In my <a href="/2021/08/14/run-confluent-cloud--serverless-apache-kafka-on-azure/">post</a> setting up Confluent Cloud I said it is a resilient, scalable streaming data service based on Apache Kafka, delivered as a fully managed service. It is Confluent Platform, running as a managed service in the cloud, and you can run it on Azure, AWS, and Google Cloud.</p>

<p>As it is Confluent Platform, you get so much more than <em>just</em> Kafka. You get built-in stream processing through ksqlDB, schema registry for data integrity, managed Kafka Connect connectors for data sources/sinks, and more.</p>

<h4 id="kafka-connect">Kafka Connect</h4>

<p>Kafka Connect is a tool for streaming data between Apache Kafka and other systems scalable and reliable. The way you move data between systems and Kafka is using connectors, and there are two flavors of connectors:</p>

<ul>
<li>Source connectors that understand how to interact with the source system send records into Kafka</li>
<li>Sink connectors that propagate records from Kafka topics to other systems.</li>
</ul>

<p>Kafka Connect is a JVM process, and it operates separately from the Kafka Broker. The process loads connectors, and the connectors know how to interact with Kafka and the source/sink systems. The connectors are written in Java and packaged into <code>.jar</code> files.</p>

<h4 id="kafka-connect-kusto-sink-connector">Kafka Connect Kusto Sink Connector</h4>

<p>The Kusto Sink Connector is a Kafka Connect connector. It is - as the name implies - a sink connector, dequeuing events from Kafka topics and ingesting them into Azure Data Explorer. The ingestion is, for now, queued ingestion leveraging the Azure Data Explorer Java SDK, i.e. batch mode.</p>

<p>Since the Kusto connector is not a managed connector, we need to decide where and how to run it.</p>

<h2 id="options">Options</h2>

<p>Robin covered the various options in his <a href="https://rmoff.net/2021/01/11/running-a-self-managed-kafka-connect-worker-for-confluent-cloud/">post</a> a lot better than I ever could, so I won&rsquo;t repeat that. Furthermore, seeing that I say in the title of this post <strong>Azure Container Instances</strong> (ACI), it is probably safe to assume that&rsquo;s the option we&rsquo;ll go with. As a picture says more than a thousand words, our solution looks something like so:</p>

<p><img src="/images/posts/kusto-aci-conn.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Kafka - ADX Architecture</em></p>

<p>We see in <em>Figure 1</em> how we have:</p>

<ul>
<li>Kafka in Azure (Confluent Cloud).</li>
<li>an Azure Container Instances running the Kusto Sink Connector.</li>
<li>data ingested from the connector into Azure Data Explorer.</li>
</ul>

<p>Oh, and yes - the data being ingested is published to Kafka from the publisher we see in the upper left-hand corner.</p>

<p>In this post, the term Azure Container Instances has been mentioned a couple of times. What is that, then?</p>

<h4 id="azure-container-instances">Azure Container Instances</h4>

<p>ACI gives you an easy way to run containers in the Azure cloud, eliminating the need to manage virtual machines (VMs) or using more complex container orchestration services, like Kubernetes. For me, just testing this out, ACI is &ldquo;good enough&rdquo;.</p>

<p>There are a couple of ways you can deploy and run a container in ACI:</p>

<ul>
<li>Utilise the integration between Docker and Azure and execute <code>docker run</code>. You do this mostly in test scenarios.</li>
<li>Deploy the container image to ACI and run it in ACI.</li>
</ul>

<p>Even though I said I am using ACI for testing at the beginning of this section, I will not use <code>docker run</code> but instead do a &ldquo;proper&rdquo; deployment to ACI.</p>

<h2 id="create-kusto-sink-connector-image">Create Kusto Sink Connector Image</h2>

<p>We start with creating the <code>Dockerfile</code> for the image we want to deploy to ACI:</p>

<p><img src="/images/posts/kusto-aci-dockerfile.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Dockerfile</em></p>

<p>As we need to run the connector in Kafka Connect, we base the image on the <code>cp-server-connect-base</code> image (the <code>FROM</code> statement). This image contains the bare minimum for Kafka Connect.</p>

<p>In the Pre-Reqs section, I mentioned how I downloaded the Kusto connector. I downloaded it as a zip file and unzipped it to the same directory the <code>Dockerfile</code> is in. In line 3, we see how I copy the connector&rsquo;s <code>.jar</code> file to <code>/usr/share/java</code> in the base image. That path is a &ldquo;well known&rdquo; path to load connectors from.</p>

<p>The <code>...OVERRIDE_POLICY=All</code> statement on line 5 allows this connector to override consumer and producer properties to not impact all connectors running in that worker.</p>

<p>Lines 7 - 10 in the docker file are core config security settings that need to be set at the Kafka connect worker level and need to be &ldquo;baked&rdquo; into the Docker image. These settings are related to authentication and authorization against the Confluent Cloud Kafka.</p>

<h4 id="build-the-image">Build the Image</h4>

<p>Having created the <code>Dockerfile</code>, we can build the image:</p>

<p><img src="/images/posts/kusto-aci-docker-build.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Docker Build</em></p>

<p>In <em>Figure 3</em>, we see us executing the <code>docker build</code> command and the outcome. When the build has finished, we can check that all looks OK by running <code>docker images</code>:</p>

<p><img src="/images/posts/kusto-aci-kusto-image.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Kusto Docker Image</em></p>

<p>Success, we have an image, as we see in <em>Figure 4</em>. Well, at least partial success; we don&rsquo;t know if it works yet. Let us find out.</p>

<h4 id="run-locally">Run Locally</h4>

<p>To find out if it works, we can run the container image locally using <code>docker-compose</code>. We create a <code>docker-compose.yml</code> file containing the bare minimum for running the connector:</p>

<p><img src="/images/posts/kusto-aci-docker-compose.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Docker Compose File</em></p>

<p>We see in <em>Figure 5</em> the compose file I use to test that my container works. As mentioned before, the file contains the required properties to get this connector up and running. What you see outlined in red are properties naming topics needed to store Kafka offsets, configs and statuses. After we have &ldquo;spun up&rdquo; the container, we can check for these topics in our Kafka installation. Let us execute `docker-compose up -d&rsquo;:</p>

<p><img src="/images/posts/kusto-aci-docker-compose-up.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>Run Docker Compose</em></p>

<p>In <em>Figure 6</em>, we see that it looks like everything has worked OK and that we have created a connector instance <code>kusto-conn-1</code>. We can confirm that it has worked by checking the topics mentioned above or execute a REST call against the Kafka Connect API to <code>GET</code> the available connectors:</p>

<p><img src="/images/posts/kusto-aci-get-connectors.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>GET Connectors</em></p>

<p>From what we see in <em>Figure 7</em> it looks like we are OK. We see, outlined in blue, the <code>GET</code> call on port <code>8083</code>, and in the result below, we see the connector outlined in red.</p>

<h2 id="azure-container-instances-1">Azure Container Instances</h2>

<p>When we have confirmed that our image works, it is time to deploy it to ACI. When we run a container in ACI, the container is stored in <a href="https://azure.microsoft.com/en-us/services/container-registry/#overview">Azure Container Registry</a> (ACR).</p>

<p>We&rsquo;ll create a new container registry in a second, but before we do that, let us log in to Azure and set the subscription to use. To log in, we run <code>az login</code>. The command may take a second or two, and a dialog in your browser may ask you for login credentials. When login is done, you will see a JSON blob with information about the subscriptions you have access to. Choose the one you want to use:</p>

<pre><code class="language-bash"># az login
# az login above returns a JSON blob with subscriptions.
# set the subscription you want to use
az account set --subscription 00000000-0000-0000-0000-000000000000 
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Set Subscription</em></p>

<p>After we have logged in, we execute the code in <em>Code Snippet 1</em> to set the subscription we want to use. And no, the subscription Id in <em>Code Snippet 1</em> is not mine.</p>

<p>We are almost at the stage where we can create the ACR, but we need one more thing before creating the ACR. That one more thing is a resource group. I will use an existing resource group for this post, so I will not create a new one. If you need to create a resource group, you do:</p>

<pre><code class="language-bash">az group create --name name-of-rg --location azure-location  
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Create Resource Group</em></p>

<p>To see what locations are available for the <code>--location</code> parameter in <em>Code Snippet 2</em>, you can execute: <code>az account list-locations</code>.</p>

<p>Right, we now have a resource group. Let us press on.</p>

<h4 id="create-azure-container-registry">Create Azure Container Registry</h4>

<p>ACR is a private Docker registry service, similar to Dockerhub. As with Dockerhub, you push container images to your container registry.</p>

<p>To create a container registry, we use <code>az acr create ...</code>:</p>

<pre><code class="language-bash">az acr create --resource-group rg-kafka \
              --name nielsblog1 \
              --sku Basic \
              --admin-enabled true \
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Create Container Registry</em></p>

<p>In <em>Code Snippet 3</em>, we see how we create a container registry. The first two parameters define the resource group to create it in: <code>rg-kafka</code>, and the name of the registry: <code>nielsblog1</code>. You may ask what the last two parameters are:</p>

<ul>
<li><code>--sku</code>: this parameter defines the service tier: <code>Basic</code>, <code>Standard</code>, or <code>Premium</code>. Read more about service tiers <a href="https://docs.microsoft.com/en-us/azure/container-registry/container-registry-skus">here</a>.</li>
<li><code>--admin-enabled</code>: an admin user account is included when creating a container registry. The account is disabled by default. For testing purposes, you may want to have it enabled, so that is why I have included it in the creation. Read more about the admin account <a href="https://docs.microsoft.com/en-us/azure/container-registry/container-registry-authentication?tabs=azure-cli#admin-account">here</a>.<br /></li>
</ul>

<p>When you execute the code in <em>Code Snippet 3</em>, it will take a little while. When it finishes, you will see a JSON blob with some information about the created registry.</p>

<p>Having created the registry, we can now log in to it: <code>az acr login --name regname</code>. We are almost ready to push our image to the registry, but there are two things we need to do before that.</p>

<p><strong>Login Server</strong></p>

<p>When we push an image to the registry, we need an address to push to; the login server. To retrieve the login server, we do:</p>

<pre><code class="language-bash">az acr show --name registryname --query loginServer
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Get Login Server</em></p>

<p>Most likely, when you execute the code in <em>Code Snippet 4</em>, you get back <code>your-registry-name.azurecr.io</code>, but it is good practice to explicitly retrieve the login server.</p>

<p><strong>Credentials</strong></p>

<p>The second thing we need to do is get the credentials for the admin user we enabled in <em>Code Snippet 3</em>. We use the credentials later when we deploy our container:</p>

<p><img src="/images/posts/kusto-aci-acr-credentials.png" alt="" /></p>

<p><strong>Figure 8:</strong> <em>ACR Credentials</em></p>

<p>In <em>Figure 8</em>, we see outlined in yellow:ish the code to retrieve the credentials. Outlined in red, we see the two passwords. These passwords are created when the admin account is enabled, and they can also be re-generated. Finally, outlined in blue is the user name to use for the admin account.</p>

<h4 id="push-image-to-acr">Push Image to ACR</h4>

<p>We are getting there. Now, the time has come to push the image we built in <em>Figure 3</em> to the ACR. We do it in a two-step process:</p>

<p><strong>Tag the Image</strong></p>

<p>Tag the image with the login server string:</p>

<p><img src="/images/posts/kusto-aci-docker-tag.png" alt="" /></p>

<p><strong>Figure 9:</strong> <em>Tag the Image</em></p>

<p>We see in <em>Figure 9</em> outlined in:</p>

<ul>
<li>Blue: the <code>docker tag</code> statement we use.</li>
<li>Yellow: the name of the image we want to tag.</li>
<li>Red: the &ldquo;tagged&rdquo; new name of the image.</li>
</ul>

<p><strong>Push the Image</strong></p>

<p>Having tagged the image with the login server, we can push it to ACR:</p>

<p><img src="/images/posts/kusto-aci-docker-push-acr.png" alt="" /></p>

<p><strong>Figure 10:</strong> <em>Docker Push to ACR</em></p>

<p>When you run the code outlined in blue in <em>Figure 10</em>, you see how the image layers are pushed to the registry.</p>

<p>Looking at <em>Figure 10</em> everything looks OK, but - look at the statement outlined in red. What is this &ldquo;repository&rdquo; thing?</p>

<p>It turns out that when we push an image to the ACR, we push it not directly into the ACR. Instead, a repository is created, and we push it into that repository. A repository is a collection of container images or other artefacts in a registry with the same name but different tags.</p>

<p>That explains why we, when looking for images in the ACR we do something like so:</p>

<p><img src="/images/posts/kusto-aci-acr-repo-images.png" alt="" /></p>

<p><strong>Figure 11:</strong> <em>ACR Repository with Image(s)</em></p>

<p>In <em>Figure 11</em>, we see how we execute <code>az acr repository list ...</code> when looking for images (outlined in yellow) and how the result comes back as an array (outlined in red).</p>

<h2 id="deploy-the-container">Deploy the Container</h2>

<p>It is now time to deploy and run the image in Azure Container Instances. To create the container, we use the <code>az container create ...</code> command:</p>

<pre><code class="language-bash">az container create --resource-group rg-kafka `
&gt;&gt; --name nielsblog1 `
&gt;&gt; --image nielsblog1.azurecr.io/kusto-conn-1:latest `
&gt;&gt; --restart-policy OnFailure `
&gt;&gt; --ip-address Public `
&gt;&gt; --ports 8083 `
&gt;&gt; --registry-login-server nielsblog1.azurecr.io `
&gt;&gt; --registry-username nielsblog1 `
&gt;&gt; --registry-password some-super-secret-password
</code></pre>

<p><strong>Code Snippet 5:</strong> <em>Create Container - I</em></p>

<p>The code in <em>Code Snippet 5</em> is part of the code we need to run to create the container. The parameters we see are straightforward. The only thing worth mentioning is the <code>--ip-address Public</code> and <code>--ports 8083</code>. We need to indicate that we need a public IP address and that port 8083 should be open.</p>

<p>In the previous paragraph, I mentioned that the code in <em>Code Snippet 5</em> is only part of what we need to run. So what else do we need? Remember what we did when we tested the container locally, how we had a <code>.yml</code> file (<em>Figure 5</em>), with properties required to run Kafka Connect? We need the same here!</p>

<p>The question is, how do we supply those properties? The answer is that <code>az container create</code> has an <code>--environment-variables</code> parameter. This parameter is a list of environment variables for the container, where the list contains space-separated values in &lsquo;key=value&rsquo; format, something like so:</p>

<p><img src="/images/posts/kusto-aci-create-container-I.png" alt="" /></p>

<p><strong>Figure 12:</strong> <em>Create Container - II</em></p>

<p>In <em>Figure 12</em>, we see the entire command, including the required properties for Kafka.</p>

<blockquote>
<p><strong>NOTE:</strong> Notice how the <code>--environment-variables</code> do not have an quotes around them. That is because I am running from PowerShell. If you run the command from command prompt you need the environment variables be enclosed in single quotes.</p>
</blockquote>

<p>The text outlined in red at the bottom of <em>Figure 12</em> shows that the screenshot is taken while the command executes. While the command is running, you can execute the following to see the state it is in:</p>

<pre><code class="language-bash"> az container show --resource-group rg-kafka `
 &gt;&gt;--name nielsblog1 --query instanceView.state
</code></pre>

<p><strong>Code Snippet 6:</strong> <em>View Container State</em></p>

<p>Eventually, the creation finishes and results in a JSON blog with some information about the container. Most of the information in the blob is available from <code>az container ...</code> commands:</p>

<pre><code class="language-bash"># get log information from the container
az container logs --resource-group rg-kafka --name nielsblog1

# get ip address information
az container show --resource-group  rg-kafka `
&gt;&gt;--name nielsblog1 --query ipAddress
</code></pre>

<p><strong>Code Snippet 7:</strong> <em>Get Logs &amp; IP Address</em></p>

<p>The code in <em>Code Snippet 7</em> retrieves log information from the running container and the public IP address. Looking at the logs, everything looks OK, so let us use the IP address we retrieved in <em>Code Snippet 7</em> and do what we did in <em>Figure 7</em> (but now against the container in Azure):</p>

<p><img src="/images/posts/kusto-aci-get-connectors-az.png" alt="" /></p>

<p><strong>Figure 13:</strong> <em>Get Connectors - II</em></p>

<p>Yay, from what we see in <em>Figure 13</em> all is OK. We see the Kusto Sink Connector, outlined in red, being part of the returned result from the <code>GET</code> call. And in the <code>GET</code> call outlined in blue, we see we use the Azure IP address (highlighted in yellow). Well, you don&rsquo;t necessarily know it is the Azure IP, but trust me - it is. Yay, again!</p>

<h2 id="summary">Summary</h2>

<p>Wow, that was a lot! In this post, we saw how to:</p>

<ul>
<li>Build a docker image for the Kusto Kafka Sink Connector.</li>
<li>Test it locally.</li>
<li>Create an Azure Container Registry.</li>
<li>Push the image to the registry.</li>
<li>Deploy the image to, and run it in Azure Container Instances.</li>
</ul>

<p>Look out for a post covering how to configure and use the Kusto Sink Connector.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 36, 2021]]></title>
    <link href="https://nielsberglund.com/2021/09/05/interesting-stuff---week-36-2021/" rel="alternate" type="text/html"/>
    <updated>2021-09-05T10:38:44+02:00</updated>
    <id>https://nielsberglund.com/2021/09/05/interesting-stuff---week-36-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="misc">Misc.</h2>

<ul>
<li><a href="https://www.hanselman.com/blog/my-ultimate-powershell-prompt-with-oh-my-posh-and-the-windows-terminal">My Ultimate PowerShell prompt with Oh My Posh and the Windows Terminal</a>. I certainly hope that <a href="https://www.hanselman.com/">Scott Hanselman</a> doesn&rsquo;t need an introduction, but if you haven&rsquo;t heard of him, <a href="https://www.hanselman.com/blog">here</a> is the link to his blog. Anyway, he has blogged a bit about setting up the Windows terminal, so it looks cool. The post I link to here is the latest and greatest in setting up your terminal.</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://ably.com/blog/8-fallacies-of-distributed-computing">Navigating the 8 fallacies of distributed computing</a>. This post reviews the eight fallacies of distributed computing and provides several hints at how to handle them. I think I&rsquo;ll, in my next job interview, ask some questions about the eight fallacies!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://eng.uber.com/kafka-async-queuing-with-consumer-proxy/">Enabling Seamless Kafka Async Queuing with Consumer Proxy</a>. This post from Uber discusses how Kafka being a stream-oriented system, where message order is assumed in the system&rsquo;s design, can hinder certain message delivery patterns. The post talks about how more than 300 microservices at Uber are leveraging Kafka for pub-sub message queueing between microservices and how Uber developed the Consumer Proxy to work around some of the drawbacks with Kafka&rsquo;s message order oriented design.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 35, 2021]]></title>
    <link href="https://nielsberglund.com/2021/08/29/interesting-stuff---week-35-2021/" rel="alternate" type="text/html"/>
    <updated>2021-08-29T13:37:54+02:00</updated>
    <id>https://nielsberglund.com/2021/08/29/interesting-stuff---week-35-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="big-data-machine-learning">Big Data / Machine Learning</h2>

<ul>
<li><a href="https://rudderstack.com/blog/churn-prediction-with-bigqueryml">Churn Prediction With BigQueryML to Increase Mobile Game Revenue</a>. Seeing what we do at <a href="/derivco">Derivco</a>, this post is exciting. The post looks at how machine learning can identify high-value mobile game players dangerously close to churning. Very interesting!</li>
</ul>

<h2 id="data-architecture">Data Architecture</h2>

<ul>
<li><a href="https://medium.com/@jordan_volz/five-predictions-for-the-future-of-the-modern-data-stack-435b4e911413">Five Predictions for the Future of the Modern Data Stack</a>. This post looks at the developments of the modern data stack and the bright side of &ldquo;Modern Data Stack V2&rdquo;, focusing on AI, Data Sharing, Data Governance, Streaming &amp; Application Serving.</li>
</ul>

<h2 id="azure-data-explorer">Azure Data Explorer</h2>

<ul>
<li><a href="https://mrfoxsql.wordpress.com/2021/08/25/timeseries-analytics-capabilities-and-azure-data-explorer-adx/">Timeseries Analytics Capabilities, and Azure Data Explorer (ADX)</a>. I guess that for you who read my blog, it doesn&rsquo;t come as a surprise that I have a thing for Azure Data Explorer. The post here looks at time-series analytics and explores the types of core functionality typical for time-series data processing applications. It further looks at how functionality built into ADX aligns exceptionally well to meet these challenges head-on.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://towardsdatascience.com/real-time-anomaly-detection-with-apache-kafka-and-python-3a40281c01c9">Real-time anomaly detection with Apache Kafka and Python</a>. In this post, the author looks at making real-time anomaly predictions over streaming data coming from Kafka using Python.</li>
<li><a href="https://www.confluent.io/blog/ksqldb-architecture-and-advanced-features/">How ksqlDB Works: Internal Architecture and Advanced Features</a>. To effectively use ksqlDB, you should, apart from being familiar with its features and syntax, also have an understanding of what&rsquo;s happening &ldquo;under the cover&rdquo; of ksqlDB. This post covers some of the &ldquo;under the cover&rdquo; topics as well as points to resources at <a href="https://developer.confluent.io/learn-kafka/inside-ksqldb/streaming-architecture/">Confluent Developer</a>.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>By now, you probably know that I:</p>

<p><img src="/images/posts/Neils_Berglund_Breakout_Session.jpg" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Breakout Session</em></p>

<p>Yes, as we see in <em>Figure 1</em> I am presenting at the <a href="https://dataplatformgeeks.com/dps2021/"><strong>2021 Data Platform Summit</strong></a>:</p>

<ul>
<li><a href="https://dataplatformgeeks.com/dps2021/sessions-agenda-schedule/"><strong>How to do Real-Time Analytics Using Apache Kafka and Azure Data Explorer</strong></a>. We are looking at how to stream events from Apache Kafka to Azure Data Explorer and perform user-facing analytics in near real-time.</li>
</ul>

<p>I mentioned in a previous roundup how the organizers have managed to increase the capacity of the virtual platform to 10,000! So, they have opened up <strong>FREE</strong> booking for <strong>LIVE</strong> attendance for a limited time. They have an internal quota, and once that is full, the free booking will close.  So, what are you waiting for? Hurry up to <a href="https://dataplatformgeeks.com/dps2021/complimentary-registration"><strong>register for FREE</strong></a>!</p>

<p>Oh, I am not only doing the conference session above, but also a post-conference training class; 4 hours per day over 2 days:</p>

<ul>
<li><a href="https://dataplatformgeeks.com/dps2021/product/big-data-analytics-with-sql-server-2019-big-data-cluster-by-niels-berglund/"><strong>Big Data &amp; Analytics with SQL Server 2019 Big Data Cluster</strong></a>.</li>
</ul>

<p>There are still a couple of seats (virtual) available for my class, so - if you are interested - register <a href="https://dataplatformgeeks.com/dps2021/product/big-data-analytics-with-sql-server-2019-big-data-cluster-by-niels-berglund/">here</a>.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 34, 2021]]></title>
    <link href="https://nielsberglund.com/2021/08/22/interesting-stuff---week-34-2021/" rel="alternate" type="text/html"/>
    <updated>2021-08-22T08:35:37+02:00</updated>
    <id>https://nielsberglund.com/2021/08/22/interesting-stuff---week-34-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="misc">Misc.</h2>

<ul>
<li><a href="https://headleysj.medium.com/having-testers-made-my-team-worse-99d4cb9866aa">Having testers made my team worse</a>. This post, from a colleague and mate of mine, has caused a bit of stir here at <a href="/derivco">Derivco</a>. In the post, Simon talks about how his team lost all of their functional testers and how due to this, the developers had to sort out their CI/CD pipelines and write meaningful tests. This lead them to be in a much better position at the end of the day than before. I mentioned how the post had caused a stir; the stir was from the testers in the company. After reading the post, I thought it would be the developers wanting to &ldquo;lynch&rdquo; Simon. Primarily due to this: <em>but because having testers meant that as developers, we could get away with being lazy and not truly putting in the effort to write meaningful tests that run both in our CI and CD pipelines</em>. Anyway, it is an excellent post - I suggest you read it and think about how you can approve your pipelines and testing while reading it.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/ksqldb-2-0-introduces-date-and-time-data-types/">Announcing ksqlDB 0.20.0</a>. As the title says; ksqlDB version 0.20 is out &ldquo;in the wild&rdquo;. One big new feature of this version is support for <code>DATE</code> and <code>TIME</code> datatypes! Very cool!!</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>In last week&rsquo;s roundup, I mentioned how one of the webinars I presented a week or two back had still not come up on YouTube. Well, one or two days after I published the post, there it was:</p>

<ul>
<li><a href="https://youtu.be/DdyZgFErLFI"><strong>Let SQL Server Be the Central Hub For All Types of Data</strong></a>. In this webinar, I look at <strong>SQL Server 2019 Big Data Cluster</strong> and how Microsoft positions it to be a central hub for all types of data - not only relational data.</li>
</ul>

<p>Oh, don&rsquo;t forget the register for <a href="https://azurebootcamp.co.za/"><strong>Azure Bootcamp 2021 South Africa</strong></a>. It will be a fantastic event, and I have just submitted some talks to it. Hopefully, one or two will be accepted.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 33, 2021]]></title>
    <link href="https://nielsberglund.com/2021/08/15/interesting-stuff---week-33-2021/" rel="alternate" type="text/html"/>
    <updated>2021-08-15T09:27:43+02:00</updated>
    <id>https://nielsberglund.com/2021/08/15/interesting-stuff---week-33-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://eng.uber.com/challenges-opportunities-to-reduce-cost-ubers-big-data/">Challenges and Opportunities to Dramatically Reduce the Cost of Uber&rsquo;s Big Data</a>. I think we all agree that Big Data is good. However, there is no doubt that Big Data incurs costs, especially in large organisations. This post from Uber looks at the top challenges they had when assessing their Big Data Platform&rsquo;s costs and the overall strategy they devised to address them. Very interesting!</li>
<li><a href="https://eng.uber.com/cost-efficient-big-data-platform/">Cost-Efficient Open Source Big Data Platform at Uber</a>. Another post by Uber. In the previous post, Uber discussed their initiative to reduce costs on their data platform. They looked at three broad pillars: platform efficiency, supply, and demand. In this post, they discuss the efforts to improve the efficiency of the data platform and bring down costs.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://engineering.linkedin.com/blog/2021/lambda-learner--nearline-learning-on-data-streams">Lambda Learner: Nearline learning on data streams</a>. In this post, LinkedIn discusses an in-house system called Lambda Learner. Lambda Learner is a library for iterative, incremental training of a class of supervised machine learning models. The discussion is about how the Lambda Learner system allows for near real-time re-training of machine learning models. This is a very interesting post!</li>
<li><a href="/2021/08/14/run-confluent-cloud--serverless-apache-kafka-on-azure/">Run Confluent Cloud &amp; Serverless Apache Kafka on Azure</a>. This is a post by yours truly. As you may know, I have some conferences coming up, and Azure features in quite a few of the talks, together with Apache Kafka. I thought it would be cool if I could run Apache Kafka on Azure and bonus points if I could run it as SaaS, i.e. Confluent Cloud. So in this post, I look at what it takes to deploy Confluent Cloud on Azure.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>Apart from publishing the blog post mentioned above, I am prepping for the upcoming <a href="https://dataplatformgeeks.com/dps2021/"><strong>2021 Data Platform Summit</strong></a>. Speaking about the <strong>Data Platform Summit</strong>, the organizers have managed to increase the capacity of the virtual platform to 10,000! So, they have opened up <strong>FREE</strong> booking for <strong>LIVE</strong> attendance for a limited time. They have an internal quota, and once that is full, the free booking will close. Hurry up to <a href="https://dataplatformgeeks.com/dps2021/complimentary-registration"><strong>https://dataplatformgeeks.com/dps2021/complimentary-registration</strong></a> to register for <strong>FREE</strong>!</p>

<p>Related to conferences; during the last couple of weeks, I did two webinars, of which one is up on YouTube (I expect the other one to be up soon as well):</p>

<ul>
<li><a href="https://youtu.be/dmsM_NKjFGs"><strong>Stream Processing with Apache Kafka and .NET</strong></a>. A presentation about Apache Kafka for the .NET developer and some stuff about stream-processing and ksqlDB. Due to a power failure, there is a break and some distortion in this video; sorry about that.</li>
</ul>

<p>Obviously I&rsquo;ll let you know when the second webinar is up.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Run Confluent Cloud &amp; Serverless Apache Kafka on Azure]]></title>
    <link href="https://nielsberglund.com/2021/08/14/run-confluent-cloud--serverless-apache-kafka-on-azure/" rel="alternate" type="text/html"/>
    <updated>2021-08-14T12:58:24+02:00</updated>
    <id>https://nielsberglund.com/2021/08/14/run-confluent-cloud--serverless-apache-kafka-on-azure/</id>
    <content type="html"><![CDATA[<p>For readers of my blog, it should not come as a surprise that I think Apache Kafka is <a href="https://www.merriam-webster.com/dictionary/the%20best%2Fgreatest%20thing%20since%20sliced%20bread">&ldquo;the greatest thing since sliced bread&rdquo;</a>, and I have written <a href="https://nielsberglund.com/categories/kafka/">some posts</a> about it. The posts I have written has been geared towards the setup/deployment of Kafka, with a Windows/.NET slant to it. This post is in the same vein; how to set up Kafka but in the cloud.</p>

<p>The &ldquo;conference season&rdquo; is upon us, and I have some conference talks coming up. This year quite a lot of what I am talking about is in the cloud, specifically Azure. As some of the talks involve Apache Kafka, I thought it would be good if Kafka also ran in the cloud. Sure, I could always run Kafka on Docker on a VM in the cloud, but &ldquo;they&rdquo; say serverless is the new &ldquo;in&rdquo; thing, as well as managed services, so why not try that out.</p>

<p>So, this post is about how to deploy Confluent Cloud on Azure!</p>

<p></p>

<h2 id="background">Background</h2>

<p>Let us start with what Confluent Cloud is. I shamelessly stole the following paragraph from <a href="https://docs.confluent.io/cloud/current/get-started/index.html">here</a>:</p>

<p><em>Confluent Cloud is a resilient, scalable streaming data service based on Apache Kafka, delivered as a fully managed service. Confluent Cloud has a web interface and local command line interface. You can manage cluster resources, settings, and billing with the web interface.</em></p>

<p>OK, enough of marketing talk; Confluent Cloud is the Confluent Platform running as a managed service in the cloud, and you can run it on Azure, AWS, and Google Cloud. It was <a href="https://www.confluent.io/blog/announcing-confluent-cloud-apache-kafka-as-a-service/">introduced in 2017</a>, and - if I remember correctly - it initially ran on the Google Cloud Platform (GCP). Shortly after the introduction on GCP, it also became available on AWS.</p>

<p>Towards the end of 2019, Confluent and Microsoft released Confluent Cloud on Azure, but it was like a separate service; you did not provision it from the Azure Portal. You provisioned it from Confluent and chose Azure as your platform. One of the drawbacks with this was that you had to sign up specifically for Confluent Cloud and provide a credit card, as the billing was separate. This changed at the beginning of 2020 when Confluent Cloud became wholly integrated with Azure.</p>

<p>The cost for Confluent Cloud is now billed to your Azure subscription, and you provision a Confluent Cloud cluster from the Azure Marketplace! In the rest of this post, we see how that is done!</p>

<h2 id="pre-reqs">Pre-Reqs</h2>

<p>This post would not be a &ldquo;Niels post&rdquo; if there were no pre-reqs section. So, the pre-reqs - if you want to follow along - are:</p>

<ul>
<li>An Azure account. If you don&rsquo;t have an Azure subscription, sign up for a <a href="https://azure.microsoft.com/en-us/free/">free account</a>.</li>
</ul>

<p>That was not so bad, was it? Oh, one more thing; in your Azure subscription, you need to have the role of <em>Owner</em> or <em>Contributor</em>.</p>

<h2 id="provision-confluent-cloud">Provision Confluent Cloud</h2>

<p>When we provision Confluent Cloud on Azure, what we do is we are creating a Confluent Cloud organization inside Azure. A Confluent Cloud organization is a resource that provides the mapping between the Azure and Confluent Cloud resources. It&rsquo;s the parent resource for other Confluent Cloud resources.</p>

<p>Within a Confluent organization, you can create multiple environments, clusters, topics, and connectors. The environments, clusters, etc., are created from within Confluent Cloud <em>Software as a Service</em> (SaaS) resources.</p>

<h4 id="create-confluent-cloud-organization">Create Confluent Cloud Organization</h4>

<p>To create a Confluent organization, we must be signed in to your subscription in the <a href="https://portal.azure.com/">Azure Portal</a>.</p>

<p>What you see after you have signed in my vary, but what you want to do is to <em>Create a resource</em>:</p>

<p><img src="/images/posts/ccl-azure-create-resource.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Create Resource</em></p>

<p>I create a resource in Azure by expanding the hamburger menu in the upper left-hand corner of the portal (outlined in blue in <em>Figure 1</em>), and then click on <em>Create a resource</em> (outlined in red in <em>Figure 1</em>). You may see <em>Azure Services</em> in the portal, and underneath <em>Azure Services</em>, you can click on <em>Create a resource</em>, outlined in yellow in <em>Figure 1</em>. Clicking on <em>Create a resource</em> takes you to a page with a search box:</p>

<p><img src="/images/posts/ccl-azure-confl-cloud.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Search for Confluent Cloud</em></p>

<p>You start to write <em>Confluent Clo &hellip;</em> in the search box as you see in <em>Figure 2</em>. Text completion &ldquo;pops up&rdquo; some options where <em>Apache Kafka on Confluent Cloud</em> is one (outlined in red). You choose that one, you hit enter and you see:</p>

<p><img src="/images/posts/ccl-azure-setup-subscribe.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Setup and Subscribe</em></p>

<p>Choosing <em>Apache Kafka on Confluent Cloud</em> as in <em>Figure 2</em> takes you to the page where you start the process of setting up Apache Kafka on Confluent Cloud. When you click on the <em>Setup + subscribe</em> button outlined in red in <em>Figure 3</em> you see something like so:</p>

<p><img src="/images/posts/ccl-azure-create-org2.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Create Organization</em></p>

<p>I mentioned above about Confluent Cloud organizations, and we see in <em>Figure 4</em> the first step in creating an organization. You provide values for the following properties:</p>

<ul>
<li><strong>Subscription</strong>: the Azure subscription to deploy to.</li>
<li><strong>Resource group</strong>: here, you choose an existing resource group. There is the option to create a new resource group, but I got an error when the deployment started when I had chosen to create a new group. The error was along the lines of that the resource group needed a location. So what I do now is to first create a resource group (and define a region) and use that resource group.</li>
<li><strong>Confluent organization name</strong>: This is the name of the Software as a Service resource.</li>
<li><strong>Region</strong>: in what Azure region that you want to place this deployment.</li>
<li><strong>Plan</strong>: the billing plan. I  most instances, this is <em>Pay as you Go</em>.</li>
<li><strong>Billing term</strong>: prefilled based on your chosen <em>Plan</em>.</li>
<li><strong>Price</strong>: as with <em>Billing term</em>, it is prefilled.</li>
</ul>

<p>Having filled in the values for the properties above, you can now click the <em>Review + create</em> button (outlined in red in <em>Figure 4</em>), and you see:</p>

<p><img src="/images/posts/ccl-azure-create-org-validation.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Validate Organization</em></p>

<p>Clicking the <em>Review + create</em> starts a validation of the values you have entered. In <em>Figure 5</em> we see what it looks like when the validation has successfully passed.</p>

<p>We can now go ahead and create and deploy the organization. At the bottom of the page, we see in <em>Figure 5</em> (we only see the top part in <em>Figure 5</em>) is a <em>Create</em> button. Clicking that button starts the deployment. After a while, you see:</p>

<p><img src="/images/posts/ccl-azure-org-complete.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>Deployment Complete</em></p>

<p>As we see in <em>Figure 6</em>, the deployment has now been completed, and everything should be set up. To see that everything is indeed set up, click on the <em>Go to resource</em> button outlined in red in <em>Figure 6</em>. Clicking on that button results in something like so:</p>

<p><img src="/images/posts/ccl-azure-manage-org.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>Manage on Confluent Cloud</em></p>

<p>In <em>Figure 7</em>, we see how we are inside the Confluent organization, and we are more or less done. However, remember how I above mentioned that the organization we just have created is the mapping between Azure and Confluent Cloud resources, and how we use Confluent Cloud resources to create Kafka clusters, Topics, etc. So, to seamlessly move between Azure and Confluent Cloud, we need to enable Single Sign-On (SSO).</p>

<h4 id="single-sign-on-sso">Single Sign-On (SSO)</h4>

<p>Enabling SSO allows us to transparently move from Azure to Confluent Cloud and directly login to Confluent Cloud with an SSO URL.</p>

<p>To enable SSO, we click on the link outlined in red in <em>Figure 7</em>. Clicking on that link gives us this:</p>

<p><img src="/images/posts/ccl-azure-sso-perm.png" alt="" /></p>

<p><strong>Figure 8:</strong> <em>Permissions</em></p>

<p>In <em>Figure 8</em>, we see a request for permissions from Confluent Cloud to do what is outlined in <em>Figure 8</em>. That is the final thing we do in Azure because when we accept the request, we see something like so:</p>

<p><img src="/images/posts/ccl-azure-welcome.png" alt="" /></p>

<p><strong>Figure 9:</strong> <em>Welcome</em></p>

<p>We have now arrived at Confluent Cloud, and the <em>Welcome</em> page, as we see in <em>Figure 9</em>, asks us some questions about what we want to do with Kafka, our experience, etc.  I am not that interested in answering those questions so I just hit the <em>Skip</em> link we see outlined in red in <em>Figure 9</em>, and I see this:</p>

<p><img src="/images/posts/ccl-azure-welcome-done.png" alt="" /></p>

<p><strong>Figure 10:</strong> <em>Ready to Roll</em></p>

<p>Right, we are done with creating the organization, and we have enabled SSO. As we see in <em>Figure 10</em> we can now start doing the &ldquo;cool stuff&rdquo;! However, let us not click on any links on this page, but just close down the page and go back to Azure and our Confluent organization in Azure.</p>

<p>Having enabled SSO, we have three ways of logging into/signing-in to our Confluent Cloud SaaS resources:</p>

<ul>
<li>from inside Azure Portal: by clicking on the <em>Manage on Confluent Cloud</em> link in <em>Figure 7</em> (the one we used to set up SSO).</li>
<li>from the <a href="https://confluent.cloud"><strong>Confluent Cloud</strong> login page</a>, where we use the email and password we use to authenticate against Azure.</li>
<li>use the SSO login URL. You get the URL by right-clicking on the <em>Manage on Confluent Cloud</em> link and choose <em>Copy link</em>.</li>
</ul>

<p>Up until now we have created a Confluent Cloud organization in Azure and registered that organization with Confluent Cloud. It is now time to do the interesting stuff: Kafka Clusters, Topics etc.</p>

<h2 id="create-a-kafka-cluster">Create a Kafka Cluster</h2>

<p>Login to Confluent Cloud by using one of the methods above. After having logged in we see something like so:</p>

<p><img src="/images/posts/ccl-azure-defaukt-environment.png" alt="" /></p>

<p><strong>Figure 11:</strong> <em>Default Environment</em></p>

<p>At the beginning of this post, I said how a Confluent Cloud organization was the parent resource for other Confluent Cloud resources. When we create an organization, an Environment is also created by default (named <code>default</code>), which we see in <em>Figure 11</em>.</p>

<blockquote>
<p><strong>NOTE:</strong> We can change the name of the environment by clicking on the link in <em>Figure 11</em> outlined in yellow.</p>
</blockquote>

<p>An environment is a container for Kafka clusters, and we see in <em>Figure 11</em> a button (outlined in red) <em>Create cluster on my own</em>. When clicking that button we get:</p>

<p><img src="/images/posts/ccl-azure-create-cluster-I.png" alt="" /></p>

<p><strong>Figure 12:</strong> <em>Create Cluster - I</em></p>

<p>In <em>Figure 12</em>, we see the <em>Create cluster</em> page we end up on after clicking the <em>Create cluster on my own</em> in <em>Figure 11</em>. We see how we can choose what cluster type to create, and for our purposes, the <em>Basic</em> type is more than enough. Having selected the cluster type, we are now ready to configure the cluster. Clicking on the button outlined in red in <em>Figure 12</em>: the <em>Begin configuration</em> button, we see this:</p>

<p><img src="/images/posts/ccl-azure-create-cluster-II.png" alt="" /></p>

<p><strong>Figure 13:</strong> <em>Create Cluster - II</em></p>

<p>The first thing to do when configuring a new cluster is what we see in <em>Figure 13</em>, we choose:</p>

<ul>
<li>the region we want the cluster in. A good practice is to choose the same region as our resource group and organization.</li>
<li>the level of availability we want: single zone or multi. In our case, using the <em>Basic</em> cluster type, single-zone is our only choice.</li>
</ul>

<p>Having decided on the <em>Region</em> we click <em>Continue</em> (outlined in red), and we get this:</p>

<p><img src="/images/posts/ccl-azure-create-cluster-III.png" alt="" /></p>

<p><strong>Figure 14:</strong> <em>Create Cluster - III</em></p>

<p>What is left for us to do is to give the cluster a name. In <em>Figure 14</em>, we see how I named it <code>test_cluster_1</code> (outlined in yellow). Then I click on the <em>Launch cluster</em> button, which in <em>Figure 14</em> is outlined in red:</p>

<p><img src="/images/posts/ccl-azure-cluster-overview.png" alt="" /></p>

<p><strong>Figure 15:</strong> <em>Cluster Created</em></p>

<p>The cluster is now created, and we are presented with an overview of the cluster as in <em>Figure 15</em>. For you who have used Confluent Platform, this looks almost like the overview page in Confluent Control Center (the Web UI for Confluent Platform). We see in <em>Figure 15</em>:</p>

<ul>
<li>outlined in blue: a menu covering the cluster&rsquo;s main components; topics, ksqlDB, etc.</li>
<li>red: the link to create and manage topics.</li>
<li>green: to connect to the cluster from the outside world, we need an API key (and a secret). The link here allows us to create that.</li>
<li>yellow: a link to instructions on downloading and installing the Confluent Cloud CLI (CCLI). You can definitely manage your cluster, topics, etc., using the Web UI, but to be more efficient, you want to use the Confluent Cloud CLI.</li>
</ul>

<blockquote>
<p><strong>NOTE:</strong> Above the CCLI box is a link giving examples of how to configure clients. Very useful.</p>
</blockquote>

<p>Speaking about clients: to access the cluster and publish to or consume from a topic you need the address of the cluster. If you click on the <em>Cluster settings</em> link we see in <em>Figure 15</em> you get all sorts of information about the cluster, including the the value of <code>bootstrap.servers</code> that clients need.</p>

<p>OK, so we now have a cluster; let us create a topic.</p>

<h2 id="create-topic-s">Create Topic(s)</h2>

<p>Click on the <em>Topics</em> link outlined in red in <em>Figure 15</em>:</p>

<p><img src="/images/posts/ccl-azure-topics-create-I.png" alt="" /></p>

<p><strong>Figure 16:</strong> <em>Topics</em></p>

<p>What we see in <em>Figure 16</em> should be self-explanatory. It is the page for our topics in this cluster. We don&rsquo;t have any topics yet, so we create one by clicking on the <em>Create topic</em> button outlined in red in <em>Figure 16</em>. Doing that, we see:</p>

<p><img src="/images/posts/ccl-azure-topics-new.png" alt="" /></p>

<p><strong>Figure 17:</strong> <em>New Topic</em></p>

<p>In <em>Figure 17</em>, we see the form for creating a new topic. We fill in the <em>Topic name</em> field outlined in blue, we set the number of partitions we want in the field outlined in yellow, and then we are ready to click the <em>Create with defaults</em> button.</p>

<blockquote>
<p><strong>NOTE:</strong> By default number of partitions is set to 6.</p>
</blockquote>

<p>When we click on the <em>Create with defaults</em> button, the result looks like so:</p>

<p><img src="/images/posts/ccl-azure-topics-topic.png" alt="" /></p>

<p><strong>Figure 18:</strong> <em>Topic Created</em></p>

<p>Yay - we have a topic! In <em>Figure 18</em>, we see an overview of the topic, and we see we have some tabs: <em>Overview</em>, <em>Messages</em>, <em>Schema</em>, and <em>Configuration</em>. I will not go into detail about the various tabs, but - nevertheless - let us have a brief look at the <em>Messages</em> tab</p>

<h4 id="messages">Messages</h4>

<p>Click on the <em>Messages</em> tab and you see something like so:</p>

<p><img src="/images/posts/ccl-azure-topics-messages.png" alt="" /></p>

<p><strong>Figure 19:</strong> <em>Messages</em></p>

<p>As we see in <em>Figure 19</em>, the <em>Messages</em> tab has to do with messages - duh! I mention this tab because it gives us the ability to quickly produce messages to the topic and view messages that have been published to the topic. Being able to publish messages gives us a quick and easy way to ensure the topic is set up correctly. Viewing messages is good because we can quickly ensure messages have arrived in the topic.</p>

<p>To publish, we click on what is outlined in red in <em>Figure 19</em>, and we see the following:</p>

<p><img src="/images/posts/ccl-azure-topics-publish.png" alt="" /></p>

<p><strong>Figure 20:</strong> <em>Publish</em></p>

<p>In <em>Figure 20</em>, we see how a message in JSON format has been generated (you can edit the message as you want), and we publish it by clicking on the <em>Produce</em> button outlined in red. Clicking on the button, you see a notification saying the message is being processed. When the notification disappears, you enter an offset in the field outlined in blue in <em>Figure 19</em>. Enter <code>0</code>, hit return, and you see under the <em>Produce</em> button something like so:</p>

<p><img src="/images/posts/ccl-azure-topics-published=message.png" alt="" /></p>

<p><strong>Figure 21:</strong> <em>Published Message</em></p>

<p>Another Yay! We have a message in the topic, as we see in <em>Figure 21</em>.</p>

<blockquote>
<p><strong>NOTE:</strong> When you are <del>playing around</del> doing tests, you may want to delete all messages from a topic. The only way to do it from the UI is to delete the topic. You then need to re-create it. To delete the topic, you go to the <em>Configuration</em> tab we see in <em>Figure 18</em> (and 19) and click on the <em>Delete topic</em> link in the lower left-hand corner.</p>
</blockquote>

<p>We are almost done. What is left is to create an API key and a secret. We use the API key and the secret to access the cluster from the outside world.</p>

<h2 id="api-access">API Access</h2>

<p>To enable API access, we use the <em>API access</em> link we see outlined in green in <em>Figure 15</em>. Clicking on that link results in this:</p>

<p><img src="/images/posts/ccl-azure-api-access-I.png" alt="" /></p>

<p><strong>Figure 22:</strong> <em>API Access</em></p>

<p>In <em>Figure 22</em> we click on the <em>Create Key</em> button, which is outlined in red, and we get this:</p>

<p><img src="/images/posts/ccl-azure-api-create-key-I.png" alt="" /></p>

<p><strong>Figure 23:</strong> <em>Create API Key</em></p>

<p>An API key can have different scope. In our case, we choose the create the with Global scope as we see in <em>Figure 24</em> and having selected the scope, we click <em>Next</em> (outlined in red in <em>Figure 23</em>):</p>

<p><img src="/images/posts/ccl-azure-api-create-key-II.png" alt="" /></p>

<p><strong>Figure 24:</strong> <em>API Key Created</em></p>

<p>Please read the highlighted section, in <em>Figure 24</em>, carefully!</p>

<p>After creating the key, you see the key and the secret as in <em>Figure 24</em>. However, you will no longer see the secret after clicking the <em>Save</em> button outlined in red. So ensure you have written down the key and the secret somewhere (and remember where you saved it to).</p>

<h2 id="summary">Summary</h2>

<p>This post looked at how to run Apache Kafka in Confluent Cloud, where Azure is our cloud environment.</p>

<p>We saw how we:</p>

<ul>
<li>Create a Confluent Cloud organization from within Azure.</li>
<li>Enable SSO between Azure and Confluent Cloud.</li>
</ul>

<p>The above creates the organization in Azure and &ldquo;registers&rdquo; it with Confluent Cloud. We then:</p>

<ul>
<li>Log into the Confluent Cloud.</li>
<li>Create a Kafka cluster.</li>
<li>In the Kafka cluster, we create one or more topics.</li>
</ul>

<p>To connect to the cluster from the &ldquo;outside&rdquo; world, we let Confluent Cloud generate an API key with a corresponding secret.</p>

<p>That&rsquo;s it!</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 32, 2021]]></title>
    <link href="https://nielsberglund.com/2021/08/08/interesting-stuff---week-32-2021/" rel="alternate" type="text/html"/>
    <updated>2021-08-08T09:09:24+02:00</updated>
    <id>https://nielsberglund.com/2021/08/08/interesting-stuff---week-32-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="big-data-data-analytics">Big Data / Data Analytics</h2>

<ul>
<li><a href="https://medium.com/event-driven-utopia/building-a-low-latency-fitness-leaderboard-with-apache-pinot-40a4da672cf0">Building a Low-Latency Fitness Leaderboard with Apache Pinot</a>. The terms user-facing/site-facing analytics are &ldquo;popping up&rdquo; more and more. When I first heard it, I was pretty confused (pretty typical for me) about what it means - analytics is analytics, after all. But when reading this post, it dawned on me what it is. However, I won&rsquo;t &ldquo;spoil&rdquo; the explanation here. Apart from explaining what user-facing analytics mean, this post covers using Apache Pinot to ingest fitness band events from a Kafka topic and make them available for immediate querying. Very cool!</li>
<li><a href="https://eng.uber.com/orders-near-you/">&lsquo;Orders Near You&rsquo; and User-Facing Analytics on Real-Time Geospatial Data</a>. When it rains, it pours, hey? Another post about user-facing analytics and Apache Pinot. In this post, Uber explains the implementation of the &lsquo;Orders Near You&rsquo; feature and how they generate insights across geospatial data.</li>
<li><a href="https://eng.uber.com/ubers-finance-computation-platform/">Uber&rsquo;s Finance Computation Platform</a>. For a company of Uber&rsquo;s size and scale, it is required to have robust, accurate, and compliant accounting and analytics. The post looks at how they built their own in-house platform - the Finance Computation Platform - to meet their demanding requirements.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/confluent-developer-launches-free-apache-kafka-courses-and-tutorials-online/">The New One-Stop Shop for Learning Apache Kafka</a>. This is awesome, awesome, awesome! Did I say it was awesome? OK, Niels, calm down - what is this? The post announces an all-new website dedicated to Apache Kafka, event streaming, and associated cloud technologies. As the title says, the site is really a one-stop-shop for everything Kafka! Have a look at the various courses they offer - it is a gold mine!</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>Well, apart from spending waaaayyy too much time on <a href="https://developer.confluent.io/"><strong>Confluent Developer</strong></a>, I am prepping for the upcoming <a href="https://dataplatformgeeks.com/dps2021/"><strong>2021 Data Platform Summit</strong></a> where I am doing one conference presentation:</p>

<ul>
<li><strong>How to do Real-Time Analytics Using Apache Kafka and Azure Data Explorer</strong>: This session shows how to do near-real-time analysis on data streaming from Apache Kafka (running on Confluent Cloud in Azure) using Azure Data Explorer.</li>
</ul>

<p>In addition to the session above, I am also doing an eight-hour post-con training class (split over two days):</p>

<ul>
<li><a href="https://dataplatformgeeks.com/dps2021/product/big-data-analytics-with-sql-server-2019-big-data-cluster-by-niels-berglund/"><strong>Big Data &amp; Analytics with SQL Server 2019 Big Data Cluster</strong></a>: This training covers big data, data virtualization and analytics in SQL Server 2019 Big Data cluster. There are still some seats left, so you can <a href="https://dataplatformgeeks.com/dps2021/product/big-data-analytics-with-sql-server-2019-big-data-cluster-by-niels-berglund/">sign up here</a> if you are interested. Apart from getting to know BDC, an added benefit of signing up is getting a free submission to the summit!</li>
</ul>

<p>Lately, I have been investigating SQL Server CDC and the use of Debezium to publish data from SQL Server. For my investigations, I have used Kafka running in Docker. Every time I have set this up, I have struggled with deploying the Debezium SQL Server Connector to the Kafka Connect container. I finally decided to write a blog post about so I have something to go back to for next time, and I published the post yesterday:</p>

<ul>
<li><a href="/2021/08/07/how-to-deploy-the-debezium-sql-server-connector-to-docker/"><strong>How to Deploy the Debezium SQL Server Connector to Docker</strong></a></li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to Deploy the Debezium SQL Server Connector to Docker]]></title>
    <link href="https://nielsberglund.com/2021/08/07/how-to-deploy-the-debezium-sql-server-connector-to-docker/" rel="alternate" type="text/html"/>
    <updated>2021-08-07T06:02:12+02:00</updated>
    <id>https://nielsberglund.com/2021/08/07/how-to-deploy-the-debezium-sql-server-connector-to-docker/</id>
    <content type="html"><![CDATA[<p>I have been doing a couple of conference talks lately (virtual, of course) about streaming data from SQL Server to Kafka. The title of the presentation is <strong>Free Your SQL Server Data With Kafka</strong>.</p>

<p>In the presentation, I talk (and show) various ways of getting data from SQL Server to Kafka. One of the ways I cover is Microsoft CDC, together with Debezium.</p>

<p>When I do the presentation, I always have a SQL Server installed locally, and I run Kafka in Docker. Without fail, every time I set up the environment, I cannot remember how to deploy the Debezium SQL Server Connector into Docker. Therefore I decided to write this post to have something to go back to for next time.</p>

<p></p>

<blockquote>
<p><strong>NOTE:</strong> This post does not cover the intricacies of how to configure Debezium for SQL Server. I leave that for a future post.</p>
</blockquote>

<h2 id="background">Background</h2>

<p>Before diving into how to do this, let us look at the moving parts of this.</p>

<h4 id="kafka-connect">Kafka Connect</h4>

<p>Kafka Connect is a tool for streaming data between Apache Kafka and other systems in a scalable and reliable way. The way you move data between systems and Kafka is using connectors, and there are two flavors of connectors:</p>

<ul>
<li>Source connectors which understand how to interact with the source system send records into Kafka</li>
<li>Sink connectors that propagate records from Kafka topics to other systems.</li>
</ul>

<p>Kafka Connect is a JVM process, and it operates separately from the Kafka Broker. Connectors are <code>.jar</code> files loaded by the connect process. The diagram below shows a high-level overview of what it looks like:</p>

<p><img src="/images/posts/kafka-connect-1.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Kafka Connect Overview</em></p>

<p>In <em>Figure 1</em> we see, (from left to right):</p>

<ul>
<li>Source systems, i.e. systems we want to get data from. These systems can be databases, Hadoop, files, etc.</li>
<li>The Kafka Connect worker with source connectors. The connectors know how to interact with the source system, whether querying a database, using CDC, reading from a filesystem, etc. The connectors publish data to Kafka topics.</li>
<li>The Kafka broker(s). The broker(s) contain topics that are the &ldquo;sinks&rdquo; for the source connectors.</li>
<li>Kafka Connect worker with sink connectors. Source and sink connectors can be in the same Kafka Connect worker. The sink connectors know how to consume events from Kafka topics and ingest them into sink systems.</li>
<li>Sink systems. These are systems we ingest data into. As with source systems, these can be databases, Hadoop, files, etc.</li>
</ul>

<h4 id="debezium">Debezium</h4>

<p>Debezium is an open source distributed platform for change data capture, (I &ldquo;stole&rdquo; the preceding shamelessly from <a href="https://debezium.io/">here</a>). It captures changes in your database(s) and publishes those changes to topics in Kafka.</p>

<blockquote>
<p><strong>NOTE:</strong> Debezium <em>can</em> work without a Kafka cluster, in which case it is embedded in your application, and the application receives the change notifications. Read more about that <a href="https://debezium.io/documentation/reference/1.4/development/engine.html">here</a>.</p>
</blockquote>

<p>Debezium has Kafka Connect connectors for a multitude of source systems. When interacting with Kafka, the connector(s) is deployed to Kafka Connect.</p>

<p>With the above in mind, let us look at how this works with SQL Server.</p>

<h4 id="sql-server-debezium-and-kafka">SQL Server, Debezium, and Kafka</h4>

<p>As I mentioned at the beginning of this post, the aim is to get data out of some table(s) in a database(s) and stream it to a topic(s) in Kafka.</p>

<p>We do not necessarily need to use Debezium as there are other Kafka Connect connectors. We could, for example, use the Confluent SQL Server connector. However, as we want to stream the data in near real-time, with the least amount of work on our side, the Debezium connector is our choice. Coming back to <em>Figure 1</em> it would look something like so:</p>

<p><img src="/images/posts/kafka-connect-cdc.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Kafka Connect SQL Server &amp; Debezium</em></p>

<p>We see in <em>Figure 2</em> how the source system is SQL Server and how the source connector is the Debezium SQL Server connector. In the diagram we publish to one topic as we only retrieve data from one table. If we were to retrieve data from multiple tables, we&rsquo;d publish to multiple topics.</p>

<p>We have several sink connectors reading from our topic and ingest into various sink systems.</p>

<p>Ok, enough background; before we get into the &ldquo;nitty-gritty&rdquo;, let&rsquo;s see what you need if you want to follow along.</p>

<h2 id="pre-reqs-code">Pre-reqs &amp; Code</h2>

<p>There are not many pre-reqs, but here goes:</p>

<ul>
<li><strong>SQL Server</strong>: well, duh - as we want to set up CDC and Debezium to stream data from SQL Server, we would need SQL Server installed somewhere. I have SQL Server 2019 installed on my local dev machine.</li>
<li><strong>Docker Desktop</strong>: another duh - this post is all about how to set up Kafka Connect and Debezium in Docker, so yes - we need Docker Desktop.</li>
</ul>

<h4 id="test-code">Test Code</h4>

<p>I mentioned in the beginning that this post is not about configuring Debezium to read data from SQL Server, so I won&rsquo;t discuss CDC in any detail. However, we need something to test that what we are doing works, so here&rsquo;s some code to set up a database on SQL Server:</p>

<pre><code class="language-sql">USE master;
GO

--  to start from scratch drop the database if exists
IF EXISTS(SELECT * FROM sys.databases WHERE name = 'DebeziumTest')
BEGIN
  ALTER DATABASE DebeziumTest
  SET SINGLE_USER
  WITH ROLLBACK IMMEDIATE;

  DROP DATABASE DebeziumTest;
END
GO

-- create the database
CREATE DATABASE DebeziumTest;
GO

USE DebeziumTest;
GO

-- this statement just if we don't want to drop the db, 
-- but still start over with the table
-- DROP TABLE dbo.tb_CDCTab1;

-- table which we later will CDC enable
CREATE TABLE dbo.tb_CDCTab1 (RowID int identity primary key,
                      Col1 int,
                      Col2 nvarchar(25));
GO

</code></pre>

<p><strong>Code Snippet 1:</strong> <em>DB Objects Creation Script</em></p>

<p>The code in <em>Code Snippet 1</em> creates a database, <code>DebeziumTest</code> and a table, <code>dbo.tb_CDCTab1</code> in the database. Later in the post, we enable the table for CDC. Enabling it allows us to check that our Kafka Connect/Debezium &ldquo;stuff&rdquo; works as expected. So, if you want to follow along, run the script, and after you have run it, ensure you have the database and the table.</p>

<p>Now, let us get into what we are supposed to do; to set this up in Docker.</p>

<h2 id="docker">Docker</h2>

<p>Let us start with getting the necessary Docker images and compose files.</p>

<h4 id="docker-kafka-image">Docker Kafka Image</h4>

<p>There are quite a few Docker images, and Docker composes files around for setting up Kafka and Kafka Connect. The ones I usually use are from Confluent&rsquo;s <a href="https://github.com/confluentinc/cp-all-in-one/"><code>cp-all-in-one</code></a> repository.</p>

<p>Let us get started:</p>

<pre><code class="language-bash">mkdir kafka
cd kafka
git clone https://github.com/confluentinc/cp-all-in-one.git
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Clone the Repo</em></p>

<p>In <em>Code Snippet 2</em>, we create a directory for the repo files and clone the <a href="https://github.com/confluentinc/cp-all-in-one/"><code>cp-all-in-one</code></a>. After cloning, we have a directory named <code>cp-all-in-one</code> under the <code>kafka</code> directory. The <code>cp-all-in-one</code> directory looks like so:</p>

<p><img src="/images/posts/kafka-connect-docker-1.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Kafka Connect SQL Server &amp; Debezium</em></p>

<p>In <em>Figure 3</em>, we see that <code>cp-all-in-one</code> (outlined in red) has some sub-directories. These directories contain Docker Compose files for various setups of Kafka. We are interested in the directory outlined in blue: <code>cp-all-in-one</code>, (yeah I know - the same name as the parent directory).</p>

<p>A quick side note here about <code>cp-all-in-one</code>. This directory contain the image for Confluent Platform, which is the enterprise edition of Confluent. As with most enterprise editions this requires a licennse. However, with the introduction of Confluent Platform 5.2 &ldquo;back in the day&rdquo;, Confluent <a href="https://www.confluent.io/blog/introducing-confluent-platform-5-2">announced</a> that Confluent Platform is &ldquo;free forever&rdquo; on a single Kafka broker! In other words, it is like a &ldquo;Developer Edition&rdquo; of Confluent Platform. The benefit of using Confluent Platform is that you get ALL the goodies, including <strong>Control Center</strong>, which is the WEB UI for Confluent Platform.</p>

<p>When going into that directory, we see a <code>docker-compose.yml</code>. Opening it in an editor, we see the various images deployed as services when running the <code>docker-compose</code> command. From a Kafka Connect perspective, we are interested in the <code>connect</code> service (and image):</p>

<p><img src="/images/posts/kafka-connect-docker-connect-yml.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Docker Compose - Connect Service</em></p>

<p>What <em>Figure 3</em> shows is a &ldquo;condensed&rdquo; part of the <code>yml</code> for the Kafka Connect image. There are three outlined section in the figure:</p>

<ul>
<li>Outlined in green: <code>image</code>, the image the service is built on.</li>
<li>Yellow: <code>CONNECT_REST_PORT</code>, the port to access the service on.</li>
<li>Red: <code>CONNECT_PLUGIN_PATH</code>, the path from where the service loads plugins (connectors).</li>
</ul>

<p>We see in <em>Figure 3</em> how the image is <code>cp-server-connect-datagen</code>. That image is an image containing some &ldquo;base&rdquo; connectors and also tools for generating data. There are other Connect images, and we see one other later in this post. But for now, let us use this image.</p>

<h2 id="run-kafka-connect">Run Kafka &amp; Connect</h2>

<p>Having retrieved the required files as in <em>Code Snippet 2</em> it is time to &ldquo;spin up&rdquo; the Kafka cluster, including Kafka Connect. We do that by <code>cd</code>:ing into the directory where the <code>docker-compose.yml</code> file is and execute: <code>docker-compose up -d</code>.</p>

<p>When running the code you see how Docker is pulling images, starts up the various services, and finally:</p>

<p><img src="/images/posts/kafka-connect-docker-compose-up.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Kafka Started</em></p>

<p>We see in <em>Figure 4</em> how all services have started. Well, that is not exactly true - some services are still &ldquo;spinning&rdquo; up, but after a minute or two, you can browse to the Confluent Control Center and see your Kafka cluster in all its &ldquo;glory&rdquo;:</p>

<p><img src="/images/posts/kafka-connect-control-center.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Confluent Control Center</em></p>

<p>In <em>Figure 5</em>, we see the overview page for the Kafka cluster. Outlined in red at the top, we see how we access it from port 9021 on the box where Docker runs. Outlined in blue, we see that we have one Connect cluster.</p>

<h4 id="installed-connectors">Installed Connectors</h4>

<p>I mentioned above that the Connect image has some connectors installed by default. To see what connectors are pre-installed, we use the Kafka REST API:</p>

<pre><code class="language-bash">GET http://127.0.0.1:8083/connector-plugins
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>GET Connectors</em></p>

<p>We can use whatever tool we are comfortable with to call the REST API (<code>curl</code>, Postman, etc.). Personally, I prefer Postman, and in <em>Code Snippet 3</em> we see how we call into the <code>connector-plugins</code> endpoint and how we use the port I mentioned above: <code>8083</code>:</p>

<p><img src="/images/posts/kafka-connect-get-connectors.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>Installed Connectors</em></p>

<p>Executing the code in <em>Code Snippet 3</em> I get the result we see in <em>Figure 6</em>. We see various Connect connectors, but nothing for SQL Server. So, we need to install it somehow.</p>

<h2 id="installing-debezium-sql-server-connector">Installing Debezium SQL Server Connector</h2>

<p>How do we go about installing a connector into the Connect service? Above I mentioned how a connector is a <code>.jar</code> file, which the service (JVM process) loads. I also mentioned the <code>CONNECT_PLUGIN_PATH</code>, which indicates where the service loads connectors from. So, with that in mind, we can imagine the process to install a connector being:</p>

<ol>
<li>Get the specific connector from &ldquo;somewhere&rdquo;.</li>
<li>Copy it into the path where the service loads connectors from.</li>
<li>Restart the service.</li>
</ol>

<p>If we ran the Kafka cluster as a local install, installing a connector would be as easy as above, but what about installing it when running Kafka in Docker containers?</p>

<p>Well, we could do something similar:</p>

<ol>
<li>Download the connector we want to install (the file is most likely &ldquo;tar&rdquo;:ed).</li>
<li>&ldquo;spin up&rdquo; the Kafka cluster.</li>
<li>Use <code>docker cp</code> to copy the connector into the connect container.</li>
<li>Use <code>docker exec</code> to get to the bash shell in the connect container.</li>
<li>Un-tar the file to the plugin load path.</li>
<li>Back out from the container and <code>docker commit</code> the changes to a new image name.</li>
<li>Tear down the running Kafka cluster: <code>docker-compose down</code>.</li>
<li>Use that image in the docker compose file in place of the &ldquo;original&rdquo; connect image.</li>
</ol>

<p>Yes, we could do something like that, and that is how I did it initially (yeah I know - I am a &ldquo;noob&rdquo;, so sue me). There are however better ways of doing it:</p>

<ul>
<li>Confluent Hub</li>
<li>Create a new image from a <code>Dockerfile</code>.</li>
</ul>

<p>Let us look at the two options.</p>

<h4 id="confluent-hub">Confluent Hub</h4>

<p>Confluent is like the App Store (or NuGet), but for Kafka. The <a href="https://docs.confluent.io/home/connect/confluent-hub/">home page</a> expresses it a lot better than what I can do:</p>

<p><em>Confluent Hub is an online library of pre-packaged and ready-to-install extensions or add-ons for Confluent Platform and Apache Kafka. You can browse the large ecosystem of connectors, transforms, and converters to find the components that suit your needs and easily install them into your local Confluent Platform environment.</em></p>

<p>Using Confluent Hub, you can install Kafka Connect connectors, whether you do it locally or in Docker. This is made possible via the <a href="https://docs.confluent.io/home/connect/confluent-hub/client.html">Confluent Hub Client</a>. The client is part of the Confluent Platform and is located in the <code>/bin</code> directory. If you are not using the Confluent Platform, you can download and install the client locally. Since I am using Confluent Platform, all is good.</p>

<p>Right, so we want to install the Debezium SQL Server connector. Let us go to the hub and look for it. Browse to <a href="https://www.confluent.io/hub/">here</a>, and in the search box, enter &ldquo;SQL Server&rdquo;, followed by a carriage return:</p>

<p><img src="/images/posts/kafka-connect-confl-hub-dbz.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>Debezium SQL Server Connector</em></p>

<p>The result coming back from the search looks like what we see in <em>Figure 7</em>; one entry: <strong>Debezium SQL Server CDC Source Connector</strong>. When you click on the result, you end up at a page with some more information about the connector, and more importantly, the syntax of how to install the connector:</p>

<pre><code class="language-bash">confluent-hub install debezium/debezium-connector-sqlserver:1.6.0
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Install SQL Server Connector</em></p>

<p>Ok, cool - so we see the syntax to install a connector in <em>Code Snippet 4</em>, but this looks suspiciously like how we do it from a bash shell. How do I do it for a Docker container without resorting to the &ldquo;hack&rdquo; I mentioned at the beginning of this section?</p>

<p>Ah, that&rsquo;s where the &ldquo;magic&rdquo; of Docker compose files comes in. It turns out that when you define a container, you can also specify configuration options. One such option is the <code>command</code> option, which allows you to execute arbitrary commands.</p>

<p>Let us edit our <code>docker-compose.yml</code> file and add the command configuration:</p>

<pre><code class="language-bash">connect:
    image: cnfldemos/cp-server-connect-datagen:0.5.0-6.2.0
    
    [snip]

    environment:

      [snip]

      CONNECT_PLUGIN_PATH: &quot;/usr/share/java,/usr/share/confluent-hub-components&quot;
      CONNECT_LOG4J_LOGGERS: org.apache.zookeeper=ERROR,org.I0Itec.zkclient=ERROR,org.reflections=ERROR
    command: 
      - bash 
      - -c 
      - |
        echo &quot;Installing connector plugins&quot;
        confluent-hub install --no-prompt debezium/debezium-connector-sqlserver:latest
        #
        echo &quot;Launching Kafka Connect worker&quot;
        /etc/confluent/docker/run &amp; 
        #
        sleep infinity

</code></pre>

<p><strong>Code Snippet 5:</strong> <em>Installing Connector from Confluent Hub in Container</em></p>

<p>In <em>Code Snippet 5</em>, we see how we use the <code>docker-compose.yml</code> file from before (heavily &ldquo;snipped&rdquo;), but we have added the <code>command</code> configuration option.</p>

<p>In the code, we see how we:</p>

<ul>
<li>&ldquo;spin up&rdquo; the bash shell.</li>
<li>set some options, <code>-c</code> and <code>|</code>.</li>
<li>executing the <code>install</code> command.</li>
<li>making sure we start up the worker process</li>
<li>we finally do <code>sleep infinity</code> to keep the container alive.</li>
</ul>

<p>It is worth noting that instead of a version number of the connector, I say <code>latest</code> to always get the latest release. Having edited the <code>docker-compose.yml</code> file, we now start the cluster: <code>docker-compose up -d</code>.</p>

<blockquote>
<p><strong>NOTE:</strong> Above I say *&hellip; get the latest <strong>release</strong> &hellip;*. What to be aware of is that the Confluent Hub only contains released versions of connectors (AFAIK).</p>
</blockquote>

<p>Wait a little while for the cluster to start up, and then use Postman to retrieve the installed connectors as in <em>Code Snippet 3</em>. Executing the Postman <code>GET</code> command, we see:</p>

<p><img src="/images/posts/kafka-connect-dbz-sql-conn.png" alt="" /></p>

<p><strong>Figure 8:</strong> <em>Installed Debezium SQL Server Connector</em></p>

<p>Yay, the Debezium SQL Server Connector is now deployed to Kafka Connect, as we see in <em>Figure 8</em> (outlined in red). What you see outlined in yellow is the datagen connector which is one of the extra parts of the <code>cp-server-connect-datagen</code> image.</p>

<h4 id="dockerfile">Dockerfile</h4>

<p>Ok, so we have seen how we can install a connector using <code>confluent-hub install</code> in the <code>command</code> option in the <code>docker-compose.yml</code>. That is awesome; however, this requires you to have an internet connection every time you &ldquo;spin up&rdquo; the Kafka cluster.</p>

<p>So, what you can do instead is build your own connect worker image, include the connector(s) you want and use that image in your compose file. To do this we need a base image and a file that tells Docker what to do to build our own image. For the base image, we can definitely use the image we have seen so far, the <code>cp-server-connect-datagen</code> image, but in reality, you want an image containing the bare minimum as the base. For that, we use the <code>cp-server-connect-base</code> image.</p>

<p>I mentioned above that we need a file telling Docker what to do. This file is essentially a build file. It is common practice to name that file <code>Dockerfile</code>. To achieve what we did above in <em>Code Snippet 5</em>, we create an empty file, name it <code>Dockerfile</code> and add the following:</p>

<pre><code class="language-bash">FROM confluentinc/cp-server-connect-base:6.2.0

RUN confluent-hub install --no-prompt \
            debezium/debezium-connector-sqlserver:latest
</code></pre>

<p><strong>Code Snippet 6:</strong> <em>File to Create Image</em></p>

<p>In <em>Code Snippet 6</em>, we see how we first define what image to use, and then we tell Docker we want to run the command to add the connector. Having saved the <code>Dockerfile</code>, we now build the image:</p>

<pre><code class="language-bash">docker build . -t dbz-sql-conn
</code></pre>

<p><strong>Code Snippet 7:</strong> <em>Docker Build</em></p>

<p>The code in <em>Code Snippet 7</em> shows the syntax to build the image, where:</p>

<ul>
<li><code>docker build</code> is the build command.</li>
<li>the <code>.</code> tells docker to pick the file named <code>Dockerfile</code>.</li>
<li>the <code>-t dbz-sql-conn</code> tags the image with a name. Here we can also assign a version number.</li>
</ul>

<p>After having run the code in <em>Code Snippet 7</em>, you can execute <code>docker images</code>, and in the list of images being returned, you should now see the <code>dbz-sql-conn</code>. Notice how it automatically has been assigned the <code>latest</code> tag, as we did not give it a version.</p>

<p>If the Kafka cluster is still up bring it down. Edit the <code>docker-compose.yml</code> file and replace the <code>cnfldemos/cp-server-connect-datagen:0.5.0-6.2.0</code> image with the image we just built: <code>dbz-sql-conn</code>, and bring up the cluster again. We do as we did before; wait a while for the cluster to come up and then use Postman to see the installed connectors. If everything has gone according to plan you now - once again - see the SQL Server connector. This time though, you will not see the datagen connector as it is not part of the <code>cp-server-connect-base</code> image.</p>

<h4 id="pre-release-connector-versions">Pre-Release Connector Versions</h4>

<p>I mentioned previously that Confluent Hub contains released versions of connectors. What if you want to use an Alpha/Beta version of a connector? For example, the latest release of the Debezium SQL Server connector is version 1.6.0, but there is a version 1.7.0 in testing, and I would like to test that version.</p>

<p>First of all, how do you know what versions there are? Well, browse to <a href="https://debezium.io/releases/">Debezium Releases Overview</a>, where you see what version is in development and what version is released:</p>

<p><img src="/images/posts/kafka-connect-dbz-release-overview.png" alt="" /></p>

<p><strong>Figure 9:</strong> <em>Installed Debezium SQL Server Connector</em></p>

<p>Click on the <em>More Info</em> button, which you see outlined in red in <em>Figure 9</em>. That takes you to the <a href="https://debezium.io/releases/1.7/">Debezium Release Series</a> page, where you at the bottom of the page have a <em>Downloads</em> button. Click on that button and then right-click on the connector for the product you want (SQL Server Connector Plug-in), and copy the link. The link for the latest development release for the SQL Server Connector Plug-in at the time of me writing this post (August 2021) is:</p>

<pre><code class="language-bash">https://repo1.maven.org/maven2/io/debezium/ \
       debezium-connector-sqlserver/1.7.0.Alpha1/ \
       debezium-connector-sqlserver-1.7.0.Alpha1-plugin.tar.gz
</code></pre>

<p><strong>Code Snippet 8:</strong> <em>Link to Connector Plugin</em></p>

<p>When you look at <em>Code Snippet 8</em> beware of the line continuations <code>\</code>. As the connector is not available via Confluent Hub, how do we get it?</p>

<p>The answer to that is that nothing much changes. We can either install it via the <code>docker-compose.yml</code> file similar to what we did in <em>Code Snippet 5</em> or what we did in <em>Code Snippet 6</em>. The only difference is that we cannot use <code>confluent-hub install</code>, but we have to:</p>

<ul>
<li>download it using <code>wget</code>.</li>
<li>un-tar it into the plugin path.</li>
</ul>

<p>I will do it by building a new image, and the code in <code>Dockerfile</code> for this looks like so:</p>

<pre><code class="language-bash">FROM confluentinc/cp-server-connect-base:6.2.0

RUN wget https://repo1.maven.org/maven2/io/debezium/ \
        debezium-connector-sqlserver/1.7.0.Alpha1/ \
        debezium-connector-sqlserver-1.7.0.Alpha1-plugin.tar.gz \
    &amp;&amp; tar -xvf ./debezium-connector-sqlserver-1.7.0.Alpha1-plugin.tar.gz \
     -C /usr/share/java/
</code></pre>

<p><strong>Code Snippet 9:</strong> <em>Dockerfile Downloading Connector</em></p>

<p>As with <em>Code Snippet 8</em>, beware of the line continuations in <em>Code Snippet 9</em>. In <em>Code Snippet 9</em>, we see how we download it using <code>wget</code> and then un-tar it into the plugin path. Having edited and saved the <code>Dockerfile</code>, we build it like so:</p>

<pre><code class="language-bash">docker build . -t dbz-sql-conn:1.7.Alpha1
</code></pre>

<p><strong>Code Snippet 10:</strong> <em>Building Alpha Release</em></p>

<p>As we see in <em>Code Snippet 10</em>, I have given the image a tag of <code>1.7.Alpha1</code>. To ensure that it works we:</p>

<ul>
<li>replace the <code>dbz-sql-conn</code> image in the <code>docker-compose.yml</code> file with dbz-sql-conn:1.7.Alpha1</li>
<li>tear down the Kafka cluster</li>
<li>spin up the Kafka cluster again.</li>
</ul>

<p>When we use Postman to get installed connectors, we see the following:</p>

<p><img src="/images/posts/kafka-connect-dbz-sql-conn-alpha.png" alt="" /></p>

<p><strong>Figure 10:</strong> <em>Installed Debezium SQL Server Connector 1.7.0</em></p>

<p>We see in <em>Figure 10</em> how we indeed have installed a pre-release version of the connector - yay!</p>

<h2 id="test">Test</h2>

<p>So far, we have seen that the SQL Server connector is installed, but we have not made sure it does what it is supposed to do, i.e. retrieve data into a Kafka topic.</p>

<p>As I mentioned initially, this post is not about SQL Server CDC or configuring the Debezium connector. In any case, let us quickly go over how to test it works. We start with enabling SQL Server, and we use the database and table we created above:</p>

<pre><code class="language-sql">USE DebeziumTest;
GO
-- before we enable CDC ensure the SQL Server Agent is started
-- we need first to enable CDC on the database
EXEC sys.sp_cdc_enable_db;

-- then we can enable CDC on the table
EXEC sys.sp_cdc_enable_table @source_schema = N'dbo',
                               @source_name   = N'tb_CDCTab1',
                               @role_name = NULL,
                               @supports_net_changes = 0;
</code></pre>

<p><strong>Code Snippet 11:</strong> <em>Enabling Database and Table for CDC</em></p>

<p>The code comments in <em>Code Snippet 10</em> should be pretty self-explanatory. The only thing to think about is that the SQL Server Agent needs to be started for this to work.</p>

<p>Having enabled CDC, we now configure and create the connector instance. We do it using Postman, <code>POST</code>:ing to the <code>connectors</code> endpoint:</p>

<p><img src="/images/posts/kafka-connect-dbz-create-connector.png" alt="" /></p>

<p><strong>Figure 11:</strong> <em>Create Connector</em></p>

<p>In <em>Figure 11</em>, we see a straightforward connector configuration. You can find out more about the various configuration properties <a href="https://debezium.io/documentation/reference/connectors/sqlserver.html#sqlserver-example-configuration">here</a>. When looking at the Kafka topics in Control Center after sending the <code>POST</code> request, you see something like so:</p>

<p><img src="/images/posts/kafka-connect-dbz-topics.png" alt="" /></p>

<p><strong>Figure 12:</strong> <em>Debezium Topics</em></p>

<p>We see in <em>Figure 12</em> how two new topics were automatically created when we created the connector. These topics are Debezium specific topics, and you as a user would not do much with them. When you look at the topics in Kafka at this stage, you do not see any topic related to the table we want to stream data from. That changes as soon as you insert some data into the table:</p>

<pre><code class="language-sql">USE DebeziumTest;
GO

INSERT INTO dbo.tb_CDCTab1(Col1, Col2)
VALUES(1, 'Hello Number 1')
</code></pre>

<p><strong>Code Snippet 12:</strong> <em>Ingest Data</em></p>

<p>When you refresh the Topics page in Control Center after executing the code in <em>Code Snippet 12</em>, you see this:</p>

<p><img src="/images/posts/kafka-connect-dbz-table-topic.png" alt="" /></p>

<p><strong>Figure 13:</strong> <em>Table Topic</em></p>

<p>Cool, in <em>Figure 13</em>, outlined in red, we see a new topic. This is the topic for the table we want to stream data from into Kafka. If you now were to look at the messages in the topic, you would see the data we just inserted. I leave that for you, my dear readers, to do on your own. We have now confirmed that everything works!</p>

<h2 id="summary">Summary</h2>

<p>In this post, I describe what to do if you want to run the Debezium SQL Server connector in a Docker environment.</p>

<p>We started by looking at what Kafka Connect and Debezium is. We said that:</p>

<ul>
<li>Kafka Connect is a JVM process allowing us to stream data between Apache Kafka and other systems. It works by the use of connectors which are <code>.jar</code> files. There are two types of connectors:

<ul>
<li>source connectors that understand how to retrieve data from a system and publish it to Kafka.</li>
<li>sink connectors that read data from Kafka topics and ingests that data into target systems.</li>
</ul></li>
<li>Debezium is a distributed platform with connectors for a multitude of database systems. It uses the underlying system&rsquo;s CDC functionality to capture database changes and publish those changes to topics in Kafka.</li>
</ul>

<p>We then looked at how to deploy a Debezium connector, more specifically the SQL Server connector, to a Kafka Connect Docker container. We saw there are two main ways to get a connector into the Kafka Connect container, and both ways use the Confluent Hub client:</p>

<ul>
<li>In the <code>command</code> option for the Kafka Connect container, run the install command.</li>
<li>Create your own image, and in the <code>Dockerfile</code> file, run the install command.</li>
</ul>

<p>That was about it. Once again, this post was not about how CDC works or the various Debezium configuration options. That may be covered in a future post.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 31, 2021]]></title>
    <link href="https://nielsberglund.com/2021/08/01/interesting-stuff---week-31-2021/" rel="alternate" type="text/html"/>
    <updated>2021-08-01T07:40:28+02:00</updated>
    <id>https://nielsberglund.com/2021/08/01/interesting-stuff---week-31-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/articles/saga-orchestration-outbox/">Saga Orchestration for Microservices Using the Outbox Pattern</a>. The last few weeks, at <a href="/derivco">Derivco</a>, I have been ~playing around~ researching the use of CDC, Debezium and the outbox pattern (a blog post or two may come soon). I&rsquo;ve been looking at it in relation to publishing events from the database. It was then interesting to come across this post discussing CDC and Debezium and how these technologies combined can be used for implementing the SAGA pattern. Very cool!</li>
</ul>

<h2 id="data-architecture">Data Architecture</h2>

<ul>
<li><a href="https://netflixtechblog.com/data-movement-in-netflix-studio-via-data-mesh-3fddcceb1059">Data Movement in Netflix Studio via Data Mesh</a>. I have previously covered posts discussing Data Mesh. In this post, Netflix talks about their Data Mesh. Data Mesh, in this context, is a fully managed, streaming data pipeline product used for enabling Change Data Capture (CDC) use cases. The post is very informative, and there are quite a few concepts worth investigating!</li>
</ul>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://eng.uber.com/pinot-real-time-ingestion/">Pinot Real-Time Ingestion with Cloud Segment Storage</a>. This post, by Uber, discusses how Uber added a deep store to Pinot&rsquo;s real-time ingestion protocol.</li>
<li><a href="https://towardsdatascience.com/getting-started-with-azure-data-explorer-and-azure-synapse-analytics-for-big-data-processing-25500821e370">Getting started with Azure Data Explorer and Azure Synapse Analytics for Big Data processing</a>. Azure Data Explorer is a fully managed data analytics service that can handle large volumes of diverse data from any data source, such as websites, applications, IoT devices, and more. This post looks at leveraging integration between Azure Data Explorer and Azure Synapse for processing data with Apache Spark.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/designing-an-elastic-apache-kafka-for-the-cloud/">Making Apache Kafka Serverless: Lessons From Confluent Cloud</a>. From a developers perspective, serverless in the cloud is awesome and easy to use. However, the system designer and the engineer who has to design and implement a serverless system have challenges. This post starts with looking at the confluent cloud architecture and then dives into how some of the difficulties mentioned above have been overcome.</li>
<li><a href="https://www.confluent.io/blog/from-apache-kafka-to-confluent-cloud-optimizing-for-speed-scale-storage/">Speed, Scale, Storage: Our Journey from Apache Kafka to Performance in Confluent Cloud</a>. Hmm, Confluent Cloud seemed popular this week. This post looks at optimizing Apache Kafka for Confluent Cloud. Even if you are not interested in the cloud, the post is full of good advice and best practices. Oh, and I have to look at the test framework mentioned in the post: <a href="https://github.com/apache/kafka/blob/db3e5e2c0de367ffcfe4078359d6d208ba722581/TROGDOR.md">Trogdor</a>.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 30, 2021]]></title>
    <link href="https://nielsberglund.com/2021/07/25/interesting-stuff---week-30-2021/" rel="alternate" type="text/html"/>
    <updated>2021-07-25T09:21:37+02:00</updated>
    <id>https://nielsberglund.com/2021/07/25/interesting-stuff---week-30-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="data-architecture">Data Architecture</h2>

<ul>
<li><a href="https://medium.com/datahub-project/enabling-data-discovery-in-a-data-mesh-the-saxo-journey-451b06969c8f">Enabling Data Discovery in a Data Mesh: The Saxo Journey</a>. In the <a href="/2021/06/27/interesting-stuff---week-26-2021/">roundup for week 26</a>, I wrote about how Saxo Bank in Denmark has moved to a domain-driven data architecture with Kafka as an integral part and the lessons learned/best practices. This post is a follow-up, and in this post, Saxo Bank writes about its data infrastructure with an in-house central data management application and how Saxo Bank approaches and solves data inconsistency issues. As with the previous article, this is a must-read!<br /></li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://medium.com/event-driven-utopia/the-duality-of-streams-and-tables-why-it-matters-ed9bb17e7505">The Duality of Streams and Tables - Why It Matters?</a>. When you read articles, blogs about Kafka and stream processing, you may have come across the statement about the duality between streams and tables. When I have read about it, I must admit that I have not really &ldquo;grasped&rdquo; it, that is, until now. The post linked to explains in a way that even I can understand the concepts behind the stream-table duality. Good job!</li>
<li><a href="https://www.confluent.io/blog/ksqldb-0-19-adds-data-modeling-foreign-key-joins/">Announcing ksqlDB 0.19.0</a>. In the <a href="/2021/07/04/interesting-stuff---week-27-2021/">roundup three weeks ago</a> I wrote about the new functionality in KStreams - the foreign-key join, and how I looked forward to it appearing in ksqlDB soon. Well, it must be Christmas, as my wish has come through. The post I link to announces ksqlDB 0.19, and one of the new features in the release is the foreign-key join. How awesome is that?!</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 29, 2021]]></title>
    <link href="https://nielsberglund.com/2021/07/18/interesting-stuff---week-29-2021/" rel="alternate" type="text/html"/>
    <updated>2021-07-18T14:05:36+02:00</updated>
    <id>https://nielsberglund.com/2021/07/18/interesting-stuff---week-29-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p>Speaking about the week just ending - wow, what a week! As you may know or not, I live in Durban, South Africa. Durban is in the province of KZN (KwaZulu-Natal), home to Jacob Zuma, ex-president of SA. On July 8, Jacob Zuma was put in jail after being found guilty of contempt of court. The jailing of Jacob Zuma led to protests that initially were peaceful. However, this changed Sunday, July 11, when small scale rioting broke out in and around Durban. On Monday (July 12), it escalated to full-scale rioting, looting, and factories burning.</p>

<p>The police were not prepared for the scale of the riots and could not offer any form of protection and help, so the local communities took it upon themselves to protect the local areas. Roadblocks were put up to prevent rioters from entering the neighbourhoods. The roadblocks were manned <sup>24</sup>&frasl;<sub>7</sub> by people from the community, and we did nighttime patrols to show presence and act as a deterrent. So I have done night patrols from last Monday until Friday night.</p>

<p>What we did seem to have helped, and fortunately we had no issues in the area I live in. I must say however, that it is a bit frightening when you are out in the middle of the night, and you hear gunshots some hundred meters away!</p>

<p>That&rsquo;s enough of my &ldquo;adventures&rdquo;, let&rsquo;s get back to what this post is all about.</p>

<p></p>

<h2 id="sql-server">SQL Server</h2>

<ul>
<li><a href="https://azure.microsoft.com/en-us/blog/accelerate-big-data-analytics-with-the-spark-30-connector-for-sql-server-now-generally-available/">Accelerate big data analytics with the Spark 3.0 connector for SQL Server—now generally available</a>. The title of this post says it all; the Apache Spark 3.0 connector is now available. The connector is a high-performance connector that enables you to use transactional data in big data analytics, and persists results for ad-hoc queries or reporting.</li>
</ul>

<h2 id="big-data">Big data</h2>

<ul>
<li><a href="https://towardsdatascience.com/identity-keyrings-201d17295954">Identity keyrings</a>. In an enterprise various parts of the business may have different identifiers for the same entity, and correlating these different identities may be difficult. The post linked to looks at how we can use Azure Data Explorer to build an identity keyring that brings together the various identifier. This is a very interesting read, showing what Azure Data Explorer can do!</li>
<li><a href="https://medium.com/pinterest-engineering/interactive-querying-with-apache-spark-sql-at-pinterest-2a3eaf60ac1b">Interactive Querying with Apache Spark SQL at Pinterest</a>. This post looks at how Pinterest built a scalable, reliable, and efficient interactive querying platform that processes hundreds of petabytes of data daily with Apache Spark SQL. Very, very cool!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://medium.com/pinterest-engineering/unified-flink-source-at-pinterest-streaming-data-processing-c9d4e89f2ed6">Unified Flink Source at Pinterest: Streaming Data Processing</a>. The post here is another post by Pinterest. In the post they drill down and look at how they use Apache Spark for stream processing.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>Above I wrote about the problems we had in Durban last week. I wrote about doing night patrols. Well, it looks like we need to start them up again, as rumours are floating around about new attacks. So, back to night shifts, <em>sad-face</em>!</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 28, 2021]]></title>
    <link href="https://nielsberglund.com/2021/07/11/interesting-stuff---week-28-2021/" rel="alternate" type="text/html"/>
    <updated>2021-07-11T09:07:39+02:00</updated>
    <id>https://nielsberglund.com/2021/07/11/interesting-stuff---week-28-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="sql-server">SQL Server</h2>

<ul>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2021/07/08/whats-new-with-sql-server-big-data-clusters-cu11-release/">What&rsquo;s new with SQL Server Big Data Clusters—CU11 Release</a>. As the title implies, this blog post announces the latest release of SQL Server Big Data Cluster (BDC). Read the post to learn more about new security features as well as PolyBase enhancements.</li>
</ul>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://arxiv.org/pdf/2104.00087.pdf">Real-time Data Infrastructure at Uber</a>. For followers of my blog, it should not come as a surprise that I like both Kafka and Apache Pinot. The link here is to a white paper around Uber looking at how Uber uses, among other tech, Kafka and Pinot as a foundation for their real-time data infrastructure. It is an awesome and very informative read!</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>As I have mentioned previously, I am doing both a training class and a conference session at the <a href="https://dataplatformgeeks.com/dps2021/"><strong>2021 Data Platform Summit</strong></a>. Related to that, I had the opportunity to pen down a new article in the <a href="https://www.sqlservergeeks.com/magazine/"><strong>SQLServerGeeks Magazine</strong></a>, where I wrote about <strong>SQL Server 2019 Big Data Cluster</strong>. The magazine has good content overall for the SQL professionals and contains a lot of really cool material. The best part of the magazine is that it is free for the community! <a href="https://www.sqlservergeeks.com/magazine/">Download</a> your copy now.</p>

<p>Oh, and speaking of speaking:</p>

<p><img src="/images/posts/bdc.jpg" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Webinar</em></p>

<p>As you see in <em>Figure 1</em> above, I am delivering a free session at DataPlatformGeeks Webinars about SQL Server and how it can be the central hub for all your data. Do join me! Oh, you ask where to register for the webinar? You register <a href="https://bit.ly/DataPlatformGeeks_Events">here</a>!</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 27, 2021]]></title>
    <link href="https://nielsberglund.com/2021/07/04/interesting-stuff---week-27-2021/" rel="alternate" type="text/html"/>
    <updated>2021-07-04T09:04:04+02:00</updated>
    <id>https://nielsberglund.com/2021/07/04/interesting-stuff---week-27-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="sql-server">SQL Server</h2>

<ul>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2021/06/30/looking-to-the-future-for-r-in-azure-sql-and-sql-server/">Looking to the future for R in Azure SQL and SQL Server</a>. If you follow my blog, you know that I have written a lot about <a href="https://nielsberglund.com/categories/sql-server-machine-learning-services/">SQL Server Machine Learning Services</a> (SQLML) and R in SQL Server throughout the last couple of years. The post linked to lays out the plans for R in SQL Server in upcoming SQL Server versions. The short version of the post is that Microsoft will go away from the proprietary R and Python packages in SQLML in favour of the open-source versions. If you are interested and want more than what is in the post, my good friend <a href="https://tecflix.com/instructors/rafal-lukawiecki">Rafal Lukawiecki</a> has written an <a href="https://tecflix.com/news/microsoft-open-sources-sql-server-machine-learning-and-discontinues-ml-server">excellent post</a> explaining in detail the changes.</li>
</ul>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://nemeiskiy-lef.medium.com/implementing-distributed-transaction-in-net-using-saga-pattern-1641172c122">Implementing distributed transaction in .NET using Saga pattern</a>. One of the biggest issues when moving from a monolithic system to a distributed microservices system is handling transactions. One solution to distributed transactions in a microservices system is using the <a href="https://microservices.io/patterns/data/saga.html">Saga</a> pattern. In this post, the author does an excellent job explaining the Saga pattern and how to implement it in .NET.</li>
</ul>

<h2 id="machine-learning">Machine Learning</h2>

<ul>
<li><a href="https://eng.uber.com/continuous-integration-deployment-ml/">Continuous Integration and Deployment for Machine Learning Online Serving and Models</a>. This post from Uber looks at how they implement CI/CD and model serving in their environment. This is a must-read if you are in the ML world!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/easily-manage-database-migrations-with-evolving-schemas-in-ksqldb/">Online, Managed Schema Evolution with ksqlDB Migrations</a>. In the database world, managing changes to the schema is (somewhat) easy. Well, at least you probably have some workflows for that. In the streaming world, it may not be that &ldquo;straightforward&rdquo;. In this post, the author looks at the tooling available for managing schema evolution in ksqlDB.</li>
<li><a href="https://itnext.io/eventual-consistency-with-spring-for-apache-kafka-cfbbed450b5e">Eventual Consistency with Spring for Apache Kafka: Part 1 of 2</a>. This post looks at how Spring for Kafka is used to manage a distributed data model across multiple microservices. You find <a href="https://itnext.io/eventual-consistency-with-spring-for-apache-kafka-part-2-of-2-23bedd512ccf">Part 2 here</a>.</li>
<li><a href="https://www.confluent.io/blog/data-enrichment-with-kafka-streams-foreign-key-joins/">Crossing the Streams: The New Streaming Foreign-Key Join Feature in Kafka Streams</a>. In relational databases, you more often than not have multiple one-to-many relationships (foreign keys). This is not well supported in KTables and streams in Kafka. At least it was not until Kafka 2.4, where non-key joining between KTables was introduced. The post I link to looks more in detail at how foreign-key joins are implemented in KStreams. This is only available in KStreams, but according to the post, we may expect to see it in the next release of ksqlDB; 0.19 - Yay!</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 26, 2021]]></title>
    <link href="https://nielsberglund.com/2021/06/27/interesting-stuff---week-26-2021/" rel="alternate" type="text/html"/>
    <updated>2021-06-27T09:21:01+02:00</updated>
    <id>https://nielsberglund.com/2021/06/27/interesting-stuff---week-26-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="machine-learning-ai">Machine Learning / AI</h2>

<ul>
<li><a href="https://databricks.com/blog/2021/06/23/need-for-data-centric-ml-platforms.html">Need for Data-centric ML Platforms</a>. When we do ML projects, it comes naturally to have a model-centric approach to the project. This Databricks blog post argues that having a data-centric view increases the chances for success. Very interesting!</li>
</ul>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://www.sqlshack.com/azure-data-explorer-and-the-kusto-query-language/">Azure Data Explorer and the Kusto Query Language</a>. This blog post is part one of a two-part series looking at Azure Data Explorer (ADX) and Kusto; its query language. This post introduces ADX and looks briefly into what you can do with Kusto. After having read this post, I look forward to part two!</li>
<li><a href="https://techcommunity.microsoft.com/t5/azure-data-explorer/update-on-geospatial-functions/ba-p/2446350">Update on geospatial functions</a>. In last weeks <a href="/2021/06/20/interesting-stuff---week-25-2021/">roundup</a>, I mentioned a post about geospatial functions in Apache Pinot. The post I link to is also about geospatial functions, but this time in Azure Data Explorer. Man, you can do some cool stuff with geospatial queries!</li>
<li><a href="https://databricks.com/blog/2021/06/22/get-your-free-copy-of-delta-lake-the-definitive-guide-early-release.html">Get Your Free Copy of Delta Lake: The Definitive Guide (Early Release)</a>. Databricks has for a long time been talking about the Lakehouse architecture and how their Delta Lake table format can help create Lakehouses. This post announces the first book about Delta Lake and Lakehouses. I have already downloaded the book, and I suggest you do the same.</li>
</ul>

<h2 id="streaming-data-architecture">Streaming / Data Architecture</h2>

<ul>
<li><a href="https://www.confluent.io/blog/distributed-domain-driven-architecture-data-mesh-best-practices/">Saxo Bank&rsquo;s Best Practices for a Distributed Domain-Driven Architecture Founded on the Data Mesh</a>. I have covered posts about the Data Mesh in quite a few previous roundups, and this post is another one around data meshes. It looks at how Saxo Bank in Denmark has moved to a domain-driven data architecture with Kafka as an integral part and the lessons learned/best practices. The post also contains very, very useful references! The post is a must-read if you are interested in data architecture!</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p><img src="/images/posts/Neils_Berglund_IAmSpeaking.jpg" alt="" /></p>

<p><strong>Figure 1:</strong> <em>I Am Speaking</em></p>

<p>As I mentioned in the <a href="/2021/06/13/interesting-stuff---week-24-2021/">roundup</a> for week 24, I am speaking about Apache Kafka and Azure Data Explorer at the <a href="https://dataplatformgeeks.com/dps2021/"><strong>2021 Data Platform Summit</strong></a>. So right now, I am researching and prepping for that.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
</feed>

