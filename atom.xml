<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Niels Berglund</title>
  <link href="https://nielsberglund.com/atom.xml" rel="self"/>
  <link href="https://nielsberglund.com"/>
  <updated>2021-02-14T07:28:01+02:00</updated>
  <id>https://nielsberglund.com/</id>
  <generator uri="http://gohugo.io/">Hugo</generator>

  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 7, 2021]]></title>
    <link href="https://nielsberglund.com/2021/02/14/interesting-stuff---week-7-2021/" rel="alternate" type="text/html"/>
    <updated>2021-02-14T07:28:01+02:00</updated>
    <id>https://nielsberglund.com/2021/02/14/interesting-stuff---week-7-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="data-architecture">Data Architecture</h2>

<ul>
<li><a href="https://alibaba-cloud.medium.com/data-lake-concepts-characteristics-architecture-and-case-studies-28be1b265624">Data Lake: Concepts, Characteristics, Architecture, and Case Studies</a>. This is a long post where the authors try to explain what a Data Lake is, characteristics of a Data Lake, the architecture of a Data Lake, and a lot more. It is an excellent read!</li>
</ul>

<h2 id="big-data-analytics">Big Data Analytics</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/low-latency-high-throughput/">Building Latency Sensitive User Facing Analytics via Apache Pinot</a>. In this <a href="https://www.infoq.com/">InfoQ</a> presentation, the presenter discusses how LinkedIn, Uber and other companies managed to have low latency for analytical database queries despite high throughput.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/introducing-confluent-platform-6-1/">Introducing Confluent Platform 6.1</a>. The post linked to here announces, as the title implies, the 6.1 version of Confluent Platform. There are quite a few new exciting features in this release. What excites me the most are the enhanced functionality of ksqlDB! That is something that will help us at <a href="/derivco">Derivco</a> a lot.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>While writing my blog posts about the open-sourced Python SQL Server language extension, I wanted to install SQL Server on a new, clean server. I decided to do it on an Azure VM instead of &ldquo;messing&rdquo; with VM&rsquo;s on my box.</p>

<p>It was not as straight forward as I thought, so I am now about to finish a post about what I did.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 6, 2021]]></title>
    <link href="https://nielsberglund.com/2021/02/07/interesting-stuff---week-6-2021/" rel="alternate" type="text/html"/>
    <updated>2021-02-07T13:05:26+02:00</updated>
    <id>https://nielsberglund.com/2021/02/07/interesting-stuff---week-6-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://medium.com/apache-pinot-developer-blog/real-time-analytics-with-presto-and-apache-pinot-part-i-cc672caea307">Real-time Analytics with Presto and Apache Pinot â€” Part I</a>. I have written in previous roundups about Presto, which is now called Trino, and Apache Pinot. The blog post linked to here is the first in a two part series about how to use Presto and Pinot together. The second part is <a href="https://medium.com/apache-pinot-developer-blog/real-time-analytics-with-presto-and-apache-pinot-part-ii-3d09ff937713">here</a>.</li>
</ul>

<h2 id="data-architecture">Data Architecture</h2>

<ul>
<li><a href="https://towardsdatascience.com/the-3-things-to-keep-in-mind-while-building-the-modern-data-stack-5d076743b33a">The 3 Things to Keep in Mind While Building the Modern Data Stack</a>. Building a data stack, never mind a modern data stack, can be confusing and complicated. This post proposes a simplified framework for creating the stack. The post looks at a conceptual model to help us when we pick the tools for the stack. I found the post very informative!</li>
<li><a href="https://towardsdatascience.com/what-is-data-mesh-and-should-you-mesh-it-up-too-364b28fe2ae9">What Is Data Mesh? And Should You Mesh It Up Too?</a>. Recently I have mentioned data meshes quite a lot. Here is another post about data meshes. It looks at what a Data Mesh is, and why more and more companies are looking to implement them.</li>
<li><a href="https://databricks.com/blog/2021/02/04/how-lakehouses-solve-common-issues-with-data-warehouses.html">How Lakehouses Solve Common Issues With Data Warehouses</a>. In <a href="/2021/01/31/interesting-stuff---week-5-2021/">last weeks roundup</a> I linked to a video about data Lakehouses. The post I link to here is the first in a series about Lakehouses, and it is based on <a href="http://cidrdb.org/cidr2021/papers/cidr2021_paper17.pdf">this white-paper</a>. I am certainly looking forward to the other posts in the series.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/consume-avro-data-from-kafka-topics-and-secured-schema-registry-with-databricks-confluent-cloud-on-azure/">Consuming Avro Data from Apache Kafka Topics and Schema Registry with Databricks and Confluent Cloud on Azure</a>. Last <a href="/2021/01/31/interesting-stuff---week-5-2021/">week</a> I posted a <a href="https://azure.microsoft.com/en-us/blog/introducing-seamless-integration-between-microsoft-azure-and-confluent-cloud/">link</a> about integration between Confluent Cloud and Microsoft Azure. I wrote that I hoped to see blog posts from the Confluent guys, (and girls), where they do &ldquo;cool stuff&rdquo; on Azure and not only AWS and Google Cloud. Well ask, and you shall be given! The post linked to here discusses how to configure Azure Databricks to interact with Confluent Cloud so that you can ingest, process, store, make real-time predictions and gain business insights from your data.</li>
<li><a href="https://joshua-robinson.medium.com/simplify-kafka-at-scale-with-confluent-tiered-storage-ae8c1a2c9c80">Simplify Kafka at Scale with Confluent Tiered Storage</a>. In October 2020, Confluent announced Confluent Platform 6.0, and how one of the new features was <a href="https://www.confluent.io/blog/confluent-platform-6-0-delivers-the-most-powerful-event-streaming-platform-to-date/#tired-storage">tiered storage</a>. This post looks at how tiered storage works, how to set it up, and performance implications. Very interesting!</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 5, 2021]]></title>
    <link href="https://nielsberglund.com/2021/01/31/interesting-stuff---week-5-2021/" rel="alternate" type="text/html"/>
    <updated>2021-01-31T12:37:43+02:00</updated>
    <id>https://nielsberglund.com/2021/01/31/interesting-stuff---week-5-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://www.youtube.com/watch?v=T70jTTYhYyM">Intro to Apache Pinot</a>. In <a href="/2021/01/24/interesting-stuff---week-4-2021/">last weeks roundup</a>, I posted a video link about doing real-time analytics using Apache Pinto and Kafka. What I have linked to here is to an awesome video introducing what Pinot is. If you are interested, it is a must-see!</li>
<li><a href="https://www.youtube.com/watch?v=RU2dXoVU8hY">Lakehouse: A New Generation of Open Platforms that Unify Data Warehousing and Advanced Analytics</a>. In some of the previous roundups I have written about Data Meshes, and how the Data Mesh is a hot topic today in the Big Data world. The video I have linked to here discusses another hot topic: the Lakehouse architecture. A Lakehouse is a data management system based on lowcost and directly-accessible storage that also provides traditional analytical DBMS management and performance features.</li>
<li><a href="https://medium.com/expedia-group-tech/a-short-introduction-to-apache-iceberg-d34f628b6799">A Short Introduction to Apache Iceberg</a>. Part of the Lakehouse architecture is the table format. The table format allows for ACID transaction capability as well as data versioning, etc. Some table formats out there are Databricks Delta Lake, Apache Hudi, and Apache Iceberg. The post linked to here looks at Apache Iceberg, and what we can do with it.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://azure.microsoft.com/en-us/blog/introducing-seamless-integration-between-microsoft-azure-and-confluent-cloud/">Introducing seamless integration between Microsoft Azure and Confluent Cloud</a>. Well, I guess the title says it all! We finally have a transparent integration between Azure and Confluent Cloud. Hopefully, we&rsquo;ll now start to see posts from the Confluent guys, (and girls), where they do &ldquo;cool stuff&rdquo; on Azure and not only AWS and Google Cloud.</li>
<li><a href="https://www.youtube.com/watch?v=wjEYH41nvBM">Streaming Machine Learning with Apache Kafka and without another Data Lake by Kai Waehner</a>. Usually, when we do Machine Learning, both training and inference, we use a data lake - perhaps even a Lakehouse as mentioned above. But it&rsquo;s possible to avoid such a data store altogether, using an event streaming architecture. The video linked to explains how this can be achieved leveraging Apache Kafka, Tiered Storage and TensorFlow.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 4, 2021]]></title>
    <link href="https://nielsberglund.com/2021/01/24/interesting-stuff---week-4-2021/" rel="alternate" type="text/html"/>
    <updated>2021-01-24T09:42:57+02:00</updated>
    <id>https://nielsberglund.com/2021/01/24/interesting-stuff---week-4-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="data-architecture">Data Architecture</h2>

<ul>
<li><a href="https://medium.com/codex/how-to-build-a-modern-data-lake-with-minio-db0455eec053">How to Build a Modern Data Lake with MinIO</a>. This is a very &ldquo;cool&rdquo; post looking at creating a &ldquo;poor man&rsquo;s data lake&rdquo;, by using open source technologies. In this case the technologies used are <a href="https://min.io/"><strong>MinIO</strong></a>, and <a href="https://trino.io/"><strong>Trino</strong></a>. MinIO is an object store compatible with S3, and Trino is a distributed SQL query engine, (formerly known as Presto). As I said, a very interesting post! See below for a follow-up post.</li>
<li><a href="https://medium.com/codex/modern-data-platform-using-open-source-technologies-212ba8273eab">Modern Data Platform using Open Source Technologies</a>. This is the follow-up post, mentioned above. This post gives an overview of Trino and MinIO, and it also touches upon some features that they offer when implemented together as a data platform.</li>
<li><a href="https://www.dataengineeringweekly.com/p/data-engineering-weekly-21-metadata">Data Engineering Weekly #21: Metadata Edition</a>. This particular post is from the <a href="https://www.dataengineeringweekly.com/">Data Engineering Weekly</a> newsletter. This edition focuses on recent breakthroughs in metadata management. Very interesting! Oh, and do yourself a favor and subscribe to the newsletter!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/best-kafka-tools-that-boost-developer-productivity/">Helpful Tools for Apache Kafka Developers</a>. What the title says; this post looks at some useful tools for Kafka developers. At <a href="/derivco">Derivco</a> we are using some of these, and the <strong>Kafka Streams Topology Visualizer</strong> is a particular favorite.</li>
<li><a href="https://eng.uber.com/gairos-scalability/">Uber&rsquo;s Real-time Data Intelligence Platform At Scale: Improving Gairos Scalability/Reliability</a>. Gairos is Uber&rsquo;s real-time data processing, storage, and querying platform. This post gives an overview of Gairos and what is done to ensure scalability and reliability. Cool stuff!</li>
<li><a href="https://zoom.us/rec/play/iXyDwNqRjmKQTp7MKkYPp8fiBvW-z84PmDlkkXldu26xMzjuxE7jaAJOvKjF3L1WRHHpXakwp6-ISB8.CVQAVTpF7RWtwneQ?continueMode=true&amp;_x_zm_rtaid=YAm9SyaCSZOQRlCQ3LgfEw.1611288712092.8aa0dac1ffc00a955a58260d99c4945e&amp;_x_zm_rhtaid=745">Using Kafka and Pinot for Real-Time, User-Facing Analytics</a>. This video looks at how <a href="https://pinot.apache.org/">Apache Pinot</a>, and Apache Kafka can work together and enable real-time analytics.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<ul>
<li><a href="/2021/01/24/sql-server-2019-external-libraries-and-your-python-runtime/">SQL Server 2019 External Libraries and Your Python Runtime</a>. I managed to publish this post that I have mentioned in the last couple of weeks roundups. In the post, we look at how we can create external libraries for our Python external language.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SQL Server 2019 External Libraries and Your Python Runtime]]></title>
    <link href="https://nielsberglund.com/2021/01/24/sql-server-2019-external-libraries-and-your-python-runtime/" rel="alternate" type="text/html"/>
    <updated>2021-01-24T06:06:25+02:00</updated>
    <id>https://nielsberglund.com/2021/01/24/sql-server-2019-external-libraries-and-your-python-runtime/</id>
    <content type="html"><![CDATA[<p>The last month or so I have written some blog posts about how Microsoft open-sourced the SQL Server language extensions for R and Python back in September 2020. These language extensions add to the Java extension which was open-sourced in March 2020. My posts have been about bringing your own Python runtime into SQL Server 2019, and the potential pitfalls you may encounter:</p>

<ul>
<li><a href="/2020/12/29/updated-bring-your-own-r--python-runtimes-to-sql-server-extensibility-framework/"><strong>Bring Your Own R &amp; Python Runtimes to SQL Server Extensibility Framework</strong></a></li>
<li><a href="/2021/01/05/updated-write-a-python-3.9-language-extension-for-sql-server-machine-learning-services/"><strong>Write a Python 3.9 Language Extension for SQL Server Machine Learning Services</strong></a></li>
<li><a href="/2021/01/09/solve-python-issues-in-sql-server-machine-learning-services-after-deploying-python-3.9/"><strong>Solve Python Issues in SQL Server Machine Learning Services After Deploying Python 3.9</strong></a></li>
</ul>

<p>I have touched upon the subject of the <code>PYTHONHOME</code> environment variable in the posts, and I have said that it is not needed - <strong>UNLESS</strong> you want to create Python external libraries.</p>

<p>In this post, we look more at <code>PYTHONHOME</code> and creating external libraries.</p>

<p></p>

<h2 id="pre-reqs">Pre-reqs</h2>

<p>I will not list the pre-reqs here, as all the other posts list them.</p>

<p>I just want to mention something important about installing Python on the SQL Server box:</p>

<ul>
<li>run the Python installation as Administrator.</li>
<li>install Python for all users.</li>
</ul>

<p>The reason for mentioning this is that you may get some strange errors when executing if you do not do that.</p>

<h2 id="demo-code">Demo Code</h2>

<p>The starting point for this post is that we have a Python external language created as per:</p>

<ul>
<li>the end of the <a href="/2020/12/29/updated-bring-your-own-r--python-runtimes-to-sql-server-extensibility-framework/"><strong>Bring Your Own R &amp; Python Runtimes to SQL Server Extensibility Framework</strong></a>, in which case it is Python 3.7.</li>
<li>or the Python externa language is based on the <a href="/2021/01/05/updated-write-a-python-3.9-language-extension-for-sql-server-machine-learning-services/"><strong>Write a Python 3.9 Language Extension for SQL Server Machine Learning Services</strong></a> post, where the language is Python 3.9.</li>
</ul>

<p>Furthermore, the expectation is that the language has been created without the <code>PYTHONHOME</code> system environment variable. In this post, I am running against the extension built in the <a href="/2021/01/05/updated-write-a-python-3.9-language-extension-for-sql-server-machine-learning-services/">Python 3.9 post</a>.</p>

<p>As in the other posts, I have created the <code>ExtLangDB</code> database. This is the database the external language above is created in. I also use the &ldquo;built-in&rdquo; Python in SQL Server Machine Learning Services in this database.</p>

<p>Now is a good time to see that everything is set up correctly. To do that we use the same code as in the other posts:</p>

<pre><code class="language-sql">EXEC sp_execute_external_script
@language =N'&lt;lang&gt;',
@script=N'
import pandas as pd
import sys
df = pd.DataFrame(columns=[&quot;Version&quot;])
ver = sys.version
pth = sys.executable
df = df.append({&quot;Version&quot;: ver}, ignore_index=True)
OutputDataSet = df'
WITH RESULT SETS (([Python Version] nvarchar(256)));
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Test Code</em></p>

<p>Run the code in <em>Code Snippet 1</em>, and replace the <code>&lt;lang&gt;</code> placeholder first with <code>Python</code>, and then with the name you gave the external Python language. In my case, it is <code>p39</code>. Both times you run the code, all should work fine.</p>

<h2 id="installing-packages-modules">Installing Packages/Modules</h2>

<p>In R and Python, you install packages/modules typically by being on the machine in question and executing R/Python code:</p>

<pre><code class="language-bash"># R
install.packages(&quot;&lt;some-package&gt;&quot;)

# Python
python.exe -m pip install &lt;some-module&gt;
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Install Packages/Modules</em></p>

<p>In <em>Code Snippet 2</em>, we see one way to install an R package and a Python module.</p>

<p>What about doing if for SQL Server? Sure you can do it that way. However, you would need access to the SQL Server box, and those &ldquo;pesky&rdquo; administrators may not want to give you access to the box the production SQL Server sits on. What could possibly go wrong?!</p>

<p>So, apart from being on the actual SQL Server box itself, we can install packages by connecting to SQL Server from a remote machine and execute. The different ways are:</p>

<ul>
<li>RevoScale: a Microsoft proprietary technology for the Microsoft R and Python SQL Server Machine Learning services. It is exposed via <code>RevoScaleR</code> for R and <code>RevoScalePy</code> for Python. Back in 2018, I wrote about using RevoScaleR for R package installation in the post <a href="/2018/06/30/installing-r-packages-in-sql-server-machine-learning-services---ii/"><strong>Installing R Packages in SQL Server Machine Learning Services - II</strong></a>. You execute using an R or Python client.</li>
<li><a href="https://docs.microsoft.com/en-us/sql/machine-learning/package-management/install-additional-python-packages-on-sql-server?view=sql-server-ver15"><code>sqlmlutils</code></a>: a package which helps you execute R/Python code in SQL Server from any R/Python client.</li>
<li><a href="https://docs.microsoft.com/en-us/sql/t-sql/statements/create-external-library-transact-sql?view=sql-server-ver15"><code>CREATE EXTERNAL LIBRARY</code></a>: this installs an R package or a Python module using T-SQL DDL statements.</li>
</ul>

<p>The ability to using <code>CREATE EXTERNAL LIBRARY</code> for Python was introduced in SQL Server 2019.</p>

<h2 id="external-libraries">External Libraries</h2>

<p>An external library is a package, module, jar file deployed to a specific database using the <code>CREATE EXTERNAL LIBRARY</code> DDL statement. The library is stored in the database and the system DMV <code>sys.external_libraries</code> exposes the installed libraries. The signature for the statement looks like so:</p>

<pre><code class="language-sql">CREATE EXTERNAL LIBRARY library_name  
[ AUTHORIZATION owner_name ]  
FROM (CONTENT = { &lt;file_spec&gt; }  
    [, PLATFORM = &lt;platform&gt; ]) 
WITH ( LANGUAGE = '&lt;language&gt;' )  
[ ; ] 
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Signature CREATE EXTERNAL LIBARY</em></p>

<p>The arguments we see in <em>Code Snippet 3</em> are:</p>

<ul>
<li><code>library_name</code>: A unique name for the package. When I say the package name has to be unique, the unique:ness is based on the name and the principal id under which it is created.</li>
<li><code>owner_name</code>: This optional parameter specifies the name of the user or role that owns the external library.</li>
<li><code>file_spec</code>: The <code>file_spec</code> specifies the package&rsquo;s content for a specific platform, and it can either be in the form of a file location (local path/network path) or a hex literal. If <code>file_spec</code> is a path, then the path needs to be readable by SQL Server. For R/Python and any languages created from the R/Python language extensions, the package/module needs to be inside a zipped archive file.</li>
<li><code>platform</code>: The <code>PLATFORM</code> parameter, which defines the platform for the content of the library. The <code>PLATFORM</code> parameter defaults to the platform SQL Server runs on, (Windows or Linux).</li>
<li><code>language</code>: Specifies the language of the package. This is either R or Python for the SQL Server machine learning languages or the name you created your external language as.</li>
</ul>

<p>Let us see how this works. What we will do is download a Python package that we want to create as an external library. The package is <code>text-tools</code>.</p>

<h4 id="text-tools">text-tools</h4>

<p>The <code>text-tools</code> package contains various tools for manipulating text. To use it, we download the <code>.whl</code> file from <a href="https://pypi.org/project/text-tools/#files">here</a>. After downloading the package, we need to put it into a zipped archive file. My zipped file is named <code>text_tools-1.0.0-py3-none-any.zip</code>. As we want to install from a file path the zip file needs to be placed in a location where SQL Server can read it.</p>

<h4 id="installation">Installation</h4>

<p>Before we do any installation, let&rsquo;s check what external libraries we have in the database already:</p>

<pre><code class="language-sql">USE ExtLangDB
GO

SELECT * FROM sys.external_libraries
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Retrieve External Libraries</em></p>

<p>When I run the code in <em>Code Snippet 4</em> I am not getting any results back as <code>ExtLangDB</code> is a newly created database.</p>

<p>OK then, let&rsquo;s get on with it. We first want to install the <code>text-tools</code> package for the SQL Server Machine Learning Services Python:</p>

<pre><code class="language-sql">CREATE EXTERNAL LIBRARY text_tools
FROM (CONTENT = 'W:\text_tools-1.0.0-py3-none-any.zip')
WITH (LANGUAGE = 'Python');
</code></pre>

<p><strong>Code Snippet 5:</strong> <em>Create text-tools for Python</em></p>

<p>In <em>Code Snippet 5</em> we see how I:</p>

<ul>
<li>give the library the name of <code>text_tools</code>.</li>
</ul>

<blockquote>
<p><strong>NOTE:</strong> When you create the external library for SQL Server Machine Learning Services Python, the name must match the module name you use in your Python script. When you create the library for your &ldquo;own&rdquo; Python, the name does not matter.</p>
</blockquote>

<ul>
<li>indicate that this library is for the &ldquo;built-in&rdquo; Python, (<code>LANGUAGE=Python</code>).</li>
</ul>

<p>After I execute the code in <em>Code Snippet 5</em> I run the code in <em>Code Snippet 4</em>, and now I get:</p>

<p><img src="/images/posts/ext-lib-py-external-library-python.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Python External Library</em></p>

<p>As we see in <em>Figure 1</em> we now have an external library for Python. The question is now if we can use it, and to do that we will use a method in <code>text-tools</code> called <code>find_best_string</code>. The method finds the best matching string in a larger string. It returns either the first match or all matches, and their match value:</p>

<pre><code class="language-sql">EXEC sp_execute_external_script
@language =N'Python',
@script=N'

from text_tools.finders import find_best_string

corpus = &quot;is simply dummy text of the printing and typesetting 
industry. Lorem Ipsum has been the industry''s standard dummy 
text ever since the 1500s.&quot;

query = &quot;Ipsum&quot;

first_match = find_best_string(query, corpus, flex = 2)

print(first_match)
'
</code></pre>

<p><strong>Code Snippet 6:</strong> <em>Execute using Python</em></p>

<p>In <em>Code Snippet 6</em> we see how we:</p>

<ul>
<li>import <code>find_best_string</code> from <code>text_tools.finders</code>.</li>
<li>set the variable <code>corpus</code> to be the string we want to search for.</li>
<li>set <code>query</code> to the string we look for.</li>
</ul>

<p>We then call the method with the <code>query</code>, and <code>corpus</code> as parameters, and the result looks like so:</p>

<p><img src="/images/posts/ext-lib-py-find-best-string-1.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Result</em></p>

<p>In <em>Figure 2</em> (outlined in red), we see that we get back the string that matches the best and the match value, cool!</p>

<p>Now, what if we do this using &ldquo;our own&rdquo; Python. When I say &ldquo;our own&rdquo; I refer to the Python external language mentioned in the <em>Demo Code</em> section above.</p>

<p>I mentioned above how I use Python 3.9 as the external language. If you remember from the <a href="/2021/01/05/updated-write-a-python-3.9-language-extension-for-sql-server-machine-learning-services/">Write a Python 3.9 Language Extension &hellip;</a>, I recompiled the language extension source code against Python 3.9, and then created the language as so:</p>

<pre><code class="language-sql">USE ExtLangDB;
GO

CREATE EXTERNAL LANGUAGE p39
FROM (CONTENT = 'W:\python-lang-extension.zip'
      , FILE_NAME = 'pythonextension.dll');
GO
</code></pre>

<p><strong>Code Snippet 7:</strong> <em>Create External Language</em></p>

<p>We see in <em>Code Snippet 7</em> how I created the language and gave it the name <code>p39</code>. We ensure all works OK by changing the <code>@language</code> parameter in <em>Code Snippet 1</em> to <code>@language=p39</code> and then execute. Yes, it works just fine. To ensure that we have not caused any &ldquo;drama&rdquo; with SQL Server Machine Learning Services Python, we can change back the language to <code>Python</code> and execute. This works fine as well.</p>

<p>As the above worked, let us now create the <code>text-tools</code> package as an external library for our newly created language: <code>p39</code>:</p>

<pre><code class="language-sql">CREATE EXTERNAL LIBRARY text_tools_p39
FROM (CONTENT = 'W:\text_tools-1.0.0-py3-none-any.zip')
WITH (LANGUAGE = 'p39');
</code></pre>

<p><strong>Code Snippet 8:</strong> <em>Create text-tools for p39</em></p>

<p>The code we see in <em>Code Snippet 8</em> looks the same as what we see in <em>Code Snippet 5</em>, except for:</p>

<ul>
<li>the name is now just a string. If I try to name it as in <em>Code Snippet 5</em> I get an error saying there is already an external library with that name and the same owner. As mentioned above, for external languages, we can give the library an arbitrary name.</li>
<li>the <code>LANGUAGE</code> parameter points to the language we created in <code>Code Snippet 7</code>.</li>
</ul>

<p>After we run the code in <em>Code Snippet 8</em> we check that all is OK by executing <code>SELECT * FROM sys.external_libraries</code>:</p>

<p><img src="/images/posts/ext-lib-py-external-library-py39.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>External Libraries</em></p>

<p>It seems that we succeeded in creating the external library as we see, outlined in red, in <em>Figure 3</em> the library we just created. Right, so let&rsquo;s run the code in <em>Code Snippet 6</em>, but use <code>@language=p39</code> instead, and watch the awesomeness:</p>

<p><img src="/images/posts/ext-lib-py-pyhome-error.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>PYTHONHOME Error</em></p>

<p>Eish, the awesomeness turned out to be not so awesome after all. We got a big fat error as in <em>Figure 4</em>. OK, I suppose that if you have read the previous posts, this error might not be that unexpected. This as I wrote about how the <code>PYTHONHOME</code> environment variable is not needed <strong>UNLESS</strong> you create an external library.</p>

<blockquote>
<p><strong>NOTE:</strong> Eish is South African slang, and it can express anything from excitement to horror.</p>
</blockquote>

<p>Oh, you may wonder why we could create the external library, as in <em>Code Snippet 8</em>, but when we tried to run it we failed? The short answer is to think about creating external libraries like you create stored procedures. When you run <code>CREATE PROCEDURE</code> the procedure is &ldquo;registered&rdquo;, it is not until you execute against it, it is being &ldquo;compiled&rdquo;. The same thing is true for external libraries. We will talk about external libraries in more detail and explain what happened here in a future post.</p>

<p>Another question that may &ldquo;pop up&rdquo; is why we could execute using the &ldquo;built-in&rdquo; Python language without having <code>PYTHONHOME</code> set? The short answer is that I have no clue. I assume it has to do with the tight integration between Python and SQL Server.</p>

<h2 id="the-fix">The Fix</h2>

<p>I see <code>PYTHONHOME</code> as &ldquo;the gift that keeps giving&rdquo;, but not in a good way! Let us see what we can do to fix this.</p>

<h4 id="set-the-environment-variable">Set the Environment Variable</h4>

<p>One way to fix this would be to set the <code>PYTHONHOME</code> system environment variable. We create a new system environment variable, we name it <code>PYTHONHOME</code>, and we set the variable value to the Python directory:</p>

<p><img src="/images/posts/byor-r-and-p-pythonhome.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>PYTHONHOME Environment Variable</em></p>

<p>The figure above is from a previous post, where I created <code>PYTHONHOME</code> to point to my Python 3.7 installation. In my case with Python 3.9, I point it to my Python 3.9 installation: <code>C:\Python39</code>. Having done that, I restart the <em>Launchpad</em> service and execute the code again:</p>

<pre><code class="language-bash">STDOUT message(s) from external script: Processing c:\program files
\microsoft sql server\mssql15.inst2\mssql\externallibraries\5\65538
\1\tmp\text_tools-1.0.0-py3-none-any.whl 
Installing collected packages: text-tools 
Successfully installed text-tools-1.0.0

STDOUT message(s) from external script: ('ipsum', 1.0)
</code></pre>

<p><strong>Code Snippet 9:</strong> <em>Result</em></p>

<p>The output from running the code looks something like what we see in <em>Code Snippet 9</em>:</p>

<ul>
<li>first something about where it is installed from. Subsequent executions of the code will not show this.</li>
<li>then the result. The result matches what we see in <em>Figure 2</em>.</li>
</ul>

<p>That&rsquo;s cool - we can use our external library. However, if we now went back and executed any code, for example, what we see in <em>Code Snippet 1</em>, using <code>@language=Python</code> we would fail. We discussed why in the <a href="/2021/01/09/solve-python-issues-in-sql-server-machine-learning-services-after-deploying-python-3.9/"><strong>Solve Python Issues in SQL Server Machine Learning Services After Deploying Python 3.9</strong></a> post.</p>

<p>After we have executed against the external library for the first time, one potential solution is to remove the <code>PYTHONHOME</code> environment variable and restart the <em>Launchpad</em> service. That works but is not very practical or sustainable. So we need something else.</p>

<h4 id="source-code-change">Source Code Change</h4>

<p>As we were getting an error about not finding the environment variable, (<em>Figure 5</em>), we can assume that something in the source code is looking for that particular variable.</p>

<p>In the <a href="/2021/01/05/updated-write-a-python-3.9-language-extension-for-sql-server-machine-learning-services/">Write a Python 3.9 Language Extension &hellip;</a> post, we cloned the GitHub library with the Python language extension source code. Then we recompiled the code to cater for Python 3.9, and that is the language extension used in this post. Having the source code, we can look for where the variable is used and change the name.</p>

<p>If we search for <code>PYTHONHOME</code> in the source code we find it in the method <code>PythonExtensionUtils::GetPathToPython()</code> in the source file <code>PythonExtensionUtils_win.cpp</code>:</p>

<p><img src="/images/posts/ext-lib-py-get-pyhome.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>PYTHONHOME Error</em></p>

<p>The code in <em>Figure 6</em> looks up the environment variable, and we see the variable name outlined in red. We can change that name to something arbitrarily, <code>PYTHONHOME39</code> for example and:</p>

<ul>
<li>recompile the code.</li>
<li>delete the <code>PYTHONHOME</code> variable.</li>
<li>create a new system environment variable named as per what we changed it to in the code.</li>
<li>restart the <em>Launchpad</em> service.</li>
<li>drop the external library.</li>
<li>drop the language.</li>
<li>create the language again based on the recompiled dll.</li>
<li>create the library.</li>
</ul>

<p>When we execute the code after doing the above steps, we get the result as we see in <em>Code Snippet 9</em>, <strong>AND</strong> we can also execute any code using the <code>Python</code> language with no errors. Win!</p>

<p>To change the source code works, but it may be too convoluted - especially if you are not a C++ developer. So let us look at another option.</p>

<h4 id="create-external-language">CREATE EXTERNAL LANGUAGE</h4>

<p>In <em>Code Snippet 7</em> we see how we create the external language, and we have used similar code in the previous posts. It turns out that in the <code>FROM(...)</code> clause, we can have a couple of more parameters other than <code>CONTENT</code> and <code>FILE_NAME</code>. One of those parameters is <code>ENVIRONMENT_VARIABLES</code>. The <code>ENVIRONMENT_VARIABLES</code> parameter allows us to provide a set of environment variables to the external language runtime. If we do this, the environment variable is not in global scope, but bound to that particular language, in that specific database:</p>

<pre><code class="language-sql">USE ExtLangDB;
GO

ALTER EXTERNAL LANGUAGE p39
SET 
(
  CONTENT = 'W:\python-lang-extension.zip'
  , FILE_NAME = 'pythonextension.dll'
  , ENVIRONMENT_VARIABLES = N'{&quot;PYTHONHOME&quot;:&quot;C:\\Python39&quot;}'
);
GO
</code></pre>

<p><strong>Code Snippet 10:</strong> <em>Alter the External Language</em></p>

<p>Instead of doing a <code>CREATE EXTERNAL LANGUAGE</code> and having to drop the external library and the language, we can <code>ALTER</code> the language as in <em>Code Snippet 10</em>. The assumption before we run the code above is that we are back to where we are at <em>Figure 4</em>. We have:</p>

<ul>
<li>created the language as in <em>Code Snippet 7</em>.</li>
<li>created the external library, (<em>Code Snippet 8</em>).</li>
<li>executed the code and received the error as in <em>Figure 4</em>.</li>
</ul>

<p>With the above requirements met, we can now run the code in <em>Code Snippet 10</em>. After running the code, we execute the code in <em>Code Snippet 6</em> with the <code>@language</code> parameter set to <code>p39</code>. And it works! Furthermore, executing with the <code>@language</code> set to <code>Python</code> also works!</p>

<h2 id="summary">Summary</h2>

<p>To create external libraries for external languages, not the &ldquo;built-in&rdquo; Python, the <code>PYTHONHOME</code> systems environment variable needs to be set. Having that variable set causes issues for both the &ldquo;built-in&rdquo; Python and other Python external languages based on earlier/later versions of Python other than the version we created the external library for.</p>

<p>In this post we looked at how we can work around that problem, and we saw following options:</p>

<ul>
<li>create the environment variable but delete it after the first execution of code using the external library. Not practical, nor sustainable.</li>
<li>create a new environment variable with a different name. Change the source code where the variable name is defined to use the new name, and recompile it.</li>
<li>define the variable using the <code>ENVIRONMENT_VARIABLES</code> in <code>CREATE/EDIT EXTERNAL LANGUAGE</code>.</li>
</ul>

<p>Of the three options above, the last seems to be the most practical and least invasive.</p>

<p>Finally, as mentioned previously in the post, a future post or posts will look into how creating external libraries work for external languages.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 3, 2021]]></title>
    <link href="https://nielsberglund.com/2021/01/17/interesting-stuff---week-3-2021/" rel="alternate" type="text/html"/>
    <updated>2021-01-17T07:29:05+02:00</updated>
    <id>https://nielsberglund.com/2021/01/17/interesting-stuff---week-3-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending. Admittedly not much this week but here goes:</p>

<p></p>

<h2 id="data-architecture">Data Architecture</h2>

<ul>
<li><a href="https://thenewstack.io/data-mesh-liberate-business-value-from-data-lakes-data-warehouses/">Data Mesh Liberates Business Value from Data Lakes, Data Warehouses</a>. Recently I have posted quite a bit around data architecture and data meshes. Here is another post about this topic. The post discusses how data meshes can add value to data lakes and traditional data warehouses. This is quite interesting to me as we are looking at this at <a href="/derivco">Derivco</a> right now.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://rmoff.net/2021/01/11/running-a-self-managed-kafka-connect-worker-for-confluent-cloud/">Running a self-managed Kafka Connect worker for Confluent Cloud</a>. Another post that comes at the right time. I have been looking at how to run Kafka in the cloud and use Kafka Connect. The problem is that not all Kafka connectors are cloud-enabled yet. Fortunately, this post comes to help and explains how to, for non cloud-enabled connectors, you can run your own Kafka Connect worker on-prem, which then connects to Confluent Cloud. Awesome!</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>In <a href="/2021/01/10/interesting-stuff---week-2-2021/">last weeks roundup</a>, I mentioned I was looking more into the SQL Server managed language extensions and how some posts would hopefully be forthcoming. Well, I am close to publishing a post around the Python extension and issues around creating external libraries.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 2, 2021]]></title>
    <link href="https://nielsberglund.com/2021/01/10/interesting-stuff---week-2-2021/" rel="alternate" type="text/html"/>
    <updated>2021-01-10T08:22:46+02:00</updated>
    <id>https://nielsberglund.com/2021/01/10/interesting-stuff---week-2-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="data-architecture">Data Architecture</h2>

<ul>
<li><a href="https://ananthdurai.medium.com/data-mesh-simplified-a-reflection-of-my-thoughts-on-data-mesh-4d4f01c37185">Data Mesh Simplified: A Reflection Of My Thoughts On Data Mesh</a>. The post linked to here gives an interesting take on what a Data Mesh is, and what problems it solves.</li>
<li><a href="https://databricks.com/blog/2021/01/08/lakehouse-architecture-realized-enabling-data-teams-with-faster-cheaper-and-more-reliable-open-architectures.html">Lakehouse Architecture Realized: Enabling Data Teams With Faster, Cheaper and More Reliable Open Architectures</a>. This post by <a href="https://databricks.com/">Databricks</a> serves as a review of 2020 of what has happened at Databricks and in the Big Data world.</li>
</ul>

<h2 id="data">Data</h2>

<ul>
<li><a href="https://abhishek-ch.github.io/around-dataengineering/">around-dataengineering</a>. The link here is not, so much a blog post, but a page with links to various interesting machine learning and data engineering technologies. When reading this, I found a lot of new interesting &ldquo;stuff&rdquo;.</li>
<li><a href="https://medium.com/apache-pinot-developer-blog/change-data-analysis-with-debezium-and-apache-pinot-b4093dc178a7">Change Data Analysis with Debezium and Apache Pinot</a>. This blog post looks at real-time analytics based on combining Debezium, with the real-time OLAP datastore, <strong>Apache Pinot</strong>. Very, very cool!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://rmoff.net/categories/twelvedaysofsmt/">TwelveDaysOfSMT</a>. Kafka has a functionality called Single Message Transforms (SMT). Using SMT, you can modify the data and its characteristics as it passes through the Kafka Connect pipeline, without needing additional stream processors. This page I have linked to here contains a list of blog posts by <a href="https://twitter.com/rmoff">Robin Moffat</a>, where he looks at various SMT types.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>Recently I have looked quite a bit at the open sourced SQL Server Language extensions, and the Python one specifically. In last weeks <a href="/2021/01/03/interesting-stuff---christmas-new-year-week-1-2021/">roundup</a> I mentioned I had written a <a href="/2020/12/29/updated-bring-your-own-r--python-runtimes-to-sql-server-extensibility-framework/">blog post</a> looking at how to using the language extension with a Python runtime other than the one shipping in SQL Server Machine Learning Services. That post resulted in a couple of follow up posts, published the last few days:</p>

<ul>
<li><a href="/2021/01/03/build-boost.python--numpy-in-windows/">How to build Boost.Python with the view to be able to create a Python SQL Language extension.</a>. Since the SQL Server language extensions are open-sourced, you can build your own language extension. I started a post looking at recompiling the Python extension to cater for a newer Python version. To recompile the Python extension, you need to use Boost.Python. It turned out that was more complex than I initially thought, so it deserved its own post. In the post linked to we look at building Boost.Python so we can create a Python SQL Language extension.</li>
<li><a href="/2021/01/05/updated-write-a-python-3.9-language-extension-for-sql-server-machine-learning-services/">Write a Python 3.9 Language Extension for SQL Server Machine Learning Services</a>. This post is the one resulting in the Boost.Python post. This post looks at how to write a Python 3.9 SQL Server Language extension to use in SQL Server Machine Learning Services.</li>
<li><a href="/2021/01/09/solve-python-issues-in-sql-server-machine-learning-services-after-deploying-python-3.9/">Solve Python Issues in SQL Server Machine Learning Services After Deploying Python 3.9</a>. Having written the posts above and trying to use the deployed languages, I realized that I could not execute against other Python languages after deploying a new language. After some investigation, I managed to figure out why, and the post linked to tries to explain what the problem is, and how to solve it.</li>
</ul>

<p>There are a couple of more things I would like to look at around language extensions, so, you can expect some more posts about the new open-sourced language extensions in the following weeks.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Solve Python Issues in SQL Server Machine Learning Services After Deploying Python 3.9]]></title>
    <link href="https://nielsberglund.com/2021/01/09/solve-python-issues-in-sql-server-machine-learning-services-after-deploying-python-3.9/" rel="alternate" type="text/html"/>
    <updated>2021-01-09T11:38:09+02:00</updated>
    <id>https://nielsberglund.com/2021/01/09/solve-python-issues-in-sql-server-machine-learning-services-after-deploying-python-3.9/</id>
    <content type="html"><![CDATA[<p>In September 2020, Microsoft open-sourced SQL Server Machine Learning Services, (SQLMLS), language extensions for R and Python. If you want more information, here are some blog posts I have written about it:</p>

<ul>
<li><a href="/2020/12/29/updated-bring-your-own-r--python-runtimes-to-sql-server-extensibility-framework/"><strong>Bring Your Own R &amp; Python Runtimes to SQL Server Extensibility Framework</strong></a></li>
<li><a href="/2021/01/05/updated-write-a-python-3.9-language-extension-for-sql-server-machine-learning-services/"><strong>Write a Python 3.9 Language Extension for SQL Server Machine Learning Services</strong></a></li>
</ul>

<p>In the <a href="/2021/01/05/updated-write-a-python-3.9-language-extension-for-sql-server-machine-learning-services/">last post</a>, which looks at using Python 3.9 in SQL Server Machine Learning Services, I wrote this at the very end:</p>

<p><em>It looks like all is good, but maybe not? In a future post we&rsquo;ll look at an issue we have introduced - but for now, let us bask in the glory of having created a new Python language extension.</em></p>

<p>In the post, we wrote a new language extension to handle Python 3.9, and that just worked fine. However, when I was doing some other things, I noticed some side effects, and in this post, we look at those side effects and how to solve them.</p>

<blockquote>
<p><strong>NOTE:</strong> The two posts mentioned above has been updated as a result of this post. If you now follow the posts above you may not see the errors we discuss here.</p>
</blockquote>

<p></p>

<h2 id="pre-reqs">Pre-reqs</h2>

<p>Before we dive in, let us look at the pre-reqs (if you want to follow along).</p>

<p>In this post, I am not going into detail about language extensions and external languages and such. If you are unsure about this, please read the posts linked to above.</p>

<h4 id="environment-variables">Environment Variables</h4>

<p>If you followed along in post about Python 3.9 and created the <code>PYTHONHOME</code> environment variable, please delete it. After you have deleted it restart the <em>Launchpad</em> service for the SQL Server instance where you run the code.</p>

<h4 id="sql-server">SQL Server</h4>

<p>We need SQL Server CU3+ with <em>Machine Learning Services and Language Extensions</em> installed together with Python. Execution of external scripts needs to be enabled as well. You also need a database where to deploy the external languages to. In this post I use a database called <code>ExtLangDB</code>:</p>

<pre><code class="language-sql">USE Master;
GO

DROP DATABASE IF EXISTS ExtLangDB;
GO

CREATE DATABASE ExtLangDB;
GO
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Create Database</em></p>

<p>Since in my previous posts I have used the same database name we see in <em>Code Snippet 1</em> we see how I drop the database if it exists before I create it. I do this to clear out any old &ldquo;cruft&rdquo; in the db.</p>

<blockquote>
<p><strong>NOTE:</strong> Above I say <em>Machine Learning Services and Language Extensions</em> installed together with Python. I mean that, during the installation of SQL and when you tick the box for <em>Machine Learning Services and Language Extensions</em>, you also choose Python. I refer to this as <em>PythonSQLMLS</em> - Python for SQL Server Machine Learning Services, the &ldquo;native&rdquo; Python.</p>
</blockquote>

<p>Let us make sure that everything looks OK in your database:</p>

<pre><code class="language-sql">USE ExtLangDB;
GO

SELECT * FROM sys.EXTERNAL_LANGUAGES
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Make Sure All is OK</em></p>

<p>In <em>Code Snippet 2</em> we look for what external languages we have installed, and the result is like so:</p>

<p><img src="/images/posts/fix-python-ext-langs.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>External Languages</em></p>

<p>OK, so according to <em>Figure 1</em> all seems OK in the database. We have both R, and Python outlined in red, installed as part of <em>Machine Learning Services and Language Extensions</em>.</p>

<p>The last thing we want to do related to SQL Server is to install Python 3.7.9, and Python 3.9.1 on the SQL Server box. Both installations need <code>pandas</code>, and Python needs to be added to the <code>PATH</code>, (root directory, and <code>.\Scripts</code> directory). The Python installations need to be done with administrator permissions: &ldquo;Run as administrator&rdquo;.</p>

<h4 id="language-extensions">Language Extensions</h4>

<p>We need the open-sourced Microsoft Python language extension, which you download from <a href="https://github.com/microsoft/sql-server-language-extensions/releases">here</a>, (click on &ldquo;Python language extension&rdquo;). As I mentioned in <a href="/2020/12/29/updated-bring-your-own-r--python-runtimes-to-sql-server-extensibility-framework/">this post</a>, it supports Python 3.7.x. In the rest of this post, I call it the <em>Python37</em> extension.</p>

<p>We also need an extension for Python 3.9. You can follow my post <a href="/2021/01/05/updated-write-a-python-3.9-language-extension-for-sql-server-machine-learning-services/">here</a>, and build it yourself. If you don&rsquo;t feel like building it, you can download the extension we built in the <a href="/2021/01/05/updated-write-a-python-3.9-language-extension-for-sql-server-machine-learning-services/">post</a> in the previous sentence from <a href="/downloads/fixpython/python-lang-extension-windows-3.9.zip">here</a>. Going forward I call this extension <em>Python39</em>.</p>

<p>After downloading/building, place them in a location from where SQL Server can access them, and name them so you can see who is who.</p>

<h2 id="pythonsqlmls">PythonSQLMLS</h2>

<p>Let us make sure that the &ldquo;native&rdquo; Python, the one that comes as part of SQLMLS, (seen in <em>Figure 1</em>), works correctly in our database:</p>

<pre><code class="language-sql">EXEC sp_execute_external_script
@language =N'Python',
@script=N'
import pandas as pd
import sys
df = pd.DataFrame(columns=[&quot;Version&quot;])
ver = sys.version
pth = sys.executable
df = df.append({&quot;Version&quot;: ver}, ignore_index=True)
OutputDataSet = df'
WITH RESULT SETS (([Python Version] nvarchar(256)));

</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Get Path &amp; Executable</em></p>

<p>The code we see in <em>Code Snippet 3</em> is the same as in the previous posts.</p>

<p>You see in <em>Code Snippet 3</em> how the <code>@language</code> parameter is set to <code>Python</code>. That indicates using the Python installed as part of <em>Machine Learning Services and Language Extensions</em> when we check the Python check-box, (<em>PythonSQLMLS</em>).</p>

<p>The result of running the code in <em>Code Snippet 3</em> is like so:</p>

<p><img src="/images/posts/byor-r-and-p-python-version-l.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Python SQLMLS</em></p>

<p>We see in <em>Figure 2</em> how the SQLMLS Python is version 3.7.1.</p>

<h2 id="python-3-7-as-language-extension">Python 3.7 as Language Extension</h2>

<p>We did the same as above in the <a href="/2020/12/29/updated-bring-your-own-r--python-runtimes-to-sql-server-extensibility-framework/">Bring Your Own R &amp; Python Runtimes to SQL Server Extensibility Framework</a>, and we then continued with creating an external language from the <a href="https://github.com/microsoft/sql-server-language-extensions/releases">Microsoft Python language extension</a>:</p>

<pre><code class="language-sql">USE ExtLangDB;
GO

CREATE EXTERNAL LANGUAGE p37
FROM (CONTENT = 'W:\python-lang-extension-windows-3.7.zip'
      , FILE_NAME = 'pythonextension.dll');
GO
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Create Python 3.7 External Language</em></p>

<p>In <em>Code Snippet 4</em> we see how:</p>

<ul>
<li>we create an external language with the name of <code>p37</code>. We cannot use <code>Python</code> as it is a reserved word.</li>
<li>I renamed the Microsoft language extension to <code>python-lang-extension-windows-3.7.zip</code>.</li>
</ul>

<p>We check that the creation of <code>p37</code> succeeded by executing the code in <em>Code Snippet 2</em>. In the result, we should now see <code>p37</code> in addition to what we see in <em>Figure 1</em>.</p>

<p>When we see that we indeed have a new external language we:</p>

<ul>
<li>create a system environment variable <code>PYTHONHOME</code> pointing to the Python 3.7 install directory on the SQL box (in my case <code>C:\Python37</code>). This is required as per the <a href="https://docs.microsoft.com/en-us/sql/machine-learning/install/custom-runtime-python">Microsoft documentation</a>. We&rsquo;ll see later that this may not be required.</li>
<li>assign read and write permissions to the <em>Launchpad</em> server for the SQL Server instance and <code>ALL APPLICATIONS GROUP</code>. The permissions are given to the root directory and underlying files and directories of <code>PYTHONHOME</code>, (read more about assigning permissions in <a href="/2020/12/29/updated-bring-your-own-r--python-runtimes-to-sql-server-extensibility-framework/">this post</a>).</li>
</ul>

<p>Having created the <code>PYTHONHOME</code> variable, we restart the <em>Launchpad</em> service. We are now ready to execute the code in <em>Code Snippet 3</em>, but we need to change the <code>@language</code> parameter to <code>p37</code> instead of <code>Python</code>.</p>

<p>The result we get back when executing the code should look like so:</p>

<p><img src="/images/posts/byor-r-and-p-python-version-2.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Python 3.7.9 External Language</em></p>

<p>Yay, we see from <em>Figure 3</em> how we now execute against Python 3.7.9 instead of 3.7.1. So yes, we are executing against another Python runtime than <em>PythonSQLMLS</em>.</p>

<p>If you get an error when you execute the code and the error looks something like this: &ldquo;A <your-language-name> script error occurred during execution of &lsquo;sp_execute_external_script&rsquo; with HRESULT 0x80004004.&ldquo;, the most common causes for this are:</p>

<ul>
<li>Your Python installation is not on the <code>PATH</code>.</li>
<li>Permissions have not been set on the Python directories and files.</li>
</ul>

<p>Cool, we have now executed using a later version of the Python runtime than what is installed through SQLMLS. That&rsquo;s where we finished the <a href="/2020/12/29/updated-bring-your-own-r--python-runtimes-to-sql-server-extensibility-framework/">Bring Your Own R &amp; Python Runtimes to SQL Server Extensibility Framework</a> post.</p>

<p>What we didn&rsquo;t do was to see what happens if we now want to run the code in <em>Code Snippet 3</em> and use <em>PythonSQLMLS</em>, (<code>@language=Python</code>):</p>

<p><img src="/images/posts/fix-python-sqlmls-error1.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Python Error - I</em></p>

<p>This.Is.Not.Good! We get a <code>ModuleNotFoundError</code>, as in <em>Figure 4</em>, and the module we cannot find is <code>revoscalepy</code>, which one of the proprietary Microsoft Python modules.</p>

<p>What we see here is one of the side effects I observed when doing my previous blog posts. We&rsquo;ll come back to this after looking at using the Python 3.9.1 runtime.</p>

<h2 id="python-3-9-as-language-extension">Python 3.9 as Language Extension</h2>

<p>In the <a href="/2021/01/05/updated-write-a-python-3.9-language-extension-for-sql-server-machine-learning-services/">Write a Python 3.9 Language Extension for SQL Server Machine Learning Services</a> post we said that the Microsoft open-sourced language extension can only be used for Python 3.7.x. If we want to use another runtime, like 3.9.x, we need to re-compile the source code against the Python version we want to use. In the <a href="/2021/01/05/updated-write-a-python-3.9-language-extension-for-sql-server-machine-learning-services/">post</a> we did that and saw how we executed against Python 3.9.1.</p>

<p>For Python 3.9.1, let us do what we did above with 3.7.9:</p>

<ul>
<li>create an external language. Let&rsquo;s call it <code>p39</code>, based on the extension for Python 3.9.1 I built, which is [here][].</li>
<li>change/create the <code>PYTHONHOME</code> variable to point to the Python 3.9 installation.</li>
<li>assign the necessary permissions against the root directory, subdirectories, and Python 3.9 installation files.</li>
<li>restart the <em>Launchpad</em> service.</li>
<li>change the @language parameter in <em>Code Snippet 3</em> to <code>@language=p39</code>.</li>
</ul>

<p>When we run the code, we see:</p>

<p><img src="/images/posts/py39-ext-exec-success.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Python Version</em></p>

<p>In <em>Figure 5</em> we see, as we did in the <a href="/2021/01/05/updated-write-a-python-3.9-language-extension-for-sql-server-machine-learning-services/">Python 3.9 post</a>, how we get back Python 3.9.1. We are running against the Python 3.9 runtime. Awesome!</p>

<p>What happens if we now change back and execute with <code>@language=Python</code>, or <code>@language=p37</code>:</p>

<pre><code class="language-bash">STDERR message(s) from external script: 
Fatal Python error: init_sys_streams: can't initialize 
sys standard streams Traceback (most recent call last): 
File &quot;C:\Python39\lib\io.py&quot;, line 54, in ImportError: 
cannot import name 'open_code' from 'io' (unknown location)
</code></pre>

<p><strong>Code Snippet 5:</strong> <em>Python Error - II</em></p>

<p>Once again - not good! As we see in <em>Code Snippet 5</em> we get a new error back saying something about not initialising something.</p>

<p>The question now is if this elated to Python in SQL Server only, or to Python in general on the SQL Server box?</p>

<h2 id="python-stand-alone">Python Stand-alone</h2>

<p>As Python 3.9 seems to work, let us just confirm that. From command prompt we <code>cd</code> into the Python 3.9 installation. For me, it looks like: <code>cd C:\Python39</code>.</p>

<p>In there, we execute:</p>

<pre><code class="language-bash">.\python.exe -c &quot;import sys; print(sys.version);&quot;
</code></pre>

<p><strong>Code Snippet 6:</strong> *Execute Python 3.9 from Command Prompt&rdquo;</p>

<p>In <em>Code Snippet 6</em> we see how we want to execute some Python code that prints out the Python version we execute. When we run the code, we get back the same we see in <em>Figure 5</em>. So that works.</p>

<p>What about Python 3.7? We do the same:</p>

<ul>
<li><code>cd</code> into the Python 3.7 installation, (for me: <code>C:\Python37</code>).</li>
<li>execute the code in <em>Code Snippet 6</em>.</li>
</ul>

<p>Hmm, that - once again - does not look good:</p>

<p><img src="/images/posts/fix-python-python37-error1.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>Python Error - III</em></p>

<p>We see in <em>Figure 6</em> how we get exactly the same error as in <em>Code Snippet 5</em>. What is going on here?</p>

<h2 id="pythonhome">PYTHONHOME</h2>

<p>A clue to the issue is highlighted in yellow in <em>Figure 6</em>: <code>File &quot;C:\Python39\lib\io.py&quot;</code>. We are executing against Python 3.7, but for some reason Python tries to touch a file in the Python 3.9 <code>lib</code> directory. Huh?</p>

<p>The culprit here is the <code>PYTHONHOME</code> system environment variable we created. We created it as the documentation around the Python language extension states it is required.</p>

<p>The <code>PYTHONHOME</code> system environment variable is a well known variable to the Python engine. The variable changes the location of the standard Python libraries. In the example where the variable was set to Python 3.9 and we executed against the Python 3.7 executable, Python tried to load files from the 3.9 installation. Now we have a version mismatch, and that&rsquo;s why we got the error.</p>

<p>In the <em>PythonSQLMLS</em> example where the variable was set to Python 3.7, we still got an error, why is that? In this case, the <em>PythonSQLMLS</em> runtime tried to load a Microsoft specific module that does not exist in the &ldquo;normal&rdquo; Python 3.7 runtime.</p>

<h2 id="solution">Solution</h2>

<p>How do we solve this? Well, above I say that the <code>PYTHONHOME</code> system environment variable is required. That is not entirely true. It is required if you want to create an external library for your language. If you only want to execute Python code, then you don&rsquo;t need it. Having Python on the <code>PATH</code> is required though in either case.</p>

<p>So to confirm that I am not lying, (too much), to you let&rsquo;s go ahead and:</p>

<ul>
<li>delete the <code>PYTHONHOME</code> system environment variable.</li>
<li>restart the <em>Launchpad</em> service.</li>
</ul>

<p>When we now execute the code in <em>Code Snippet 3</em> with the <code>@language</code> parameter being <code>Python</code>, <code>p37</code>, and <code>p39</code> respectively we see the expected result for all three executions!</p>

<p>But what do we do if we also want to create external libraries for for example Python 3.9, and at the same time in the database where we want to create the external library also need to execute <em>PythonSQLMLS</em> code?</p>

<p>In that case, we can not use <code>PYTHONHOME</code> as an environment variable, so we need to create a variable with a random name pointing to the Python extension.</p>

<p>Having done that we:</p>

<ul>
<li>go to the source code for the extension.</li>
<li>change the variable name in the method where it is used: <code>std::string PythonExtensionUtils::GetPathToPython()</code>. This is in the source file <code>PythonExtensionUtils_win.cpp</code>, (remember, I do this for SQL on Windows).</li>
<li>recompile the extension, and redeploy.</li>
</ul>

<p>In a future post, we will look more in detail at how to do this.</p>

<h2 id="summary">Summary</h2>

<p>This post came about as I saw strange errors after having deployed Python language extensions. Well, what I saw was that if I deployed a Python 3.7 extension, (<em>Python37</em>), I could not execute code with the &ldquo;native&rdquo; Python for SQL Server Machine Learning Services, (<em>PythonSQLMLS</em>). If I deployed an extension for Python 3.9, I could not execute code for either <em>PythonSQLMLS</em>, or <em>Python37</em>.</p>

<p>Based on the errors we received, we determined the issue was with the <code>PYTHONHOME</code> system environment variable. The official Microsoft documentation says this variable needs to be set for the Python extension. That requirement is only partially true, yes - it needs to be set if you want to create an external library for the language extension. However, if you just want to execute code, it is not needed.</p>

<blockquote>
<p><strong>NOTE:</strong> Something that is not entirely clear in the official documentation is that the Python installation needs to be on the <code>PATH</code> in either case.</p>
</blockquote>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[UPDATED: Write a Python 3.9 Language Extension for SQL Server Machine Learning Services]]></title>
    <link href="https://nielsberglund.com/2021/01/05/updated-write-a-python-3.9-language-extension-for-sql-server-machine-learning-services/" rel="alternate" type="text/html"/>
    <updated>2021-01-05T04:57:43+02:00</updated>
    <id>https://nielsberglund.com/2021/01/05/updated-write-a-python-3.9-language-extension-for-sql-server-machine-learning-services/</id>
    <content type="html"><![CDATA[<blockquote>
<p><strong>NOTE:</strong> This post has been updated after it was originally published. I have edited information about the <code>PYTHONHOME</code> system variable, as well as <code>PATH</code> requirements.</p>
</blockquote>

<p>In my post, <a href="/2020/12/29/updated-bring-your-own-r--python-runtimes-to-sql-server-extensibility-framework/"><strong>Bring Your Own R &amp; Python Runtimes to SQL Server Extensibility Framework</strong></a> I wrote about how we can use other R and Python runtimes in SQL Server Machine Learning Services than the ones that come &ldquo;out of the box&rdquo;. In the post, I wrote that if you want to bring a Python runtime other than version 3.7.x, (like 3.8, 3.9, etc.), you need to build your own Python language extension, and we&rsquo;d look at it in a future post.</p>

<blockquote>
<p><strong>NOTE:</strong> For R and Java the existing language extension can be used regardless of R/Java version, (at least as far as I know).</p>
</blockquote>

<p>When I wrote we&rsquo;d look at it in a future post I thought to myself; &ldquo;how hard can it be?&rdquo;. I had read the steps of how to build a Python language extension for Windows <a href="https://github.com/microsoft/sql-server-language-extensions/tree/master/language-extensions/python">here</a>, and it didn&rsquo;t seem that hard: some Boost, CMake, compile, and Bob&rsquo;s your uncle! Well, it turned out it was somewhat more complicated than what I anticipated. So, if you are interested - read on!</p>

<p></p>

<h2 id="background">Background</h2>

<p>As I wrote in the <a href="/2020/12/29/updated-bring-your-own-r--python-runtimes-to-sql-server-extensibility-framework/">post mentioned above</a>, Microsoft open-sourced R and Python language extensions in September 2020. This allows us to bring our own R/Python runtime to SQL Server 2019, (CU3+), and use those runtimes in SQL Server Machine Learning Services. We may want to do this because we want to use a later version of the runtime, for example.</p>

<p>Language extensions are C++ dll&rsquo;s acting as a bridge between SQL Server and the external runtime. The language extensions implement the <a href="https://docs.microsoft.com/en-us/sql/language-extensions/reference/extensibility-framework-api?view=sql-server-ver15">Extensibility Framework API for SQL Server</a></p>

<p>In the post, we looked at using Python 3.7.9, and what we did was:</p>

<ul>
<li>installed Python 3.7.9.</li>
<li>after installing Python, we installed the <code>pandas</code> module, as it is not installed by default: <code>pip install pandas</code>.</li>
<li>ensured that Python was on the <code>PATH</code>,</li>
<li>created a system environment variable <code>PYTHONHOME</code> pointing to the install directory of Python 3.7.9. The  post has been updated since, and the <code>PYTHONHOME</code> variable is not necessarily needed.</li>
<li>assigned read and write permissions for the SQL Server instance-specific <em>Launchpad</em> service, and the <code>ALL APPLICATION PACKAGES</code> group to the Python install directory.</li>
<li>downloaded the Python language extension zip file to a location accessible by SQL Server.</li>
</ul>

<p>We then created an external language from the downloaded zip file:</p>

<pre><code class="language-sql">USE ExtLangDB;
GO

CREATE EXTERNAL LANGUAGE p379
FROM (CONTENT = 'W:\python-lang-extension-windows.zip'
      , FILE_NAME = 'pythonextension.dll');
GO
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Create Python Language</em></p>

<p>We see in <em>Code Snippet 1</em> how we:</p>

<ul>
<li>create the language in a database named <code>ExtLangDB</code>.</li>
<li>give the language the name of <code>p379</code>. We cannot use the name Python, (or R if it is an R extension), as it is a reserved keyword.</li>
<li>point to the path of the zip file.</li>
<li>indicate what file in the zip file is the extension dll.</li>
</ul>

<p>To use the newly created language we:</p>

<pre><code class="language-sql">EXEC sp_execute_external_script
@language =N'p379',
@script=N'
import pandas as pd
import sys
df = pd.DataFrame(columns=[&quot;Version&quot;])
ver = sys.version
df = df.append({&quot;Version&quot;: ver}, ignore_index=True)
OutputDataSet = df'
WITH RESULT SETS (([Python Version] nvarchar(256)));
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Version Check Python</em></p>

<p>The code in <em>Code Snippet 2</em> outputs the Python version we execute against, and by setting the <code>@language</code> parameter to <code>p379</code>, we ensure we use the language we created in <em>Code Snippet 1</em>. When we run the code, we see <code>3.7.9</code> being output.</p>

<p>That&rsquo;s how we used another runtime than the one coming with SQL Server. However, when we tried to do the same with Python 3.9, we failed, as - as mentioned above - the Python language extension is version bound to Python 3.7.x. So, to use another Python version than 3.7.x, we need to re-build the existing Python extension, and - as I said above- how hard can that be, the code is open-source after all.</p>

<h2 id="pre-reqs">Pre-reqs</h2>

<p>Let us look at what we need to do this.</p>

<p>In the last post, we downloaded the zip file for the Python language extension, but in this post, we will build from source, so we start with cloning the GitHub repo for <a href="https://github.com/microsoft/sql-server-language-extensions.git">SQL Server Language extensions</a>.</p>

<p>When you drill down into the <a href="https://github.com/microsoft/sql-server-language-extensions/tree/master/language-extensions/python">root for the Python extension</a>, there is a <code>README.md</code> file that explains the steps, (those steps are the ones I referred to in the beginning), and what is needed to build the Python extension. There, (in the root for the Python extension), is also the Python extension source code.</p>

<p>Reading the steps in the <code>README.md</code> file, we see some requirements:</p>

<ul>
<li>CMake for Windows</li>
<li>Boost Python.</li>
<li>C++ tools for CMake.</li>
</ul>

<p>In addition to this we obviously need SQL Server and Python.</p>

<h4 id="sql-server">SQL Server</h4>

<p>No changes from <a href="/2020/12/29/updated-bring-your-own-r--python-runtimes-to-sql-server-extensibility-framework/">previous post</a>. We need CU3+ with <em>Machine Learning Services and Language Extensions</em> installed. Execution of external scripts needs to be enabled as well. We covered all this in detail in the <a href="/2020/12/29/updated-bring-your-own-r--python-runtimes-to-sql-server-extensibility-framework/">last post</a>. In this post, we use the same database as in the last post: <code>ExtLangDB</code>.</p>

<h4 id="python">Python</h4>

<p>Instead of Python 3.7.9, we use Python 3.9.1. When installing Python on the SQL Server machine, make sure you &ldquo;Install for all users&rdquo;:</p>

<p><img src="/images/posts/py39-ext-install-python.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Python Install on the SQL Server Box</em></p>

<p>In <em>Figure 1</em> we see how I have checked the &ldquo;Install for all users&rdquo; box. On the SQL Server box you also should install <code>pandas</code>: <code>pip install pandas</code>, as we will use that in our testing script. When you install <code>pandas</code>, actually when installing any Python modules, ensure they are installed system-wide, and not for the installing user.</p>

<blockquote>
<p><strong>NOTE:</strong> Make sure the Python installation on the SQL Server box is on the <code>PATH</code>.</p>
</blockquote>

<p>If you create the Python extension on another machine than where SQL Server is, you do not need to install <code>pandas</code>, but you should at least install <code>numpy</code>, (<code>numpy</code> is part of <code>pandas</code>).</p>

<h4 id="cmake-for-windows">CMake for Windows</h4>

<p>CMake is an open-source, cross-platform family of tools designed to build, test and package software. I install it via Chocolatey: <code>choco install cmake -y</code>. Alternatively, you can download CMake from <a href="https://cmake.org/download/">here</a>.</p>

<p>CMake is used when building the extension, so install it on the box where you will do the build.</p>

<h4 id="c-tools-for-cmake">C++ Tools for CMake</h4>

<p>This is the C++ compiler that CMake will use. In the instructions on the <a href="https://github.com/microsoft/sql-server-language-extensions/tree/master/language-extensions/python">building page</a>, it says you get it from &ldquo;Build Tools for Visual Studio 2017&rdquo;. I am using &ldquo;Build Tools for Visual Studio 2019&rdquo; instead. As with CMake, this should be on the box where you will do the build.</p>

<blockquote>
<p><strong>NOTE:</strong> If you use the 2019 compiler you will need to change part of the build script and some of the source code. More about that below.</p>
</blockquote>

<p>Whichever version you use, install by following the instructions on the page.</p>

<p>That should be it for pre-reqs, except for <code>Boost.Python</code>.</p>

<h2 id="boost">Boost</h2>

<p>Boost is a set of C++ libraries complementing the C++ standard libraries. The Boost libraries provide support for tasks and structures such as linear algebra, pseudorandom number generation, multithreading, image processing, regular expressions, and unit testing.</p>

<p>Boost also allows us to interact between C++ and Python, via <code>Boost.Python</code>. In the Python extension, Boost is used - among other things - to interact with the runtime, execute scripts, as well as to interact with <code>numpy</code>.</p>

<p>Most of the Boost libraries are pre-built, however <code>Boost.Python</code> needs to be built before we can use it. Unfortunately it is not very clear how to build <code>Boost.Python</code>, and that is - I believe - why this became complicated. Or it is because I am stupid - the jury is still out on that one.</p>

<p>Anyway, originally I had planned how to build <code>Boost.Python</code> to be part of this post - but after a while, I realized it deserved a post of its own. So, before you go any further read <a href="/2021/01/03/build-boost.python--numpy-in-windows/"><strong>Build Boost.Python &amp; Numpy in Windows</strong></a>, to build <code>Boost.Python</code>.</p>

<p>Welcome back!</p>

<h4 id="directory-layout">Directory Layout</h4>

<p>When building <code>Boost.Python</code> we tell the build engine we want the files, etc., created by the build to be output to a certain directory. In my <a href="/2021/01/03/build-boost.python--numpy-in-windows/">build post</a>, the output directory was <code>C:\boost175</code>. Looking at the content of the directory, we see something like:</p>

<p><img src="/images/posts/py39-ext-boost-output.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Boost Build Output</em></p>

<p>Outlined in yellow, (ish), in <em>Figure 2</em> we see the directory hierarchy created from the build, with <code>boost175</code> as root. We see a <code>lib</code> directory outlined in red and how that directory contains two <code>lib</code> files. These <code>lib</code> files are needed when building the extension. In green, we have an <code>include</code> directory, and two levels down in the <code>include</code> directory structure is a directory named <code>boost</code>, (outlined in blue).</p>

<p>When building the extension, the build script expects the <code>boost</code> directory to sit directly below the <code>include</code> directory, so before we go any further copy/move <code>boost</code> to underneath <code>include</code>.</p>

<h2 id="system-environment-variables">System Environment Variables</h2>

<p>We are almost ready to build the extension, but before we can build, we need to set some system environment variables, so the build scripts will work.</p>

<p>The <a href="https://github.com/microsoft/sql-server-language-extensions/tree/master/language-extensions/python">Python Language Extension</a> page mentions these variables:</p>

<ul>
<li><code>CMAKE_ROOT</code>: pointing at CMake&rsquo;s install directory. On my box, it is <code>C:\Program Files\CMake</code>.</li>
<li><code>BOOST_ROOT</code>: from my experiments that should point to the Boost output directory mentioned above, (<code>C:\boost175</code> on my box).</li>
<li><code>PYTHONHOME</code>:  <del>this is not build related, but a variable that SQL Server uses to find the Python installation.</del> This variable is required on the box where you build the extension. The variable bonds the correct Python version to the extension.</li>
</ul>

<p>When I first tried building with the variables as above the build scripts threw an error saying it needed an extra variable: <code>BOOST_PYTHON_ROOT</code>. So set <code>BOOST_PYTHON_ROOT</code> to point to the directory containing the two <code>lib</code> files we saw in <em>Figure 2</em>. For me it is <code>C:\boost175\lib</code>, (as in <em>Figure 2</em>).</p>

<p>We should now be set up for getting the show on the road.</p>

<h2 id="build">Build</h2>

<p>As we have a later version of Python than the original extension and a newer compiler we need to do some changes to build scripts and source code.</p>

<h4 id="cmakelists">CMakeLists</h4>

<p>The <code>CMakeLists.txt</code> file, located in the root of the <code>src</code> directory contains a set of directives and instructions describing the project&rsquo;s source files and targets (executable, library, or both).</p>

<p>We want to change the Python version, as well s the names of the <code>lib</code> files. As I build for Windows, I change it in the code-block related to Windows:</p>

<p><img src="/images/posts/py39-ext-cmake-change-from.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>CMakeLists Change From</em></p>

<p>We see in <em>Figure 3</em> the code blocks, (outlined in red and blue), we need to change, and we change it to:</p>

<p><img src="/images/posts/py39-ext-cmake-change-to.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>CMakeLists Change To</em></p>

<p>The two figures above show what we change:</p>

<ul>
<li>at line 74 we change the Python version from <code>python37</code>, to <code>python39</code>.</li>
<li>at lines 84 and 85 we change the Python and Numpy <code>lib</code> files to what we built in Boost, (the files we see in <em>Figure 2</em>).</li>
</ul>

<p>Notice that I only change the code for release build as we have not built debug versions of the <code>lib</code> files. If you build a debug version you should also change lines 80 and 81.</p>

<h4 id="build-script-build-python-extension-cmd">Build Script - build-python-extension.cmd</h4>

<p>The <code>build-python-extension.cmd</code> file is the script that kicks off the build. As we are using the VS 2019 compiler, we need to do some changes to this file.</p>

<p>We do the first change at line 88, where we change the path to the <code>VsDevCmd.bat</code> file. For my box I change <code>2017</code> in: <code>C:\Program Files (x86)\Microsoft Visual Studio\2017\...</code> to <code>2019</code>.</p>

<p>We also need to change the call to <code>cmake.exe</code> on line 99 - 107 to:</p>

<p><img src="/images/posts/py39-ext-change-cmake.exe.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Change to CALL cmake.exe</em></p>

<p>We see in <em>Figure 5</em> what the call to <code>cmake.exe</code> should look like. I have changed the build system generator name <code>-G</code> to point to Visual Studio 2019. Using VS 2019 also requires to define a platform name. Therefore I added the <code>-A</code> option and defined it being 64-bit via <code>x64</code>.</p>

<h4 id="source-code">Source Code</h4>

<p>When we have done the changes above, we should be ready to go. However, if we were to run the build, we would get some compiler errors. Something along the lines of:</p>

<pre><code class="language-bash">The &lt;experimental/filesystem&gt; header providing 
std::experimental::filesystem is deprecated by Microsoft 
and will be REMOVED. It is superseded by the 
C++17 &lt;filesystem&gt; header providing std::filesystem.
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Compiler Error</em></p>

<p>What <em>Code Snippet 3</em> implies is that we need to change the <code>&lt;experimental/filesystem&gt;</code> header to <code>&lt;filesystem&gt;</code>, and the reference <code>std::experimental::filesystem</code> should be changed to <code>std::filesystem</code>. We need to do it for the following source code files:</p>

<ul>
<li><code>PythonLibrarySession.h</code>: change <code>&lt;experimental/filesystem&gt;</code> to <code>&lt;filesystem&gt;</code>.</li>
<li><code>PythonExtension.cpp</code>, <code>PythonExtensionUtils_win.cpp</code>, <code>PythonLibrarySession.cpp</code>, and <code>PythonLibrarySession.cpp</code>: change <code>&lt;experimental/filesystem&gt;</code> to <code>&lt;filesystem&gt;</code> and  <code>namespace fs = std::experimental::filesystem;</code> to <code>namespace fs = std::filesystem;</code>.</li>
</ul>

<blockquote>
<p><strong>NOTE:</strong> If we use the VS 2017, (or earlier), compiler we do not need to change this.</p>
</blockquote>

<p>Save the changes, and we&rsquo;re ready to build.</p>

<h2 id="build-1">Build</h2>

<p>So to build we <code>cd</code> into <code>..\language-extensions\python\build\windows</code> and:</p>

<pre><code class="language-bash">.\build-python-extension.cmd
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Build the Extension</em></p>

<p>To kick off the build we run the code in <em>Code Snippet 4</em>. The build starts, and we see something like so:</p>

<p><img src="/images/posts/py39-ext-build-ext-1.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>Build Start</em></p>

<p>We see in <em>Figure 6</em> how the build process starts and outputs information to the console. We see what SDK is used, what compiler, and whether it is <code>release</code> or <code>debug</code>, plus other &ldquo;stuff&rdquo;.</p>

<p>After a while, the build finishes, and we see output like so:</p>

<p><img src="/images/posts/py39-ext-build-ext-2.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>Build Finished</em></p>

<p>Yay, from what we see in <em>Figure 7</em> it looks like the build succeeded, and that the extension dll has been copied to a build output directory, (outlined in red).</p>

<p>The final thing to do is to create a zip file out of the extension dll.</p>

<p>We do that by executing <code>create-python-extension-zip.cmd</code> in the same directory as <code>build-python-extension.cmd</code>:</p>

<pre><code class="language-bash">.\create-python-extension-zip.cmd
</code></pre>

<p><strong>Code Snippet 5:</strong> <em>Create the Extension zip File</em></p>

<p>When we run the script in <em>Code Snippet 5</em> the extension dll is compressed into a zip file and placed in <code>..\build-output\pythonextension\windows\release\packages</code>:</p>

<p><img src="/images/posts/py39-ext-extension-zip.png" alt="" /></p>

<p><strong>Figure 8:</strong> Output <em>Extension dll</em></p>

<p>As we see in <em>Figure 8</em> the Python extension dll is now available.</p>

<p>Actually, this last step running <code>create-python-extension-zip.cmd</code> is not entirely required. When you run <code>build-python-extension.cmd</code> an extension dll file is created and it is output to <code>..\build-output\pythonextension\target\release</code>.</p>

<p>What is left is now to use this Python 3.9 extension dll.</p>

<h2 id="usage">Usage</h2>

<p>To make sure our extension works, we do pretty much what we did in the <a href="/2020/12/29/updated-bring-your-own-r--python-runtimes-to-sql-server-extensibility-framework/"><strong>Bring Your Own R &amp; Python Runtimes to SQL Server Extensibility Framework</strong></a> post:</p>

<ol>
<li>Copy the <code>python-lang-extension.zip</code> file to a location on the SQL Server box accessible to the SQL Server instance.</li>
<li>Make sure the Python installation is on the <code>PATH</code> on the SQL Server box.</li>
<li>Ensure that the <code>PYTHONHOME</code> environment variable is set on the SQL Server box and points to to the Python 3.9 installation, (on the SQL Server box). This step is not required unless you want to be able to create external libraries for your language.</li>
</ol>

<p>Having <del>set the <code>PYTHONHOME</code> environment variable</del> done the above, we have to assign read and execute permissions to the Python location. We assign the permissions to the <em>Launchpad</em> service of the SQL Server, and to the <code>ALL APPLICATION PACKAGES</code> group. From an elevated command prompt, we do:</p>

<pre><code class="language-bash"># permissions for the Launchpad service
icacls &lt;python-path&gt; /grant &quot;NT Service\MSSQLLAUNCHPAD$INST2&quot;:(OI)(CI)RX /T

# permissions for ALL APPLICATION PACKAGES
icacls &lt;python-path&gt; /grant *S-1-15-2-1:(OI)(CI)RX /T
</code></pre>

<p><strong>Code Snippet 6:</strong> <em>Setting Permissions</em></p>

<p>Notice in <em>Code Snippet 6</em> how I define the instance with <code>$instance_name</code>. If you do this for the default instance the command is without <code>$instance_name</code>, like so: <code>icacls &quot;&lt;python-path&gt; /grant &quot;NT Service\MSSQLLAUNCHPAD&quot;:(OI)(CI)RX /T</code>. The <code>ALL APPLICATION PACKAGES</code> is defined by SID <code>S-1-15-2-1</code>.</p>

<p>Let us create the external language in SQL Server:</p>

<pre><code class="language-sql">USE ExtLangDB;
GO

CREATE EXTERNAL LANGUAGE p39
FROM (CONTENT = 'W:\python-lang-extension.zip'
      , FILE_NAME = 'pythonextension.dll');
GO
</code></pre>

<p><strong>Code Snippet 7:</strong> <em>Create External Language</em></p>

<p>After we have run the code in <em>Code Snippet 7</em> restart the <em>Launchpad</em> service, and then test what we have done:</p>

<pre><code class="language-sql">EXEC sp_execute_external_script
@language =N'p39',
@script=N'
import pandas as pd
import sys
df = pd.DataFrame(columns=[&quot;Version&quot;])
ver = sys.version
df = df.append({&quot;Version&quot;: ver}, ignore_index=True)
OutputDataSet = df'
WITH RESULT SETS (([Python Version] nvarchar(256)));
</code></pre>

<p><strong>Code Snippet 8:</strong> <em>Execute against External Language</em></p>

<p>When executing the code in <em>Code Snippet 8</em> we get:</p>

<p><img src="/images/posts/py39-ext-exec-success.png" alt="" /></p>

<p><strong>Figure 9:</strong> Output <em>Success</em></p>

<p>Wohoo, as we see in <em>Figure 9</em>, it works! We are now using version 3.9.1 of Python!</p>

<blockquote>
<p><strong>NOTE:</strong> If you are unsure of what we are doing in <em>Code Snippet 7</em>, and <em>Code Snippet 8</em>, go back to the <a href="/2020/12/29/updated-bring-your-own-r--python-runtimes-to-sql-server-extensibility-framework/"><strong>Bring Your Own R &amp; Python Runtimes to SQL Server Extensibility Framework</strong></a> post, (or the <em>Recap</em> above).</p>
</blockquote>

<p>Oh, one last thing. I have mentioned this in the <a href="/2020/12/29/updated-bring-your-own-r--python-runtimes-to-sql-server-extensibility-framework/"><strong>Bring Your Own R &amp; Python Runtimes to SQL Server Extensibility Framework</strong></a>, as well as the <a href="/2021/01/03/build-boost.python--numpy-in-windows/"><strong>Build Boost.Python &amp; Numpy in Windows</strong></a> posts, but it is worth mentioning again:</p>

<p>You may get an error when you execute the code in <em>Code Snippet 8</em> along the lines of: <em>The current Numpy installation (&lt;path to numpy) fails to pass a sanity check due to a bug in the windows runtime</em>. This is due to, as the error says, a bug in Windows which impacts Numpy version 1.19.4. The bug was introduced in build 19041.488 of Windows 10 2004/20H2. It is fixed from build 20270 and upwards. However, that build is still not generally available, but you can get it from Windows Insiders Dev channel. Microsoft estimates a fix will be rolled out sometimes in January 2021.</p>

<p>If you are affected by this and can not get a Windows 10 Dev build you can solve it by downgrading <code>numpy</code> to version 1.19.3: <code>pip install --upgrade numpy==1.19.3</code>. Personally, I am now running on build 21277, and I am not impacted by the bug. <a href="https://developercommunity.visualstudio.com/content/problem/1207405/fmod-after-an-update-to-windows-2004-is-causing-a.html">Here</a> is a link if you are interested in more details about the bug.</p>

<h2 id="summary">Summary</h2>

<p>We have now built a Python language extension for Python 3.9. Along the way, we:</p>

<ul>
<li>Built <code>Boost.Python</code>.</li>
<li>Edited the build scripts and source files to handle a new Python version and compiler.</li>
<li>Built the extension dll.</li>
<li>Created the external language against the new dll.</li>
<li>Executed against the new language.</li>
</ul>

<p>It looks like all is good, but maybe not? In a future post we&rsquo;ll look at an issue we have introduced - but for now, let us bask in the glory of having created a new Python language extension.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Build Boost.Python &amp; Numpy in Windows]]></title>
    <link href="https://nielsberglund.com/2021/01/03/build-boost.python--numpy-in-windows/" rel="alternate" type="text/html"/>
    <updated>2021-01-03T13:50:55+02:00</updated>
    <id>https://nielsberglund.com/2021/01/03/build-boost.python--numpy-in-windows/</id>
    <content type="html"><![CDATA[<p>In this post, we look at how to build <code>Boost.Python</code> and <code>Numpy</code>. We look at it from a perspective where we want to use what we build as part of a bridge between SQL Server 2019 and Python. However, if you are not interested in SQL, the post should still give you some - hopefully - useful information.</p>

<p>Please note that I am a SQL dude, and my knowledge of Boost, Python and Numpy is limited at best. So take this post for what it is; the steps I took to successfully build <code>Boost.Python</code> and <code>Numpy</code> on a Windows box.</p>

<p></p>

<h2 id="background">Background</h2>

<p>In my post, <a href="/2020/12/29/updated-bring-your-own-r--python-runtimes-to-sql-server-extensibility-framework/"><strong>Bring Your Own R &amp; Python Runtimes to SQL Server Extensibility Framework</strong></a> I wrote about how we can use other R and Python runtimes in SQL Server Machine Learning Services than the ones that come &ldquo;out of the box&rdquo;. In the post, I wrote that if you want to bring a Python runtime other than version 3.7.x, (like 3.8, 3.9, etc.), you need to build your own bridge; a SQL Server Python language extension.</p>

<p>A language extension is a C++ dll acting as a bridge between SQL Server and an external runtime - in this case Python. To interact between C++ and Python you often use Boost, and for the SQL Server Python extension, Boost libraries are required.</p>

<h2 id="pre-reqs">Pre-reqs</h2>

<p>What do we need to do this:</p>

<ul>
<li>Python: I have Python 3.9.1 installed together with <code>numpy</code>.</li>
<li>Boost: well, that&rsquo;s fairly obvious. I downloaded Boost 1.75.0 from <a href="https://www.boost.org/users/download/">Boost Downloads</a>.</li>
<li>A C++ compiler, (a <code>toolset</code> in Boost speak). I use the compiler from Visual Studio 2019 - in Boost it is defined as <code>msvc-14.2</code>.</li>
</ul>

<p>Obviously, if you want to use what we do here to build a Python SQL Server Language Extension, you need the source code for the language extensions and SQL Server. That will be covered in a future post.</p>

<p>Now, let&rsquo;s get on with it.</p>

<h2 id="boost">Boost</h2>

<p>Boost is a set of C++ libraries complementing the C++ standard libraries. The Boost libraries provide support for tasks and structures such as linear algebra, pseudorandom number generation, multithreading, image processing, regular expressions, and unit testing.</p>

<p>Boost also allows us to interact between C++ and Python, via <code>Boost.Python</code>. In the Python extension, Boost is used - among other things - to interact with the runtime, execute scripts as well as to interact with <code>numpy</code>.</p>

<p>Most of the Boost libraries are pre-built, however, <code>Boost.Python</code> needs to be built before we can use it. Initially, I thought &ldquo;how hard can it be to build this&rdquo;, well - it turned out a lot more complicated than I imagined. This, in my opinion, is due to that the Boost documentation is less than stellar, which is one big reason I wrote this post.</p>

<h4 id="boost-installation">Boost Installation</h4>

<p>There is no installation file as such, I just unzipped the file I downloaded above to a location on my box: <code>C:\</code>:</p>

<p><img src="/images/posts/byor-python-boost1.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Boost Install</em></p>

<p>We see in <em>Figure 1</em> the Boost installation. The question now is what we do with it? We know from above that we somehow have to build <code>Boost.Python</code>, but what do we build with?</p>

<h4 id="bootstrap">Bootstrap</h4>

<p>It turns out that you have to bootstrap the Boost build engine, <code>Boost.Build</code>. On Windows, you do that by running the <code>bootstrap.bat</code> file, outlined in red in <em>Figure 1</em>:</p>

<pre><code class="language-bash">c:\boost_1_75_0&gt;.\bootstrap.bat vc142
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Bootstrap Boost Build Engine</em></p>

<p>In <em>Code Snippet 1</em> we see how I from command prompt have <code>cd</code>:ed into the Boost directory. I indicate that I want to use the Visual Studio 2019 toolset by defining the <code>vc142</code> flag. When I execute the script, some information is output to the console, and when the script has finished, I see:</p>

<p><img src="/images/posts/byor-python-boost2.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Boost Install</em></p>

<p>In <em>Figure 2</em> I have a couple of things outlined in red and yellow:</p>

<ul>
<li>What is outlined in red is the command you use to build with Boost: <code>b2.exe</code>. That is the executable created by <code>bootstrap.bat</code>.</li>
<li>Outlined in yellow is a configuration file. You configure your builds using - among other things - configuration files which are <code>.jam</code> files. This file <code>project-config.jam</code> is used for project-specific configuration.</li>
</ul>

<p>Let&rsquo;s talk configuration.</p>

<h4 id="configuration">Configuration</h4>

<p>Above I mentioned the <code>project-config.jam</code> file created when you run the bootstrap script. There are two more configuration files used by <code>b2.exe</code>:</p>

<ul>
<li><code>site-config.jam</code>: usually installed and maintained by a system administrator. Not installed by default.</li>
<li><code>user-config.jam</code>: for the user to configure, not installed by default. It usually defines the available compilers and other tools. We&rsquo;ll create the file to indicate what version of Python to compile against.</li>
</ul>

<p>Create a file in your home directory and name it <code>user-config.jam</code>. Edit it to look like so:</p>

<pre><code class="language-py">using python 
   : 3.9
   : C:\\Python39\\python.exe
   : C:\\Python39\\include #directory that contains pyconfig.h
   : C:\\Python39\\libs    #directory that contains python39.lib
   ;
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Configuration File</em></p>

<p>We see in <em>Code Snippet 2</em> how we indicate where to find the Python executable, Python header files, and Python lib files.hr</p>

<blockquote>
<p><strong>NOTE:</strong> Defining the Python version is not really necessary if you have only one Python version installed.</p>
</blockquote>

<p>Now when we have a configuration, it is time to build.</p>

<h4 id="build">Build</h4>

<p>As mentioned above to build, we run <code>b2.exe</code>. If we were to <code>cd</code> into the Boost install directory and just do: <code>b2</code>, then we would build everything - and it would take a while. Here we are only interested in building Python, so we need to limit what <code>b2</code> does.</p>

<p>After browsing for information around Boost, I started with something like so:</p>

<pre><code class="language-bash">b2 --with-python --prefix=c:\\boost175 address-model=64 \
                 variant=release link=static threading=multi \
                 runtime-link=shared install
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Build Take 1</em></p>

<p>Let&rsquo;s look at the code in <em>Code Snippet 3</em>, and see what it means:</p>

<ul>
<li><code>--with-python</code>: limit the build to only build Python. This will also include <code>numpy</code>.</li>
<li><code>--prefix</code>: where to build to. In <em>Code Snippet 3</em> I want everything built to be placed in a root directory: <code>c:\\boost175</code>. Notice for the files to be put into this directory, the <code>install</code> flag needs to be set.</li>
<li><code>address-model=64</code>: specifies if 32-bit or 64-bit code should be generated by the compiler. In my case I want 64-bit.</li>
<li><code>variant=release</code>: specifies release or debug, or both.</li>
<li><code>link=static</code>: defines whether to create <code>static</code> or <code>shared</code> libraries. For the Python extension, we want <code>static</code>. Read more about <code>static</code> vs. <code>shared</code> <a href="https://www.boost.org/doc/libs/1_75_0/libs/python/doc/html/building/choosing_a_boost_python_library_.html">here</a>.</li>
<li><code>threading=multi</code>: threading model.</li>
<li><code>runtime-link=shared</code>: determines if shared or static version of C and C++ runtimes should be used.</li>
<li><code>install</code>: ensures that the built files are put into the <code>--prefix</code> directory.</li>
</ul>

<p>Running the code we see in <em>Code Snippet 3</em> &ldquo;spews&rdquo; out a lot of information to the console, and if something is not working correctly, it can be difficult to see what is going wrong, due to the amount of data being output.</p>

<p>An example of something going wrong was when I initially ran this code on Windows 10; the <code>boost175</code> directory was created as expected. However, when I drilled down in the directory, I saw only a Python lib file, but no Numpy lib file. I knew there should be both Python and Numpy files, so something was clearly not right.</p>

<p>After tearing my hair out for quite a while, I came across the <a href="https://boostorg.github.io/build/manual/develop/index.html">B2 User Manual</a>, and <a href="https://boostorg.github.io/build/manual/develop/index.html#bbv2.overview.invocation.options">invocation options</a>. In there, I found two option flags:</p>

<ul>
<li><code>--debug-configuration</code>: this flag tells <code>b2</code> to produce debug information about the loading of <code>b2</code> and toolset files.</li>
<li><code>-d0</code>: suppresses all informational messages.</li>
</ul>

<pre><code class="language-bash">b2 --with-python --prefix=c:\\boost175 \
                 --debug-configuration \
                 -d0 \
                 address-model=64 \
                 variant=release link=static threading=multi \
                 runtime-link=shared install
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Build Take 2</em></p>

<p>When I ran the build as in <em>Code Snippet 4</em> it produced some useful output:</p>

<p><img src="/images/posts/byor-python-numpy-error.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Numpy Error</em></p>

<p>We see in <em>Figure 3</em> (outlined in red) a bug in Windows 10 2004/20H2, (from build 19041.488), impacting Numpy. It is fixed from build 20270 and upwards. However, that build is still not generally available, (you can get it from Windows Insiders Dev channel). Microsoft estimates a fix will be rolled out sometimes in January 2021. If you are affected by this and you can not get a Windows 10 Dev build  you can solve it by downgrading <code>numpy</code> to version 1.19.3: <code>pip install --upgrade numpy==1.19.3</code>.</p>

<blockquote>
<p><strong>NOTE:</strong> The link <a href="https://developercommunity.visualstudio.com/content/problem/1207405/fmod-after-an-update-to-windows-2004-is-causing-a.html">here</a> has more information about the bug.</p>
</blockquote>

<p>I have since then upgraded to the latest Windows Dev build and when I run the code, everything works fine:</p>

<p><img src="/images/posts/byor-python-lib-files.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Lib Files</em></p>

<p>In <em>Figure 4</em> we see how we have lib files for both Python and Numpy! Success!</p>

<p>When you look at the console output when you do the build you may see something like so:</p>

<p><img src="/images/posts/byor-python-boost-arch.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Boost Address Model &amp; Architecture</em></p>

<p>Hmm, that does not look right. In the code we definitely said <code>address-model=64</code>, but in the output, it says 32 bit. It turns out this is a bug in the build output, so nothing to worry about.</p>

<h2 id="summary">Summary</h2>

<p>We have in this post looked at how to build <code>Boost.Python</code>, and <code>Numpy</code> on a Windows 10 box:</p>

<ol>
<li>Ensure we have Python and Numpy installed.</li>
<li>Download Boost and unzip.</li>
<li>Ensure you have a C++ compiler installed.</li>
<li>Run the <code>bootstrap.bat</code> script, and optionally define the compiler, (<code>toolset</code>).</li>
<li>Execute <code>b2.exe</code> as per <em>Code Snippet 4</em> to build <code>Boost.Python</code> and <code>Numpy</code>.</li>
</ol>

<p>You can now start to use the libs created. In a future post we&rsquo;ll see how to create the SQL Server Python extension, using the files above.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Christmas, New Year, Week 1, 2021]]></title>
    <link href="https://nielsberglund.com/2021/01/03/interesting-stuff---christmas-new-year-week-1-2021/" rel="alternate" type="text/html"/>
    <updated>2021-01-03T07:20:06+02:00</updated>
    <id>https://nielsberglund.com/2021/01/03/interesting-stuff---christmas-new-year-week-1-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This is the &ldquo;roundup&rdquo; of the posts that have been most interesting to me over the Christmas and New Year period 2020, and the first week of 2021.</p>

<p></p>

<h2 id="data-architecture">Data Architecture</h2>

<ul>
<li><a href="https://martinfowler.com/articles/data-mesh-principles.html">Data Mesh Principles and Logical Architecture</a>. The post here is a follow up to <a href="https://martinfowler.com/articles/data-monolith-to-mesh.html">How to Move Beyond a Monolithic Data Lake to a Distributed Data Mesh</a>. It summarizes the data mesh approach by enumerating its underpinning principles, and the high level logical architecture that the principles drive. If you are into data architecture, you have to read this post!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://databricks.com/blog/2020/12/15/handling-late-arriving-dimensions-using-a-reconciliation-pattern.html">Handling Late Arriving Dimensions Using a Reconciliation Pattern</a>. The blog post linked to here looks at a few use cases of late-arriving dimensions and potential solutions to handle it in Apache Spark pipelines.</li>
<li><a href="https://www.confluent.io/blog/introducing-confluent-parallel-message-processing-client/">Introducing the Confluent Parallel Consumer</a>. This blog post looks at a new Kafka consumer client: the <a href="https://github.com/confluentinc/parallel-consumer">Confluent Parallel Consumer</a>. The post covers why a new consumer client is needed and the use cases for this consumer. Very interesting!</li>
<li><a href="https://www.manning.com/books/event-streaming-with-kafka-streams-and-ksqldb">Event Streaming with Kafka Streams and ksqlDB</a>. The link here is to the revised new edition of <strong>Kafka Streams in Action</strong>. It has been expanded to cover more of the Kafka platform used for building event-based applications, including full coverage of ksqlDB. I bought it, and you should buy it as well!</li>
<li><a href="https://www.confluent.io/blog/ksqldb-0-14-0-features-updates/">Announcing ksqlDB 0.14.0</a>. As the title implies, a new version of ksqlDB is out in the wild. This post looks at some of the most notable changes, and new features of this release. Some quite &ldquo;juicy stuff&rdquo; in the release!</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>When I went on leave for Christmas, and New Year I said to myself that I had to get some blog-posts out, and for once my plans came together:</p>

<ul>
<li><a href="/2020/12/21/a-lap-around-sql-server-2019-big-data-cluster-architecture/">A Lap Around SQL Server 2019 Big Data Cluster: Architecture</a>. Finally, finally, finally! This post is a follow on from <a href="/2020/04/26/a-lap-around-sql-server-2019-big-data-cluster-background--technology/">A Lap Around SQL Server 2019 Big Data Cluster: Background &amp; Technology</a>, and it has been in the works for nearly eight months. What can I say? In the post, we look at the architecture of a SQL Server 2019 Big Data Cluster, and the various components of a BDC.</li>
<li><a href="/2020/12/29/updated-bring-your-own-r--python-runtimes-to-sql-server-extensibility-framework/">Bring Your Own R &amp; Python Runtimes to SQL Server Extensibility Framework</a>. In September 2020, Microsoft <a href="https://cloudblogs.microsoft.com/sqlserver/2020/09/24/open-sourcing-the-r-and-python-language-extensions-for-sql-server/">announced</a> that they have open-sourced the R and Python language extensions for SQL Server Machine Learning Services. As a result, we can now bring our own versions of R and Python to SQL Server 2019. In the post linked to I look at how to use a Python runtime with a later version then what is by default shipped in SQL Server Machine Learning Services.</li>
</ul>

<p>So that&rsquo;s what I have done.</p>

<p>I am now working on a couple of posts on how to create your own Python language extension from the open-sourced code Microsoft released. Expect something to be out fairly soon. Yeah, yeah, I know - that&rsquo;s what I said about the Big Data Cluster architecture post as well back in April 2020. I guess we&rsquo;ll see.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[UPDATED: Bring Your Own R &amp; Python Runtimes to SQL Server Extensibility Framework]]></title>
    <link href="https://nielsberglund.com/2020/12/29/updated-bring-your-own-r--python-runtimes-to-sql-server-extensibility-framework/" rel="alternate" type="text/html"/>
    <updated>2020-12-29T12:47:45+02:00</updated>
    <id>https://nielsberglund.com/2020/12/29/updated-bring-your-own-r--python-runtimes-to-sql-server-extensibility-framework/</id>
    <content type="html"><![CDATA[<blockquote>
<p><strong>NOTE:</strong> This post has been updated after it was originally published. I have edited information about the <code>PYTHONHOME</code> system variable, as well as <code>PATH</code> requirements.</p>
</blockquote>

<p>Back in the day I wrote quite a few blog posts about <a href="https://nielsberglund.com/categories/sql-server-machine-learning-services/"><strong>SQL Server Machine Learning Services</strong></a> as well as <a href="https://nielsberglund.com/categories/sql-server-extensibility-framework/"><strong>SQL Server Extensibility Framework</strong></a>, and <a href="https://nielsberglund.com/categories/sql-server-extensibility-framework/"><strong>External Languages</strong></a>. <strong>SQL Server Machine Learning Services</strong> is very cool, but a complaint has been that you are restricted in what versions of R and Python you use.</p>

<p>In September 2020, Microsoft <a href="https://cloudblogs.microsoft.com/sqlserver/2020/09/24/open-sourcing-the-r-and-python-language-extensions-for-sql-server/">announced</a> that they have open-sourced the technology behind SQL Server Extensibility Framework. As a result, we can now bring our own versions of R and Python to SQL Server 2019.</p>

<blockquote>
<p><strong>NOTE:</strong> To bring your own R/Python you need SQL Server 2019 CU3 or above.</p>
</blockquote>

<p>In this blog post, we look at how to do that.</p>

<p></p>

<h2 id="background">Background</h2>

<p>In SQL Server 2016, Microsoft introduced SQL Server R Services. From inside SQL Server, that allowed you to call to the R engine via a special procedure, <code>sp_execute_external_script</code>, and execute R scripts.</p>

<p>In SQL Server 2017, Microsoft added Python as an external language and renamed SQL Server R Services to SQL Server Machine Learning Services. As with R, you execute your Python scripts using <code>sp_execute_external_script</code>.</p>

<p>Both the R runtime and the Python runtime are part of the SQL Server install, but they run as an external process. The runtimes are based on the open-source versions, but they have some Microsoft proprietary additions. The communication between SQL Server and the external engine goes over the <em>Launchpad</em> service:</p>

<p><img src="/images/posts/byor-r-and-p-launchpad1.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>External Script and Language</em></p>

<p>We see in <em>Figure 1</em> how:</p>

<ol>
<li>We execute the procedure <code>sp_execute_external_script</code>, notice the <code>@language</code> parameter.</li>
<li>That calls into the <em>Launchpad</em> service.</li>
<li>The <em>Launchpad</em> service passes the script into the relevant launcher based on the <code>@language</code> parameter in <code>sp_execute_external_script</code>. The knowledge of what launcher to call lives inside of the <em>Launchpad</em> service.</li>
<li>The launcher dll, a C++ dll, loads the relevant external engine and passes the script to the engine and executes.</li>
</ol>

<p>If you want to know more about R and Python&rsquo;s implementation in SQL Server, I suggest you look at the link above about <a href="https://nielsberglund.com/categories/sql-server-machine-learning-services/"><strong>SQL Server Machine Learning Services</strong></a>.</p>

<h4 id="java">Java</h4>

<p>In CTP 2.0 of SQL Server 2019, Microsoft made Java publicly available as an external language together with R and Python. Having Java as an external language may not seem that much different from R/Python, but there are some differences:</p>

<ul>
<li>Java is a compiled language, where we call into a specific method. R/Python are scripting languages where we send a script to the engine.</li>
<li>R/Python are part of the SQL Server install, together with launcher dll&rsquo;s and so forth. There is an equivalent of a launcher dll for Java, (<code>javaextension.dll</code>), which calls into the JVM. The difference here between R/Python and Java is that the JVM is not part of the SQL Server install but must be installed separately.</li>
</ul>

<p>Microsoft could have done with the Java integration in SQL Server 2019 to just treat it as R/Python, and &ldquo;hardcode&rdquo; Java as a language in the <em>Launchpad</em> service and let the <em>Launchpad</em> service call the <code>javaextension.dll</code>.</p>

<p>However, Microsoft did not &ldquo;hack&rdquo; the <em>Launchpad</em> service, but what they did was, with the view to &ldquo;properly&rdquo; expose an extensibility framework with multiple external languages, that they introduced a new &ldquo;host&rdquo; for external languages: <code>ExtHost.exe</code>. The <em>Launchpad</em> service calls this host for all languages except the built-in R/Python:</p>

<p><img src="/images/posts/byor-r-and-p-launchpad2.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>External Host</em></p>

<p>In <em>Figure 2</em> we see how:</p>

<ol>
<li>We execute the procedure <code>sp_execute_external_script</code>, with <code>Java</code> being the value of the <code>@language</code> parameter.</li>
<li>The proc calls into the <em>Launchpad</em> service as above.</li>
<li>Since the <code>@language</code> parameter is not <code>R</code> or <code>Python</code>, the <em>Launchpad</em> service call into <code>ExtHost.exe</code>.</li>
<li>The <code>ExtHost.exe</code> calls a well-known entry point in the language extension dll.</li>
<li>The language extension dll loads the external runtime and executes the code.</li>
</ol>

<p>OK, the above does not seem that different from R/Python, but hang on a minute? With R and Python the <em>Launchpad</em> service knows about the launchers, (language extension dll&rsquo;s), what about step 4 above; how does the <em>ExternalHost</em> know which language extension to call?</p>

<h4 id="external-language">External Language</h4>

<p>The answer to the question above is <code>EXTERNAL LANGUAGE</code>. When Microsoft introduced Java, they also introduced the notion of an external language. The external language is the path, (or bytes), to a zip file containing the language extension dll. So in the case of Java, it is the path to the zip file where <code>javaextension.dll</code> is:</p>

<pre><code class="language-sql">CREATE EXTERNAL LANGUAGE Java
FROM (CONTENT = 'W:\javaextension.zip'
      , FILE_NAME = 'javaextension.dll');
GO
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Creating External Language</em></p>

<p>In <em>Code Snippet 1</em>, we set the file name in the <code>FILE_NAME</code> parameter because the zip file may contain multiple files and the file name defines the language extension.</p>

<p>When Microsoft introduced external languages, they also introduced some new system catalog views:</p>

<ul>
<li><code>sys.external_languages</code> - contains a row for each external language in the database.</li>
<li><code>sys.external_language_files</code> - contains a row for each external language extension file in the database.</li>
</ul>

<p>You use the catalog views above to see what external languages exist in a database.</p>

<blockquote>
<p><strong>NOTE:</strong> External languages are database scoped.</p>
</blockquote>

<p>The <a href="https://nielsberglund.com/categories/sql-server-extensibility-framework/"><strong>External Languages</strong></a> link above has links to blog posts to get more information about external languages.</p>

<h4 id="recap">Recap</h4>

<p>So to recap the <em>Background</em>:</p>

<ul>
<li>The R and Python runtimes have Microsoft specific additions, and they are installed together with SQL Server. The launchers are closed source C++ dll&rsquo;s.</li>
<li>The Java language extension, (&ldquo;launcher&rdquo;), is closed source - but you are not tied to a Java version. Well, the version has to be 8+.</li>
<li>When using Java, the language extension needs to be registered and tied to the language.</li>
</ul>

<p>This was the &ldquo;lie of the land&rdquo; up until September this year, (2020).</p>

<h2 id="open-source">Open Source</h2>

<p>When Java became a supported language in SQL Server 2019, Microsoft mentioned that communication between <em>ExternalHost</em> and the language extension should be based on an API, regardless of the external language. The API is the <a href="https://docs.microsoft.com/en-us/sql/language-extensions/reference/extensibility-framework-api?view=sql-server-ver15">Extensibility Framework API for SQL Server</a>. Having an API ensures simplicity and ease of use for the extension developer.</p>

<p><strong>Start edit: edited when Java was open-sourced</strong></p>

<p>From the paragraph above, one can assume that Microsoft would like to see 3rd party development of language extensions. That assumption turned out to be accurate as, Microsoft open-sourced the <a href="https://github.com/microsoft/sql-server-language-extensions/tree/master/language-extensions/java">Java language extension</a>, together with the <a href="https://github.com/microsoft/sql-server-language-extensions/tree/master/extension-host/include">include files for the extension API</a>, in <del>September</del> March 2020! This means that anyone interested can now create a language extension for their own favorite language!</p>

<p><del>However, open sourcing the Java extension was not the only thing Microsoft did.</del> Furthermore, in September 2020,  they also created and open-sourced language extensions for R and Python!</p>

<p><strong>End Edit</strong></p>

<blockquote>
<p><strong>NOTE:</strong> Microsoft did not open-source the R and Python launcher dll&rsquo;s, but they developed new extensions which do not require the Microsoft implementation of the R and Python runtimes.</p>
</blockquote>

<p>So, if you are on SQL Server 2019 CU3+, you can now bring your own R and Python runtimes to SQL server. In the rest of this post, we look at bringing a Python runtime to SQL Server 2019.</p>

<blockquote>
<p><strong>NOTE:</strong> In this post we bring our own Python runtime, but there is no difference if you want to install an R runtime instead.</p>
</blockquote>

<h2 id="pre-reqs">Pre-reqs</h2>

<p>Before we dive into the &ldquo;nitty-gritty&rdquo;, let&rsquo;s look at what you need in order to bring your own Python runtime to SQL server.</p>

<h4 id="sql-server-2019">SQL Server 2019</h4>

<p>Well, duh - that&rsquo;s quite obvious. However, don&rsquo;t forget you need to be on CU3 or above, (in this post I use CU4). Ensure that <em>Machine Learning Services and Language Extensions</em> are installed, (or part of the installation):</p>

<p><img src="/images/posts/byor-r-and-p-install.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Installation Machine Learning Services and Language Extensions</em></p>

<p>As we will use our own Python runtime, we only check the <em>Machine Learning Services and Language Extensions</em> checkbox during installation or upgrade, as in <em>Figure 3</em>. That ensures that the <em>Launchpad</em> service and <code>ExtHost.exe</code> are installed.</p>

<blockquote>
<p><strong>NOTE:</strong> If you already have installed the Microsoft R/Python runtimes the necessary, Microsoft proprietary, extensions are there. Also, you can run &ldquo;your&rdquo; R/Python runtime side by side with the Microsoft ones.</p>
</blockquote>

<p>If this is a new installation of SQL Server you also need to enable the execution of external scripts:</p>

<pre><code class="language-sql">EXEC sp_configure 'external scripts enabled', 1
RECONFIGURE WITH OVERRIDE;
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Enable Execution of External Scripts</em></p>

<p>As we see in <em>Code Snippet 2</em> we use <code>sp_configure</code> to enable external script execution.</p>

<p>When we use external languages; Java, etc., the languages are databased scoped. So the code we see in <em>Code Snippet 1</em> needs to be run in any database you want to use Java.</p>

<p>Let us create a database to use for our Python runtime:</p>

<pre><code class="language-sql">USE Master;
GO

IF EXISTS(SELECT 1 FROM sys.databases WHERE name = 'ExtLangDB')
BEGIN
  DROP DATABASE 'ExtLangDB'
END
GO

CREATE DATABASE ExtLangDB;
GO
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Creating Database</em></p>

<p>After creating the database, as in <em>Code Snippet 3</em>, we can look at Python&rsquo;s base requirements.</p>

<h4 id="python">Python</h4>

<p>Another duh! But, let us talk about why we would like to use a different runtime. One reason could be using a more recent version of the runtime than the Microsoft provided runtime. I mentioned above that I am using SQL Server 2019 CU4 in this post. When I installed SQL Server, I chose to install the Microsoft provided runtimes for R and Python. I check the version like so:</p>

<pre><code class="language-sql">EXEC sp_execute_external_script
@language =N'Python',
@script=N'
import pandas as pd
import sys
df = pd.DataFrame(columns=[&quot;Version&quot;])
ver = sys.version
df = df.append({&quot;Version&quot;: ver}, ignore_index=True)
OutputDataSet = df'
WITH RESULT SETS (([Python Version] nvarchar(256)));
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Version Check Python</em></p>

<p>In <em>Code Snippet 4</em> I use Python&rsquo;s <code>sys.version</code> to get the version, and I add that to data frame. The data frame is then assigned to the return dataset represented by <code>OutputDataSet</code>. The result when we execute:</p>

<p><img src="/images/posts/byor-r-and-p-python-version-l.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Python Version</em></p>

<p>The version of Python, as we see in <em>Figure 4</em> is 3.7.1. For some reason or another, I would like to use 3.7.9. So we need to ensure we have Python 3.7.9 <a href="https://cloudblogs.microsoft.com/sqlserver/2020/09/24/open-sourcing-the-r-and-python-language-extensions-for-sql-server/">downloaded</a> and installed on the SQL Server box.</p>

<p><strong>Start edit: following NOTE added 2021-01-09</strong></p>

<blockquote>
<p><strong>NOTE:</strong> When installing Python you need install it with administrator permissions: &ldquo;Run as administrator&rdquo;. The top level Python directory as well as the <code>.\Scripts</code> directory needs to be added to the <code>PATH</code>.</p>
</blockquote>

<p><strong>End Edit</strong></p>

<p>After you have installed Python install the <code>pandas</code> module, as it is not installed by default: <code>pip install pandas</code>.</p>

<h2 id="python-language-extension">Python Language Extension</h2>

<p>As mentioned above, the language extensions for Java, R, and Python are open-sourced, so you can download the source code and build them yourself. However, for now, we&rsquo;ll use <a href="https://github.com/microsoft/sql-server-language-extensions/releases">prebuilt binaries</a>.</p>

<p>Browse to the Python language extension page, and download the version for your platform. For me, it is <code>python-lang-extension-windows.zip</code>.</p>

<blockquote>
<p><strong>NOTE:</strong> On the page there are both release and debug versions of the extension. Use the release version in production!</p>
</blockquote>

<p>Download to somewhere SQL Server has access to. In my case, I downloaded it to the root of my <code>w:\</code> drive.</p>

<p>When the extension is downloaded, it is time to register/install the extension.</p>

<h2 id="installation">Installation</h2>

<p>Before we register the extension, let us see what the zip file contains:</p>

<p><img src="/images/posts/byor-r-and-p-python-pythonextension.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Python Extension</em></p>

<p>When I open the zip file I see it contains one file, <code>pythonextension.dll</code> - as in <em>Figure 5</em>. Knowing the file name is good as we need to supply that when we register the external extension, like in <em>Code Snippet 1</em>.</p>

<p>So, let&rsquo;s do it, let&rsquo;s register the Python extension in the database we created in <em>Code Snippet 3</em>:</p>

<pre><code class="language-sql">USE ExtLangDB;
GO

CREATE EXTERNAL LANGUAGE p379
FROM (CONTENT = 'W:\python-lang-extension-windows.zip'
      , FILE_NAME = 'pythonextension.dll');
GO
</code></pre>

<p><strong>Code Snippet 5:</strong> <em>Create Python Language</em></p>

<p>In <em>Code Snippet 5</em> we see how I:</p>

<ul>
<li>name the language <code>p379</code>. I cannot name it Python, or R, as they are reserved for Microsoft R and Python.</li>
<li>point to the path of the zip file.</li>
<li>say what file in the zip file is the extension dll.</li>
</ul>

<p>After I have executed the code I can check that it succeeded by: <code>SELECT * FROM sys.external_languages</code>:</p>

<p><img src="/images/posts/byor-r-and-p-python-ext-langs.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>External Languages</em></p>

<p>We can see from <em>Figure 6</em>, (outlined in red), that we succeeded in creating our own Python based external language. We also see R and Python as external languages.</p>

<blockquote>
<p><strong>NOTE:</strong> You see R and Python as external languages regardless if you have installed them or not. Even if you haven&rsquo;t ticked the box for <em>Machine Learning Services and Language Extensions</em> checkbox, you will see R and Python.</p>
</blockquote>

<p>Cool, let us execute the code in <em>Code Snippet 4</em>, and see what happens. Before we run the code, change the <code>@language</code> parameter to be <code>p379</code> instead of <code>Python</code>, and then execute.</p>

<p>Hmm, bummer we get a very un-informative error along the lines of:</p>

<p><img src="/images/posts/byor-r-and-p-python-exec-error2.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>Error</em></p>

<p>In <em>Figure 7</em> we see the error message with a result of <code>0x8004004</code>. The <code>HRESULT 0x8004004</code> is <code>E_ABORT</code>, which doesn&rsquo;t really tell us anything - I did say the error was not very informative.</p>

<p>So, it turns out that when installing Python, (as well as R), we need to do some extra steps so the <em>Launchpad</em> service and <em>ExtHost</em> can do its things.</p>

<h4 id="path-permissions">Path &amp; Permissions</h4>

<p><strong>Start edit: edited around paths and environment variables 2021-01-09</strong></p>

<p><del>When we execute <code>sp_execute_external_script</code> and after <em>ExternalHost</em> has loaded the python extension dll, the extension needs to know where Python is installed, so it looks for an environment variable named <code>PYTHONHOME</code>. So the first thing we need to do is to create that variable as a system variable:</del></p>

<p>When we execute <code>sp_execute_external_script</code> and after <em>ExternalHost</em> has loaded the python extension dll, the extension needs to know where the Python runtime is. For this it uses the <code>PATH</code> variable on the SQL Server box. The extension knows which version of Python it is as the extension is compiled against a specific version, (more about that below). So, as mentioned above we need to ensure that Python is on the <code>PATH</code>.</p>

<p>When executing code, the <em>Launchpad</em> service needs to read and write to the Python directory, so we need to set permissions for that:</p>

<pre><code class="language-bash">icacls &lt;python-path&gt; /grant &quot;NT Service\MSSQLLAUNCHPAD$INST2&quot;:(OI)(CI)RX /T
</code></pre>

<p><strong>Code Snippet 6:</strong> <em>Grant Permissions to Launchpad</em></p>

<p>We need to run the code in <em>Code Snippet 6</em> from an elevated command prompt, (Powershell will not work). Notice in <em>Code Snippet 6</em> how I define the instance with <code>$instance_name</code>. If you do this for the default instance the command is without <code>$instance_name</code>, like so: <code>icacls &quot;&lt;python-path&gt; /grant &quot;NT Service\MSSQLLAUNCHPAD&quot;:(OI)(CI)RX /T</code>.</p>

<p>Having granted read and execute access to the <em>Launchpad</em> service for the instance, we do the same for the <code>ALL APPLICATION PACKAGES</code> group, which is represented by the <code>SID</code> <code>S-1-15-2-1</code>:</p>

<pre><code class="language-bash">icacls &lt;python-path&gt; /grant *S-1-15-2-1:(OI)(CI)RX /T
</code></pre>

<p><strong>Code Snippet 7:</strong> <em>Grant Permissions to ALL APPLICATION PACKAGES</em></p>

<p><strong>End Edit</strong></p>

<p>The code in <em>Code Snippet 7</em>, which needs to be run from an elevated command prompt, grants read and execute permissions to the <code>ALL APPLICATION PACKAGES</code> group.</p>

<blockquote>
<p><strong>NOTE:</strong> You can read more about <code>ALL APPLICATION PACKAGES</code>, and why we need it, <a href="https://docs.microsoft.com/en-us/sql/machine-learning/install/sql-server-machine-learning-services-2019?view=sql-server-ver15">here</a>.</p>
</blockquote>

<p>After you have granted the necessary permissions, restart the <em>Launchpad</em> service, and rerun the code:</p>

<p><img src="/images/posts/byor-r-and-p-python-version-2.png" alt="" /></p>

<p><strong>Figure 8:</strong> <em>Python 3.7.9</em></p>

<p>Wohoo, as we see in <em>Figure 8</em>, it works! We are now using version 3.7.9 of Python!</p>

<blockquote>
<p><strong>NOTE:</strong> If you run the code on a Windows 10 20H2 version you may get an error along the lines of <em>The current Numpy installation (&lt;path to numpy) fails to pass a sanity check due to a bug in the windows runtime</em>. As the error says, it is an issue with Windows and <code>Numpy</code>. The easiest way to fix it is to downgrade <code>Numpy</code> to version <code>1.19.3</code>. You do it like so: <code>pip install --upgrade numpy==1.19.3</code>.</p>
</blockquote>

<p><strong>Start edit: editing around <code>PYTHONHOME</code></strong></p>

<p>When looking at the official <a href="https://docs.microsoft.com/en-us/sql/machine-learning/install/custom-runtime-python">Microsoft documentation for the Python language extension</a> it mentions we need to add a system environment variable <code>PYTHONHOME</code>, what is that?</p>

<h4 id="pythonhome">PYTHONHOME</h4>

<p>Initially I thought the <code>PYTHONHOME</code> variable was used by the extension host to find the Python runtime, but that is not entirely correct. The <code>PYTHONHOME</code> variable is used when creating external libraries for the external Python language. So if you are only calling <code>sp_execute_external_script</code> the variable is not needed.</p>

<p>However, if we want to be able to create external libraries we need to create the variable:</p>

<p><img src="/images/posts/byor-r-and-p-pythonhome.png" alt="" /></p>

<p><strong>Figure 9:</strong> <em>Create PYTHONHOME</em></p>

<p>In <em>Figure 9</em>, we see how I set the value of <code>PYTHONHOME</code> to where Python is installed. Having the variable makes it somewhat easier to set the required permissions we saw in <em>Code Snippet 6</em>, and <em>Code Snippet 7</em> as we can doe something like so: <code>icacls &quot;%PYTHONHOME%&quot; /grant ...</code>. We do not need to explicitly set the path.</p>

<p>There is a downside with <code>PYTHONHOME</code> where it can cause unexpected errors, and we look at that in the <a href="/2021/01/09/solve-python-issues-in-sql-server-machine-learning-services-after-deploying-python-3.9/"><strong>Solve Python Issues in SQL Server Machine Learning Services After Deploying Python 3.9</strong></a> post. That post led to the updates to this post.</p>

<p><strong>End Edit: <code>PYTHONHOME</code></strong></p>

<p>We are almost ready for a summary, but one last thing.</p>

<h4 id="python-extension-and-python-versions">Python Extension and Python Versions</h4>

<p>Cool, so above we&rsquo;ve seen how I can bring my own Python runtime, and in this case, it was 3.7.9. What about if I wanted a later version, let&rsquo;s say 3.9?</p>

<p>It so happens that I have Python 3.9.1 installed on my machine, so I:</p>

<ul>
<li><del>change <code>PYTHONHOME</code> to point to where 3.9 is.</del></li>
<li>ensure Python 3.9.1 is on the <code>PATH</code></li>
<li>apply the necessary permissions for the <em>Launchpad</em> service and <code>ALL APPLICATION PACKAGES</code>.</li>
</ul>

<p>However, when I execute the code as previous, I get the same error as in <em>Figure 7</em>. This is because the Python language extension is Python version-specific, and the release we use here is for Python 3.7.x. For other versions of Python (3.8, 3.9, etc.) you must modify and rebuild the Python Extension binaries. Look out for a future post around targeting different Python versions.</p>

<p>The R and Java extensions are not version specific.</p>

<h2 id="summary">Summary</h2>

<p>In this post, we discussed how Microsoft has open-sourced language extensions for R, Python and Java, which means we can bring our own R and Python runtimes. We mentioned how the language extensions are C++ dll&rsquo;s implementing the <a href="https://docs.microsoft.com/en-us/sql/language-extensions/reference/extensibility-framework-api?view=sql-server-ver15">Extensibility Framework API for SQL Server</a>.</p>

<p>We looked at using a later version of Python, (3.7.9), than the Python version included in SQL Server Machine Learning Services. To use another version, after we have installed the version in question we:</p>

<ul>
<li>ensure that the Python version you want to is on the <code>PATH</code> on the SQL Server box.</li>
<li>download the Python language extension.</li>
<li>create a system environment variable <code>PYTHONHOME</code> pointing to the install directory of the version. Not necessary unless you want to create external libraries for your language.</li>
<li>assign read and write permissions to the SQL Server instance-specific <em>Launchpad</em> service, and the <code>ALL APPLICATION PACKAGES</code> group.</li>
<li>create an external language using the <code>CREATE EXTERNAL LANGUAGE</code> syntax, with a unique name, (we cannot name it Python/R).</li>
</ul>

<p>When we&rsquo;ve done the above, we can execute using <code>sp_execute_external_script</code>.</p>

<p>The Python differs from the R and Java extensions in that it is version-specific. The extension we have used here are tied to Python runtimes 3.7.x. For other Python version, we need to modify and rebuild.</p>

<p>Oh, and if you want to bring your own version of R, you do it in the same was we did here with Python.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A Lap Around SQL Server 2019 Big Data Cluster: Architecture]]></title>
    <link href="https://nielsberglund.com/2020/12/21/a-lap-around-sql-server-2019-big-data-cluster-architecture/" rel="alternate" type="text/html"/>
    <updated>2020-12-21T09:18:00+02:00</updated>
    <id>https://nielsberglund.com/2020/12/21/a-lap-around-sql-server-2019-big-data-cluster-architecture/</id>
    <content type="html"><![CDATA[<p>This post is the second in series about <strong>SQL Server 2019 Big Data Cluster</strong> based on a presentation I do: <strong>A Lap Around SQL Server 2019 Big Data Cluster</strong>.</p>

<p>In the first post <a href="/2020/04/26/a-lap-around-sql-server-2019-big-data-cluster-background--technology/"><strong>A Lap Around SQL Server 2019 Big Data Cluster: Background &amp; Technology</strong></a> we looked at - as the title implies - the background of SQL Server 2019 Big Data Cluster, (BDC), and the technology behind it.</p>

<p>In this post, we look at the architecture and components of a BDC.</p>

<p></p>

<p>Before we dive into the architecture, let&rsquo;s refresh our memories around what we covered in the <a href="/2020/04/26/a-lap-around-sql-server-2019-big-data-cluster-background--technology/">previous</a> post.</p>

<h2 id="recap">Recap</h2>

<p>We are getting more and more data, and the data comes in all types and sizes. We need a system to be able to manage, integrate, and analyze all this data. That&rsquo;s where SQL Server comes into the picture.</p>

<p>SQL Server has continuously evolved from its very humble beginnings based on Ashton Tate/Sybase code-base to where it is now:</p>

<ul>
<li>SQL Server in Linux.</li>
<li>SQL Server in Containers.</li>
<li>SQL Server on Kubernetes.</li>
</ul>

<p>With all the capabilities now in SQL Server, it is the ideal platform to handle big data.</p>

<p>The SQL Server itself is not enough to achieve what we want, so in addition to SQL Server a BDC includes quite a few open-source technologies:</p>

<ul>
<li>Apache Spark</li>
<li>Hadoop File System (HDFS)</li>
<li>Influx DB</li>
<li>Graphana</li>
<li>Kibana</li>
<li>Elasticsearch</li>
<li>more &hellip;</li>
</ul>

<p>Oh, and a BDC is not only one SQL server, but quite a few instances. The SQL Server instances are SQL on Linux containers, and the whole BDC are deployed to and runs on Kubernetes (k8s).</p>

<p>We spoke a bit about k8s, and what constitutes a k8s cluster, (nodes, pods, etc.). In the post we tried to illustrate a k8s cluster like so:</p>

<p><img src="/images/posts/bdc-lap-around-bdc-kubernetes-1.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Kubernetes</em></p>

<p>In <em>Figure 1</em> we see some of the parts of a two-node Kubernetes cluster, with a Master node. Later in this post, we talk some more about the Master node, and the role it plays.</p>

<p>In the post, we briefly mentioned how we deploy a BDC, and we said we have essentially two options:</p>

<ul>
<li>Deploy via Python scripts.</li>
<li>Deploy using <strong>Azure Data Studio</strong>.</li>
</ul>

<p>We looked at how to manage and monitor a BDC, and we spoke about the tools for managing and monitoring:</p>

<ul>
<li><code>kubectl</code> - used to manage the Kubernetes cluster the BDC is deployed to.</li>
<li><code>azdata</code> - manage the BDC.</li>
</ul>

<p>In this post, looking at the architecture, we use the tools above, so ensure you have them installed if you want to follow along.</p>

<h2 id="architecture">Architecture</h2>

<p>How can we figure out what the architecture looks like? Well, in the <a href="/2020/04/26/a-lap-around-sql-server-2019-big-data-cluster-background--technology/"><strong>A Lap Around SQL Server 2019 Big Data Cluster: Background &amp; Technology</strong></a> post, we discussed k8s pods and how we could get information about the pod by executing some <code>kubectl</code> commands.</p>

<p>So let us go back to the pod we looked at briefly in the last post: <code>master-0</code>, which is the pod containing the SQL Server master instance. We look at that pod to see if we can get some information from it, which will help us in gaining insight into the architecture of a BDC. The code we use looks like so:</p>

<pre><code class="language-bash">kubectl get pods master-0 -n sqlbdc-cluster -o JSON
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Get Pod Information</em></p>

<p>In <em>Code Snippet 1</em> above we see how we use <code>kubectl get pods</code> and we:</p>

<ul>
<li>Send in the name of the pod we are interested in,</li>
<li>Indicate the k8s namespace the pod is in.</li>
<li>Want the output, <code>-o</code> flag, formatted as JSON.</li>
</ul>

<p>When we execute the code in <em>Code Snippet 1</em> we see something like so:</p>

<p><img src="/images/posts/bdc-lap-around-arch-pods1.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Get Pods</em></p>

<p>The <code>kubectl get pods master-0</code> command returns all information about that particular pod, and in <em>Figure 2</em> we see the first 20 lines or so of the JSON output.</p>

<p>Notice the section outlined in red, the <code>metadata</code> section. This section contains general information about the pod, and if we look closer, we can see three labels outlined in, purple, yellow and green respectively:</p>

<ul>
<li><code>app</code> with a value of <code>master</code>.</li>
<li><code>plane</code> with a value of <code>data</code>.</li>
<li><code>role</code> with a value of <code>master-pool</code>.</li>
</ul>

<p>Maybe these labels would give us some insight if we were to look at all pods? Ok, so let&rsquo;s do that, and we will use some <code>kubectl -o</code> &ldquo;magic&rdquo; to get the information we want:</p>

<pre><code class="language-bash">kubectl get pods -n sqlbdc-cluster \ 
                 -o custom-columns=NAME:.metadata.name, \
                    APP:.metadata.labels.app, \
                    ROLE:.metadata.labels.role,\
                    PLANE:.metadata.labels.plane
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Custom Columns</em></p>

<p>To retrieve the information we want, we use the <code>custom-columns</code> output option. We see in <em>Code Snippet 2</em> how we say we want four columns back: <code>NAME</code>, <code>APP</code>, <code>ROLE, and</code>PLANE`, and what labels those are, (we talk more about labels below). We then execute:</p>

<p><img src="/images/posts/bdc-lap-around-arch-roles-planes.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Pods with Custom Output</em></p>

<p>In <em>Figure 3</em> we see the result from executing the code in <em>Code Snippet 2</em> and we see all pods in the <code>sqlbdc-cluster</code> namespace, i.e. all pods in the BDC. From the <code>PLANE</code> column we see how the BDC has two planes, the control plane and the data plane.</p>

<h4 id="control-data-plane">Control &amp; Data Plane</h4>

<p>Let us make a short diversion here and talk a bit about control and data planes.</p>

<p>In distributed systems/services, we need a way to manage and monitor our services, and that is the role of the control plane. In the previous <a href="/2020/04/26/a-lap-around-sql-server-2019-big-data-cluster-background--technology/">post</a> we spoke about the master node in a k8s cluster and how we interact with the master node for management of the cluster.</p>

<p>However, k8s has no idea about a SQL Server 2019 Big Data Cluster; in which order pods should be deployed etc. This is where the BDC control plane comes in. It knows about the BDC, so whenever there needs to be an interaction between the Kubernetes cluster and the BDC the k8s master node interacts with the BDC&rsquo;s control plane. Take a deployment as an example; when deploying to the BDC, the control plane acts as the coordinator and ensures services, etc., are &ldquo;spun up&rdquo; in the correct order.</p>

<p>That is, however, not the only thing the control plane does. Remember from my previous <a href="/2020/04/26/a-lap-around-sql-server-2019-big-data-cluster-background--technology/">post</a> how we briefly discussed monitoring of a BDC and how we said we use Grafana, and Kibana together with the underlying InfluxDB and EleasticSearch as persistent stores. Well, the control plane is also responsible for monitoring, and in <em>Figure 3</em> you see some examples of this, where there are <code>APP</code>&rsquo;s related to logs and metrics.</p>

<p>The data plane is what we communicate with when working with the BDC, doing queries etc. - the application traffic. In addition to that, the data plane is also responsible for:</p>

<ul>
<li>Routing.</li>
<li>Load balancing.</li>
<li>Observability.</li>
</ul>

<p>Now, knowing a bit about the planes, let us have a look at roles.</p>

<h4 id="role">role</h4>

<p>In Kubernetes, you have the notion of a <code>Role</code>, and that has to do with security: a <code>Role</code> sets permissions within a namespace. The <code>role</code> I refer to here has nothing to do with that. No, a <code>role</code> in this context is a label attributed to a pod in a k8s cluster.</p>

<blockquote>
<p><strong>NOTE:</strong> In <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/">k8s documentation Labels</a> are described as follows: &ldquo;Labels are key/value pairs that are attached to objects, such as pods. Labels are intended to be used to specify identifying attributes of objects that are meaningful and relevant to users, but do not directly imply semantics to the core system. Labels can be used to organize and to select subsets of objects.&rdquo;</p>
</blockquote>

<p>Seeing the description about Labels above, we can deduce that a <code>role</code> describes the &ldquo;role&rdquo; of a Kubernetes component, i.e. what it does or belongs to. With that in mind, we can get some information/insight around the architecture of the BDC from it.</p>

<p>So let us once again look at the pods in the cluster and see what the <code>role</code> labels tell us:</p>

<pre><code class="language-bash">kubectl get pods -n sqlbdc-cluster2 \
                 -o custom-columns=PODNAME:.metadata.name,\
                 ROLE:.metadata.labels.role
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Pods &amp; Roles</em></p>

<p>The code in <em>Code Snippet 3</em> above is almost the same as in <em>Code Snippet 2</em>, but without the <code>APP</code> and <code>PLANE</code> columns. When we execute, we see:</p>

<p><img src="/images/posts/bdc-lap-around-arch-roles2.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Pods with Custom Output</em></p>

<p>In <em>Figure 4</em> we see how the various pods belong to quite few <code>ROLE</code>&rsquo;s. Some of the <code>ROLE</code>&rsquo;s we see are familiar, like: <code>controller</code> and <code>monitoring</code>, and we will not talk about them that much more. However, in <em>Figure 4</em> we also see some <code>ROLE</code>s named <code>xxx-pool</code>. Above we said that the <code>master-0</code> belonged to a role named <code>master-pool</code>, and we see that in <em>Figure 4</em> as well.</p>

<p>It turns out that the functionality, (SQL Server Master, Hadoop, etc.), of a BDC, is split into pools.</p>

<h2 id="pools">Pools</h2>

<p>When looking at <em>Figure 4</em> we see that we have different type of pools, and some of them have more than one pod. The pools are:</p>

<ul>
<li><code>compute-pool</code></li>
<li><code>data-pool</code></li>
<li><code>master-pool</code></li>
<li><code>storage-pool</code></li>
</ul>

<p>Let us look somewhat more in-depth into the pools above.</p>

<h4 id="master-pool">Master Pool</h4>

<p>From above we see how <code>master-0</code> belongs to the <code>master-pool</code>. In the last post as well as in this, we have mentioned how the <code>master-0</code> pod represents the SQL Server master instance, i.e. the &ldquo;normal&rdquo; SQL Server where your OLTP databases sit. So, let us see if we can prove that, by looking at what containers the pod has:</p>

<pre><code class="language-bash">Â kubectl get pods master-0 -n sqlbdc-cluster2 \
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â -o custom-columns=PODNAME:.metadata.name,\
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â ROLE:.metadata.labels.role,\
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â CONTAINERS:.spec.containers[*].name
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Master Pod Containers</em></p>

<p>The code in <em>Code Snippet 4</em> is almost the same as in <em>Code Snippet 3</em>. The difference is that we also want to see the containers in the pod. When executing, we get:</p>

<p><img src="/images/posts/bdc-lap-around-arch-master-0-containers.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Containers in Master Pod</em></p>

<p>We see in <em>Figure 5</em> that the <code>master-0</code> pod is part of the <code>master-pool</code>, and it consists of three containers. From the <a href="/2020/04/26/a-lap-around-sql-server-2019-big-data-cluster-background--technology/">previous post</a> we already know about <code>collectd</code> and <code>fluentbit</code>. It is the third container, (first in the list), that is interesting - <code>mssql-server</code>, (highlighted in yellow).</p>

<p>To find out some more about the container we change the code in <em>Code Snippet 4</em> to the following:</p>

<pre><code class="language-bash">Â kubectl get pods master-0 -n sqlbdc-cluster2 \
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â -o custom-columns=PODNAME:.metadata.name,\
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â CONTAINERS:.spec.containers[0].name,\
                     IMAGE:.spec.containers[0].image
</code></pre>

<p><strong>Code Snippet 5:</strong> <em>Container Image</em></p>

<p>In the code in <em>Code Snippet 5</em> we see how we retrieve the first container and the first image in the pod. We assume that as the SQL Server container is listed first, (see <em>Figure 5</em>), the container image will also be first. When we execute the code, the result looks like so:</p>

<p><img src="/images/posts/bdc-lap-around-arch-master-0-image.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>Containers in Master Pod</em></p>

<p>What we see in <em>Figure 6</em> is that the SQL Server instance in the master pool is SQL Server 2019 CU8, and it is SQL Server on Linux.</p>

<p>We will see later how there are more SQL Server instances in a BDC, but the master instance is what the user is interacting with. The master instance is also where read-write OLTP or dimensional data is stored, something like so:</p>

<p><img src="/images/posts/bdc-lap-around-arch-master-pool.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>BDC and Master Pool</em></p>

<p>What <em>Figure 7</em> shows us is a partial BDC cluster. The left is the control plane as discussed above and then beside it is the master pool with the one SQL Server instance.</p>

<p>At the bottom - outlined in red - we see a screen which illustrates a user and the interaction with the master instance. We also see a picture showing data stores (outlined in blue). What this means is that in SQL Server 2019, (not only BDC), you can query other data stores outside of SQL Server. This is thanks to Data Virtualization and PolyBase.</p>

<blockquote>
<p><strong>NOTE:</strong> A future post will cover Data Virtualization in SQL Server 2019 BDC.</p>
</blockquote>

<p>So, that is the master pool and the SQL Server master instance, what is next?</p>

<h4 id="compute-pool">Compute Pool</h4>

<p>In <em>Figure 4</em> we see how we have one pod belonging to the compute pool, the <code>compute-0-0</code>. Let us find out what containers are in the pod. We use code similar to <em>Code Snippet 4</em>, and when we execute we see:</p>

<p><img src="/images/posts/bdc-lap-around-arch-compute-containers.png" alt="" /></p>

<p><strong>Figure 8:</strong> <em>Compute Pool Containers</em></p>

<p>Hmm, the compute pool pod looks the same as the master pool pod - a SQL Server instance. If we were to look at the container images, we&rsquo;d see the same as the master instance. So what is this?</p>

<p>As the name implies, the compute pool provides scale-out computational resources for a SQL Server BDC. They are used to offload computational work, or intermediate result sets, from the SQL Server master instance. For you who have worked with PolyBase before it is a fully configured Polybase Scale-Out Group.</p>

<p>The SQL Server instance in the compute pool is - as mentioned before - for computational purposes, not for storing data. The only time there may be data persistence is if it is needed for data shuffling, and in that case, <code>tempdb</code> is used.</p>

<p>If we add the compute pool to what we have in <em>Figure 7</em> we get something like so:</p>

<p><img src="/images/posts/bdc-lap-around-arch-compute-pool.png" alt="" /></p>

<p><strong>Figure 9:</strong> <em>Compute Pool</em></p>

<p>By looking at <em>Figure 9</em> we understand that the compute pool is mostly used when accessing external data, and we see more of this as we go along.</p>

<h4 id="data-pool">Data Pool</h4>

<p>When we look at <em>Figure 4</em> we see we have two pods belonging to the data pool. Let us run the same code as in <em>Figure 8</em> but replace <code>compute-0-0</code> with <code>data-0-0</code> and see what we get:</p>

<p><img src="/images/posts/bdc-lap-around-arch-data-0-containers.png" alt="" /></p>

<p><strong>Figure 10:</strong> <em>Data Pool &amp; Containers</em></p>

<p>In <em>Figure 10</em> we see the same as for the pods in the master and data pools, and if we looked at the second pod it would be the same; one SQL Server instance, together with <code>collectd</code> and <code>fluentbit</code>. So the only difference between the data pool and the other pools is that we have two SQL Server pods in the data pool. Building on the architectural diagram, it looks like so:</p>

<p><img src="/images/posts/bdc-lap-around-arch-data-pool.png" alt="" /></p>

<p><strong>Figure 11:</strong> <em>Data Pool</em></p>

<p>We see the data pool in <em>Figure 11</em>, and how it has the two SQL Server instances mentioned above. The reason it has two is that the data pool acts as a persisting and caching layer of external data. The data pool allows for performance querying of cached data against external data sources and offloading of work.</p>

<p>You ingest data into the data pool using either T-SQL queries or from Spark jobs. When you ingest data into the pool, the data is distributed into shards and stored across all SQL Server instances in the pool.</p>

<blockquote>
<p><strong>NOTE:</strong> The data pool is append only, you cannot edit data in the pool.</p>
</blockquote>

<p>At the beginning of the post, we mentioned Apache Spark and Hadoop, but so far we have only seen SQL Server &ldquo;stuff&rdquo;. Where is Hadoop?</p>

<h4 id="storage-pool">Storage Pool</h4>

<p>In the previous paragraph, we asked where Hadoop comes into the picture, and the answer to that is the storage pool. Let us have a look at one of the pods in the storage pool and see what information we get. We use the same code as for the other pods, and when we execute, we see:</p>

<p><img src="/images/posts/bdc-lap-around-arch-storage-0-containers.png" alt="" /></p>

<p><strong>Figure 12:</strong> <em>Storage Pool &amp; Containers</em></p>

<p>That is interesting! In <em>Figure 12</em> we see the &ldquo;usual suspects&rdquo;; <code>mssql-server</code>, <code>collectd</code>, and <code>fluentbit</code> - but we also see a container we haven&rsquo;t seen before: <code>hadoop</code>.</p>

<p>We have two pods in the storage pool, and the <code>hadoop</code> container in each pod forms part of a Hadoop cluster. The Hadoop container provides persistent storage for unstructured and semi-structured data. Data files, such as Parquet or delimited text, can be stored in the storage pool. Not only is the Hadoop File System, (HDFS), within the container but also Apache Spark:</p>

<p><img src="/images/posts/bdc-lap-around-arch-storage-pool.png" alt="" /></p>

<p><strong>Figure 13:</strong> <em>Storage Pool</em></p>

<p>We have added to our architectural diagram the storage pool cluster as in <em>Figure 13</em>, and at the bottom of the picture, outlined in red, something that looks like files. That represents the ability to mount external HDFS data sources into the storage pool cluster. You access the data via the SQL Server master instance, and PolyBase external tables or you can use the Apache Knox Gateway which sits in the Hadoop name-node: <code>nmnode-0-0</code>, which you see in <em>Figure 4</em>.</p>

<p>You may ask why we have SQL Server instances in the storage pool pods? The Big Data Cluster uses the SQL Servers to optimize the access of the data stored in the HDFS Data Nodes.</p>

<p>We have now looked at the various pools we listed at the beginning of this post, and we should have a relatively good grasp of the architecture of a SQL Server 2019 Big Data Cluster.</p>

<h2 id="applications">Applications</h2>

<p>However, there is one thing more to look at. If we look at <em>Figure 4</em> we see at the very top a pod named <code>appproxy-nsp2m</code>, and its role is <code>proxy</code>. What is this? Well, let us run the same code we have done so many times before and see that containers this pod has:</p>

<p><img src="/images/posts/bdc-lap-around-arch-appproxy-containers.png" alt="" /></p>

<p><strong>Figure 14:</strong> <em>Application Proxy</em></p>

<p>In <em>Figure 14</em> we see that the <code>appproxy-nsp2m</code> has a container named <code>app-service-proxy</code>. This container is used, amongst other things, to enable applications to be deployed to a BDC.</p>

<h4 id="application-pool">Application Pool</h4>

<p>The reason for deploying applications to the BDC is so the applications can benefit from the computational power of the cluster and can access the data that is available on the cluster. Supported runtimes are:</p>

<ul>
<li>R</li>
<li>Python</li>
<li>SSIS</li>
<li>MLeap</li>
</ul>

<p>When we deploy an application to a BDC, it is deployed into the Application Pool:</p>

<p><img src="/images/posts/bdc-lap-around-arch-application-pool.png" alt="" /></p>

<p><strong>Figure 15:</strong> <em>Application Proxy</em></p>

<p>What we see in <em>Figure 15</em> is an example of the application pool where we have a user, outlined in red, interacting with the applications in the pool.</p>

<h2 id="summary">Summary</h2>

<p>This is the second post in a series about SQL Server 2019 Big Data Cluster. In the first post: <a href="/2020/04/26/a-lap-around-sql-server-2019-big-data-cluster-background--technology/">A Lap Around SQL Server 2019 Big Data Cluster: Background &amp; Technology</a> we looked at the reason why SQL Server 2019 Big Data Cluster came about and the tech behind it.</p>

<p>In this post, we looked at the architecture of the BDC. We discussed in this post about:</p>

<ul>
<li>Master Pool: the master instance of SQL Server, which also acts as an entry point into the BDC.</li>
<li>Compute Pool: provides scale-out computational resources for a SQL Server big data cluster.</li>
<li>Data Pool: persistence and caching layer for external data.</li>
<li>Storage Pool: provides persistent storage for unstructured and semi-structured data.</li>
<li>Application Pool: hosts applications running inside the BDC.</li>
</ul>

<p>In future posts, we will look at data virtualization, and how the various pools work.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 50, 2020]]></title>
    <link href="https://nielsberglund.com/2020/12/13/interesting-stuff---week-50-2020/" rel="alternate" type="text/html"/>
    <updated>2020-12-13T09:49:39+02:00</updated>
    <id>https://nielsberglund.com/2020/12/13/interesting-stuff---week-50-2020/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<blockquote>
<p><strong>NOTE:</strong> <em>It is now coming up on Christmas and New Year, and I will take a break with these posts and come back in the beginning of next year.</em></p>
</blockquote>

<p></p>

<h2 id="sql-server-2019-big-data-cluster-bdc">SQL Server 2019 Big Data Cluster (BDC)</h2>

<ul>
<li><a href="https://techcommunity.microsoft.com/t5/sql-server/sql-server-big-data-clusters-cu8-release-surfaces-encryption-at/ba-p/1956946">SQL Server Big Data Clusters CU8 release surfaces Encryption at Rest capabilities and more</a>. The latest cumulative update, (CU8), for SQL Server 2019 BDC includes several fixes, optimizations and adds two main capabilities for SQL Server BDC. This post looks at some of the major improvements, provides additional context to understand the design behind these capabilities better, and points you to relevant resources to learn more and get you started.</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/minibooks/chaos-engineering/">The InfoQ eMag - Real World Chaos Engineering</a>. This <a href="https://www.infoq.com/">InfoQ</a> post links to a download of an &ldquo;eMag&rdquo; around chaos engineering. The eMag pulls together a variety of case studies to show mechanisms by which you can implement chaos engineering.</li>
<li><a href="https://www.infoq.com/news/2020/12/grafana-tempo-distributed-tracin/">Grafana Announces Grafana Tempo, a Distributed Tracing System</a>. The <a href="https://www.infoq.com/">InfoQ</a> article linked to here looks at Grafana Tempo, the distributed tracing backend recently released by Grafana Labs. Grafana Tempo integrates with any existing logging system to create links from trace IDs in log lines, and it only requires object storage like Amazon S3 or Google Cloud Storage (GCS) to operate.</li>
</ul>

<h2 id="data">Data</h2>

<ul>
<li><a href="https://www.theseattledataguy.com/how-can-presto-and-starburst-data-improve-your-data-analytics/">How Can Presto And Starburst Data Improve Your Data Analytics</a>. At <a href="/derivco">Derivco</a> we have started looking at Presto. Presto is an open-source distributed SQL query engine for running interactive analytic queries against data sources of all sizes ranging from gigabytes to petabytes. This post looks more in detail what Presto is and why companies are using it.</li>
<li><a href="https://towardsdatascience.com/deploying-a-python-sql-engine-to-your-cluster-76a590940977">Mix SQL and Machine Learning and leverage your computation cluster</a>. The post linked to above discussed Presto. This post looks at another distributed SQL query engine - <a href="https://nils-braun.github.io/dask-sql/">dask-sql</a>. In the post, the author examines what dask-sql is and how it can be used in machine learning scenarios. Very cool!</li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://medium.com/microsoftazure/no-code-data-enhancement-with-azure-synapse-analytics-and-azure-auto-ml-cb9d97fb0c26">No Code Data Enrichment with Azure Synapse and Azure Machine Learning</a>. This post will walk through how to train and evaluate Azure ML AutoML Regressions model on your data using Azure Synapse Analytics Spark and SQL pools. Quite interesting!</li>
<li><a href="https://www.confluent.io/blog/transactional-machine-learning-with-maads-viper-and-apache-kafka/">Transactional Machine Learning at Scale with MAADS-VIPER and Apache Kafka</a>. The post linked to here shows how transactional machine learning (TML) integrates data streams with automated machine learning (AutoML). Apache Kafka is used as the data backbone, and it allows the creation of a frictionless machine learning process.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.youtube.com/watch?v=KR9yvcNBFIw">Event Streaming Applications with Zero Infrastructure</a>. In this YouTube video, <a href="https://twitter.com/tlberglund">Tim Berglund</a>, (from Confluent), demos how you can quickly spin up new event streaming applications with ksqlDB, Kafka, and connectors, all in a fully managed way on Confluent Cloud.</li>
<li><a href="https://www.confluent.io/blog/kafka-lag-monitoring-and-metrics-at-appsflyer/">Apache Kafka Lag Monitoring at AppsFlyer</a>. One crucial aspect of every distributed system is visibility - how do you see what&rsquo;s going on? In streaming applications, it is vital that we can see if consumers are lagging. The post linked to here looks at how one can implement a system for monitoring lag in Kafka.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 49, 2020]]></title>
    <link href="https://nielsberglund.com/2020/12/06/interesting-stuff---week-49-2020/" rel="alternate" type="text/html"/>
    <updated>2020-12-06T09:06:09+02:00</updated>
    <id>https://nielsberglund.com/2020/12/06/interesting-stuff---week-49-2020/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://www.infoq.com/news/2020/11/microsoft-releases-dotnet-spark/">Microsoft Releases .NET for Apache Spark 1.0</a>. This is an <a href="https://www.infoq.com/">InfoQ</a> article looking at the first major version of .NET for Apache Spark, an open-source package that brings .NET development to the Apache Spark platform. The new release allows .NET developers to write Apache Spark applications using .NET user-defined functions, Spark SQL, and additional libraries such as Microsoft Hyperspace and ML.NET. I can say that the developers here at <a href="/derivco">Derivco</a> are quite excited about this!</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/podcasts/service-mesh-interface/">Michelle Noorali on the Service Mesh Interface Spec and Open Service Mesh Project</a>. The <a href="https://www.infoq.com/">InfoQ</a> podcast linked to here covers quite a few topics: the service mesh interface (SMI) spec, the open service mesh (OSM) project, and the future of application development on Kubernetes.</li>
<li><a href="https://www.infoq.com/articles/microservice-monitoring-right-way/">Monitoring Microservices the Right Way</a>. Another article from <a href="https://www.infoq.com/">InfoQ</a>. The article looks at how recent innovations in open-source time-series databases have improved the scalability of monitoring tools such as Prometheus. These solutions can handle microservices large scale of data while providing metric scraping, querying, and visualization based on Prometheus and Grafana.</li>
<li><a href="https://www.infoq.com/news/2020/12/microservices-strangler-fig/">Migrating a Monolith towards Microservices with the Strangler Fig Pattern</a>. The article linked to here looks at how a company, <a href="https://scholarpack.com">ScholarPack</a>, managed to migrate away from a monolith backend using a Strangler Fig pattern. They applied incremental development and continuous delivery to target customersâ€™ needs, in the meanwhile strangling their monolith. All this is very interesting for us, <a href="/derivco">Derivco</a>!</li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://engineering.linkedin.com/blog/2020/open-sourcing-dagli">Dagli: Faster and easier machine learning on the JVM, without the tech debt</a>. This post by the LinkedIn engineering team is about the release of Dagli. Dagli is an open source machine learning library for Java (and other JVM languages) that makes it easy to write bug-resistant, readable, modifiable, maintainable, and trivially deployable model pipelines without incurring technical debt.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/apache-kafka-spring-cloud-data-flow-tutorial/">Getting Started with Spring Cloud Data Flow and Confluent Cloud</a>. This blog post gives you the foundation for event streaming and designing and implementing real-time patterns. Using Confluent Schema Registry, ksqlDB, and fully managed Apache Kafka as a service, you can experience clean, seamless integrations with your existing cloud provider. The post also discusses Spring Cloud Data Flow which is a microservices-based toolkit for building streaming and batch data processing pipelines.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 48, 2020]]></title>
    <link href="https://nielsberglund.com/2020/11/29/interesting-stuff---week-48-2020/" rel="alternate" type="text/html"/>
    <updated>2020-11-29T09:01:28+02:00</updated>
    <id>https://nielsberglund.com/2020/11/29/interesting-stuff---week-48-2020/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/netflix-api-graphql-federation/">How Netflix Scales Its API with GraphQL Federation</a>. This is an <a href="https://www.infoq.com/">InfoQ</a> presentation, where the presenters look at how Netflix uses GraphQL to scale its API&rsquo;s. The usage of GraphQL looks very cool, and maybe we can use it at <a href="/derivco">Derivco</a> as well!</li>
<li><a href="https://medium.com/swlh/the-6-things-you-need-to-know-about-event-driven-architectures-38e11fdcb5a">The 6 Things You Need to Know About Event-Driven Architectures</a>. The post linked to is the second in a series about event driven architectures, (the first is <a href="https://medium.com/swlh/the-engineers-guide-to-event-driven-architectures-benefits-and-challenges-3e96ded8568b">here</a>). In this post, the author looks at, and explains, what in his mind are key concepts of event-driven architectures.</li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://towardsdatascience.com/deploy-machine-learning-applications-to-kubernetes-using-streamlit-and-polyaxon-49bf4b963515">Deploy Machine Learning applications to Kubernetes using Streamlit and Polyaxon</a>. This is a step-by-step guide on how to train, analyze, and deploy a containerized Streamlit machine learning application on Kubernetes using Polyaxon.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://netflixtechblog.com/simple-streaming-telemetry-27447416e68f">Simple streaming telemetry</a>. This post looks at the Netflix <code>gnmi-gateway</code> project. <code>gnmi-gateway</code> is a modular, distributed, and highly available service for modern network telemetry via OpenConfig and gNMI. The <code>gnmi-gateway</code> project looks interesting, and I&rsquo;ll forward the post to our network guys.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>If you have not signed up for the <a href="https://dataplatformgeeks.com/dps2020/"><strong>Data Platform Virtual Summit 2020</strong></a> yet, you still have some time!</p>

<p>The <strong>Data Platform Virtual Summit 2020</strong>, (DPS), is a 100% technical learning event with 200 Breakout Sessions, 30 Training Classes, 72 hours of non-stop conference sessions.</p>

<p>DPS 2020 is the largest online learning event on Microsoft Azure Data, Analytics &amp; Artificial Intelligence. Delegates get the recordings at no extra cost, which is quite a wonderful thing. Also, the conference virtual platform looks amazing, <a href="https://www.linkedin.com/posts/amitbansal2010_dps2020-sqlserver-powerbi-activity-6728885748755374080-a8QL/">have a look</a>.</p>

<p>You <a href="https://dataplatformgeeks.com/dps2020/booking/">book here</a>. Oh, and since I am a speaker I get a discount code to hand out to you guys! Use the discount code <strong>DPSSPEAKER</strong> to book your seat at <strong>55%</strong> off.</p>

<p>If you wonder what I am speaking about, this should give you an idea:</p>

<p><img src="/images/posts/Niels_Berglund.jpg" alt="" /></p>

<p>In my talk, I will be talking about Kafka and SQL Server, and various ways we can &ldquo;set our SQL Server data free&rdquo;!</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 47, 2020]]></title>
    <link href="https://nielsberglund.com/2020/11/22/interesting-stuff---week-47-2020/" rel="alternate" type="text/html"/>
    <updated>2020-11-22T08:52:30+02:00</updated>
    <id>https://nielsberglund.com/2020/11/22/interesting-stuff---week-47-2020/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="azure-data-studio">Azure Data Studio</h2>

<ul>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2020/11/16/building-and-sharing-jupyter-books-in-azure-data-studio/">Building and sharing Jupyter Books in Azure Data Studio</a>. We all should know by now that Azure Data Studio allows us to use Jupyter notebooks. This post looks at how we can not only use Jupyter books but also create and share them. Very cool!</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://medium.com/expedia-group-tech/autoscaling-in-kubernetes-a-primer-on-autoscaling-7b8f0f95a928">Autoscaling in Kubernetes: A Primer on Autoscaling</a>. This post is the first in a series looking at application autoscaling in Kubernetes. I was going to write that I really look forward to the second instalment when I realized it already had been <a href="https://medium.com/expedia-group-tech/autoscaling-in-kubernetes-options-features-and-use-cases-c8a6ce145957">published</a>! Awesome!</li>
<li><a href="https://martin.kleppmann.com/2020/11/18/distributed-systems-and-elliptic-curves.html">New courses on distributed systems and elliptic curve cryptography</a>. As the title says; Martin Kleppman of <a href="http://dataintensive.net/">Designing Data-Intensive Applications</a> fame have released some new training courses. I am very interested in the distributed systems course; the videos look awesome! This course is a must for anyone interested in distributed systems!</li>
<li><a href="https://medium.com/swlh/distributed-systems-and-asynchronous-i-o-ef0f27655ce5">Distributed Systems and Asynchronous I/O</a>. The post linked to here looks at how different forms of handling I/O affect the performance, availability, and fault-tolerance of network applications.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://medium.com/@dt_23597/if-youre-using-kafka-with-your-microservices-you-re-probably-handling-retries-wrong-8492890899fa">If Youâ€™re Using Kafka With Your Microservices, Youâ€™re Probably Handling Retries Wrong</a>. In this excellent article, the author looks at various ways of handling retries in Kafka. The article presents a potential solution together with the downsides of that particular solution. As I said in the beginning - this is an excellent article!</li>
<li><a href="https://www.confluent.io/blog/how-real-time-stream-processing-safely-scales-with-ksqldb/">How Real-Time Stream Processing Safely Scales with ksqlDB, Animated</a>. This post is the third in a series around ksqlDB and how it executes stateless and stateful operations. The two previous posts have looked at a single server setup. This post looks at how stateless and stateful operations work when ksqlDB is deployed with many servers, and more importantly, how it linearly scales the work it is performingâ€”even in the presence of faults.</li>
<li><a href="https://www.confluent.io/blog/using-kafka-ksqldb-kibana-to-stream-data-and-get-real-time-analytics/">Analysing historical and live data with ksqlDB and Elastic Cloud</a>. This is a great post by <a href="https://twitter.com/rmoff">Robin Moffat</a>. He looks at how you can take &ldquo;messy and imperfect&rdquo; data, (think CSV), from a &ldquo;raw data&rdquo; Kafka topic, re-format it, and make it presentable with ksqlDB, push it into another topic, and from there stream it into an analytical dashboard. Awesome stuff!</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>Don&rsquo;t forget Data Platform Summit 2020.</p>

<p><img src="/images/posts/dps_2020.png" alt="" /></p>

<p>I am super excited to be speaking at the <a href="https://dataplatformgeeks.com/dps2020/"><strong>Data Platform Virtual Summit 2020</strong></a>:</p>

<p><img src="/images/posts/Niels_Berglund.jpg" alt="" /></p>

<p>and as you see in the figure above, my presentation is about Kafka and SQL Server.</p>

<p>The <strong>Data Platform Virtual Summit 2020</strong>, (DPS), is a 100% technical learning event with 200 Breakout Sessions, 30 Training Classes, 72 hours of non-stop conference sessions. DPS 2020 is the largest online learning event on Microsoft Azure Data, Analytics &amp; Artificial Intelligence. Delegates get the recordings at no extra cost, which is quite a wonderful thing. Also, the conference virtual platform looks amazing, <a href="https://www.linkedin.com/posts/amitbansal2010_dps2020-sqlserver-powerbi-activity-6728885748755374080-a8QL/">take a look</a>.</p>

<p>If you want to attend and hear industry experts talk about really exciting stuff you can <a href="https://dataplatformgeeks.com/dps2020/booking/">book here</a>. Oh, and the coolest thing is that as I am a speaker I get a discount code to hand out to you guys! Use the discount code <strong>DPSSPEAKER</strong> to book your seat at <strong>55%</strong> off.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 46, 2020]]></title>
    <link href="https://nielsberglund.com/2020/11/15/interesting-stuff---week-46-2020/" rel="alternate" type="text/html"/>
    <updated>2020-11-15T09:31:21+02:00</updated>
    <id>https://nielsberglund.com/2020/11/15/interesting-stuff---week-46-2020/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="misc">Misc.</h2>

<ul>
<li><a href="https://towardsdatascience.com/how-to-integrate-python-and-r-in-visual-studio-code-496a47c90422">How to integrate Python and R in Visual Studio Code</a>. I really like VSCode, but I always have issues with how to configure for Python and R when installing from scratch. So this blog-post comes in real handy, as it explains what one needs to do to get Python and R up and running in VSCode. Awesome!</li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://towardsdatascience.com/synthetic-data-vault-sdv-a-python-library-for-dataset-modeling-b48c406e7398">Synthetic Data Vault (SDV): A Python Library for Dataset Modeling</a>. When doing machine learning/data science, you need realistic data to work with, and that can sometimes be a problem. This post introduces the Synthetic Data Vault, which is a tool to generate complex datasets using statistical &amp; machine-learning models. It looks very interesting!</li>
<li><a href="https://eng.uber.com/metadata-insights-databook/">Turning Metadata Into Insights with Databook</a>. This post looks at Uber&rsquo;s system for handling metadata - Databook.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://medium.com/apache-pinot-developer-blog/using-apache-pinot-and-kafka-to-analyze-github-events-93cdcb57d5f7">Using Apache Pinot and Kafka to Analyze GitHub Events</a>. Apache Pinot is a real-time distributed OLAP datastore, which is used to deliver scalable real-time analytics with low latency. The post I linked to here discusses how to ingest Kafka events into Pinot. What we see in the blog post is very interesting for <a href="/derivco">us</a>, and we will definitely look at it in more detail.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>In <a href="/2020/11/08/interesting-stuff---week-45-2020/">last weeks roundup</a> I wrote about my recording of video for the Data Platform Summit 2020.</p>

<p><img src="/images/posts/dps_2020.png" alt="" /></p>

<p>If I haven&rsquo;t said it before:</p>

<p>I am super excited to be speaking at the <a href="https://dataplatformgeeks.com/dps2020/"><strong>Data Platform Virtual Summit 2020</strong></a>. A 100% technical learning event with 200 Breakout Sessions, 30 Training Classes, 72 hours of non-stop conference sessions â€“ DPS 2020 is the largest online learning event on Microsoft Azure Data, Analytics &amp; Artificial Intelligence. Delegates get the recordings at no extra cost, which is quite a wonderful thing. Also, the conference virtual platform looks amazing, <a href="https://www.linkedin.com/posts/amitbansal2010_dps2020-sqlserver-powerbi-activity-6728885748755374080-a8QL/">take a look</a>.</p>

<p>If you want to attend and hear industry experts talk about really exciting stuff you can <a href="https://dataplatformgeeks.com/dps2020/booking/">book here</a>. Oh, and the coolest thing is that as I am a speaker I get a discount code to hand out to you guys! Use the discount code <strong>DPSSPEAKER</strong> to book your seat at <strong>55%</strong> off.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 45, 2020]]></title>
    <link href="https://nielsberglund.com/2020/11/08/interesting-stuff---week-45-2020/" rel="alternate" type="text/html"/>
    <updated>2020-11-08T10:32:09+02:00</updated>
    <id>https://nielsberglund.com/2020/11/08/interesting-stuff---week-45-2020/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://blog.acolyer.org/2020/11/02/helios-part-ii/">Helios: hyperscale indexing for the cloud &amp; edge (part II)</a>. In last weeks <a href="/2020/11/01/interesting-stuff---week-44-2020/">roundup</a>, I pointed to a post by <a href="https://twitter.com/adriancolyer">Adrian</a>, where he dissected a white-paper about Helios, and I said how I looked forward to part 2. Well, I got what I wanted, and the post I link to here is the follow-up. Enjoy!</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://vlfig.me/posts/microservices">Microservices â€” architecture nihilism in minimalism&rsquo;s clothes</a>. This post looks at microservices and argues that there is no such thing as a microservice architecture, but software architecture. The author also looks at the size of a microservice and argues that there is no prescribed size of a microservice. I found the post very interesting!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://itnext.io/change-data-capture-with-azure-postgresql-and-kafka-4598dbf0b57a">Change Data Capture with Azure, PostgreSQL, and Kafka</a>. In this blog post, the author looks at how one can use Change Data Capture to stream database modifications from PostgreSQL to Azure Data Explorer, (Kusto), using Apache Kafka. It is an interesting post! As a side note, I must say that Azure Data Explorer looks really interesting!</li>
<li><a href="https://www.confluent.io/blog/pull-queries-in-preview-confluent-cloud-ksqdb/">Announcing Pull Queries in Preview in Confluent Cloud ksqlDB</a>. ksqlDB has two types of queries: push and pull. Push queries allow you subscribe to a query&rsquo;s result as it changes in real-time, whereas with a pull query you fetch the current state of a materialized view. Both types of queries have been in Confluent Platform for a while, but Confluent Cloud has up until now only supported push queries. That changes now, and the post I linked to here discusses more in detail about the support for pull queries in Confluent Cloud.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>Well, it is not so much about what I am doing, as it is of what I have been doing the last couple of days. You who read my blog are probably aware that:</p>

<p><img src="/images/posts/Niels_Berglund.jpg" alt="" /></p>

<p><strong>Figure 1:</strong> <em>SQL Server &amp; Kafka</em></p>

<p>Yes, I am speaking at the conference! My topic is about, as you can see in <em>Figure 1</em>, the various ways we can stream data from SQL Server to Apache Kafka. In a previous roundup, I wrote how I was prepping for the talk. That is all good and well, but a while ago a &ldquo;curve-ball&rdquo; was thrown: the conference is obviously virtual, but: we are not presenting live! Which means that the last few days, I have been recording, and editing the recording. Geez, recording and editing is hard work, compared to &ldquo;just&rdquo; deliver. Well, it is done now, and if you <a href="https://dataplatformgeeks.com/dps2020/booking/https://dataplatformgeeks.com/dps2020/booking/">register</a> you have the chance to see other speakers and me in a recorded fashion.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 44, 2020]]></title>
    <link href="https://nielsberglund.com/2020/11/01/interesting-stuff---week-44-2020/" rel="alternate" type="text/html"/>
    <updated>2020-11-01T11:01:29+02:00</updated>
    <id>https://nielsberglund.com/2020/11/01/interesting-stuff---week-44-2020/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://blog.acolyer.org/2020/10/26/helios-part-1/">Helios: hyperscale indexing for the cloud &amp; edge â€“ part 1</a>. In this post <a href="https://twitter.com/adriancolyer">Adrian</a> from the <a href="https://blog.acolyer.org">morning paper</a> dissects a white-paper about Helios. Helios is a distributed, highly-scalable system used at Microsoft for flexible ingestion, indexing, and aggregation of large streams of real-time data that is designed to plug into relational engines. Adrian is as thorough as usual, and the conclusions he draws are very interesting. I can&rsquo;t wait for part 2.</li>
</ul>

<h2 id="distributed-systems">Distributed Systems</h2>

<ul>
<li><a href="https://medium.com/@polyglot_factotum/how-i-am-learning-distributed-systems-7eb69b4b51bd">How I am learning distributed systems</a>. This post looks, from one person&rsquo;s perspective, how one can learn to design distributed systems. What is interesting in this post is the use of <a href="https://raft.github.io/">Raft</a>, (no, not Raft the game - but the consensus algorithm), as a learning tool. I will definitely point to this post as a learning resource for my developers.</li>
<li><a href="https://thenewstack.io/nginx-steps-into-the-service-mesh-fray-promising-a-simpler-alternative/">NGINX Steps into the Service Mesh Fray Promising a Simpler Alternative</a>. The post linked to here points discusses how NGINX introduces its own service mesh: <a href="https://www.nginx.com/products/nginx-service-mesh">NGINX Service Mesh</a>, (NSM). It promises to be less complicated than ISTIO, so I will definitely have a look.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/how-to-prepare-for-kip-500-kafka-zookeeper-removal-guide/">Preparing Your Clients and Tools for KIP-500: ZooKeeper Removal from Apache Kafka</a>. The Kafka community has for quite a while been talking about removing the dependency of ZooKeeper, (ZK), from Kafka, and it seems we are getting closer. In the post I have linked to here, the author looks at what is needed to do in Kafka consumers so that nothing &ldquo;bad&rdquo; happens when ZK is eventually removed.</li>
<li><a href="https://www.kai-waehner.de/blog/2020/10/27/streaming-machine-learning-kafka-native-model-server-deployment-rpc-embedded-streams/">Streaming Machine Learning with Kafka-native Model Deployment</a>. Kafka is used more and more for real-time machine learning purposes, and we are moving towards Kafka as a native streaming model server. This blog post explores the architectures and trade-offs between various options for model deployment with Kafka.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
</feed>

