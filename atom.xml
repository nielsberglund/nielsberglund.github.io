<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Niels Berglund</title>
  <link href="https://nielsberglund.com/atom.xml" rel="self"/>
  <link href="https://nielsberglund.com"/>
  <updated>2019-12-01T08:03:26+02:00</updated>
  <id>https://nielsberglund.com/</id>
  <generator uri="http://gohugo.io/">Hugo</generator>

  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 48, 2019]]></title>
    <link href="https://nielsberglund.com/2019/12/01/interesting-stuff---week-48-2019/" rel="alternate" type="text/html"/>
    <updated>2019-12-01T08:03:26+02:00</updated>
    <id>https://nielsberglund.com/2019/12/01/interesting-stuff---week-48-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="azure">Azure</h2>

<ul>
<li><a href="https://www.youtube.com/watch?v=ddo3e0IxAAA">Azure Arc for data services, including SQL and PostgreSQL</a>. At <a href="https://www.microsoft.com/en-us/ignite">Microsoft Ignite</a> earlier this year, Microsoft introduced <a href="https://azure.microsoft.com/en-us/services/azure-arc/">Azure Arc</a>, a service which enables deployment of Azure services anywhere and extends Azure management to any infrastructure. In the video linked to here <a href="https://twitter.com/radtravis">Travis Wright</a> takes a deep-dive into Azure Data Services as part of Azure Arc. He shows how you can provision and manage SQL Server or PostgreSQL databases regardless if they are on-prem, in Azure, or in any other data centers. Very, very Powerful!</li>
</ul>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://towardsdatascience.com/big-data-from-b-to-a-the-hadoop-distributed-filesystem-hdfs-992612cbf8aa">Big Data From B to A: The Hadoop Distributed Filesystem — HDFS</a>. If you work with or are interested in Big Data, you have most likely heard of Hadoop File System, (HDFS). The blog post linked to here discusses HDFS and looks at the design, architecture, and the data flow.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://towardsdatascience.com/https-medium-com-ekoutanov-introduction-to-event-streaming-with-kafka-and-kafdrop-303d5d0ceeec">Introduction to Event Streaming with Kafka and Kafdrop</a>. This post I link to here is an awesome introduction to Kafka and event streaming. I recommend the post highly!<br /></li>
<li><a href="https://charlla.com/building-a-donut-shop-on-ksqldb/">Kafka Donuts - on KSQLDB - #1</a>. In las weeks <a href="/2019/11/24/interesting-stuff---week-47-2019/">roundup</a> I covered the <a href="https://www.dataplatformgeeks.com/">introduction of <strong>ksqlDB</strong></a>. My good friend, and colleague, <a href="https://twitter.com/charllamprecht">Charl Lamprecht</a> who loves Kafka almost as much as he loves doughnuts had dive into how <strong>ksqlDB</strong> works and the post I link to here is the result. Please go ahead and read it - it gives you a good insight into <strong>ksqlDB</strong>!</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>This coming week I am delivering two webinars for <a href="https://www.dataplatformgeeks.com/">DataPlatformGeeks</a>:</p>

<p><strong>3rd December:</strong></p>

<ul>
<li><a href="https://www.dataplatformgeeks.com/events/dpg-webinar-03rd-december-2019-0300-pm-0400-pm-ist-niels-berglund-a-lap-around-sql-server-2019-big-data-cluster/">A Lap Around SQL Server 2019 Big Data Cluster</a>. An overview of SQL Server 2019 Big Data Cluster.</li>
</ul>

<p><strong>5th December:</strong></p>

<ul>
<li><a href="https://www.dataplatformgeeks.com/events/dpg-webinar-05th-december-2019-0300-pm-0400-pm-ist-niels-berglund-azure-data-studio-and-sql-server-2019-big-data-cluster/">Azure Data Studio and SQL Server 2019 Big Data Cluster</a>. We look at using Azure Data Studio to deploy and manage a SQL Server 2019 Big Data Cluster.</li>
</ul>

<p>The webinars are free, so if you have time, register and join the fun! Notice that the times in the links above are Indian Standard Time, so 3 pm IST is 09:30, (am), UTC.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 47, 2019]]></title>
    <link href="https://nielsberglund.com/2019/11/24/interesting-stuff---week-47-2019/" rel="alternate" type="text/html"/>
    <updated>2019-11-24T08:37:03+02:00</updated>
    <id>https://nielsberglund.com/2019/11/24/interesting-stuff---week-47-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/conde-nast-chaos-resilience/">The Future of Chaos Engineering: In Pursuit of the Unknown Unknowns</a>. This is an <a href="https://www.infoq.com/">InfoQ</a> presentation where the presenter discusses how chaos and resilience practices must evolve to keep pace with the challenges of growing complexity.</li>
</ul>

<h2 id="sql-server-2019">SQL Server 2019</h2>

<ul>
<li><a href="/2019/11/20/fix-polybase-in-sql-server-2019-developers-edition/">Fix PolyBase in SQL Server 2019 Developers Edition</a>. This post is by yours truly, discussing how to fix an issue with PolyBase not starting up correctly after installing SQL Server 2019 Developers Edition.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://medium.com/trending-information-technologies/stream-processing-17384a23111f">Real-World Data Stream Processing</a>. In this post, the author discusses using streaming technologies with Kafka, Spark, and Cassandra to gain insights on data.</li>
<li><a href="https://www.confluent.io/blog/intro-to-ksqldb-sql-database-streaming/">Introducing ksqlDB</a>. This post discusses <a href="https://ksqldb.io/"><strong>ksqlDB</strong></a>, the latest release of KSQL. There are essentially two new features in the release, but they are so far-reaching that Confluent decided to rename KSQL to <strong>ksqlDB</strong>. I will keep an eye on what Confluent is doing in this space - I think we will see loads of interesting &ldquo;stuff&rdquo; happening!</li>
<li><a href="https://www.confluent.io/blog/kafka-streams-vs-ksqldb-compared/">Kafka Streams and ksqlDB Compared – How to Choose</a>. So, KSQL and <strong>ksqlDB</strong>, (as per above), uses the Kafka Streams API, (KStreams), under the covers. This blog-post discusses when to use KStreams vs <strong>ksqlDB</strong>.</li>
<li><a href="https://www.youtube.com/watch?v=D5QMqapzX8o">ksqlDB Demo | The Event Streaming Database in Action</a>. Here is a <strong>ksqlDB</strong> demo video to round off the <strong>ksqlDB</strong> coverage with.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>The first week of December, I am delivering two webinars for <a href="https://www.dataplatformgeeks.com/">DataPlatformGeeks</a>:</p>

<p><strong>3rd December:</strong></p>

<ul>
<li><a href="https://www.dataplatformgeeks.com/events/dpg-webinar-03rd-december-2019-0300-pm-0400-pm-ist-niels-berglund-a-lap-around-sql-server-2019-big-data-cluster/">A Lap Around SQL Server 2019 Big Data Cluster</a>. An overview of SQL Server 2019 Big Data Cluster.</li>
</ul>

<p><strong>5th December:</strong></p>

<ul>
<li><a href="https://www.dataplatformgeeks.com/events/dpg-webinar-05th-december-2019-0300-pm-0400-pm-ist-niels-berglund-azure-data-studio-and-sql-server-2019-big-data-cluster/">Azure Data Studio and SQL Server 2019 Big Data Cluster</a>. We look at using Azure Data Studio to deploy and manage a SQL Server 2019 Big Data Cluster.</li>
</ul>

<p>The webinars are free, so if you have time, register and join the fun! Notice that the times in the links above are Indian Standard Time, so 3 pm IST is 09:30, (am), UTC.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Fix PolyBase in SQL Server 2019 Developers Edition]]></title>
    <link href="https://nielsberglund.com/2019/11/20/fix-polybase-in-sql-server-2019-developers-edition/" rel="alternate" type="text/html"/>
    <updated>2019-11-20T05:52:17+02:00</updated>
    <id>https://nielsberglund.com/2019/11/20/fix-polybase-in-sql-server-2019-developers-edition/</id>
    <content type="html"><![CDATA[<p>At <a href="https://www.microsoft.com/en-us/ignite">MS Ignite</a> in Orlando November 4 - 8, 2019, Microsoft announced the general availability of SQL Server 2019. At the same time, the SQL Server 2019 Developers Edition appeared as an MSDN download, and of course, I downloaded it and installed it on my dev box.</p>

<p>After the installation, I noticed that PolyBase did not start up correctly, and I saw dump files all over the place. After some investigation, I figured out what the issue was, and this blog post describes the fix.</p>

<p></p>

<h2 id="polybase-intro">PolyBase Intro</h2>

<p>Before we go into the issue and solution, let us do a quick introduction of PolyBase and also look at the installation of PolyBase.</p>

<p>PolyBase enables you to, in your SQL Server instance, process Transact-SQL queries that read data from external data sources. Microsoft introduced PolyBase in SQL Server 2016, and in that version of PolyBase the only data-sources you could process data from were: Hadoop and Azure Blob Storage. That changed in SQL Server 2019 where PolyBase now supports:</p>

<ul>
<li>Hadoop.</li>
<li>Azure Blob Storage.</li>
<li>SQL Server.</li>
<li>Oracle.</li>
<li>Teradata.</li>
<li>MongoDB</li>
<li>ODBC Generic Types, (only PolyBase in SQL Server 2019 on Windows).</li>
</ul>

<h4 id="installation">Installation</h4>

<p>You either install PolyBase as part of a regular installation of a SQL Server instance, or you add it after installation. In both cases, you select it during feature selection:</p>

<p><img src="/images/posts/sql-2k19-polybase-issue-install.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>PolyBase Feature Selection</em></p>

<p>In <em>Figure 1</em> we see how I chose both the <em>PolyBase Query Service</em> as well as the <em>Java connector for HDFS data sources</em>. After clicking on from there passing <em>Feature Rules</em>, and <em>Instance Configuration</em> we get to <em>PolyBase Configuration</em>:</p>

<p><img src="/images/posts/sql-2k19-polybase-issue-install2.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>PolyBase Configuration</em></p>

<p>As we see from <em>Figure 2</em> I choose a standalone PolyBase-enabled instance.</p>

<p>I click on from there and eventually the installation finishes:</p>

<p><img src="/images/posts/sql-2k19-polybase-issue-install3.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Installation Complete</em></p>

<p>We see in <em>Figure 3</em> how all components are green which indicates that the installation has succeeded. Cool!</p>

<h2 id="problem">Problem</h2>

<p>But has the installation succeeded? What usually I do after a SQL installation is to go to <em>Services</em> and change properties for the various SQL services, (automatic startup to manual startup, etc.). When I do this, I see:</p>

<p><img src="/images/posts/sql-2k19-polybase-issue-services.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Services</em></p>

<p>What we see in <em>Figure 4</em> is a view of services a couple of minutes after the installation. We see that the SQL Server service, (<code>MSSQLSERVER</code>), is in <code>Running</code> state. However, the two PolyBase services are still in a <code>Starting</code> state. Something must have gone wrong, but what!</p>

<blockquote>
<p><strong>NOTE:</strong> Depending on how long after installation you look at services you may see the PolyBase Engine in a stopped state, but the Data Movement still in a <code>Starting</code> state.</p>
</blockquote>

<p>Let us see if we can try and figure out what is wrong. We start by going to the SQL Server log files directory, which is at: <code>&lt;path_to_sql_instance&gt;\MSSQL\Log</code>. In there you find a <code>Polybase</code> directory with a <code>dump</code> subdirectory:</p>

<p><img src="/images/posts/sql-2k19-polybase-issue-dump-dir.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>PolyBase Dump Directory</em></p>

<p>In <em>Figure 5</em> we see how the <code>dump</code> directory contains a <code>.dmp</code> file. Maybe we can find any clues if we open it in <strong>WinDbg</strong>:</p>

<pre><code class="language-bash">Comment: 'Stack Trace'
Comment: 
  '&lt;Identity&gt;
     &lt;Element key=&quot;CodePackageName&quot; val=&quot;DwDms.Code&quot;/&gt;
     [snip]
     &lt;Element key=&quot;ExceptionMessage&quot; val=&quot;A network-related or 
      instance-specific error occurred while establishing a connection 
      to SQL Server. The server was not found or was not accessible. 
      Verify that the instance name is correct and that SQL Server is 
      configured to allow remote connections. 
      (provider: TCP Provider, error: 0 - The remote computer refused 
       the network connection.)&quot;/&gt;
     &lt;Element key=&quot;ExceptionType&quot; val=&quot;System.Data.SqlClient.SqlException&quot;/&gt;
     [snip]
     &lt;/Identity&gt;'
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Exception in File</em></p>

<p>Hmm, in <em>Code Snippet 1</em> we see an excerpt of the top of the file, (I have &ldquo;snipped&rdquo; out parts for brevity), and we see an error about how PolyBase cannot connect to SQL Server via the TCP provider.</p>

<h2 id="fix">Fix</h2>

<p>The error we see in <em>Code Snippet 1</em> is a big clue since TCP is not an enabled network protocol by default for SQL Server Developer edition.</p>

<p>Let us enable TCP/IP as a network protocol for SQL Server. We enable network protocols using the <em>SQL Server Configuration Manager</em>. Open <em>SQL Server Configuration Manager</em> either via the Windows <strong>Start</strong> menu or from <em>Computer Manager</em>:</p>

<p><img src="/images/posts/sql-2k19-polybase-issue-compmngr.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>SQL Server Configuration Manager</em></p>

<p>In <em>Figure 6</em> we see, outlined in blue, <em>SQL Server Configuration Manager</em>, and how we have expanded <em>SQL Server Network Configuration</em>, (outlined in red). On the right-hand side in <em>Figure 6</em> we see how the TCP/IP protocol is indeed disabled.</p>

<p>To enable we right-click on the TCP/IP protocol and in the menu choose <strong>Enable</strong>. When we do that a <em>Warning</em> dialog &ldquo;pops&rdquo; up:</p>

<p><img src="/images/posts/sql-2k19-polybase-issue-enable-warning.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>Warning Dialog</em></p>

<p>So, we see in <em>Figure 7</em> that any changes will not happen until the SQL Server instance is restarted. Cool, let us restart the service. We go back to <em>Services</em> as in <em>Figure 4</em>, and we right-click on the SQL Server service, and choose <strong>Restart</strong>:</p>

<p><img src="/images/posts/sql-2k19-polybase-issue-stop-service.png" alt="" /></p>

<p><strong>Figure 8:</strong> <em>Stop Services</em></p>

<p>When we try to restart we get the warning we see in <em>Figure 8</em>, telling us we need to also restart the <em>PolyBase Data Movement</em> service, and the <em>Launchpad</em> service.</p>

<blockquote>
<p><strong>NOTE:</strong> The <em>Launchpad</em> service is part of SQL Server Machine Learning Services.</p>
</blockquote>

<p>I click <strong>Yes</strong> as in <em>Figure 8</em>, and almost immediately I see:</p>

<p><img src="/images/posts/sql-2k19-polybase-issue-stop-control.png" alt="" /></p>

<p><strong>Figure 9:</strong> <em>Can Not Stop Service</em></p>

<p>The problem that manifests itself by what we see in <em>Figure 9</em> is that the <em>PolyBase Data Movement</em> service is in a <code>Starting</code> state, and cannot be stopped.</p>

<p>What do we do now? Well, we can try and restart the machine, but that seems like overkill, (and it may not work). So let us try to use a command from the command prompt, where we can force the termination of a service: <code>taskkill</code>.</p>

<p>To use the <code>taskkill</code> command, we need to know either the process-id, (<code>PID</code>), of the process we want to terminate, or the process name. To get the <code>PID</code> we open <em>Task Manager</em> and under the services tab we find the SQL Server service:</p>

<p><img src="/images/posts/sql-2k19-polybase-issue-task-manager.png" alt="" /></p>

<p><strong>Figure 10:</strong> <em>Task Manager</em></p>

<p>We see in <em>Figure 10</em> the <code>PID</code>, (highlighted in yellow), which we now use to terminate the SQL Server process:</p>

<p><img src="/images/posts/sql-2k19-polybase-issue-task-kill.png" alt="" /></p>

<p><strong>Figure 11:</strong> <em>Task Manager</em></p>

<p>As we see in <em>Figure 11</em> I called <code>taskkill</code> from command prompt which I opened as administrator. I used the process-id from <em>Figure 10</em>, and I used the <code>/F</code> flag to indicate I wanted to force the termination. The command executed successfully, and as we see in <em>Figure 11</em> the process has been terminated.</p>

<p>What is left to do now is to start the SQL Server process, followed by the PolyBase Data Movement service and the PolyBase Engine service. The PolyBase Data Movement service may still be in the state of `Starting, but by right-clicking on the service and choose <strong>Start</strong> it will start:</p>

<p><img src="/images/posts/sql-2k19-polybase-issue-services-running.png" alt="" /></p>

<p><strong>Figure 12:</strong> <em>Services Running</em></p>

<p>We see in <em>Figure 12</em> how the services are up and running.</p>

<h2 id="summary">Summary</h2>

<p>In this post, we saw how we fix the issue where the PolyBase services do not start on a SQL Server 2019 Developer Edition.</p>

<p>We said it was due to that TCP/IP is not by default an enabled SQL Server network protocol for the Developer edition. Therefore the PolyBase services cannot connect to the SQL Server instance at startup. We fixed it by:</p>

<ul>
<li>Enable the TCP/IP protocol under <em>SQL Server Network Configuration</em> in <em>SQL Server Configuration Manager</em>.</li>
<li>Retrieve the process-id, (<code>PID</code>), for the SQL Server service.</li>
<li>Forcing the process to be terminated by the <code>taskkill /PID &lt;process-id&gt; /F</code> command.</li>
<li>Restart the SQL Server service, followed by the PolyBase services.</li>
</ul>

<p>Another way to do this is to install SQL Server without the PolyBase features. After installation you enable the TCP/IP protocol, restart SQL Server, and then install the PolyBase features.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 46, 2019]]></title>
    <link href="https://nielsberglund.com/2019/11/17/interesting-stuff---week-46-2019/" rel="alternate" type="text/html"/>
    <updated>2019-11-17T07:47:05+02:00</updated>
    <id>https://nielsberglund.com/2019/11/17/interesting-stuff---week-46-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://devblogs.microsoft.com/dotnet/building-modern-cloud-applications-using-pulumi-and-net-core/">Building Modern Cloud Applications using Pulumi and .NET Core</a>. As you probably know, <a href="https://www.pulumi.com/">Pulumi</a> is an open source infrastructure as code tool which makes it easy to create, deploy, and manage cloud applications, using your favorite development languages. The post I link to announces .NET Core support for Pulumi.</li>
</ul>

<h2 id="data-science-machine-learning">Data Science / Machine Learning</h2>

<ul>
<li><a href="https://blog.revolutionanalytics.com/2019/11/azure-ai-and-machine-learning-talk-series.html">Azure AI and Machine Learning talk series</a>. This is a post by <a href="https://twitter.com/revodavid">David</a>, where lists the talks he and his team did at Microsoft Ignite. These talks are also presented worldwide at <a href="https://www.microsoft.com/en-us/ignite-the-tour/"><strong>Microsoft Ignite The Tour</strong></a> during the next six months.<br /></li>
</ul>

<h2 id="sql-server-2019-big-data-cluster">SQL Server 2019 Big Data Cluster</h2>

<ul>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2019/11/11/build-an-intelligent-analytics-platform-with-sql-server-2019-big-data-clusters/">Build an intelligent analytics platform with SQL Server 2019 Big Data Clusters</a>. This post gives a high-level view of one can create a powerful analytics platform on top of SQL Server 2019 Big Data Cluster.</li>
</ul>

<h2 id="microsoft-ignite-the-tour">Microsoft Ignite The Tour</h2>

<ul>
<li><p><a href="https://www.microsoft.com/en-za/ignite-the-tour/johannesburg">Microsoft Ignite The Tour Johannesburg</a>. I mentioned <strong>Microsoft Ignite the Tour</strong> above, and this is for my readers in South Africa. <strong>Microsoft Ignite the Tour</strong> visits Johannesburg, January 30 - 31, 2020, so register now to not miss out on the latest in cloud technologies and developer tools. Oh, and by the way, I deliver three talks in Johannesburg:</p>

<ul>
<li><strong>A Lap Around SQL Server Big Data Cluster</strong>.</li>
<li><strong>Improve Customer Lifetime Value using Azure Databricks &amp; Delta Lake</strong>.</li>
<li><strong>Simplify and Scale Your Data Pipelines with Azure Delta Lake</strong>.</li>
</ul></li>
</ul>

<p>So <a href="https://www.microsoft.com/en-za/ignite-the-tour/johannesburg">register</a> now, and come and say Hi!</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 45, 2019]]></title>
    <link href="https://nielsberglund.com/2019/11/10/interesting-stuff---week-45-2019/" rel="alternate" type="text/html"/>
    <updated>2019-11-10T09:26:05+02:00</updated>
    <id>https://nielsberglund.com/2019/11/10/interesting-stuff---week-45-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://itnext.io/tutorial-using-azure-event-hubs-with-the-dapr-framework-81c749b66dcf">Tutorial: Using Azure Event Hubs with the Dapr framework</a>. A while ago, Microsoft introduced <a href="https://dapr.io/">Dapr</a>, (Distributed Application Runtime), which is an event-driven, portable runtime for building microservices on cloud and edge. The post linked to here walks through an example of how to integrate Azure Event Hubs with Dapr.</li>
<li><a href="https://medium.com/swlh/a-quick-introduction-to-service-meshes-c4c47c6894b1">A Quick Introduction to Service Meshes</a>. This article explores the basics of service mesh use-cases and architecture. It gives you an understanding of whether a service mesh is the right solution for you and your organization.</li>
<li><a href="https://medium.com/@nwillc/thoughts-on-event-driven-architectures-cbf31c553f4">Thoughts on Event Driven Architectures</a>. For you who follow my &ldquo;ramblings&rdquo; here on my blog, you have probably noticed that I am quite partial to streaming and events. So it should not come as a big surprise that I really liked the post linked to, where the author posts his thoughts around event driven architectures.</li>
</ul>

<h2 id="sql-server-2019">SQL Server 2019</h2>

<ul>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2019/11/04/sql-server-2019-is-now-generally-available/">SQL Server 2019 is now generally available</a>. The title says it all! At the MS Ignite conference in Orlando, Microsoft announced that SQL Server 2019 is now generally available - it is released!</li>

<li><p><a href="https://cloudblogs.microsoft.com/sqlserver/2019/11/05/the-november-2019-release-of-azure-data-studio-is-now-available/">The November 2019 release of Azure Data Studio is now available</a>. Another one of those &ldquo;the title says it all&rdquo; posts. Also at Ignite Microsoft announced the November version of Azure Data Studio. There are some exciting features in the release:</p>

<ul>
<li>Deploy SQL Server 2019 Big Data Clusters with the Big Data Clusters deploy wizard.</li>
<li>Manage SQL Server 2019 Big Data Clusters health with the controller dashboard.</li>
<li>Jupyter Book support. Read more about Jupyter Books <a href="https://jupyterbook.org/intro.html">here</a>.</li>
</ul></li>

<li><p><a href="https://azure.microsoft.com/en-us/blog/simply-unmatched-truly-limitless-announcing-azure-synapse-analytics/">Simply unmatched, truly limitless: Announcing Azure Synapse Analytics</a>. This was an announcement at Ignite that very few saw coming. Azure Synapse is formerly known as Azure SQL Data Warehouse, but it is so much more. It brings together enterprise data warehousing and Big Data analytics in a truly transparent way. There is an excellent introductory video <a href="https://www.youtube.com/watch?v=tMYOi5E14eU">here</a> if you want to know more about Azure Synapse</p></li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/kafka-connect-single-message-transformation-tutorial-with-examples">How to Use Single Message Transforms in Kafka Connect</a>. When you use Kafka Connect to stream data from source to sink, you may want to transform records as they flow through. The Kafka Connect API provides a simple interface for doing this: the Single Message Transforms API. This post covers how to use the API.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 44, 2019]]></title>
    <link href="https://nielsberglund.com/2019/11/03/interesting-stuff---week-44-2019/" rel="alternate" type="text/html"/>
    <updated>2019-11-03T16:45:41+02:00</updated>
    <id>https://nielsberglund.com/2019/11/03/interesting-stuff---week-44-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="database-technology">Database Technology</h2>

<ul>
<li><a href="https://www.cockroachlabs.com/blog/how-we-built-a-vectorized-sql-engine/">How We Built a Vectorized SQL Engine</a>. This post by <a href="https://www.cockroachlabs.com/blog/how-we-built-a-vectorized-sql-engine/">Cockroach Labs</a> discusses how they re-wrote their SQL execution engine to get better performance for analytical type of workloads, (read: queries).</li>
</ul>

<h2 id="sql-server-2019">SQL Server 2019</h2>

<ul>
<li><a href="https://www.cockroachlabs.com/">Introducing the SQL Server 2019 video learning series</a>. This post by Microsoft&rsquo;s <a href="https://twitter.com/bobwardms">Bob Ward</a> points to resources for learning more about SQL Server 2019.</li>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2019/10/31/unify-your-data-lakes-with-hdfs-tiering-in-sql-server-big-data-clusters/">Unify your data lakes with HDFS tiering in SQL Server Big Data Clusters</a>. What is HDFS tiering in SQL Server Big Data Cluster? Well, this post by <a href="https://twitter.com/nelliegson">Nellie Gustafsson</a> explains what it is and how to do it. It also has a link to a video about it. Great stuff!</li>
<li><a href="/2019/11/03/sql-server-2019--java-parameters/">SQL Server 2019 &amp; Java: Parameters</a>. My, my: haven&rsquo;t I been a &ldquo;busy bee&rdquo;. Earlier today I published this post about how to handle parameters between SQL Server 2019 and your Java code.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://itnext.io/creating-a-real-time-flight-info-data-pipeline-with-streamsets-kafka-elasticsearch-and-kibana-dc40868c1021">Creating a Real-Time Flight-info Data Pipeline with StreamSets, Kafka, ElasticSearch and Kibana</a>. The title says it all. This is an awesome post about creating data pipelines. It is a must-read!</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SQL Server 2019 &amp; Java: Parameters]]></title>
    <link href="https://nielsberglund.com/2019/11/03/sql-server-2019--java-parameters/" rel="alternate" type="text/html"/>
    <updated>2019-11-03T11:07:23+02:00</updated>
    <id>https://nielsberglund.com/2019/11/03/sql-server-2019--java-parameters/</id>
    <content type="html"><![CDATA[<p>Microsoft introduced the ability to call Java code from SQL Server in around the SQL Server 2019 CTP 2.0 release, and I have written some posts posts about this:</p>

<ul>
<li><a href="/s2k19_ext_framework_java"><strong>SQL Server 2019 Extensibility Framework &amp; Java</strong></a>.</li>
</ul>

<p>Throughout the CTP releases some functionality has changed, and in CTP 2.5 Microsoft introduced the Java Language Extension SDK. The Java code we write needs now to include the Java SDK and extend an abstract base from the SDK.</p>

<p>With the introduction of the Java Language SDK, came some changes to how you handle method parameters, and I touched upon it briefly in the <a href="/2019/05/26/java--sql-server-2019-extensibility-framework-the-sequel/"><strong>Java &amp; SQL Server 2019 Extensibility Framework: The Sequel</strong></a> post. In this post, I want to look more in detail at parameters and how to handle them.</p>

<p></p>

<blockquote>
<p><strong>DISCLAIMER:</strong> <em>This post contains Java code. I am not a Java guy, in fact, the only Java I have ever written is the code for the SQL Server 2019 Java posts. So, the code is not elegant in any shape or form, and I am absolutely certain it can be done in a much better way. However, this is not about Java as such, but how you call Java code from SQL Server, and what you need to implement on the Java side.</em></p>
</blockquote>

<h2 id="java-code-in-sql-refresh">Java Code in SQL Refresh</h2>

<p>If you have not done any Java code in SQL Server, or at least not recently, here are some posts which introduce you to Java in SQL Server:</p>

<ul>
<li><a href="/2019/03/10/sql-server-2019-java--external-libraries---i/"><strong>SQL Server 2019, Java &amp; External Libraries - I</strong></a>. Part I of a couple of posts where we look at how we can deploy Java code to the database, so it can be loaded from there.</li>
<li><a href="/2019/05/26/java--sql-server-2019-extensibility-framework-the-sequel/"><strong>Java &amp; SQL Server 2019 Extensibility Framework: The Sequel</strong></a>. We look at the implications of the introduction of the Java Language Extension SDK.</li>
<li><a href="/2019/06/06/sql-server-2019-extensibility-framework--external-languages/"><strong>SQL Server 2019 Extensibility Framework &amp; External Languages</strong></a>. We look at SQL Server 2019 Extensibility Framework and Language Extensions.</li>
<li><a href="/2019/08/03/sql-server-2019-ctp3.2--java/"><strong>SQL Server 2019 CTP3.2 &amp; Java</strong></a>. SQL Server 2019 CTP 3.2 and Azule OpenJDK.</li>
<li><a href="/2019/10/26/sql-server-2019--java-null-handling-take-two/"><strong>SQL Server 2019 &amp; Java Null Handling: Take Two</strong></a>. We take a second look at how to handle null values in datasets being passed to and from Java code.</li>
</ul>

<h2 id="pre-req">Pre-req</h2>

<p>If you want to follow along with what we are doing in this post, you need:</p>

<ul>
<li>A SQL Server 2019 CTP 3.0, (or later), instance.</li>
<li>A database where you have registered Java as a language. Read <a href="/2019/06/06/sql-server-2019-extensibility-framework--external-languages/">this post</a> if you are unsure how to register Java as an external language.</li>
<li>The Java Language SDK deployed to the database above. <a href="/2019/05/26/java--sql-server-2019-extensibility-framework-the-sequel/">This post</a> explains how.</li>
<li>Understand how to deploy Java code to the database. The post <a href="/2019/03/10/sql-server-2019-java--external-libraries---i/">here</a> helps with that.</li>
</ul>

<p>You also need an editor with which you can write Java code. I am partial to <strong>Visual Studio Code</strong> myself, and I wrote a post about VS Code and Java <a href="/2019/01/17/sql-server-2019--java-with-visual-studio-code/">here</a>.</p>

<h2 id="parameter-recap">Parameter Recap</h2>

<p>Let us take a look back at parameters to refresh our memory.</p>

<h4 id="bjlsdk-before-java-language-sdk">BJLSDK (Before Java Language SDK)</h4>

<p>Before the introduction of the Java Language SDK, (before CTP 2.5), you called into a method specified in the <code>@script</code> variable of the <code>sp_execute_external_script</code>, (SPEES), procedure. You defined the parameters you wanted to send to the Java code as class variables with the same name as the SQL parameters but without the <code>@</code>:</p>

<pre><code class="language-sql">DECLARE @p1 int = 21;
DECLARE @p2 int = 21;
EXEC sp_execute_external_script
  @language = N'Java'
, @script = N'JavaTest1.adder'
, @params = N'@x int, @y int'
, @x = @p1
, @y = @p2   
GO
</code></pre>

<p><strong>Code Snippet 1</strong> <em>Call from T-SQL to Java with Parameters</em></p>

<p>We see in <em>Code Snippet 1</em> how we want to call into the <code>adder</code> method in the <code>JavaTest1</code> class, passing in two parameters: <code>@x</code>, and <code>@y</code>. When we execute the code the Java C++ language extension gets the parameter values, and looks in the code for two class-level variables named <code>x</code>, and <code>y</code>, and assigns the values to those variables. The <code>adder</code> method then uses <code>x</code>, and <code>y</code>.</p>

<h4 id="ajlsdk-after-java-language-sdk">AJLSDK (After Java Language SDK)</h4>

<p>I mentioned above how a couple of things changed after Microsoft introduced the Java Language SDK. One of them was that you no longer define a method in SPEES&rsquo;s <code>@script</code> parameter. The parameter instead defines a class you want to call into.</p>

<p>The class must extend the <code>AbstractSqlServerExtensionExecutor</code> abstract base class and implement the <code>execute</code> method from the base class. The <code>execute</code> method is the method the Java C++ language extension calls into, and the signature looks like so:</p>

<pre><code class="language-java">public AbstractSqlServerExtensionDataset execute(
                                AbstractSqlServerExtensionDataset input, 
                                LinkedHashMap&lt;String, Object&gt; params) {
  throw new UnsupportedOperationException(&quot;AbstractSqlServerExtensionExecutor 
                                          execute() is not implemented&quot;);
}
</code></pre>

<p><strong>Code Snippet 2</strong> <em>The <code>execute</code> Method</em></p>

<p>From the signature in <em>Code Snippet 2</em> we see how the <code>execute</code> method takes two parameters and has a return type.</p>

<p>To call into Java after the introduction of the Java Language SDK has not changed much:</p>

<pre><code class="language-sql">DECLARE @p1 int = 21;
DECLARE @p2 int = 21;
EXEC sp_execute_external_script
  @language = N'Java'
, @script = N'sql.JavaTest1'
, @params = N'@x int, @y int'
, @x = @p1
, @y = @p2   
GO
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Call from T-SQL to Java with Parameters</em></p>

<p>The only real difference between <em>Code Snippet 3</em> and <em>Code Snippet 2</em> is that the <code>@script</code> parameter is now <code>package.class</code>, instead of <code>class.method</code>.</p>

<p>For the parameters the Java C++ language extension creates a new instance of a <code>LinkedHashMap</code> and populates it with the parameter names, (sans <code>@</code>), and the values.</p>

<h2 id="parameters">Parameters</h2>

<p>Having the recap out of the way, let us see how we handle the parameters in our Java code. Let us first start with some skeleton code:</p>

<pre><code class="language-java">package sql;

import com.microsoft.sqlserver.javalangextension.*;
import java.util.LinkedHashMap;

public class JavaTest1 extends AbstractSqlServerExtensionExecutor {

  public JavaTest1() {
    executorExtensionVersion = SQLSERVER_JAVA_LANG_EXTENSION_V1;
    executorInputDatasetClassName = PrimitiveDataset.class.getName();
    executorOutputDatasetClassName = PrimitiveDataset.class.getName();
  }

  public PrimitiveDataset execute(PrimitiveDataset input, 
                                  LinkedHashMap&lt;String, Object&gt; params) {
    
    return null;

  }
}
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Java Starter Code</em></p>

<p>We see in <em>Code Snippet 4</em> that we have a class, <code>JavaTest1</code>, which extends the <code>AbstractSqlServerExtensionExecutor</code> class. In the constructor, we set some required class members, and the <code>execute</code> method is empty.</p>

<blockquote>
<p><strong>NOTE:</strong> The post <a href="/2019/05/26/java--sql-server-2019-extensibility-framework-the-sequel/"><strong>Java &amp; SQL Server 2019 Extensibility Framework: The Sequel</strong></a> explains about the constructor in <em>Code Snippet 4</em>.</p>
</blockquote>

<p>The code we want to write is to handle the parameters we send in when we execute the code in <em>Code Snippet 3</em>, and in our code, we want to add the two parameters together. With that in mind, our code looks like so:</p>

<pre><code class="language-java">public PrimitiveDataset execute(PrimitiveDataset input, 
                                LinkedHashMap&lt;String, Object&gt; params) {

  int x = (int)params.get(&quot;x&quot;);
  int y = (int)params.get(&quot;y&quot;);
  System.out.printf(&quot;The result of adding %d and %d = %d&quot;, x, y, x + y);
  return null;
}
</code></pre>

<p><strong>Code Snippet 5:</strong> <em>Handle Parameters</em></p>

<p>We see in <em>Code Snippet 5</em> how we handle the parameters that come in from SQL Server:</p>

<ul>
<li>As we said above, the Java C++ language extension populates the <code>LinkedHashMap</code> with the parameters from SQL Server, so we retrieve the <code>x</code> and <code>y</code> using the <code>get</code> method. As the <code>get</code> method returns an object, we cast it to <code>int</code>.</li>
<li>We then add <code>x</code> and <code>y</code> together and we call <code>printf</code> to show the result.</li>
</ul>

<p>After we compile and package the code into a <code>.jar</code> file we deploy to our database:</p>

<pre><code class="language-sql">CREATE EXTERNAL LIBRARY SqlParamLib 
FROM (CONTENT = 'W:\sql-params-1.0.jar')
WITH (LANGUAGE = 'Java');
</code></pre>

<p><strong>Code Snippet 6:</strong> <em>Deploy to the Database</em></p>

<p>From <em>Code Snippet 6</em> we see how I deploy to my SQL Server based on a file-path: <code>W:\sql-params-1.0.jar</code>, and how I indicate that what I deploy is Java code: <code>LANGUAGE = 'Java'</code>. The post <a href="/2019/03/10/sql-server-2019-java--external-libraries---i/"><strong>SQL Server 2019, Java &amp; External Libraries - I</strong></a> covers in detail how to deploy Java code to SQL Server.</p>

<blockquote>
<p><strong>NOTE:</strong> I can use a file path when I deploy as my SQL Server is on my local machine.</p>
</blockquote>

<p>When we have deployed as in <em>Code Snippet 6</em> we execute the code in <em>Code Snippet 3</em>:</p>

<p><img src="/images/posts/sql-2k19-java-params-add.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Add Result</em></p>

<p>In <em>Figure 1</em> we see that our code worked, cool!</p>

<h4 id="null-values">Null Values</h4>

<p>Now then, what if one of the parameters in <em>Code Snippet 3</em> is not <code>DECLARE</code>:ed and set, but instead generated by something, (perhaps a <code>SELECT</code>), and it doesn&rsquo;t have a value:</p>

<pre><code class="language-sql">DECLARE @p1 int;
DECLARE @p2 int;

SELECT @p1 = null, @p2 = 21;

EXEC sp_execute_external_script
  @language = N'Java'
, @script = N'sql.JavaTest1'
, @params = N'@x int, @y int'
, @x = @p1
, @y = @p2   
GO
</code></pre>

<p><strong>Code Snippet 7:</strong> <em>Null Parameter</em></p>

<p>In <em>Code Snippet 7</em> we see how we retrieve the values of the two parameters, and one is null, (no value). Let us see what happens when we execute the code:</p>

<p><img src="/images/posts/sql-2k19-java-params-null.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Null Parameter</em></p>

<p>Hmm, not good, we get a <code>NullPointerException</code> as we see in <em>Figure 2</em>. As a Java developer, this is what you would expect, whereas if we executed similar code in T-SQL, it would work - but the result would be <code>NULL</code>. I wrote <a href="/2019/10/26/sql-server-2019--java-null-handling-take-two/">here</a> about null handling in datasets, and how there is a &ldquo;convenience&rdquo; method <code>getColumnNullMap</code> which indicates null values for a column in the dataset.</p>

<p>Unfortunately, we do not have the same for parameters, so we need to handle it ourselves. Since the parameter value&rsquo;s type in the <code>LinkedHashMap</code> is an <code>Object</code>, we should be able to do a null check quite easily before we do the cast. Alternatively, we can use the primitive type&rsquo;s wrapper class to accomplish the same thing:</p>

<pre><code class="language-java">public PrimitiveDataset execute(PrimitiveDataset input, 
                                LinkedHashMap&lt;String, Object&gt; params) {

  boolean isNull = false;

  Integer x = (Integer)params.get(&quot;x&quot;);
  Integer y = (Integer)params.get(&quot;y&quot;);

  if(x == null || y == null) {
    isNull = true;
  }

  if(isNull) {
    String xStr = x != null ? x.toString() : &quot;null&quot;;
    String yStr = y != null ? y.toString() : &quot;null&quot;;
    System.out.printf(&quot;The result of adding %s and %s = %s&quot;, 
                       xStr, yStr, &quot;null&quot;);
  }
  else {
    System.out.printf(&quot;The result of adding %d and %d = %d&quot;, 
                       x, y, x + y);
  }

  return null;
}
</code></pre>

<p><strong>Code Snippet 8:</strong> <em>Handle Null Parameter</em></p>

<p>In <em>Code Snippet 8</em> we see an example of how we can handle null parameters. As you see in the code snippet, we use the <code>Integer</code> wrapper class for <code>int</code>. Our logic is that if we get a null value, then the result should also be null.</p>

<h4 id="output-parameters">Output Parameters</h4>

<p>In SQL Server, when we execute a procedure, we can return values as output parameters, and the way we indicate that a parameter is for output is by using the <code>OUT</code> statement:</p>

<pre><code class="language-sql">DECLARE @p1 int = 21;
DECLARE @p2 int = 21;
DECLARE @ret int;
EXEC dbo.pr_MyAdder @x = @p1,
                    @y = @p2,
                    @retVal = @ret OUT;
SELECT @ret AS ReturnValue;                    
</code></pre>

<p><strong>Code Snippet 9:</strong> <em>Output Parameters - I</em></p>

<p>We see in <em>Code Snippet 9</em> how we assign <code>@ret</code> as an output parameter and then select the value after the execution of the procedure.</p>

<p>How do we do this in Java? Well, initially output parameters were not supported, but that changed with SQL Server CTP 3.0, (or 3.1). The reason why output parameters were not supported was that the Java C++ language extension did not have an implementation for output parameters. However, as I said above - that changed in CTP 3.0/3.1, and output parameters are now part of the <code>LinkedHashMap</code> parameter.</p>

<p>The Java C++ language extension populates, as I said above, the <code>LinkedHashMap</code> and since it is a reference type when the call returns the Java C++ language extension loops through the parameters and outputs the parameters adorned with <code>OUT</code>, (or <code>OUTPUT</code>).</p>

<p>The code to implement output parameters in our Java code looks like so:</p>

<pre><code class="language-java">public PrimitiveDataset execute(PrimitiveDataset input, 
                                LinkedHashMap&lt;String, Object&gt; params) {

  boolean isNull = false;

  Integer x = (Integer)params.get(&quot;x&quot;);
  Integer y = (Integer)params.get(&quot;y&quot;);
  Integer retVal = (Integer)params.get(&quot;retVal&quot;);

  if(x == null || y == null) {
    isNull = true;
  }

  if(isNull) {
    params.put(&quot;retVal&quot;, null);
  }
  else {
    params.put(&quot;retVal&quot;, x + y);
  }

  return null;
}
</code></pre>

<p><strong>Code Snippet 10:</strong> <em>Output Parameters in Java</em></p>

<p>We see in <em>Code Snippet 10</em> how we have changed the Java code to return the result of the addition of <code>x</code> and <code>y</code> as an output parameter. We retrieve the <code>x</code> and <code>y</code> values as before, and dependent on whether <code>x</code> or <code>y</code> is null or not we either <code>put</code> the <code>retVal</code> parameter as <code>null</code> or the result of the addition.</p>

<blockquote>
<p><strong>NOTE:</strong> It is not necessary to retrieve the <code>retVal</code> parameter as in <em>Code Snippet 10</em> since in this case, it is a pure output parameter. SQL Server, however, allows output parameters both to be used as in as out. So in your Java code, you may get a value coming in for an output parameter.</p>
</blockquote>

<p>The SQL code to execute against the code in <em>Code Snippet 10</em> looks like so:</p>

<pre><code class="language-sql">DECLARE @p1 int = 21;
DECLARE @p2 int = 21;
DECLARE @ret int;
EXEC sp_execute_external_script
  @language = N'Java'
, @script = N'sql.JavaTest1'
, @params = N'@x int, @y int, @retVal int OUT'
, @x = @p1
, @y = @p2  
, @retVal = @ret OUT;

SELECT @ret AS ReturnValue;
GO
</code></pre>

<p><strong>Code Snippet 11:</strong> <em>Output Parameters - II</em></p>

<p>We see in <em>Code Snippet 11</em> how we:</p>

<ul>
<li>Declare a third parameter<code>@ret</code>.</li>
<li>In the <code>@params</code> parameter we define the <code>@retVal</code> parameter.</li>
<li>Assign <code>@ret</code> to <code>@retVal</code>.</li>
</ul>

<p>The result of executing the code in <em>Code Snippet 11</em> is:</p>

<p><img src="/images/posts/sql-2k19-java-params-outputparam.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Output Parameter</em></p>

<p>As we see in <em>Figure 3</em> it works. If we were to send in null for <code>@x</code> or <code>@y</code>, we get back <code>NULL</code> in the <code>SELECT</code>.</p>

<h2 id="summary">Summary</h2>

<p>In this post, we saw how to handle parameters, and we said:</p>

<ul>
<li>The <code>LinkedHashMap</code> parameter in the <code>execute</code> method exposes parameters.</li>
<li>For primitive data type parameters, we can use Java&rsquo;s wrapper classes to handle null values.</li>
<li>The Java C++ language extension handles now output parameters.</li>
<li>We assign a value to an output parameter by doing a <code>put</code>.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 43, 2019]]></title>
    <link href="https://nielsberglund.com/2019/10/27/interesting-stuff---week-43-2019/" rel="alternate" type="text/html"/>
    <updated>2019-10-27T09:15:54+02:00</updated>
    <id>https://nielsberglund.com/2019/10/27/interesting-stuff---week-43-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://mattwarren.org/2019/10/25/Research-based-on-the-.NET-Runtime/">Research based on the .NET Runtime</a>. This post by <a href="https://twitter.com/matthewwarren">Matthew</a> is a collection of Common Language Runtime, (CLR), research papers. It is an impressive list, and if you are interested in CLR, you should read some of the papers.</li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://blog.acolyer.org/2019/10/23/futzing-and-moseying/">Futzing and moseying: interviews with professional data analysts on exploration practices</a>. In this post, <a href="https://twitter.com/adriancolyer">Adrian</a> dissects a white paper from 2018 around what data analysts do when they do Explanatory Data Analysis, (EDA). If you are a data analyst, or if you are interested in data in general, then you should read this.</li>
<li><a href="https://towardsdatascience.com/predicting-customer-churn-with-pyspark-95cd352d393">PySpark &amp; AWS | Predicting Customer Churn</a>. This is a very interesting post where the author takes us through how to work with PySpark on your local computer and then move to AWS to handle large data volumes.</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://medium.com/better-programming/thinking-in-kubernetes-k8s-3770bf14c463">Thinking in Kubernetes (K8s)</a>. Due to what Kubernetes does, and how it does it, starting with Kubernetes may require a mind-shift. This post looks at certain things to look at, and perhaps look at differently when you start your Kubernetes journey.</li>
<li><a href="https://www.infoq.com/presentations/adtech-fraud-detection">High Performance Cooperative Distributed Systems in Adtech</a>. This is an <a href="https://www.infoq.com/">InfoQ</a> presentation where the presenter explores a set of core building blocks exhibited by Adtech platforms and applies them towards building a fraud detection platform. The presenter also touches on critical attributes of system reliability and quality in an Adtech system.</li>
<li><a href="https://itnext.io/tutorial-auto-scale-your-kubernetes-apps-with-prometheus-and-keda-c6ea460e4642">Autoscaling Kubernetes apps with Prometheus and KEDA</a>. There are a couple of ways to scale your application in a Kubernetes cluster. In this post, the author looks at using the Kubernetes Event Driven Autoscaling, (KEDA), Kubernetes operator. What KEDA does is it provides fine-grained autoscaling for event-driven workloads. Very cool article!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/create-dynamic-kafka-connect-source-connectors">4 Steps to Creating Apache Kafka Connectors with the Kafka Connect API</a>. Kafka Connect connectors are used to stream data into Kafka or stream data out of it. In this post, the author discusses how you can create your own connectors using the Kafka Connect API.</li>
</ul>

<h2 id="sql-server-2019">SQL Server 2019</h2>

<ul>
<li><a href="/2019/10/26/sql-server-2019--java-null-handling-take-two/">SQL Server 2019 &amp; Java Null Handling: Take Two</a>. I eventually managed to finish and publish the post I mentioned a couple of weeks ago. Anyway, this post is about how to handle null values passed back and forth between SQL Server and your Java code, after the introduction of the SQL Server Java language SDK.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SQL Server 2019 &amp; Java Null Handling: Take Two]]></title>
    <link href="https://nielsberglund.com/2019/10/26/sql-server-2019--java-null-handling-take-two/" rel="alternate" type="text/html"/>
    <updated>2019-10-26T05:32:54+02:00</updated>
    <id>https://nielsberglund.com/2019/10/26/sql-server-2019--java-null-handling-take-two/</id>
    <content type="html"><![CDATA[<p>You who read my blog know that during the last year, (or so), I have been writing about SQL Server 2019 and the ability to call into Java code from SQL Server:</p>

<ul>
<li><a href="/s2k19_ext_framework_java"><strong>SQL Server 2019 Extensibility Framework &amp; Java</strong></a>.</li>
</ul>

<p>It has been a fascinating &ldquo;journey&rdquo;, since SQL Server 2019 is still in preview, and there have been changes in how you call Java code along the way. In this post, we look at some relatively recent changes to how we handle null values in datasets.</p>

<p></p>

<h2 id="java-code-in-sql-refresh">Java Code in SQL Refresh</h2>

<p>If you have not done any Java code in SQL Server, or at least not recently, here are a couple of posts which introduces you to Java in SQL Server:</p>

<ul>
<li><a href="/2019/03/10/sql-server-2019-java--external-libraries---i/"><strong>SQL Server 2019, Java &amp; External Libraries - I</strong></a>. Part I of a couple of posts where we look at how we can deploy Java code to the database, so it can be loaded from there.</li>
<li><a href="/2019/05/26/java--sql-server-2019-extensibility-framework-the-sequel/"><strong>Java &amp; SQL Server 2019 Extensibility Framework: The Sequel</strong></a>. We look at the implications of the introduction of the Java Language Extension SDK.</li>
<li><a href="/2019/06/06/sql-server-2019-extensibility-framework--external-languages/"><strong>SQL Server 2019 Extensibility Framework &amp; External Languages</strong></a>. We look at SQL Server 2019 Extensibility Framework and Language Extensions.</li>
<li><a href="/2019/08/03/sql-server-2019-ctp3.2--java/"><strong>SQL Server 2019 CTP3.2 &amp; Java</strong></a>. SQL Server 2019 CTP 3.2 and Azule OpenJDK.</li>
</ul>

<h2 id="demo-code">Demo Code</h2>

<h4 id="data">Data</h4>

<p>In today&rsquo;s post, we use some data from the database. The code to insert the data is the same we used in a previous post: <a href="/2018/12/19/sql-server-2019-extensibility-framework--java---null-values/"><strong>SQL Server 2019 Extensibility Framework &amp; Java - Null Values</strong></a>:</p>

<pre><code class="language-sql">USE master;
GO
SET NOCOUNT ON;
GO
DROP DATABASE IF EXISTS JavaNullDB;
GO
CREATE DATABASE JavaNullDB;
GO
USE JavaNullDB;
GO
DROP TABLE IF EXISTS dbo.tb_NullRand10
CREATE TABLE dbo.tb_NullRand10(RowID int identity primary key, 
                          x int, y int, col1 nvarchar(50));
GO
INSERT INTO dbo.tb_NullRand10(x, y, col1)
SELECT TOP(10) CAST(ABS(CHECKSUM(NEWID())) % 14 AS int) 
  , CAST(ABS(CHECKSUM(NEWID())) % 20 AS int)
  , N'Hello ' + CAST(CAST(ABS(CHECKSUM(NEWID())) % 25 AS int) AS nvarchar(50))
FROM sys.objects o1
CROSS JOIN sys.objects o2
CROSS JOIN sys.objects o3
CROSS JOIN sys.objects o4;
GO
UPDATE dbo.tb_NullRand10
  SET y = NULL
WHERE RowId = 3
UPDATE dbo.tb_NullRand10
  SET Col1 = NULL
WHERE RowId = 5    
UPDATE dbo.tb_NullRand10
  SET x = NULL
WHERE RowId = 6 
UPDATE dbo.tb_NullRand10
  SET y = NULL
WHERE RowId = 8  
UPDATE dbo.tb_NullRand10
  SET col1 = NULL
WHERE RowId = 9 
GO
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Create Database Objects</em></p>

<p>We see from *Code Snippet 1 * how we:</p>

<ul>
<li>Create a database: <code>JavaNullDB</code>.</li>
<li>Create a table: <code>dbo.tb_NullRand10</code>.</li>
<li>Insert some data into the table.</li>
</ul>

<p>The data we insert is entirely random, but it gives us something to &ldquo;play around&rdquo; with.</p>

<p>Let us see what the data looks like, by executing: <code>SELECT RowID, x, y FROM dbo.tb_NullRand10;</code>:</p>

<p><img src="/images/posts/sql-2k19-java-null2-result1.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>T-SQL Result</em></p>

<p>As we see in <em>Figure 1</em>, we get back 10 rows, and rows 3, 6, and 8 contains null values.</p>

<h4 id="enable-java-in-the-database">Enable Java in the Database</h4>

<p>To use Java in the database, you need to do a couple of more things:</p>

<ul>
<li>Register Java as an external language in your database. The following post describes what to do: <a href="/2019/06/06/sql-server-2019-extensibility-framework--external-languages/"><strong>SQL Server 2019 Extensibility Framework &amp; External Languages</strong></a>.</li>
<li>Create an external library in the database for the Java SDK. The post <a href="/2019/05/26/java--sql-server-2019-extensibility-framework-the-sequel/"><strong>Java &amp; SQL Server 2019 Extensibility Framework: The Sequel</strong></a> shows you how to do it.</li>
</ul>

<p>Now, when we have a database with some data and Java enabled we, can start.</p>

<h2 id="null-values">Null Values</h2>

<p>In the <a href="/2018/12/19/sql-server-2019-extensibility-framework--java---null-values/">null values post</a> mentioned above, I mentioned that there are differences between SQL Server and Java in how they handle null. So, when we call into Java from SQL Server, we may want to treat null values the same way as we do in SQL Server.</p>

<p>I wrote about this in the <a href="/2018/12/19/sql-server-2019-extensibility-framework--java---null-values/"><strong>SQL Server 2019 Extensibility Framework &amp; Java - Null Values</strong></a> post mentioned above. However, that post was written before SQL Server 2019 CTP 2.5. In CTP 2.5 Microsoft introduced the Java SDK, and certain things changed. Amongst the things that changed is the way we handle nulls when we receive datasets from SQL Server in our Java code.</p>

<p>Let us see how null handling works now post CTP 2.5. We use similar code to what we saw in the <a href="/2018/12/19/sql-server-2019-extensibility-framework--java---null-values/">null post</a> above:</p>

<pre><code class="language-java">public PrimitiveDataset execute(PrimitiveDataset input, 
                               LinkedHashMap&lt;String, Object&gt; params) {
    
  int[] inputCol1 = input.getIntColumn(0);
  int[] inputCol2 = input.getIntColumn(1);
  int[] inputCol3 = input.getIntColumn(2);

  PrimitiveDataset output = new PrimitiveDataset();

  output.addColumnMetadata(0, &quot;RowID&quot;, Types.INTEGER, 0, 0);
  output.addColumnMetadata(1, &quot;x&quot;, Types.INTEGER, 0, 0);
  output.addColumnMetadata(2, &quot;y&quot;, Types.INTEGER, 0, 0);

  output.addIntColumn(0, inputCol1, null);
  output.addIntColumn(1, inputCol2, null);
  output.addIntColumn(2, inputCol3, null);
 
  return output;
}
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Java Input &amp; Output Data</em></p>

<p>The code in <em>Code Snippet 3</em> &ldquo;echoes&rdquo; back what it receives as input dataset and we see how we:</p>

<ul>
<li>Load three arrays with the three columns in the dataset.</li>
<li>Create a new <code>PrimitiveDataset</code> to use as the return type.</li>
<li>Set metadata for the return dataset.</li>
<li>Assign the columns for the return dataset.</li>
<li>Return the dataset.</li>
</ul>

<blockquote>
<p><strong>NOTE:</strong> If you wonder about why the method is named <code>execute</code> and what the <code>PrmitiveDataset</code> is; the post <a href="/2019/05/26/java--sql-server-2019-extensibility-framework-the-sequel/">here</a> explains it.</p>
</blockquote>

<p>After we have compiled and packaged the code into a <code>.jar</code> file we can deploy:</p>

<pre><code class="language-sql">CREATE EXTERNAL LIBRARY SqlNullLib 
FROM (CONTENT = 'W:\sql-null-1.0.jar')
WITH (LANGUAGE = 'Java');
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Deploy Java Code</em></p>

<p>As we see in <em>Code Snippet 3</em> I named my <code>.jar</code> file <code>sql-null-1.0.jar</code> and I deployed it as an external library: <code>SqlNullLib</code>. Since I deploy to a local SQL Server instance, I can use a file location for my <code>.jar</code>.</p>

<blockquote>
<p><strong>NOTE:</strong> The <a href="/2019/03/10/sql-server-2019-java--external-libraries---i/">Java &amp; External Libraries</a> post mentioned above goes into details about external libraries.</p>
</blockquote>

<p>The code to execute looks something like so:</p>

<pre><code class="language-sql">EXEC sp_execute_external_script
  @language = N'Java'
, @script = N'sql.NullValues'
, @input_data_1 = N'SELECT RowID, x, y FROM dbo.tb_NullRand10'
WITH RESULT SETS((RowID int, x int, y int))
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Execute Code</em></p>

<p>We see in <em>Code Snippet 4</em> how we call into the <code>NullValues</code> class in the <code>sql</code> package and how we use the same <code>SELECT</code> statement that generated the resultset we saw in <em>Figure 1</em>. When we execute the code, we see:</p>

<p><img src="/images/posts/sql-2k19-java-null2-result2.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Java Code Result</em></p>

<p>Compare the result we see in <em>Figure 2</em> with the result in <em>Figure 1</em>, and we see the difference in the outlined rows, (3, 6, 8), and the highlighted columns, In <em>Figure 1</em> the columns are <code>NULL</code>, whereas in <em>Figure 2</em> they are <code>0</code>. So why are the columns <code>0</code>?</p>

<p>Well, as we said in the <a href="/2018/12/19/sql-server-2019-extensibility-framework--java---null-values/">null post</a>, this is because the Java language C++ extension converts the null values to the default value for the data type in question.</p>

<blockquote>
<p><strong>NOTE:</strong> The Java language C++ extension is the bridge between SQL Server and your Java code. The <a href="/2019/06/06/sql-server-2019-extensibility-framework--external-languages/"><strong>SQL Server 2019 Extensibility Framework &amp; External Languages</strong></a> post covers it in some detail.</p>
</blockquote>

<p>The question is, why do we care that a null value comes across as zero, at least we do not get a null exception? Let us take a look at the following Java code:</p>

<pre><code class="language-java">public PrimitiveDataset execute(PrimitiveDataset input, 
                                LinkedHashMap&lt;String, Object&gt; params) {
    
  int[] inputCol3 = input.getIntColumn(2);

  int numRows = inputCol3.length;
  int sum = 0;

  for(int x = 0; x &lt; numRows; x++) {
    sum += inputCol3[x];

  }

  double avg = (double)sum / numRows;

  System.out.printf(&quot;Average value of y is: %f&quot;, avg);

  return null;
}

</code></pre>

<p><strong>Code Snippet 5:</strong> <em>Average Value</em></p>

<p>The code in <em>Code Snippet 5</em> expects the same input data as we generated in <em>Code Snippet 4</em>, and it calculates the average value of the <code>y</code> column of that dataset. When we execute the code in <em>Code Snippet 4</em>, after having compiled, packaged and deployed, (comment out <code>WITH RESULT SETS</code>), we see the result as so:</p>

<p><img src="/images/posts/sql-2k19-java-null2-average.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Result of Average Calculation</em></p>

<p>The result in <em>Figure 3</em> looks OK, so let us see what it looks like if we run a similar query in SQL Server:</p>

<pre><code class="language-sql">SELECT CAST(AVG(CAST(y AS DECIMAL(4,2)))AS DECIMAL(4,2))
FROM dbo.tb_NullRand10;
</code></pre>

<p><strong>Code Snippet 6:</strong> <em>T-SQL Average Calculation</em></p>

<p>The result of the query in <em>Code Snippet 6</em> looks like so:</p>

<p><img src="/images/posts/sql-2k19-java-null2-average2.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Result of T-SQL Average Calculation</em></p>

<p>We see in <em>Figure 4</em> how the result of the average calculation, (outlined in red), differs from the Java calculation. The question is why this is; it was the same data in both calculations? Well, was it; we see in <em>Figure 4</em> the highlighted part at the top: &ldquo;Null value is eliminated &hellip;&rdquo;. So what happens is that for certain operations SQL Server eliminates null values, as SQL Server treats nulls as unknown.</p>

<p>As the Java language C++ extension converts nulls, we need to handle it in our Java code.</p>

<h4 id="input-nulls">Input Nulls</h4>

<p>In the previous <a href="/2018/12/19/sql-server-2019-extensibility-framework--java---null-values/">null post</a> we saw that, when we want to handle null input, we use a required class level variable <code>inputNullMap</code>, which the Java language extension populates &ldquo;automagically&rdquo;. However, after the introduction of the Java language SDK, this variable is not required any more. Even if you declared it, the Java language extension does not populate it.</p>

<p>So how do we then figure out whether a column has a null value? Well, since the Java language extension passes data into the <code>execute</code> method via the <code>PrimitiveDataSet</code> class, let us have a look at the base class for <code>PrimitiveDataSet</code>: <code>AbstractSqlServerExtensionDataset</code>:</p>

<pre><code class="language-java">public class AbstractSqlServerExtensionDataset {
  /**
   * Column metadata interfaces
   */
  public void addColumnMetadata(int columnId, String columnName, int columnType, 
                                int precision, int scale) {
    throw new UnsupportedOperationException(&quot;addColumnMetadata is not implemented&quot;);
  }

  ...

  public boolean[] getColumnNullMap(int columnId) {
    throw new UnsupportedOperationException(&quot;getColumnNullMap is not implemented&quot;);
  }

  ...
}
</code></pre>

<p><strong>Code Snippet 7:</strong> <em>AbstractSqlServerExtensionDataset</em></p>

<p>We see in <em>Code Snippet 7</em> how the <code>AbstractSqlServerExtensionDataset</code> has a section for metadata, and in that section is a method: <code>getColumnNullMap</code>. The method takes an integer as an input parameter, and it returns an array of type <code>boolean</code>.</p>

<blockquote>
<p><strong>NOTE:</strong> The Java SDK is open source, and you find it <a href="https://github.com/microsoft/sql-server-language-extensions">here</a>.</p>
</blockquote>

<p>This is what happens when the Java language C++ extension populates the dataset which is used as an input parameter:</p>

<ul>
<li>The extension creates a <code>boolean</code> array for each non-nullable Java datatype columns.</li>
<li>The extension loops each row for each column in the dataset.</li>
<li>Where there is a null value, for a primitive data type, the extension assigns the default value of the data type to that column.</li>
<li>When the extension comes across a null value in a non-nullable Java data type column, it sets the boolean array value to <code>true</code> in the column array.</li>
</ul>

<p>With this in mind we can change the code in <em>Code Snippet 5</em> to handle null values, or rather handle values in the dataset that originates from a SQL Server null value:</p>

<pre><code class="language-java">public PrimitiveDataset execute(PrimitiveDataset input, 
                                LinkedHashMap&lt;String, Object&gt; params) {
    
  int[] inputCol3 = input.getIntColumn(2);

  int numRows = inputCol3.length;
  boolean[] nullMap = input.getColumnNullMap(2);
  int nonNull= 0;
  int sum = 0;

  for(int x = 0; x &lt; numRows; x++) {
    if(!nullMap[x]) {
      nonNull ++;
      sum += inputCol3[x];
    }

  }

  double avg = (double)sum / nonNull;

  System.out.printf(&quot;Average value of y is: %f&quot;, avg);

  return null;
}
</code></pre>

<p><strong>Code Snippet 8:</strong> <em>Null Handling</em></p>

<p>So, in <em>Code Snippet 8</em> we see how we:</p>

<ul>
<li>Get the column we want to create the average over.</li>
<li>Use <code>getColumnNullMap</code> to retrieve the null map for the column we use for the calculation.</li>
<li>In the <code>for</code> loop check whether the column value is null or not. If it is not null, we include the value and increase the row count.</li>
<li>Finally do the average calculation.</li>
</ul>

<p>The result when executing the code in <em>Code Snippet 4</em> against our new code looks like so:</p>

<p><img src="/images/posts/sql-2k19-java-null2-average3.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>New Java Average Calculation</em></p>

<p>We see in <em>Figure 5</em> how our Java calculation now gives the same result as the T-SQL calculation. Awesome!</p>

<h4 id="output-nulls">Output Nulls</h4>

<p>We have now seen how to use <code>getColumnNullMap</code> to distinguish input values that come in as <code>NULL</code> from SQL Server, which the Java language C++ extension converts to the default value for the Java data type.</p>

<p>What about if we need to return null values to SQL Server in a return dataset, but the Java data type is non-nullable? I.e. we receive data in the input dataset, and some column values for a non-nullable Java type are null when passed in from SQL Server. If we wanted to, for example, add the column to another column, the sum should be <code>NULL</code> if we were to handle it the same way as SQL Server does.</p>

<p>So how do we indicate to SQL Server that a column value is null, even though it has a value in Java? Let us go back to <em>Code Snippet 2</em> where we discussed how to return data to SQL Server from Java code. After &ldquo;newing&rdquo; up an instance of <code>PrimitiveDataset</code>, we defined the metadata for the columns via the <code>addColumnMetadata</code> method. We then added the row arrays for the columns through the <code>add*Type*Column</code>, (in our case it was <code>addIntColumn</code>), and it is in that method the &ldquo;secret&rdquo; to null values lies. Let us go back to <code>AbstractSqlServerExtensionDataset</code> and look at the signature for <code>addIntColumn</code>:</p>

<pre><code class="language-java">public class AbstractSqlServerExtensionDataset {
  
  /**
   * Adding column interfaces
   */
  public void addIntColumn(int columnId, int[] rows, 
                           boolean[] nullMap) {
    throw new UnsupportedOperationException(...);
  }
}
</code></pre>

<p><strong>Code Snippet 9:</strong> <em>Add Column Method</em></p>

<p>Look in <em>Code Snippet 9</em> at the third parameter in the add method. See how it takes a <code>boolean</code> array, and how the name &ldquo;gives it away&rdquo;: <code>nullMap</code>. If we look at other methods for non-nullable Java types, we see that all of them have this parameter, whereas add methods for types that are nullable do not have it.</p>

<p>So for non-nullable types, we define <code>boolean</code> arrays, and in those arrays, we indicate what row value(s) is <code>null</code>. Let us see an example:</p>

<pre><code class="language-java">public PrimitiveDataset execute(PrimitiveDataset input, 
                                LinkedHashMap&lt;String, Object&gt; params) {
    
  PrimitiveDataset output = new PrimitiveDataset();

  int numRows = 5;

  output.addColumnMetadata(0, &quot;RowID&quot;, Types.INTEGER, 0, 0);
  output.addColumnMetadata(1, &quot;IntCol&quot;, Types.INTEGER, 0, 0);
  output.addColumnMetadata(2, &quot;StringCol&quot;, Types.NVARCHAR, 256, 0);

  int[] rowIdRows = new int[numRows];
  int[] intColRows = new int[numRows];
  String[] stringColRows = new String[numRows];

  boolean[] intColNullMap = new boolean[numRows];

  for(int x = 0; x &lt; numRows; x++){
    rowIdRows[x] = x+1;
    if(x % 2 ==0) {
      intColRows[x] = 0;
      intColNullMap[x] = true;
    }
    else {
      intColRows[x] = x + 2;
      intColNullMap[x] = false;
    }
    if(x % 3 ==0) {
      stringColRows[x] = null;
    }
    else {
      stringColRows[x] = &quot;Hello number: &quot; + (x + 1);
    }
  }

  output.addIntColumn(0, rowIdRows, null);
  output.addIntColumn(1, intColRows, intColNullMap);
  output.addStringColumn(2, stringColRows);
 
  return output;
}
</code></pre>

<p><strong>Code Snippet 10:</strong> <em>Return Dataset</em></p>

<p>What we see in <em>Code Snippet 10</em> is a somewhat contrived example where we return a dataset in which we want certain column values to be <code>NULL</code> in SQL Server. We see how we:</p>

<ul>
<li>Create metadata for the dataset.</li>
<li>Create arrays for the individual rows.</li>
<li>Create a null map for one of the integer columns.</li>
<li>In the <code>for</code> loop add values to the arrays, and based on some modulus operations emulate that some values are null.</li>
<li>Add the arrays to the columns, and for the second integer column, we also add the null map.</li>
<li>Finally return the dataset.</li>
</ul>

<p>A couple of things to notice in the code in <em>Code Snippet 10</em>:</p>

<ul>
<li>A null-map is not required for a non-nullable data type columns if the values are not null.</li>
<li>We do not need a null-map for nullable data type columns.</li>
</ul>

<p>To see that our code works we use following code:</p>

<pre><code class="language-sql">EXEC sp_execute_external_script
  @language = N'Java'
, @script = N'sql.NullValues'
WITH RESULT SETS((RowID int, IntCol int, StringCol nvarchar(256)))
</code></pre>

<p><strong>Code Snippet 11:</strong> <em>Execute T-SQL</em></p>

<p>The result we get when executing the code in <em>Code Snippet 11</em>, looks like so:</p>

<p><img src="/images/posts/sql-2k19-java-null2-return-null.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>Return Dataset with Null Values</em></p>

<p>When we look at <em>Figure 6</em> we see that our code worked, and how the Java C++ language extension converted the <code>0</code> values in the integer column to <code>NULL</code> based on the null map.</p>

<h2 id="summary">Summary</h2>

<p>In this post, we discussed how to handle null values in datasets passed into our Java code from SQL Server, and from our Java code back to SQL Server. I wrote about null handling in a previous <a href="/2018/12/19/sql-server-2019-extensibility-framework--java---null-values/">post</a>, but since that post, Microsoft introduced the Java language SDK for SQL Server, and null handling has changed.</p>

<p>We care about null values because, in SQL Server, all data types are nullable, whereas, in Java, that is not the case. In Java, like in .NET, primitive types, (<code>int</code>, etc.), cannot be null. The Java C++ language extension handles the mismatch between nullable in SQL Server and non-nullable in Java, whereby it converts null values in data from SQL Server to the data type&rsquo;s default value for Java. Going the other way, from Java to SQL Server, the C++ language extension converts the values supposed to be null to actual null values.</p>

<p>The way we handle null values after the introduction of the Java language SDK is that we:</p>

<ul>
<li>For input data, and each non-nullable column, we call <code>getColumnNullMap</code> and pass in the columns ordinal position. We then handle the values where the null map indicates a null value.</li>
<li>For output data we create <code>boolean</code> arrays for the columns which should contain null values in SQL Server. We pass the array in as a parameter to the <code>add*TypeName*Column</code> method.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 42, 2019]]></title>
    <link href="https://nielsberglund.com/2019/10/20/interesting-stuff---week-42-2019/" rel="alternate" type="text/html"/>
    <updated>2019-10-20T08:40:13+02:00</updated>
    <id>https://nielsberglund.com/2019/10/20/interesting-stuff---week-42-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://levelup.gitconnected.com/net-core-worker-service-as-windows-service-or-linux-daemons-a9579a540b77">.NET Core Worker Service as Windows Service or Linux Daemons</a>. This post discusses a new .NET Core 3 application template called &ldquo;Worker Service&rdquo;. The template makes it easy to write Windows Services or Linux Daemons.</li>
</ul>

<h2 id="sql-server">SQL Server</h2>

<ul>
<li><a href="https://techcommunity.microsoft.com/t5/SQL-Customer-Success-Engineering/JSON-in-your-Azure-SQL-Database-Let-s-benchmark-some-options/ba-p/909131">JSON in your Azure SQL Database? Let’s benchmark some options!</a>. This Microsoft post discusses and benchmarks options on how to handle JSON in SQL Server.</li>
<li><a href="https://techcommunity.microsoft.com/t5/SQL-Server/Optimize-OLTP-Performance-with-SQL-Server-on-Azure-VM/ba-p/916794">Optimize OLTP Performance with SQL Server on Azure VM</a>. This article shares recommendations on how to optimize SQL Server performance on Azure VMs. The article is based on performance testing with a somewhat scaled-down <a href="http://www.tpc.org/tpce/default.asp">TPC-E</a> benchmark.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://rmoff.net/2019/10/15/skipping-bad-records-with-the-kafka-connect-jdbc-sink-connector/">Skipping bad records with the Kafka Connect JDBC sink connector</a>. This post by the Kafka guru <a href="https://twitter.com/rmoff">Robin Moffat</a> discusses how to handle bad records when using the JDBC sink connector in Kafka Connect. Very interesting!</li>
<li><a href="https://yokota.blog/2018/11/19/kcache-an-in-memory-cache-backed-by-kafka/">KCACHE: AN IN-MEMORY CACHE BACKED BY KAFKA</a>. This post details how you can have an in-memory cache backed by Kafka. Excellent!</li>
<li><a href="https://rmoff.net/2019/10/16/using-kafka-connect-and-debezium-with-confluent-cloud/">Using Kafka Connect and Debezium with Confluent Cloud</a>. This is yet another awesome post by <a href="https://twitter.com/rmoff">Robin</a>. One where he discusses how you can use Debezium together with Confluent Cloud.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 41, 2019]]></title>
    <link href="https://nielsberglund.com/2019/10/13/interesting-stuff---week-41-2019/" rel="alternate" type="text/html"/>
    <updated>2019-10-13T16:09:41+02:00</updated>
    <id>https://nielsberglund.com/2019/10/13/interesting-stuff---week-41-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://databricks.com/blog/2019/10/09/democratizing-financial-time-series-analysis-with-databricks.html">Democratizing Financial Time Series Analysis with Databricks</a>. This is a very interesting post, in that it discusses how to develop financial time series analysis faster using Apache Spark, (well actually Databricks), and Koalas.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.ververica.com/blog/announcing-stateful-functions-distributed-state-uncomplicated">Announcing Stateful Functions: Distributed State, Uncomplicated</a>. This post from Ververica, (previously dataArtisans), discusses Stateful Functions which is an open-source framework that reduces the complexity of building and orchestrating distributed stateful applications at scale.</li>
<li><a href="https://www.confluent.io/blog/kafka-summit-san-francisco-2019-session-videos">Kafka Summit San Francisco 2019 Session Videos</a>. The <a href="https://kafka-summit.org/events/kafka-summit-san-francisco-2019/">Kafka Summit San Francisco 2019</a> finished last week, (October 1). The organizers have done a tremendous job and managed to put all session videos online. The post I linked to has links videos for all sessions, as well as a top ten list of sessions. Next weekend will be a Kafka Summit videos binge for me!</li>
<li><a href="https://www.infoq.com/presentations/event-driven-benefits-pitfalls">Opportunities and Pitfalls of Event-driven Utopia</a>. This is an <a href="https://www.infoq.com/">InfoQ</a> presentation where the presenter goes over the concepts, the advantages, and the pitfalls of event-driven utopia. He shares real-life stories or points to source code examples.</li>
<li><a href="https://rmoff.net/2019/10/07/kafka-connect-and-elasticsearch/">Kafka Connect and Elasticsearch</a>. This post by <a href="https://twitter.com/rmoff">Robin Moffat</a> discusses recent changes in Elasticsearch and the Kafka Connector for Elastic and what you can do to fix some of the errors you may encounter due to the changes.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>Towards the end of last year, I wrote a post about how to handle null values when calling from SQL Server into Java, (and the reverse). Since then, Microsoft released the <a href="/2019/05/26/java--sql-server-2019-extensibility-framework-the-sequel/">Java Extensibility SDK</a>, and some things changed. I am now working on a follow-up post where I look at how to handle null values post the Java Extensibility SDK. Expect it to be published in a week or so.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 40, 2019]]></title>
    <link href="https://nielsberglund.com/2019/10/06/interesting-stuff---week-40-2019/" rel="alternate" type="text/html"/>
    <updated>2019-10-06T08:09:53+02:00</updated>
    <id>https://nielsberglund.com/2019/10/06/interesting-stuff---week-40-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/evolutionary-architecture">Evolutionary Architecture</a>. This is an <a href="https://www.infoq.com/">InfoQ</a> presentation where the presenter discusses traditional approaches of evolutionary architecture showing how to use fitness functions and transition to an evolutionary architecture even in the face of legacy systems.</li>
<li><a href="https://www.infoq.com/news/2019/09/cqrs-event-sourcing-production">Day Two Problems When Using CQRS and Event Sourcing</a>. This is a summary of an <a href="https://www.infoq.com/">InfoQ</a> presentation where the presenter shared his experience running and evolving CQRS and event-sourced applications in production.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/kafka-made-serverless-with-confluent-cloud">Free Apache Kafka as a Service with Confluent Cloud</a>. The <a href="https://kafka-summit.org/events/kafka-summit-san-francisco-2019">Kafka Summit 2019</a> was held earlier this week, and this is a post about an announcement that was made about how Confluent is now offering Confluent Cloud for free. Well, for free is not exactly true, but you get $50 off the bill each month for the first three months after you have signed up. This may not sound like much, but it goes a long way. I have signed up, so expect some posts about Kafka in the cloud going forward.</li>
<li><a href="https://www.infoq.com/presentations/stream-analysis-fp">Real-Time Stream Analysis in Functional Reactive Programming</a>. This <a href="https://www.infoq.com/">InfoQ</a> presentation discusses a reactive approach to application design, and how to account for handling events in near real-time employing the Functional Reactive Programming paradigm.</li>
<li><a href="https://www.youtube.com/watch?reload=9&amp;v=4QoCbhsQeyE&amp;feature=">Jay Kreps, Confluent | Kafka Summit SF 2019 Keynote</a>. Above I mentioned <a href="https://kafka-summit.org/events/kafka-summit-san-francisco-2019">Kafka Summit 2019</a>, and the YouTube video in the link is <a href="https://www.linkedin.com/in/jaykreps/">Jay Kreps</a> keynote address. In the keynote, he explains modern stream processing, real-time databases, KSQL, and other interesting &ldquo;stuff&rdquo;.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 39, 2019]]></title>
    <link href="https://nielsberglund.com/2019/09/29/interesting-stuff---week-39-2019/" rel="alternate" type="text/html"/>
    <updated>2019-09-29T13:17:50+02:00</updated>
    <id>https://nielsberglund.com/2019/09/29/interesting-stuff---week-39-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://devblogs.microsoft.com/dotnet/announcing-net-core-3-0/">Announcing .NET Core 3.0</a>. The title says it all, .NET Core 3.0 is now released! Go and <a href="https://dotnet.microsoft.com/download/dotnet-core/3.0">get it</a>!</li>
<li><a href="https://mattwarren.org/2019/09/26/Stubs-in-the-.NET-Runtime/">&ldquo;Stubs&rdquo; in the .NET Runtime</a>. This post is another performance related post by <a href="https://twitter.com/matthewwarren">Matthew</a>. In the post, he discusses &ldquo;Stubs&rdquo; which is used in the .NET framework to provide a level of indirection. The post explores what they are, how they work, and why they&rsquo;re needed.</li>
</ul>

<h2 id="data">Data</h2>

<ul>
<li><a href="https://www.theseattledataguy.com/why-do-you-need-data-engineers-and-what-do-they-do/">Why Do You Need Data Engineers And What Do They Do?</a>. This is an interesting post discussing the importance of data engineers.</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.tbray.org/ongoing/When/201x/2019/09/25/On-Sharding">On Sharding</a>. This post by <a href="https://en.wikipedia.org/wiki/Tim_Bray">Tim Bray</a>, (of XML fame), discusses different types of sharding in high traffic scenarios. It is always interesting to read what Tim has to say, as he has tremendous experience in all aspects of computing.</li>
<li><a href="https://levelup.gitconnected.com/components-of-kubernetes-architecture-6feea4d5c712">Components of Kubernetes Architecture</a>. I can warmly recommend this post to anyone interested in Kubernetes. The post gives a handy overview of the components of Kubernetes.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/every-company-is-becoming-software">Every Company is Becoming <del>a</del> Software <del>Company</del></a>. In this blog post, <a href="https://www.linkedin.com/in/jaykreps/">Jay Kreps</a>, co-founder and CEO of Confluent, makes the point that not only businesses use more software, but that, increasingly, a business is defined in software. He goes on to discuss how Apache Kafka can help companies to re-architect themselves around event streaming, which he sees as the future.</li>
<li><a href="https://www.confluent.io/blog/analytics-with-apache-kafka-and-rockset">Real-Time Analytics and Monitoring Dashboards with Apache Kafka and Rockset</a>. This blog post looks at how we can build analytics and monitoring on top of Apache Kafka using <a href="https://rockset.com/">Rockset</a>. This is something I want to look into for a couple of our <a href="/derivco">Derivco</a> projects.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 38, 2019]]></title>
    <link href="https://nielsberglund.com/2019/09/22/interesting-stuff---week-38-2019/" rel="alternate" type="text/html"/>
    <updated>2019-09-22T17:01:04+02:00</updated>
    <id>https://nielsberglund.com/2019/09/22/interesting-stuff---week-38-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="databases">Databases</h2>

<ul>
<li><a href="https://blog.acolyer.org/2019/09/20/even-more-amazing-papers-at-vldb/">Even more amazing papers at VLDB 2019 (that I didn&rsquo;t have space to cover yet)</a>. The <a href="https://en.wikipedia.org/wiki/International_Conference_on_Very_Large_Data_Bases">VLDB (Very Large Databases)</a> was held at the end of August, and as usual, quite a lot of white papers were presented at the conference. <a href="https://twitter.com/adriancolyer">Adrian</a> has during the last week, dissected some of them. In this post, he lists some of the papers he didn&rsquo;t dissect but still finds interesting. There are some nuggets in the list!</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/news/2019/09/maesh-kube-service-mesh">Introducing Maesh: A Service Mesh for Kubernetes</a>. This article is an <a href="https://www.infoq.com/">InfoQ</a> article about Maesh. Maesh is an open source service mesh written in Golang and built on top of the reverse proxy and load balancer Traefik. Maesh promises to provide a lightweight service mesh solution that is easy to get started with and to roll out across a microservice application.</li>
<li><a href="https://medium.com/better-programming/essential-kubernetes-resources-2ccb250bcf44">Essential Kubernetes Resources</a>. This post covers a list of Kubernetes resources related to security, resilience, and availability best practices.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/multi-region-data-replication">Built-In Multi-Region Replication with Confluent Platform 5.4-preview</a>. This post covers new functionality in Confluent Platform 5.4: the ability to replicate data across data center regions. Very interesting!</li>
<li><a href="https://www.buzzsprout.com/186154/1720903-kip-500-apache-kafka-without-zookeeper-ft-colin-mccabe-and-jason-gustafson">KIP-500: Apache Kafka Without ZooKeeper ft. Colin McCabe and Jason Gustafson</a>. The link here is to a podcast, where <a href="https://twitter.com/tlberglund">Tim Berglund</a> talks with a couple of Kafka engineers and discusses new functionality coming to Kafka where Kafka do not need ZooKeeper any more.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 37, 2019]]></title>
    <link href="https://nielsberglund.com/2019/09/15/interesting-stuff---week-37-2019/" rel="alternate" type="text/html"/>
    <updated>2019-09-15T16:25:58+02:00</updated>
    <id>https://nielsberglund.com/2019/09/15/interesting-stuff---week-37-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<p>This week I do not have that much to share, partly because I have been occupied with writing a blog post about deploying <strong>SQL Server 2019 Big Data Cluster</strong> using <strong>Azure Data Studio</strong> (see below). It has also been <strong>SQL Saturday</strong> which I have been &ldquo;prepping&rdquo; for.</p>

<h2 id="data-science-machine-learning">Data Science / Machine Learning</h2>

<ul>
<li><a href="https://blog.acolyer.org/2019/09/09/ms-approx-query/">Experiences with approximating queries in Microsoft&rsquo;s production big-data clusters</a>. This is a dissection, by <a href="https://twitter.com/adriancolyer">Adrian</a>, of a white paper about approximating queries at Microsoft. Approximation of queries is used when you want to run analysis / OLAP queries against massive datasets where a query could potentially run for hours. By using approximation the time to run the query is reduced significantly.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/event-streaming-reflections-as-confluent-turns-five-part-1">Reflections on Event Streaming as Confluent Turns Five – Part 1</a>. This is a blog post by <a href="https://twitter.com/tlberglund">Tim Berglund</a>, (awesome name by the way), where he looks back at how Apache Kafka and the Confluent Platform has changed the way we build event-driven systems. Happy 5th Birthday to Confluent!</li>
</ul>

<h2 id="sql-server-2019-big-data-cluster">SQL Server 2019 Big Data Cluster</h2>

<ul>
<li><a href="/2019/09/11/install-sql-server-2019-big-data-cluster-using-azure-data-studio/">Install SQL Server 2019 Big Data Cluster using Azure Data Studio</a>. I had to tear down the <strong>SQL Server 2019 Big Data Cluster</strong> <a href="https://twitter.com/adriancolyer">Andrew</a> and me used for our workshop <strong>A Day of SQL Server 2019 Big Data Cluster</strong> in Johannesburg and rebuild it for the Cape Town leg of SQL Saturday. While I did the rebuild, I thought it would be a good idea to document what I did, and this blog post is the result.</li>
</ul>

<h2 id="sql-saturday">SQL Saturday</h2>

<p>The South African leg of SQL Saturday is done and dusted. We were in Cape Town yesterday, (Saturday, September 14), and I delivered two conference talks, (in addition to Andrew&rsquo;s and mine workshop mentioned above):</p>

<ul>
<li><a href="/download/s2k19-bdc-overview.pdf">A Lap Around SQL Server 2019 Big Data Cluster</a>: The new release of SQL Server; SQL Server 2019 includes Apache Spark and Hadoop Distributed File System (HDFS) for scalable compute and storage. This new architecture that combines together the SQL Server database engine, Spark, and HDFS into a unified data platform is called a “big data cluster.”
This session gives you an overview of what a SQL Server Big Data Cluster is, and what you can do with it.</li>
<li><a href="/download/pirate-snake-coffee.pdf">What is the PiRate, Snake, and Cup of Coffee Doing in My Database?</a>: In this session we looked at the SQL Server Extensibility Framework, and we saw how we can call out to external languages from inside SQL Server. We looked at R, Python and Java, and what we can do from SQL Server having access to those languages.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Install SQL Server 2019 Big Data Cluster using Azure Data Studio]]></title>
    <link href="https://nielsberglund.com/2019/09/11/install-sql-server-2019-big-data-cluster-using-azure-data-studio/" rel="alternate" type="text/html"/>
    <updated>2019-09-11T06:12:12+02:00</updated>
    <id>https://nielsberglund.com/2019/09/11/install-sql-server-2019-big-data-cluster-using-azure-data-studio/</id>
    <content type="html"><![CDATA[<p>I wrote a <a href="/2018/11/10/sql-server-2019-big-data-cluster-on-azure-kubernetes-service/">blog post</a> back in November 2018, about how to install and deploy <strong>SQL Server 2019 Big Data Cluster</strong> on Azure Kubernetes Service. Back then <strong>SQL Server 2019 Big Data Cluster</strong> was in private preview, (CTP 2.1 I believe), and you had to sign up, to get access to the &ldquo;bits&rdquo;. Well, you did not really get any &ldquo;bits&rdquo;; what you did get was access to Python deployment scripts.</p>

<p>Now, September 2019, the BDC is in public preview (you do not have to sign up), and it has reached Release Candidate (RC) status, RC 1. The install method has changed, or rather, in addition to installing via deployment scripts, you can now also install using <strong>Azure Data Studio</strong> deployment notebooks, and that is what this blog post is about.</p>

<p>I install it on Azure, and since I am in South Africa, I use one of the South African hosting locations.</p>

<p></p>

<h2 id="pre-reqs">Pre-Reqs</h2>

<p>To deploy using <strong>Azure Data Studio</strong>, (ADS), you need ADS (duh - see below), but you also need some other things.</p>

<h4 id="azure-subscription">Azure Subscription</h4>

<p>If you want to install on <strong>Azure Kubernetes Service</strong>, (AKS), you need an Azure subscription. If you do not have one you can sign up for a free trial subscription <a href="https://azure.microsoft.com/en-us/free/">here</a>.</p>

<p>Of course you do not need to install it on AKS, you can install it on basically any Kubernetes cluster</p>

<h4 id="python">Python</h4>

<p>Well, Python is not a tool as such, but you need Python installed on the machine you install from, as the ADS deployment runs some Python scrips. You need Python3, and on my machine, I have Python 3.7.3. Ensure that Python is on the <code>PATH</code>.</p>

<h4 id="azdata">azdata</h4>

<p><code>azdata</code> is a Python command-line tool replacing <code>mssqlctl</code>. It enables cluster administrators to bootstrap and manages the big data cluster via REST APIs.</p>

<p>There are a couple of steps to install it:</p>

<ul>
<li>If you have <code>mssqlctl</code> installed you need to uninstall it:</li>
</ul>

<pre><code class="language-bash">$ pip3 uninstall -r https://private-repo.microsoft.com/ \
                           python/ctp3.1/mssqlctl/requirements.txt
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Uninstall <code>mssqlctl</code></em></p>

<p>In <em>Code Snippet 1</em> above I have inserted a line continuation (<code>\</code>) to make the code fit the page.</p>

<ul>
<li>If you have deployed CTP 3.2 of the BDC, then you need to uninstall that version of <code>azdata</code>:</li>
</ul>

<pre><code class="language-bash">pip3 uninstall -r https://azdatacli.blob.core.windows.net/ \
                  python/azdata/2019-ctp3.2/requirements.txt
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Uninstall <code>azdata</code></em></p>

<p>In <em>Code Snippet 2</em> you see how the uninstall command takes the version of <code>azdata</code> to uninstall: <em><code>2019-ctp3.2</code></em>. If you have the 3.2 version installed you need to confirm when you run the code in <em>Code Snippet 2</em> that you want to remove some installed components:</p>

<p><img src="/images/posts/inst-bdcrc1-uninst-azdata.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Confirm Uninstall <code>azdata</code></em></p>

<p>Just click <code>y</code> when asked to proceed.</p>

<blockquote>
<p><strong>NOTE:</strong> The biggest issue causing errors in a BDC deployment, by far, is using an older version of <code>azdata</code>. So please, do not be &ldquo;that guy&rdquo; (or girl) - make sure you uninstall <code>azdata</code> if you have an earlier version. In fact, before a deployment, always uninstall <code>azdata</code> followed by an install, (see below).</p>
</blockquote>

<ul>
<li>You need the latest version of the Python <code>requests</code> package installed:</li>
</ul>

<pre><code class="language-bash">$ pip3 install -U requests
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Install/Upgrade <code>requests</code></em></p>

<ul>
<li>When you have executed the code in <em>Code Snippet 3</em> you can install <code>azdata</code>:</li>
</ul>

<pre><code class="language-bash">$ pip3 install -r https://aka.ms/azdata
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Installing <code>azdata</code></em></p>

<p>After executing the code in <em>Code Snippet 4</em> you can go ahead and install the other tools needed.</p>

<h4 id="kubectl">kubectl</h4>

<p>The <code>kubectl</code> tool is a Kubernetes command-line tool, and it allows you to run commands against Kubernetes clusters. You can use <code>kubectl</code> to deploy applications, inspect and manage cluster resources, and view logs.</p>

<p>You can install <code>kubectl</code> in different ways, and I installed it from <a href="https://chocolatey.org/packages/kubernetes-cli">Chocolatey</a>: <code>choco install kubernetes-cli</code>.</p>

<h4 id="azure-cli">Azure CLI</h4>

<p>The Azure CLI is Microsoft&rsquo;s cross-platform command-line experience for managing Azure resources, and you install it on your local machine. You find install links for Azure CLI <a href="https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest">here</a>.</p>

<h2 id="azure-data-studio">Azure Data Studio</h2>

<p>Since this post is about installing and deploying a BDC using <strong>Azure Data Studio</strong>, you also need ADS. You may already have ADS installed, but to be able to install and deploy to the release candidate of BDC you need a specific install. If you have already installed ADS, this ADS version installs side-by-side with existing ADS installations.</p>

<p>The install link to the ADS RC version is <a href="https://aka.ms/azuredatastudio-rc">here</a>.</p>

<h4 id="sql-server-2019-preview">SQL Server 2019 (Preview)</h4>

<p>In addition to ADS, you also need the <strong>SQL Server 2019 (Preview)</strong> extension, which you install after installing ADS.</p>

<p>As opposed to other ADS extensions, you need to download the extension to your machine before you can install it. You download it from <a href="https://docs.microsoft.com/en-us/sql/azure-data-studio/sql-server-2019-extension">here</a>. After download, you install it from the <strong>File</strong> menu, and the <strong>Install Extension from VSIX Package</strong> item:</p>

<p><img src="/images/posts/inst-bdcrc1-inst-extension.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Install Extension</em></p>

<p>In <em>Figure 2</em> you see the <strong>File</strong> menu (outlined in blue), and the <strong>Install Extension from VSIX Package</strong> item outlined in red.</p>

<h4 id="azure-data-studio-notebooks">Azure Data Studio Notebooks</h4>

<p>I mentioned above that you deploy the BDC using ADS deployment <em>Notebooks</em>. You may ask yourself what an <strong>Azure Data Studio Notebook</strong> is? Well, Notebooks come from the Data Science world where a Notebook can contain live code, equations, visualizations and narrative text. It is a tool for teaching or sharing information between people. A notebook makes it easy to link lots of docs and code together.</p>

<p>When Microsoft developed ADS, the embedded the <a href="https://jupyter.org/">Jupyter</a> service in ADS, which enables ADS to run Notebooks. When you talk about Notebooks, you also talk about <em>Kernels</em>. A <em>Kernel</em> is the programming language you can write and execute code in, in the <em>Notebook</em>:</p>

<p><img src="/images/posts/inst-bdcrc1-ads-notebook-kernels.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Notebook Kernels</em></p>

<p>The drop-down you see in <em>Figure 3</em> shows the <em>Kernels</em> ADS supports. When you deploy, you use the <em>Python 3</em> kernel.</p>

<p>If you have not used Python Notebooks before in ADS, you need to configure Python for use with Notebooks. You enter <strong>Ctrl+Shift+P</strong> to open the command palette, and you search for <em>Configure Python</em>:</p>

<p><img src="/images/posts/inst-bdcrc1-ads-configure-notebooks.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Configure Notebooks</em></p>

<p>In <em>Figure 4</em> you see the command palette, and you choose <em>Configure Python for Notebooks</em>, and follow the instructions.</p>

<p>When you have configured Python for the notebooks, you are ready to deploy the BDC.</p>

<h2 id="deployment-settings">Deployment Settings</h2>

<p>The first step in the deployment is to configure settings which the notebook use.</p>

<p>After you launch ADS, you click on the <strong>Connections</strong> icon in the top of the <em>Activity</em> bar (leftmost panel in ADS). That opens the sidebar where you can see your connections. Click on the ellipsis, (the three dots &ldquo;&hellip;&rdquo;), to the right in the top panel of the sidebar:</p>

<p><img src="/images/posts/inst-bdcrc1-ads-deploy.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Connections</em></p>

<p>You see in <em>Figure 5</em>:</p>

<ul>
<li>Connections icon outlined in yellow.</li>
<li>The three dots in the connections panel outlined in blue.</li>
<li>The pop-up menu items when clicking on the three dots.</li>
</ul>

<p>You also see in <em>Figure 5</em> that the pop-up menu gives you choices for SQL Server deployments. To deploy a BDC, you click on the item outlined in red:</p>

<p><img src="/images/posts/inst-bdcrc1-ads-deploy-options1.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>Connections</em></p>

<p>When you click on the *Deploy SQL Server big data cluster you see something like in <em>Figure 6</em>: the <em>Select the deployment options</em> dialog. You see in the dialog what you can deploy, and what options you have.</p>

<p>You choose <em>SQL Server big data cluster</em>, (outlined in blue), the version <em>SQL Server 2019 RC big data cluster</em>, (outlined in yellow), and the target to deploy to: <em>New Azure Kubernetes Service cluster</em>. When you click <strong>Select</strong> in the dialog, you see a new dialog:</p>

<p><img src="/images/posts/inst-bdcrc1-ads-new-cluster.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>New AKS Cluster</em></p>

<p>You see in Figure 7* a settings dialog for the creation and deployment of your new cluster. All of the settings are relatively self-explanatory. However, there are two where you may not know how to retrieve them:</p>

<ul>
<li>Subscription id - you can have multiple subscriptions in Azure. This defines under which subscription to create the cluster. If you only have one subscription you leave this as is. If you have more subscriptions see below.</li>
<li>Region - in what Azure region your cluster should be created.</li>
</ul>

<h4 id="subscription-id">Subscription id</h4>

<p>You retrieve subscription information either by using the Azure portal or logging in via the Azure CLI. I prefer to log-in using Azure CLI:</p>

<pre><code class="language-bash">$ az login
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Login to Azure</em></p>

<p>When I execute the code in <em>Code Snippet 4</em> a tab opens in my browser, and I see a dialog that asks me to pick an account to log in to Azure with:</p>

<p><img src="/images/posts/sql_2k19_bdc_az_login.png" alt="" /></p>

<p><strong>Figure 8:</strong> <em>Azure Login</em></p>

<p>I choose the account from what I see in <em>Figure 8</em>, and after a little while, I see in the browser a success message:</p>

<p><img src="/images/posts/sql_2k19_bdc_az_login_success.png" alt="" /></p>

<p><strong>Figure 9:</strong> <em>Azure Login Success</em></p>

<p>At the same time as the success message in <em>Figure 9</em>, the code in <em>Code Snippet 4</em> returns with information what subscriptions I have access to in Azure:</p>

<p><img src="/images/posts/sql_2k19_bdc_az_login_return.png" alt="" /></p>

<p><strong>Figure 10:</strong> <em>Azure Login Return</em></p>

<p>As we see in <em>Figure 10</em>, I have access to multiple subscriptions, and I enter the id for the subscription I want to use in the <em>Subscription id</em> field.</p>

<h4 id="regions">Regions</h4>

<p>To see a list of Azure regions you execute <code>az account list-locations</code>:</p>

<p><img src="/images/posts/inst-bdcrc1-azure-regions.png" alt="" /></p>

<p><strong>Figure 11:</strong> <em>Azure Regions</em></p>

<p>When you execute the code above you get back a list of all regions you have access to and what you enter in the <em>Region</em> is the <code>name</code> property of your chosen region. In my case, I choose <code>southafricnorth</code>. When you have the necessary information, you set the various settings. For me the settings dialog looks like so:</p>

<p><img src="/images/posts/inst-bdcrc1-cluster-settings.png" alt="" /></p>

<p><strong>Figure 12:</strong> <em>Azure Regions</em></p>

<p>It is worth noting that I have changed the <em>VM size</em>, and <em>VM count</em> from its default of <code>Standard_E4s_v3</code>, and <code>5</code> to <code>Standard_B8ms</code> and <code>3</code>. Reason for this is that having fewer nodes cuts down on install time. The thing to bear in mind here is that a BDC deployment requires at a minimum around 24 hard disks altogether in your cluster, and each VM has a set number of disks. In my case, each <code>Standard_B8ms</code> VM has 16 disks so I should be good (3 * 16).</p>

<h2 id="deployment-notebook">Deployment Notebook</h2>

<p>With the settings set you now click <strong>Open Notebook</strong>:</p>

<p><img src="/images/posts/inst-bdcrc1-deploy-notbook1.png" alt="" /></p>

<p><strong>Figure 13:</strong> <em>Deploy Notebook</em></p>

<p>In <em>Figure 13</em> you see the opened deployment notebook. When you scroll through the notebook, you see the various stages of the deployment, and what it does in each stage:</p>

<ul>
<li>Check dependencies.</li>
<li>Required information.</li>
<li>Azure settings.</li>
<li>Default settings.</li>
<li>Login to Azure.</li>
<li>Set active Azure subscription.</li>
<li>Create Azure resource group.</li>
<li>Create AKS cluster.</li>
<li>Set the new AKS cluster as current context.</li>
<li>Create a deployment configuration file.</li>
<li>Create SQL Server 2019 big data cluster.</li>
<li>Login to SQL Server 2019 big data cluster.</li>
<li>Show SQL Server 2019 big data cluster endpoints.</li>
<li>Connect to master SQL Server instance in Azure Data Studio.</li>
</ul>

<p>To do the deployment, you can now either run each cell independently by clicking on the cell and hit F5 or click on the <strong>Run Cells</strong> command at the top of the notebook. In either case, you see what command the cell executes as well as the result:</p>

<p><img src="/images/posts/inst-bdcrc1-cell-output.png" alt="" /></p>

<p><strong>Figure 14:</strong> <em>Cell Output</em></p>

<p>What you see in <em>Figure 14</em> is the output from creating the Azure resource group.</p>

<p>Be aware that the deployment takes a while, and especially the stage <em>Create SQL Server 2019 big data cluster</em>. Eventually, the deployment finishes, and you get an output from the cell <em>Show SQL Server 2019 big data cluster endpoints</em>:</p>

<p><img src="/images/posts/inst-bdcrc1-endpoints.png" alt="" /></p>

<p><strong>Figure 15:</strong> <em>Cell Output</em></p>

<p>The BDC exposes external endpoints for various services, and those are the ones you see in <em>Figure 15</em>. It is beyond the scope of this post to discuss what all those endpoints are, but the one outlined in red is the endpoint for the SQL Server master instance.</p>

<p>To connect to the master instance, you create a new connection:</p>

<p><img src="/images/posts/inst-bdcrc1-ads-connect.png" alt="" /></p>

<p><strong>Figure 16:</strong> <em>New Connection</em></p>

<p>When you click the <em>New Connection</em> icon in the <em>Servers</em> panel in the sidebar that you see in <em>Figure 16</em>, a <em>Connection</em> dialog &ldquo;pops up&rdquo;:</p>

<p><img src="/images/posts/inst-bdcrc1-ads-new-connection.png" alt="" /></p>

<p><strong>Figure 17:</strong> <em>Connection Dialog</em></p>

<p>In the <em>Connection</em> dialog you enter the details for your connection, including the IP address for the SQL Server master instance, and then you click <strong>Connect</strong>:</p>

<p><img src="/images/posts/inst-bdcrc1-ads-connection.png" alt="" /></p>

<p><strong>Figure 18:</strong> <em>Connected</em></p>

<p>As you see in <em>Figure 18</em>, you are now connected.</p>

<h2 id="summary">Summary</h2>

<p>In this post you saw how you can deploy a *<em>SQL Server 2019 Big Data Cluster</em> using <strong>Azure Data Studio</strong> and notebooks.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 36, 2019]]></title>
    <link href="https://nielsberglund.com/2019/09/08/interesting-stuff---week-36-2019/" rel="alternate" type="text/html"/>
    <updated>2019-09-08T20:23:48+02:00</updated>
    <id>https://nielsberglund.com/2019/09/08/interesting-stuff---week-36-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/microservices-ddd-bounded-contexts">PracticalDDD: Bounded Contexts + Events =&gt; Microservices</a>. An <a href="https://www.infoq.com/">InfoQ</a> presentation discussing the intersection of DDD as a software discipline with Messaging as a technology counterpart. DDD allows us to move faster and write high-quality code. When we start to use the technology of messaging to communicate between clean and well-defined bounded contexts, we get to remove temporal coupling. We now have microservices that are built for autonomy from the ground up. A fascinating presentation!</li>
<li><a href="https://www.infoq.com/presentations/uber-microservices-distributed-tracing">Conquering Microservices Complexity @Uber with Distributed Tracing</a>. Another <a href="https://www.infoq.com/">InfoQ</a> presentation. The presenter presents a methodology that uses data mining to learn the typical behavior of the system from massive amounts of distributed traces, compares it with pathological behavior during outages, and uses complexity reduction and intuitive visualizations to guide the user towards actionable insights about the root cause of the outages. This is a very, very interesting presentation!</li>
<li><a href="https://blog.acolyer.org/2019/09/04/slog/">SLOG: serializable, low-latency, geo-replicated transactions</a>. In this white paper, dissection, <a href="https://twitter.com/adriancolyer">Adrial</a> looks at SLOG. SLOG is a system which leverages physical region locality in an application workload to achieve: strict serializability, low-latency writes, and high transactional throughput, while also supporting online consistent dynamic &ldquo;re-mastering&rdquo; of data as application patterns change over time.</li>
</ul>

<h2 id="vs-code">VS Code</h2>

<ul>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2019/09/03/visual-studio-code-develop-pyspark-jobs-for-sql-server-2019-big-data-clusters/">Visual Studio Code: Develop PySpark jobs for SQL Server 2019 Big Data Clusters</a>. The linked blog post announces a new VS Code extension. The extension allows you, amongst other things, to deploy PySpark applications to SQL Server 2019 Big Data Cluster. This is an extension anyone developing Python applications need to have a look at.</li>
</ul>

<h1 id="sql-saturday">SQL Saturday</h1>

<p>I have just come back from SQL Saturday in Johannesburg where I delivered a workshop about SQL Server 2019 Big Data Cluster with a colleague of mine; <a href="https://twitter.com/datawookie">Andrew Collier</a>. I also delivered two presentations:</p>

<ul>
<li><strong>What is the PiRate, Snake, and Cup of Coffee Doing in My Database?</strong>.</li>
<li><strong>A Lap Around SQL Server Big Data Cluster</strong>.</li>
</ul>

<p>Next week I do the big data cluster workshop with <a href="https://twitter.com/datawookie">Andrew</a>, <a href="https://www.quicket.co.za/events/80996-sqlsaturday-cape-town-2019-precon-with-neils-berglund-mvp-and-andrew-collier-sq/#/"><strong>A Day of SQL Server 2019 Big Data Cluster</strong></a>, on Friday (Sep., 13), and two presentations on Saturday (Sep., 14). I believe the presentation will be the same as in Johannesburg:</p>

<ul>
<li><strong>What is the PiRate, Snake, and Cup of Coffee Doing in My Database?</strong>. A talk about the SQL Server Extensibility Framework and how you can use R, Python and Java from inside SQL Server.</li>
<li><strong>A Lap Around SQL Server Big Data Cluster</strong>. This session gives you an overview of what a SQL Server Big Data Cluster is, and what you can do with it. We look at the various components, (Kubernetes, Spark, HDFS, PolyBase, etc.), and what you can do with them.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 35, 2019]]></title>
    <link href="https://nielsberglund.com/2019/09/01/interesting-stuff---week-35-2019/" rel="alternate" type="text/html"/>
    <updated>2019-09-01T19:40:31+02:00</updated>
    <id>https://nielsberglund.com/2019/09/01/interesting-stuff---week-35-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://medium.com/microsoft-open-source-stories/how-microsoft-rewrote-its-c-compiler-in-c-and-made-it-open-source-4ebed5646f98">How Microsoft rewrote its C# compiler in C# and made it open source</a>. TThis is a fascinating post by Mads Torgersen, where he discusses Roslyn, the open-source compiler for C# and VB.NET.</li>
</ul>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://buckwoody.wordpress.com/2019/08/26/big-data-is-just-data/">Big Data is just Data</a>. In this blog post <a href="https://twitter.com/BuckWoodyMSFT">Buck</a> discusses what happened to <strong>Big Data</strong>, did it die or what - as no one talks about it any more? The post gives one food for thoughts! Oh, and I am so excited to get to meet Buck at the SQL Saturday&rsquo;s in <a href="https://www.sqlsaturday.com/903/EventHome.aspx">Johannesburg</a> and <a href="https://www.sqlsaturday.com/897/EventHome.aspx">Cape Town</a> the next two weekends!</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/news/2019/08/octant-kubernetes-dashboard">Octant: Local and Real-Time Dashboard for Kubernetes Workloads</a>. This <a href="https://www.infoq.com/">InfoQ</a> article is about Octant, a tool to help developers understand how their applications are running in a Kubernetes cluster. Developers can graphically visualize Kubernetes objects dependencies, forward local ports to a running pod, inspect pod logs, and navigate through different clusters.</li>
<li><a href="https://www.infoq.com/articles/cellery-code-first-kubernetes">Cellery: A Code-First Approach to Deploy Applications on Kubernetes</a>. This another Kubernetes article from <a href="https://www.infoq.com/">InfoQ</a>. The article explains Cellery, which is a code-first approach to building, integrating, running, and managing composite applications on Kubernetes, using a cell-based architecture.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/kafka-graph-visualizations">Using Graph Processing for Kafka Stream Visualizations</a>. This is a very, very cool article discussing how we can match up Kafka with graph databases, (in this case Neo4J), to visualize our streaming data and gain insight from the data.</li>
</ul>

<h1 id="sql-saturday">SQL Saturday</h1>

<p>It is early September, and that means that SQL Saturday is just around the corner. In fact, the Johannesburg leg of SQL Saturday is next weekend. I am fortunate enough to present in both Johannesburg and Cape Town.</p>

<p>Me together with a colleague - [Andrew Collier][acoll] - delivers a one day workshop: <strong>A Day of SQL Server 2019 Big Data Cluster</strong>, where we drill into - you guessed it - SQL Server 2019 Big Data Cluster. We do that Friday, September 6 in Johannesburg, and Friday, September 13 in Cape Town.</p>

<p>If you are interested to understand what SQL Server 2019 Big Data Cluster is all about, please register for Johannesburg <a href="https://www.quicket.co.za/events/81482-a-day-of-sql-server-2019-big-data-cluster-with-neils-berglund-and-andrew-collier#/">here</a>, or Cape Town <a href="https://www.quicket.co.za/events/80996-sqlsaturday-cape-town-2019-precon-with-neils-berglund-mvp-and-andrew-collier-sq/#/">here</a>. It will be a blast - I promise!</p>

<p>In Johannesburg, I deliver on Saturday, (September 7), two talks:</p>

<ul>
<li><a href="https://www.sqlsaturday.com/903/Sessions/Details.aspx?sid=95909">What is the PiRate, Snake, and Cup of Coffee Doing in My Database?</a>. A talk about the SQL Server Extensibility Framework and how you can use R, Python and Java from inside SQL Server.</li>
<li><a href="https://www.sqlsaturday.com/903/Sessions/Details.aspx?sid=95908">A Lap Around SQL Server Big Data Cluster</a>. This session gives you an overview of what a SQL Server Big Data Cluster is, and what you can do with it. We look at the various components, (Kubernetes, Spark, HDFS, PolyBase, etc.), and what you can do with them.</li>
</ul>

<p>In Cape Town, Saturday, September 14, I also deliver two talks:</p>

<ul>
<li><a href="https://www.sqlsaturday.com/897/Sessions/Details.aspx?sid=95915">Set Your SQL Server Data Free with the Use of Kafka</a>. In this talk, we look at how we can stream data from SQL Server to the de facto standard for streaming: Apache Kafka. We look at tools like Kafka Connect and external languages, and after the session, we should have a good understanding of various ways we can &ldquo;set the data free&rdquo;.</li>
<li><a href="https://www.sqlsaturday.com/897/Sessions/Details.aspx?sid=95916">SQL Server Machine Learning Services &amp; External R/Python Packages</a>. SQL Server Machine Learning Services supports both R and Python, and they come with all the regular packages installed. However, there are a lot more packages available &ldquo;out in the wild&rdquo;. In this session, you learn various ways you can install both R and Python packages in SQL Server instances.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 34, 2019]]></title>
    <link href="https://nielsberglund.com/2019/08/25/interesting-stuff---week-34-2019/" rel="alternate" type="text/html"/>
    <updated>2019-08-25T19:51:11+02:00</updated>
    <id>https://nielsberglund.com/2019/08/25/interesting-stuff---week-34-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="misc">Misc.</h2>

<ul>
<li><a href="https://medium.com/better-programming/what-to-do-when-youve-inherited-dysfunctional-code-e09822656b3a">What to Do When You&rsquo;ve Inherited Dysfunctional Code</a>. This is a very interesting post about what you can do when you have inherited &ldquo;crap&rdquo; code. Code that makes no logical sense and is coded in a way that resembles a drunk and disoriented spider trying to build a web.</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/state-serverless-computing">The State of Serverless Computing</a>. This is an <a href="https://www.infoq.com/">InfoQ</a> presentation where the presenter briefly discusses both the benefits and shortcomings of existing serverless offerings. He then projects forward to the future and highlights challenges that must be overcome to realize truly general-purpose serverless computing, as well as our efforts to get there.</li>
<li><a href="https://www.infoq.com/presentations/multi-tenancy-kubernetes">Multi-Tenancy in Kubernetes</a>. This is another <a href="https://www.infoq.com/">InfoQ</a> presentation. In this presentation the presenter discusses both the mechanics and the implications of cluster sharing on cost, isolation, and operational efficiency, including use cases, even challenging ones.</li>
</ul>

<h2 id="sql-server-2019">SQL Server 2019</h2>

<ul>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2019/08/21/sql-server-2019-release-candidate-is-now-available/">SQL Server 2019 release candidate is now available</a>. Boys and girls, start your engines - we are getting closer. Closer to the release of SQL Server 2019! This post announces, as the title implies, that RC1 of SQL Server 2019 has been released, go and get it! Worth noting, however, is that SQL Server 2019 Big Data Cluster has not reached RC1 stage yet. RC1 for SQL Server 2019 Big Data Cluster will come a little bit later.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/transactional-systems-with-apache-kafka">Building Transactional Systems Using Apache Kafka</a>. This article presents an event-based architecture that retains most transactional properties as provided by an RDBMS while leveraging Apache Kafka® as a scalable and highly available single source of truth.</li>
</ul>

<h2 id="sql-saturday">SQL Saturday</h2>

<p>It is late August, and I am in a state of panic! Why, well - the conference season is upon us, and I have months, and months ago, submitted some talks and workshops - and now it is time to get those talks and workshops ready. In other words, I am burning the midnight oil.</p>

<p>Me together with a colleague - <a href="https://twiter.com/datawookie">Andrew Collier</a> - delivers a one day workshop: <strong>A Day of SQL Server 2019 Big Data Cluster</strong>, where we drill into - you guessed it - SQL Server 2019 Big Data Cluster. We do that Friday, September 6 in Johannesburg, and Friday, September 13 in Cape Town.</p>

<p>If you are interested to understand what SQL Server 2019 Big Data Cluster is all about, please register for Johannesburg <a href="https://www.quicket.co.za/events/81482-a-day-of-sql-server-2019-big-data-cluster-with-neils-berglund-and-andrew-collier#/">here</a>, or Cape Town <a href="https://www.quicket.co.za/events/80996-sqlsaturday-cape-town-2019-precon-with-neils-berglund-mvp-and-andrew-collier-sq/#/">here</a>. It will be a blast - I promise!</p>

<p>In Johannesburg, I deliver on Saturday, (September 7), two talks:</p>

<ul>
<li><a href="https://www.sqlsaturday.com/903/Sessions/Details.aspx?sid=95909">What is the PiRate, Snake, and Cup of Coffee Doing in My Database?</a>. A talk about the SQL Server Extensibility Framework and how you can use R, Python and Java from inside SQL Server.</li>
<li><a href="https://www.sqlsaturday.com/903/Sessions/Details.aspx?sid=95908">A Lap Around SQL Server Big Data Cluster</a>. This session gives you an overview of what a SQL Server Big Data Cluster is, and what you can do with it. We look at the various components, (Kubernetes, Spark, HDFS, PolyBase, etc.), and what you can do with them.</li>
</ul>

<p>For Cape Town, the Saturday schedule is not ready yet, but I have no doubt that I will deliver one or two talks.</p>

<p>I hope to see you there!</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 33, 2019]]></title>
    <link href="https://nielsberglund.com/2019/08/18/interesting-stuff---week-33-2019/" rel="alternate" type="text/html"/>
    <updated>2019-08-18T16:08:06+02:00</updated>
    <id>https://nielsberglund.com/2019/08/18/interesting-stuff---week-33-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://devblogs.microsoft.com/dotnet/announcing-net-core-3-0-preview-8/">Announcing .NET Core 3.0 Preview 8</a>. We are getting close to an official release of .NET Core 3.0. As you can see from the title of the linked post, Microsoft just released Preview 8 of .NET Core 3.0.</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/articles/kubernetes-workloads-serverless-era/">Kubernetes Workloads in the Serverless Era: Architecture, Platforms, and Trends</a>. An <a href="https://www.infoq.com/">InfoQ</a> article about how microservices architecture has evolved into cloud-native architecture, where many of the infrastructure concerns are provided by Kubernetes in combination with additional abstractions provided by service mesh and serverless frameworks.</li>
<li><a href="https://www.infoq.com/presentations/monolith-observable-microservices-ddd">From Monolith to Observable Microservices using DDD</a>. This is an <a href="https://www.infoq.com/">InfoQ</a> presentation about how to move from a monolith to microservices applying Domain-Driven Design principles. Additionally, the presenter explains how to better maintain microservices in production by making them observable.</li>
<li><a href="https://www.infoq.com/articles/twelve-testing-techniques-microservices-intro">Testing Microservices: Overview of 12 Useful Techniques - Part 1</a>. Yet another <a href="https://www.infoq.com/">InfoQ</a> post. This is the first in a series about how to test microservices. An excellent post!</li>
</ul>

<h2 id="machine-learning-data-science">Machine Learning / Data Science</h2>

<ul>
<li><a href="https://medium.com/better-programming/fundamentals-of-time-series-data-and-forecasting-15e9490b2618">Fundamentals of Time Series Data and Forecasting</a>. This article explores the fundamentals of time series data. It talks about how very simple forecasting methods work. Plus, the article describes the most common patterns found in time-series data. I found this article very informative!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/kafka-connect-improvements-in-apache-kafka-2-3">Kafka Connect Improvements in Apache Kafka 2.3</a>. Apache Kafka 2.3 was released recently together with Confluent Platform 5.3. The release has some improvements of Kafka Connect. In this blog post, <a href="https://twitter.com/rmoff">Robin Moffat</a>, covers the - in his mind - most exciting improvements.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
</feed>

