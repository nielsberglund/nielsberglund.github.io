<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Niels Berglund</title>
  <link href="https://nielsberglund.com/atom.xml" rel="self"/>
  <link href="https://nielsberglund.com"/>
  <updated>2022-04-10T12:49:21+01:00</updated>
  <id>https://nielsberglund.com/</id>
  <generator uri="http://gohugo.io/">Hugo</generator>

  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 14, 2022]]></title>
    <link href="https://nielsberglund.com/2022/04/10/interesting-stuff---week-14-2022/" rel="alternate" type="text/html"/>
    <updated>2022-04-10T12:49:21+01:00</updated>
    <id>https://nielsberglund.com/2022/04/10/interesting-stuff---week-14-2022/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="machine-learning">Machine Learning</h2>

<ul>
<li><a href="https://towardsdatascience.com/6-time-series-predictive-tasks-you-should-know-about-b899fb83b6bf">6 Time Series Predictive Tasks You Should Know About</a>. Recently I have gotten interested in Time Series analysis because of the Time Series capabilities in <strong>Azure Data Explorer</strong> and <strong>KQL</strong> (Kusto Query Language). Therefore this post comes at an opportune time as it describes predictive tasks that can be put together with time-series data sets.</li>
</ul>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://databricks.com/blog/2022/04/05/announcing-generally-availability-of-databricks-delta-live-tables-dlt.html">Announcing General Availability of Databricks&rsquo; Delta Live Tables (DLT)</a>. This post talks about the launch of Databricks&rsquo; Delta Live Tables and how it simplifies streaming and batch ETL for data, analytics and AI applications.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/real-time-betting-platform-with-confluent-cloud-and-ably/">Building a Dependable Real-Time Betting App with Confluent Cloud and Ably</a>. As you probably know by now, I work for <a href="/derivco"><strong>Derivco</strong></a>, and at Derivco, we develop software for online gaming. That&rsquo;s why this post is so interesting, as it looks at how to build a dependable real-time betting experience by combining the power of Confluent Cloud with the capabilities of the <a href="https://ably.com/">Ably</a> edge messaging platform.</li>
<li><a href="https://www.confluent.io/blog/real-time-data-integration-analytics-confluent-azure-synapse-connector/">Migrating Data to Azure Synapse with Confluent&rsquo;s Fully Managed Connector to Unlock Real-Time Advanced Analytics</a>. This is another really interesting post for us at Derivco, as we are underway to implement an enterprise data lake using Azure. As part of the data lake, we are looking at Azure Synapse. So, reading this post about how Confluent integrates with Azure Synapse and enables real-time data processing and advanced analytics in Azure is very cool!</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 13, 2022]]></title>
    <link href="https://nielsberglund.com/2022/04/03/interesting-stuff---week-13-2022/" rel="alternate" type="text/html"/>
    <updated>2022-04-03T09:05:36+02:00</updated>
    <id>https://nielsberglund.com/2022/04/03/interesting-stuff---week-13-2022/</id>
    <content type="html"><![CDATA[<p><img src="/images/posts/mbp-box.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>The Devil Wears <del>Prada</del> Skates</em></p>

<p>What you see in <em>Figure 1</em> is being explained further down!</p>

<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that have to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="data">Data</h2>

<ul>
<li><a href="https://towardsdatascience.com/data-engineer-tools-c7e68eed28ad">Tools For Data Engineers</a>. Data Engineers are among the most valued people in modern data-driven organisations that aim to build or enhance products by utilising their most valuable asset: data! To make this happen, Data Engineers must ensure they are using the right tools. This blog post looks at some of the tools a Data Engineer should be familiar with.</li>
<li><a href="https://www.theseattledataguy.com/data-platforms-vs-data-warehouses-vs-data-virtualization-and-federated-sql-engine-how-to-store-your-data/?utm_source=pocket_mylist#page-content">Data Platforms Vs Data Warehouses Vs Data Virtualization And Federated SQL Engine â€“ How To Store Your Data</a>. There is a lot of discussion on how an organization should implement its data stacks today. This article discusses some of the popular routes data solution providers are talking about in terms of types of data layers.</li>
</ul>

<h2 id="azure-data-explorer">Azure Data Explorer</h2>

<ul>
<li><a href="https://techcommunity.microsoft.com/t5/azure-data-explorer-blog/end-to-end-azure-data-explorer-pipeline-deployment-with-arm/ba-p/3248717">End to end Azure Data Explorer pipeline deployment with ARM</a>. You, who follow my blog, know that I am doing quite a lot of Azure Data Explorer &ldquo;stuff&rdquo; now. Setting up and tearing down ADX clusters is getting tedious, so the post linked to brings some good news: The ability to use ARM templates and BICEP scripts for deployment! Expect a blog post shortly about this!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/introducing-stream-processing-use-case-recipes-for-every-industry/">Introducing Stream Processing Use Case Recipes Powered by ksqlDB</a>. ksqlDB is the &ldquo;bee&rsquo;s knees&rdquo; for real-time stream processing. However, it can be tricky to get started, and there are an infinite number of use cases. That&rsquo;s what is being announced in this blog post aims to fix. The post introduces ksqlDB &ldquo;recipes&rdquo; consisting of pre-built code samples and a step-by-step tutorial to tackle the most popular, high-impact use cases for stream processing. Awesome!!</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>So, a couple of weeks ago, I was at the SQLBits 2022 conference, where I gave some talks. This was the first &ldquo;live&rdquo; conference since the end of January 2020 (when I did MS Ignite, The World Tour). Since then, doing conference talks from home has been the norm, and I have been using my fully &ldquo;tricked out&rdquo; development PC for the talks.</p>

<p>When going to &ldquo;live&rdquo; conferences, I used my private laptop. A couple of days before leaving for London, I decided to ensure that I had all the necessary &ldquo;stuff&rdquo; on the laptop. Imagine my shock when I realized that the laptop no longer worked; I could not get it to boot! What to do? Fortunately, I have a work laptop, so I quickly copied the necessary slide decks, code, etc., and set off to London. Unfortunately, as it is a work laptop, there is some security &ldquo;stuff&rdquo; on the laptop, which meant that some of the things I was going to do didn&rsquo;t work as expected.</p>

<p>Coming home from London, I realized that I had to get a new laptop since - hopefully - we will now do in-person conferences again. The question was then, what laptop to get. Remember, I am a Windows &ldquo;dude&rdquo;. You probably now know what I got, especially seeing the picture in <em>Figure 1</em>. Yes, I got myself a MacBook Pro M1 (14 inches, 16Gb RAM):</p>

<p><img src="/images/posts/mbp-unboxed-full.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Ready for Setup</em></p>

<p>One week later, I have got a bit further than the setup in <em>Figure 2</em>. In fact, I am writing this post using my MBP. Expect a post or two about my experiences with the MBP in the coming weeks.</p>

<p>Oh, if you wonder about the <em>The Devil Wears Skates</em> title in <em>Figure 1</em>. As a hardcore Windows guy, I have been overheard saying that I will switch to Mac when &ldquo;hell freezes over&rdquo;. I guess hell must have frozen over now, and the Devil moves along on skates.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 12, 2022]]></title>
    <link href="https://nielsberglund.com/2022/03/27/interesting-stuff---week-12-2022/" rel="alternate" type="text/html"/>
    <updated>2022-03-27T08:32:00+02:00</updated>
    <id>https://nielsberglund.com/2022/03/27/interesting-stuff---week-12-2022/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="azure-data-explorer">Azure Data Explorer</h2>

<ul>
<li><a href="https://techcommunity.microsoft.com/t5/azure-data-explorer-blog/preview-configure-adx-table-retention-caching-and-batching/ba-p/3262352">(Preview) Configure ADX table retention, caching and batching policies via one-click</a>. This post looks at something I just came across the other day when I edited the lab exercises for my one day Azure Data Explorer training class <em>A Day of Azure Data Explorer</em>. So what the post covers and I found out was how you can configure retention, batching &amp; caching policies for tables &amp; materialized views via one click. Quite cool!</li>
</ul>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://sqlofthenorth.blog/2022/03/10/building-the-lakehouse-architecture-with-synapse-analytics/">BUILDING THE LAKEHOUSE ARCHITECTURE WITH AZURE SYNAPSE ANALYTICS</a>. As the title says, this post looks at using the full range of Synapse services to build a lake house architecture on top of Azure Databricks Delta Lake tables.</li>
<li><a href="https://www.infoq.com/news/2022/03/azure-purview-workflows-preview/">Orchestrate Operations, Validations, and Approvals on Data Entities with Azure Purview Workflows</a>. Azure Purview is a data governance service from Microsoft that automates the discovery and cataloging of data. It went into General Availability (GA) in October 2021. Recently Microsoft released as a preview Azure Purview Workflows. This article looks into the Purview Workflows and what you can do with them.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://medium.com/event-driven-utopia/build-a-real-time-data-analytics-pipeline-with-airbyte-kafka-and-pinot-c9ff3c42dcf2">Build a real-time data analytics pipeline with Airbyte, Kafka, and Pinot</a>. This post looks at using Airbyte, Kafka and Pinot for building analytics dash boards for user-facing analysis. Very interesting!</li>
<li><a href="https://www.confluent.io/blog/why-replace-zookeeper-with-kafka-raft-the-log-of-all-logs/">Why ZooKeeper Was Replaced with KRaft â€“ The Log of All Logs</a>. Anyone interested in Kafka probably now knows how ZooKeeper is being replaced by a Raft based quorum protocol. This post explores the rationale behind the replacement, examines why Raft was utilized and altered to become KRaft and describes the new Quorum Controller built on top of KRaft protocols.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 11, 2022]]></title>
    <link href="https://nielsberglund.com/2022/03/20/interesting-stuff---week-11-2022/" rel="alternate" type="text/html"/>
    <updated>2022-03-20T08:30:37+02:00</updated>
    <id>https://nielsberglund.com/2022/03/20/interesting-stuff---week-11-2022/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending. This week, there hasn&rsquo;t been that much that has caught my eye, but here it is.</p>

<p></p>

<h2 id="sql-server">SQL Server</h2>

<ul>
<li><a href="https://techcommunity.microsoft.com/t5/sql-server-blog/configure-msdtc-to-run-distributed-transactions-for-sql-server/ba-p/3259913">Configure MSDTC to run Distributed transactions for SQL Server Linux Containers on Azure Kubernetes</a>. This blog post does exactly what it says. It discusses setting up and configuring SQL Server and MS DTC to run in containers on the Kubernetes platform. I have been working with SQL Server since version 4.x (I believe it was 4.2), and a couple of years back, I would never have thought we&rsquo;d see SQL running on Linux, never mind on Kubernetes! As Bob Dylan sang, <a href="https://youtu.be/90WD_ats6eE">&ldquo;The Times They Are A-Changin&rsquo;&rdquo;</a>.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/announcing-ksqldb-0-24-0/">Announcing ksqlDB 0.24.0</a>. Another post doing what the title says. This post looks at some of the new features in ksqlDB 0.24.0. The post also shows some examples of how to use them. I am quite interested in the ability to access record data headers from within ksqlDB. Very cool! Another interesting feature is the new functions aiding in processing JSON-formatted strings.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 10, 2022]]></title>
    <link href="https://nielsberglund.com/2022/03/12/interesting-stuff---week-10-2022/" rel="alternate" type="text/html"/>
    <updated>2022-03-12T08:10:36+02:00</updated>
    <id>https://nielsberglund.com/2022/03/12/interesting-stuff---week-10-2022/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="azure-data-explorer">Azure Data Explorer</h2>

<ul>
<li><a href="https://youtu.be/eqh6a4kR4oc">Azure MeetUp Frankfurt - Azure Data Explorer</a>. This link is to a video where <a href="https://twitter.com/cosh23">Henning Rauch</a> (PM in the Azure Data Explorer team) introduces <strong>Azure Data Explorer</strong> at <em>Azure Meetup Frankfurt</em>.</li>
<li>[Just enough Azure Data Explorer for data enthusiasts][3]. This LinkedIn post is an excellent overview of Azure Data Explorer. There is, however, one big flaw in the post - it should have been published a week earlier. Then I would have been able to use some of it in my Azure Data Explorer talks at SQLBits 2022. Just kidding - excellent post.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://opendatascience.com/introduction-to-kafka-stream-processing-in-python/">Introduction to Kafka Stream Processing in Python</a>. This post looks at building streaming Kafka applications in Python using <a href="https://github.com/robinhood/faust">Faust</a>. Faust is a stream processing library for Python, similar to KStreams. Looks very interesting!</li>
<li><a href="https://www.confluent.io/blog/how-to-make-apache-kafka-even-faster/">How to Make Apache Kafka Clients Go Fast(er) on Confluent Cloud</a>. You want to get the best performance possible from your applications running on Confluent Cloud. There is quite a lot of information about this &ldquo;out there&rdquo;, but what should you read? This blog post highlights the top five reading recommendations for this. They cover the key concepts and provide concrete examples of how we do it and how you can do it too, with specific benchmark testing and configuration guidelines.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>For those of you who read my posts, you are probably aware that I have, the last month, been in &ldquo;prep&rdquo; mode for the <strong>SQLBits</strong> conference in London:</p>

<p><img src="/images/posts/sqlbits2022-1.jpg" alt="" /></p>

<p><strong>Figure 1:</strong> <em>SQLBits 2022</em></p>

<p>I managed to be ready in time, and the picture above is a picture I took right before starting my full day training class on <strong>Azure Data Explorer</strong>. The notebook with <em>IT&rsquo;S POSSIBLE</em> on it, is part of the Swag I received from <a href="/derivco"><strong>Derivco</strong></a> to hand out to the delegates. Thank You, <strong>Derivco</strong>!</p>

<p>Today I am flying back to SA from London, and I think my &ldquo;prep&rdquo; paid off as the attendees seemed to like what I presented.</p>

<p>I want to give a HUGE &ldquo;shout-out&rdquo; to the organizers of <strong>SQLBits</strong>. They did a fantastic job!!! Thank You!</p>

<p>Now it is back to the &ldquo;grindstone&rdquo;, trying to develop new materials for the next conferences - whatever/wherever they are!</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 9, 2022]]></title>
    <link href="https://nielsberglund.com/2022/03/05/interesting-stuff---week-9-2022/" rel="alternate" type="text/html"/>
    <updated>2022-03-05T07:57:02+02:00</updated>
    <id>https://nielsberglund.com/2022/03/05/interesting-stuff---week-9-2022/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://www.jamesserra.com/archive/2022/03/azure-synapse-and-delta-lake/">Azure Synapse and Delta Lake</a>. If you are using Azure for Big Data, the chances are high that you also use Azure Databrick&rsquo;s <strong>Delta Lake</strong> format. You may also use <strong>Azure Synapse</strong> for analysis, etc. This post looks at the support and interaction between Azure Synapse and Delta Lake.</li>
</ul>

<h2 id="azure-data-explorer">Azure Data Explorer</h2>

<ul>
<li><a href="https://azurecloudai.blog/2022/02/28/create-and-maintain-your-own-kql-demo-environment-with-the-new-start-for-free-cluster/">Create and Maintain Your Own KQL Demo Environment with the New Start-for-free Cluster</a>. <strong>Azure Data Explorer</strong> is the &ldquo;best thing since sliced bread&rdquo;, together with <strong>Kusto Query Language</strong> (KQL). One slight problem with learning ADX and KQL is that you&rsquo;s need to spin up an ADX cluster if you want to ingest data etc. Sure, there are help clusters around with data, but those are read-only. So, to make it easier to &ldquo;play around&rdquo; with ADX and KQL, Microsoft has just released a Start-for-free Cluster program. You can read all about it in the post linked.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://engineering.linkedin.com/blog/2022/near-real-time-features-for-near-real-time-personalization">Near real-time features for near real-time personalization</a>. This is an exciting post by the LinkedIn engineering team. In the post, they look at LinkedIn&rsquo;s personalization recommendation journey, from pretty much batch-oriented to doing this in near real-time. Obviously Kafka plays a big part!</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>This:</p>

<p><img src="/images/posts/London_calling_cover.jpg" alt="" />
<strong>Figure 1:</strong> <em>London Calling</em></p>

<p>Yay, finally! Later today I am flying out to London for <strong>SQLBits 2022</strong>, where I deliver the following:</p>

<ul>
<li><a href="https://arcade.sqlbits.com/session-details/?id=300019"><strong>A Day of Azure Data Explorer</strong></a>. Full day training class.</li>
<li><a href="https://arcade.sqlbits.com/session-details/?id=301807"><strong>ksqlDB - The Real-Time Streaming Database</strong></a>. Conference session.</li>
<li><a href="https://arcade.sqlbits.com/session-details/?id=301808"><strong>Analyze Billions of Rows of Data in Real-Time Using Azure Data Explorer</strong></a>. Conference session.</li>
</ul>

<p>There are still seats available for my training class and the conference itself. You register <a href="https://register.sqlbits.com/event/36db4730-583c-49c1-adcf-f76432bb6580/summary">here</a>!</p>

<p>A HUGE shout-out to my employer <a href="/derivco"><strong>Derivco</strong></a>, who helps with some of my costs for the trip. They also sponsor some <a href="https://www.vocabulary.com/dictionary/swag"><strong>SWAG</strong></a> - another reason to come to my sessions.</p>

<p><strong>THANK YOU, Derivco</strong></p>

<p>So, if you are at <a href="https://sqlbits.com/"><strong>SQLBits</strong></a>, please come by and say Hi!</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 8, 2022]]></title>
    <link href="https://nielsberglund.com/2022/02/27/interesting-stuff---week-8-2022/" rel="alternate" type="text/html"/>
    <updated>2022-02-27T09:10:43+02:00</updated>
    <id>https://nielsberglund.com/2022/02/27/interesting-stuff---week-8-2022/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://databricks.com/blog/2022/02/23/get-to-know-your-queries-with-the-new-databricks-sql-query-profile.html">Get to Know Your Queries With the New Databricks SQL Query Profile!</a>. This blog post is part of a series on Databricks SQL that covers critical capabilities across performance, ease of use, and governance. This post takes a closer look at the new Databricks SQL Query Profile feature and how it helps data teams speed up and optimize their queries.</li>
</ul>

<h2 id="ai">AI</h2>

<ul>
<li><a href="https://www.zdnet.com/article/microsoft-goes-public-with-details-on-its-singularity-ai-infrastructure-service/">Microsoft goes public with details on its &lsquo;Singularity&rsquo; AI infrastructure service</a>. A week or so ago, it was revealed how Microsoft&rsquo;s Azure and Research teams are working together to build a new AI infrastructure service, codenamed &ldquo;Singularity&rdquo;. The post linked to looks at what this could be and looks back at previous projects with the same codename. Interesting!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/real-time-systems-what-you-need-to-know/">Building Real-Time Data Systems the Hard Way</a>. This post compares the effort of building an event-driven system based on a relational database with building it using Kafka. Since the post is from Confluent, it is obvious what is the better choice. Anyway, it is an interesting post in that it covers real-world problems we run into when wanting to develop event-driven/real-time applications.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>Nothing has changed from last week, so: prepping, prepping, prepping:</p>

<p><img src="/images/posts/sqlbits-sessions.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Sessions at SQLBits 2022</em></p>

<p><strong>SQLBits 2022</strong> is very close now, and I am nowhere near ready to present the sessions you see in <em>Figure 1</em>, &lt;sigh&gt;! However, somehow everything will be OK! Details of the sessions:</p>

<ul>
<li><a href="https://arcade.sqlbits.com/session-details/?id=300019"><strong>A Day of Azure Data Explorer</strong></a>. Full day training class.</li>
<li><a href="https://arcade.sqlbits.com/session-details/?id=301807"><strong>ksqlDB - The Real-Time Streaming Database</strong></a>. Conference session.</li>
<li><a href="https://arcade.sqlbits.com/session-details/?id=301808"><strong>Analyze Billions of Rows of Data in Real-Time Using Azure Data Explorer</strong></a>. Conference session.</li>
</ul>

<p>There are still seats available for my training class as well as the conference. You register <a href="https://register.sqlbits.com/event/36db4730-583c-49c1-adcf-f76432bb6580/summary">here</a>!</p>

<p>See you in London in a bit over one week!</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 7, 2022]]></title>
    <link href="https://nielsberglund.com/2022/02/20/interesting-stuff---week-7-2022/" rel="alternate" type="text/html"/>
    <updated>2022-02-20T13:26:02+02:00</updated>
    <id>https://nielsberglund.com/2022/02/20/interesting-stuff---week-7-2022/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="misc">Misc.</h2>

<ul>
<li><a href="https://netflixtechblog.com/rapid-event-notification-system-at-netflix-6deb1d2b57d1">Rapid Event Notification System at Netflix</a>. This post from the <a href="https://netflixtechblog.com/">Neflix tech blog</a> looks at the <strong>Rapid Event Notification System</strong> (RENO), which Netflix developed to support use cases that require server-initiated communication with devices in a scalable and extensible manner. The post gives an overview of the system and shares &ldquo;lessons learned&rdquo;.</li>
</ul>

<h2 id="data-architecture">Data Architecture</h2>

<ul>
<li><p><a href="https://databricks.com/blog/2022/02/15/how-gemini-built-a-cryptocurrency-analytics-platform-using-lakehouse-for-financial-services.html">How Gemini Built a Cryptocurrency Analytics Platform Using Lakehouse for Financial Services</a>. This post is about crypto market leader Gemini and how they partnered with Databricks to build an order book analytics platform using the Lakehouse architecture. Very interesting post! I have some &ldquo;cool&rdquo; takeaways from the post!</p></li>

<li><p><a href="https://www.decipherzone.com/blog-detail/druid-vs-pinot">Apache Druid vs Apache Pinot - Which One Should You Choose?</a>. Apache Druid and Apache Pinot are two open-source OLAP datastore popular for real-time analytics. This post discusses the similarities and differences between the two, along with the use cases to help you understand which scenario favors Druid more and vice versa.</p></li>
</ul>

<h2 id="azure-data-explorer">Azure Data Explorer</h2>

<ul>
<li><a href="https://techcommunity.microsoft.com/t5/azure-data-explorer-blog/deleting-individual-records-in-a-table/ba-p/3166847">Deleting individual records in a table</a>. This post does what &ldquo;it says on the tin&rdquo;: it looks at how to delete individual records from a table in an <strong>Azure Data Explorer</strong> cluster. Very interesting!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/bring-your-own-monitoring-with-confluent-cloud/">Bringing Your Own Monitoring (BYOM) with Confluent Cloud</a>. Confluent Cloud provides a Metrics API to see performance data for a cluster. This means you can configure an application performance monitoring (APM) product with a Confluent Cloud cluster to monitor the telemetry flowing through the cluster. This blog post uses the Metrics API, Docker, Prometheus, Grafana, Splunk, and Datadog to create a complete monitoring solution for a Confluent Cloud deployment.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing )</h2>

<p>Prepping, prepping, prepping:</p>

<p><img src="/images/posts/sqlbits-sessions.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Sessions at SQLBits 2022</em></p>

<p><strong>SQLBits 2022</strong> is very close now, and I am nowhere near ready to present the sessions you see in <em>Figure 1</em>, &lt;sigh&gt;! However, somehow all will be OK! Details of the sessions:</p>

<ul>
<li><a href="https://arcade.sqlbits.com/session-details/?id=300019"><strong>A Day of Azure Data Explorer</strong></a>. Full day training class.</li>
<li><a href="https://arcade.sqlbits.com/session-details/?id=301807"><strong>ksqlDB - The Real-Time Streaming Database</strong></a>. Conference session.</li>
<li><a href="https://arcade.sqlbits.com/session-details/?id=301808"><strong>Analyze Billions of Rows of Data in Real-Time Using Azure Data Explorer</strong></a>. Conference session.</li>
</ul>

<p>There are still seats available for my training class as well as the conference. You register <a href="https://register.sqlbits.com/event/36db4730-583c-49c1-adcf-f76432bb6580/summary">here</a>!</p>

<p>See you in London in two weeks time!</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 6, 2022]]></title>
    <link href="https://nielsberglund.com/2022/02/13/interesting-stuff---week-6-2022/" rel="alternate" type="text/html"/>
    <updated>2022-02-13T17:27:47+02:00</updated>
    <id>https://nielsberglund.com/2022/02/13/interesting-stuff---week-6-2022/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://databricks.com/blog/2022/02/10/databricks-delta-live-tables-announces-support-for-simplified-change-data-capture.html">Databricks Delta Live Tables Announces Support for Simplified Change Data Capture</a>. I don&rsquo;t know what category this post falls under, but Big Data sounds OK:ish, so that&rsquo;s where it ends up. Anyway, the post looks at <a href="https://databricks.com/product/delta-live-tables">Delta Live</a> table&rsquo;s support for CDC. Cool stuff!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://databricks.com/blog/2022/02/07/structured-streaming-a-year-in-review.html">Structured Streaming: A Year in Review</a>. This post gives an overview of all the new Structured Streaming features developed in 2021 for Databricks and Apache Spark. As you see, it is quite a lot!</li>
<li><a href="https://www.confluent.io/blog/intro-to-ksqldb-sql-database-streaming/">Introducing ksqlDB</a>. The post linked to is from back in November 2019, and I must have missed it then. I came across it as I was &ldquo;researching&rdquo; ksqlDB (read browsing). The post introduces pull queries and Kafka Connect connector management from inside ksqlDB. As cool as that is, that was not what caught my eye. No, what I found interesting was the section around ksqlDB&rsquo;s internal architecture, very cool!</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 5, 2022]]></title>
    <link href="https://nielsberglund.com/2022/02/06/interesting-stuff---week-5-2022/" rel="alternate" type="text/html"/>
    <updated>2022-02-06T12:51:07+02:00</updated>
    <id>https://nielsberglund.com/2022/02/06/interesting-stuff---week-5-2022/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="cloud">Cloud</h2>

<ul>
<li><a href="https://www.tbray.org/ongoing/When/202x/2022/01/30/Cloud-Lock-In">Lock-in and Multi-Cloud</a>. This is an excellent post by <a href="https://en.wikipedia.org/wiki/Tim_Bray">Tim Bray</a>, where he looks at various options for &ldquo;going to the cloud&rdquo;. As the post title implies, he also looks at the perception of lock-in and the fear thereof. As I mentioned in the beginning, this is an excellent post. Thanks to this post, I also got to an old post (from 2003) of his: <a href="https://www.tbray.org/ongoing/When/200x/2003/03/10/GigaTeraPeta"><em>Half a Billion Bibles</em></a>, where he puts data sizes in perspective.</li>
</ul>

<h2 id="data-architecture">Data Architecture</h2>

<ul>
<li><a href="https://databricks.com/blog/2022/01/31/make-your-data-lakehouse-run-faster-with-delta-lake-1-1.html">Make Your Data Lakehouse Run, Faster With Delta Lake 1.1</a>. The 1.1 release of Databricks&rsquo; Delta Lake has some significant performance improvements. This post goes over the major changes and notable features in this release. There is some very cool stuff in there!</li>
<li><a href="https://towardsdatascience.com/data-mesh-pattern-deep-dive-event-streaming-backbone-99a5bb2a7cbf">Data Mesh Patterns: Event Streaming Backbone</a>. This post is the second in a series of articles on Foundational Data Mesh Patterns, and it discusses the <em>Event Streaming Backbone</em> pattern. The post is very interesting, and if you are interested in Data Mesh and/or Event Driven architectures you should read it.</li>
</ul>

<h2 id="azure-data-explorer">Azure Data Explorer</h2>

<ul>
<li><a href="https://techcommunity.microsoft.com/t5/azure-data-explorer-blog/kibana-dashboards-and-visualizations-on-top-of-azure-data/ba-p/3062838">Kibana dashboards and visualizations on top of Azure Data Explorer are now supported with K2Bridge</a>. If you are a Kibana user, this post is for you! It discusses how you can now easily migrate to Azure Data Explorer (ADX) while keeping Kibana as your visualization tool, alongside the other Azure Data Explorer experiences and the powerful KQL language.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/streaming-etl-sfdc-data-for-real-time-customer-analytics/">Streaming ETL SFDC Data for Real-Time Customer Analytics</a>. Confluent relies heavily on Salesforce data for marketing and other purposes, where the Salesforce data is loaded into Google Big Query. This blog post shares how Confluent leverages Confluent Cloud connectors, Schema Registry, ksqlDB, and Kafka Streams (KStreams) to build a streaming ETL pipeline to send Salesforce data to BigQuery. It is cool to see how Confluent &ldquo;eats their own dog-food&rdquo;!</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>It is getting closer:</p>


<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="//www.youtube.com/embed/wQwQIy_wwes" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"></iframe>
</div>


<p>The &ldquo;trailer&rdquo; above is my attempt to &ldquo;shameless self-promotion&rdquo; of my one day <a href="https://arcade.sqlbits.com/sessions/"><strong>Azure Data Explorer</strong> training class</a> at <strong>SQLBits 2022</strong> in London next month. There are still some seats left so, if you are interested, go ahead and <a href="https://register.sqlbits.com/event/36db4730-583c-49c1-adcf-f76432bb6580/summary"><strong>REGISTER</strong></a>!</p>

<p>Speaking of registering:</p>

<p><img src="/images/posts/stream-processing-kafka.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Stream Processing with Apache Kafka and .NET</em></p>

<p>This coming Wednesday (February 9), I present <a href="https://www.meetup.com/NET-to-the-Core/events/283278548"><strong>Stream Processing with Apache Kafka and .NET</strong></a> at the <strong>.NET to the Core</strong> meetup user group. Register for free <a href="https://www.meetup.com/NET-to-the-Core/events/283278548">here</a>.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 4, 2022]]></title>
    <link href="https://nielsberglund.com/2022/01/30/interesting-stuff---week-4-2022/" rel="alternate" type="text/html"/>
    <updated>2022-01-30T12:46:36+02:00</updated>
    <id>https://nielsberglund.com/2022/01/30/interesting-stuff---week-4-2022/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="misc">Misc.</h2>

<ul>
<li><a href="https://netflixtechblog.com/fixing-performance-regressions-before-they-happen-eab2602b86fe">Fixing Performance Regressions Before they Happen</a>. This <a href="https://netflixtechblog.com/">Netflix</a> blog post looks at one of the strategies they use to quickly and easily detect performance anomalies before they are released â€” and often before they are even committed to the codebase.</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://hongilkwon.medium.com/when-to-use-two-phase-commit-in-distributed-transaction-f1296b8c23fd">When to Use Two Phase Commit in Distributed Transaction</a>. Ah, one of my favorite topics - distributed transactions and 2 phase commit! This post looks at helping you understand the pros and cons of the 2PC algorithm and build capability to thoroughly assess the trade-off between different algorithms for your future system designs.</li>
<li><a href="http://muratbuffalo.blogspot.com/2022/01/decoupled-transactions-low-tail-latency.html">Decoupled Transactions: Low Tail Latency Online Transactions Atop Jittery Servers (CIDR 2022)</a>. In <a href="/2022/01/23/interesting-stuff---week-3-2022/">last weeks roundup</a>, I mentioned the CIDR conference and how the talks were available on YouTube. The post linked to by <a href="https://twitter.com/muratdemirbas">Murat</a> looks at a White Paper submitted to the conference by Pat Helland. If you are not interested in reading the post (or the paper), Pat&rsquo;s talk based on the white paper is now up on YouTube: <a href="https://youtu.be/72UZ8DxPa8o">Decoupled Transactions: Low Tail Latency Online Transactions Atop Jittery Servers</a>. The video is very, very &ldquo;watch worthy&rdquo;!</li>
</ul>

<h2 id="azure-data-explorer">Azure Data Explorer</h2>

<ul>
<li><a href="https://techcommunity.microsoft.com/t5/azure-data-explorer-blog/adx-dashboards-january-2022-updates/ba-p/3074426">ADX dashboards January 2022 updates</a>. As the title implies, this post looks at new functionality introduced for dashboards in <strong>Azure Data Explorer</strong>. Hmm, the ability to export dashboards, very cool!</li>
</ul>

<h2 id="data-architecture">Data Architecture</h2>

<ul>
<li><a href="https://www.datanami.com/2022/01/20/all-eyes-on-snowflake-and-databricks-in-2022/">All Eyes on Snowflake and Databricks in 2022</a>. It&rsquo;s hard to overstate Snowflake and Databricks&rsquo; impact on the data industry for customers, partners, and competitors. This blog post looks at both Databricks and Snowflake and tries to determine what 2022 has in store.</li>
<li><a href="https://medium.com/towards-data-science/modern-data-stack-which-place-for-spark-8e10365a8772">Modern Data Stack: Which Place for Spark ?</a>. This very interesting post looks at why you rarely see any mentions of Apache Spark in articles about the Modern Data Stack.</li>
<li><a href="https://medium.com/event-driven-utopia/building-reference-architectures-for-user-facing-analytics-dc11c7c89df3">Building Reference Architectures for User-Facing Analytics</a>. User-facing analytics can be challenging to implement and, more specifically, user-facing analytics <strong>at scale</strong>. The post, written by <a href="https://twitter.com/dunithd">Dunith Dhanushka</a>, discusses several blueprints for building scalable analytics infrastructure to deliver user-facing analytics. This is a <strong>MUST</strong> read post if you are interested in real-time user-facing analytics.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/apache-kafka-3-1-version-features-and-updates/">What&rsquo;s New in Apache Kafka 3.1.0</a>. This post highlights some of the more prominent features in the newly released 3.1.0 version of Apache Kafka.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>In the coming days and weeks, I will be prepping for quite a few conference talks:</p>

<p><img src="/images/posts/stream-processing-kafka.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Stream Processing with Apache Kafka and .NET</em></p>

<ul>
<li><a href="https://www.meetup.com/NET-to-the-Core/events/283278548"><strong>Stream Processing with Apache Kafka and .NET</strong></a>. This is a Meetup user group talk, where I discuss Kafka, .NET and stream-processing.</li>
</ul>

<p><img src="/images/posts/sqlbits-precon-adx.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>SQLBITS 2022 - A Day of Azure Data Explorer</em></p>

<ul>
<li><a href="https://arcade.sqlbits.com/sessions/"><strong>A Day of Azure Data Explorer</strong></a>. This is a one-day training class about Azure Data Explorer at SQLBits 2022. I am presenting live (woohoo), but you can also attend virtually.</li>
</ul>

<p><img src="/images/posts/ksqldb-streaming-db.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>ksqlDB - The Real-Time Streaming Database</em></p>

<ul>
<li><a href="https://arcade.sqlbits.com/sessions/"><strong>ksqlDB - The Real-Time Streaming Database</strong></a>. An SQLBits 2022 conference talk, where I - in this demo filled session - look at using ksqlDB to gain real-time insights into streaming data. We look at push/pull queries, User Defined Functions, tables, streams, and <em>other cool stuff</em>.</li>
</ul>

<p><img src="/images/posts/analyze-billions-adx-sqlbits.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Analyze Billions of Rows of Data in Real-Time Using Azure Data Explorer</em></p>

<ul>
<li><a href="https://arcade.sqlbits.com/sessions/"><strong>Analyze Billions of Rows of Data in Real-Time Using Azure Data Explorer</strong></a>. This is my second conference talk at SQLBits 2022 (not counting the training day). In this session (full of demos), we get an overview of Azure Data Explorer. We look at how to ingest data and query in real-time against billions of rows with sub-second latency.</li>
</ul>

<p>For the SQLBits related &ldquo;stuff&rdquo; (training day and conference), you register <a href="https://register.sqlbits.com/event/36db4730-583c-49c1-adcf-f76432bb6580/summary">here</a>.</p>

<p>I hope to see you either at the <a href="https://www.meetup.com/NET-to-the-Core/events/283278548"><strong>.NET to the Core</strong></a> session or at <a href="https://register.sqlbits.com/event/36db4730-583c-49c1-adcf-f76432bb6580/summary"><strong>SQLBits</strong></a>!</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 3, 2022]]></title>
    <link href="https://nielsberglund.com/2022/01/23/interesting-stuff---week-3-2022/" rel="alternate" type="text/html"/>
    <updated>2022-01-23T08:40:56+02:00</updated>
    <id>https://nielsberglund.com/2022/01/23/interesting-stuff---week-3-2022/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="databases">Databases</h2>

<ul>
<li><a href="http://cidrdb.org/cidr2022/program.html">CIDR 2022 Conference</a>. The Conference on Innovative Data Systems Research (CIDR) is a systems-oriented conference emphasizing the systems architecture perspective. CIDR 2022 was held a couple of weeks ago, and the link is to the table of content, which has links to the given talks. There are some very interesting presentations there.</li>
<li><a href="https://starrocks.medium.com/trip-com-starrocks-efficiently-supports-high-concurrent-queries-dramatically-reduces-labor-and-1e1921dd6bf8">Trip.comï¼šStarRocks efficiently supports high concurrent queries, dramatically reduces labor and hardware cost</a>. The last few years have seen an emergence of real-time distributed OLAP datastore&rsquo;s; Druid, Apache Pinot, Clickhouse, etc. The link here is a post about StarRocks, a fairly new entry into the market. The post compares StarRocks with ClickHouse, and StarRocks comes out quite favourable.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/real-time-data-integrations-at-scale-with-confluent-q1-22-launch/">Announcing the Confluent Q1 &lsquo;22 Launch</a>. This blog post announces new features for Confluent Cloud being launched in Q1 2022. One new &ldquo;feature&rdquo; that I like is the new Confluent CLI <code>confluent v2</code>. With <code>confluent v2</code>, I no longer need separate CLI&rsquo;s for on-prem and cloud!</li>
<li><a href="https://www.confluent.io/blog/streaming-data-pipeline-with-apache-kafka-and-ksqldb/">Introduction to Streaming Data Pipelines with Apache Kafka and ksqlDB</a>. This blog post describes the elements involved in setting up a Kafka-based data pipeline: connecting data entities together; streaming data from source(s) into the middle of the pipeline; filtering, joining, and enriching the data with ksqlDB. Cool stuff!</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>If you read my <a href="/2021/12/05/interesting-stuff---week-49-2021/">roundup from week 49 last year</a>, you do know:</p>

<p><img src="/images/posts/sqlbits-precon-adx.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>SQLBITS 2022 - A Day of Azure Data Explorer</em></p>

<p>Yes, I am presenting a one-day training class about Azure Data Explorer at SQLBits 2022. Referring back to what I am doing, I did a promo video for my class last evening, so go to <a href="https://youtu.be/wQwQIy_wwes"><strong>A Day of Azure Data Explorer</strong></a> and have a look. Then <a href="https://register.sqlbits.com/event/36db4730-583c-49c1-adcf-f76432bb6580/summary"><strong>register</strong></a> for <a href="https://register.sqlbits.com/event/36db4730-583c-49c1-adcf-f76432bb6580/summary"><strong>SQLBits 2022</strong></a>.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 2, 2022]]></title>
    <link href="https://nielsberglund.com/2022/01/16/interesting-stuff---week-2-2022/" rel="alternate" type="text/html"/>
    <updated>2022-01-16T08:56:07+02:00</updated>
    <id>https://nielsberglund.com/2022/01/16/interesting-stuff---week-2-2022/</id>
    <content type="html"><![CDATA[<p><img src="/images/posts/monitoring-1.jpg" alt="" /></p>

<p><em>Photo by <a href="https://unsplash.com/@dawson2406?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">Stephen Dawson</a> on <a href="https://unsplash.com/s/photos/technical?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">Unsplash</a></em></p>

<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="data-data-architecture">Data &amp; Data Architecture</h2>

<ul>
<li><a href="https://medium.com/event-driven-utopia/the-state-of-data-infrastructure-landscape-in-2022-and-beyond-c57b9f85505c">The State of Data Infrastructure Landscape in 2022 and Beyond</a>. This post looks at the evolution of the data infrastructure landscape over the last decade. It then goes on to look at key trends and what can be expected from now and onwards.</li>
<li><a href="https://www.infoq.com/articles/next-evolution-of-database-sharding-architecture/">The Next Evolution of the Database Sharding Architecture</a>. In this <a href="https://www.infoq.com/">InfoQ</a> article, the author discusses the data sharding architecture patterns in a distributed database system. She explains how the Apache ShardingSphere project solves the data sharding challenges. Also discussed are two practical examples of how to create a distributed database and an encrypted table with DistSQL. Very, very interesting!</li>
</ul>

<h2 id="azure-data-explorer">Azure Data Explorer</h2>

<ul>
<li><a href="https://www.meetup.com/data-platform-downunder-meetup-group/events/283207013/">Big Data Analytics using Azure Data Explorer</a>. Learn from the masters of Azure Data Explorer. This link is to register for an Azure Data Explorer talk by the Azure Data Explorer gurus <a href="https://www.linkedin.com/in/minni-walia-17968521/">Minni Walia</a> and Uri Barash<a href="https://www.linkedin.com/in/uri-barash-7820594/">5</a>. Read the abstract on the <a href="https://www.meetup.com/data-platform-downunder-meetup-group/events/283207013/">sign-up page</a> to see all the goodies you&rsquo;ll hear about. See you there!</li>
<li><a href="https://techcommunity.microsoft.com/t5/azure-data-explorer-blog/azure-data-explorer-offers-on-amd-skus/ba-p/3033197">Azure Data Explorer offers on AMD SKUs</a>. The post linked to covers &ldquo;what it says on the tin&rdquo;; it talks about how we can now run Azure Data Explorer on AMD SKUs. This is cool as we can now gain higher performance while keeping the costs low.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://databricks.com/blog/2022/01/13/confluent-streaming-for-databricks-build-scalable-real-time-applications-on-the-lakehouse.html">Confluent Streaming for Databricks: Build Scalable Real-time Applications on the Lakehouse</a>. This post is about the fully managed Confluent connector against Databricks Delta Lake: the ability to ingest directly into Delta Lake tables from Kafka topics. I got really excited when I saw this post because this would solve some issues for us: us as in <a href="/derivco">Derivco</a>. Unfortunately, we cannot use it yet, as it is AWS only - for now. Regardless of that, this is really cool!</li>
<li><a href="https://www.confluent.io/blog/ksqldb-0-23-1-features-updates/">Announcing ksqlDB 0.23.1</a>. A new version of ksqlDB is out in the wild! Some of the new exciting features are: perform pull queries on streams, access topic partition and offset through pseudo-columns, and use grace periods when joining streams. All very cool!</li>
<li><a href="https://medium.com/paypal-tech/kafka-consumer-benchmarking-c726fbe4000">Scaling Kafka Consumer for Billions of Events</a>. This post provides a &ldquo;ton&rdquo; of helpful information about configuring Kafka consumers for optimal throughput. I have made this post a mandatory read for my developers that write Kafka applications.</li>
<li><a href="https://developer.confluent.io/ksqldb-recipes/">Transform your Kafka data into real-time insights</a>. The post looks at ksqlDB recipes for the most popular stream processing use cases. Each recipe provides a set of ksqlDB queries you can run to process real-time data streams and take immediate action.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>In last week&rsquo;s roundup, I mentioned how I had started writing a post using Debezium and Kafka Connect to publish events to Event Hubs. The one blog post turned into two, and I published both during last week:</p>

<ul>
<li><a href="/2022/01/10/how-to-stream-data-to-event-hubs-from-databases-using-kafka-connect--debezium-in-docker---i/">How to Stream Data to Event Hubs from Databases Using Kafka Connect &amp; Debezium in Docker - I</a>. In this post I looked at the configuration of Kafka Connect in <code>docker-compose.yml</code> to enable connection to Event Hubs.</li>
<li><a href="/2022/01/14/how-to-stream-data-to-event-hubs-from-databases-using-kafka-connect--debezium-in-docker---ii/">How to Stream Data to Event Hubs from Databases Using Kafka Connect &amp; Debezium in Docker - II</a>. This, the second post, concluded the &ldquo;adventure&rdquo; of streaming data to Eent Hubs using Debezium and Kafka Connect. More specifically, I looked at the configuration of the Debezium connector and the various properties required to push data to Event Hubs.</li>
</ul>

<p>When I started with the two posts above I was not 100% sure it would work (publishing to Event Hubs using Kafka Connect). Fortunately it turned out that, yes - you can use Kafka Connect to publish to Event Hubs.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to Stream Data to Event Hubs from Databases Using Kafka Connect &amp; Debezium in Docker - II]]></title>
    <link href="https://nielsberglund.com/2022/01/14/how-to-stream-data-to-event-hubs-from-databases-using-kafka-connect--debezium-in-docker---ii/" rel="alternate" type="text/html"/>
    <updated>2022-01-14T05:25:03+02:00</updated>
    <id>https://nielsberglund.com/2022/01/14/how-to-stream-data-to-event-hubs-from-databases-using-kafka-connect--debezium-in-docker---ii/</id>
    <content type="html"><![CDATA[<p>In a two-post series, this second post looks at streaming data from a database to <strong>Azure Event Hubs</strong> using Kafka Connect and Debezium, where Kafka Connect and Debezium run in Docker.</p>

<p>The first post:</p>

<ul>
<li><a href="/2022/01/10/how-to-stream-data-to-event-hubs-from-databases-using-kafka-connect--debezium-in-docker---i/"><strong>How to Stream Data to Event Hubs from Databases Using Kafka Connect &amp; Debezium in Docker - I</strong></a>. This post mainly looks at the configuration of Kafka Connect&rsquo;s <code>docker-compose.yml</code> file to allow us to connect to Event Hubs.</li>
</ul>

<p>This series came about as I in the post <a href="/2022/01/02/how-to-use-kafka-client-with-azure-event-hubs/"><strong>How to Use Kafka Client with Azure Event Hubs</strong></a>, somewhat foolishly said:</p>

<p><em>An interesting point here is that it is not only your Kafka applications that can publish to Event Hubs but any application that uses Kafka Client 1.0+, like Kafka Connect connectors!</em></p>

<p>I wrote the above without testing it myself, so when I was called out on it, I started researching (read &ldquo;Googling&rdquo;) to see if it was possible. The result of the &ldquo;Googling&rdquo; didn&rsquo;t give a 100% answer, so I decided to try it out, and this series is the result.</p>

<p>In the <a href="/2022/01/10/how-to-stream-data-to-event-hubs-from-databases-using-kafka-connect--debezium-in-docker---i/">first post</a>, - as mentioned - we configured Kafka Connect to connect into Event Hubs. In this post, we look at configuring the Debezium connector.</p>

<p></p>

<h2 id="pre-reqs">Pre-reqs</h2>

<p>The pre-reqs are the same as in <a href="/2022/01/10/how-to-stream-data-to-event-hubs-from-databases-using-kafka-connect--debezium-in-docker---i/"><strong>How to Stream Data to Event Hubs from Databases Using Kafka Connect &amp; Debezium in Docker - I</strong></a> and if you followed along in that post you should now have:</p>

<ul>
<li>A SQL Server database: <code>DebeziumTest</code> (or whatever you named it).</li>
<li>A table in the database: <code>dbo.tb_CDCTab1</code>.</li>
</ul>

<p>In addition to the above, you should also have an Event Hubs namespace. In the namespace you should have created a SAS policy connection string, where the connection string looks like so:</p>

<pre><code class="language-bash">Endpoint=sb://dbzeventhubs.servicebus.windows.net/; SharedAccessKeyName=KafkaConnect; \
SharedAccessKey=&lt;secret-key&gt;
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>SAS Policy Connection String</em></p>

<p>In <em>Code Snippet 1</em>, we see the policy connection string I created in the previous post.</p>

<h2 id="recap">Recap</h2>

<p>Before diving into configuring Debezium, let us recap what we did in the <a href="/2022/01/10/how-to-stream-data-to-event-hubs-from-databases-using-kafka-connect--debezium-in-docker---i/">previous post</a>, where we configured the <code>docker-compose.yml</code> file we use for Kafka Connect. In the last post, I divided the file into three parts to look at the details. Here I show the whole file with pointers to interesting areas:</p>

<p><img src="/images/posts/dbz-evthub-docker-compose-complete-1.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Docker Compose File</em></p>

<p>The numbered/outlined areas in <em>Figure 1</em> refer to:</p>

<ol>
<li>The image we use. Here, we use the Kafka Connect base image, which contains the bare minimum for Kafka Connect.</li>
<li>Defines the Kafka endpoint for the worker process. This is the Kafka endpoint of the Event Hubs namespace for Event Hubs. You get the endpoint from the SAS policy&rsquo;s connection string, and you append it with port, <code>9093</code>, which is the Event Hubs Kafka API endpoint.</li>
<li>Kafka Connect uses topics to store connectors config, offsets, and statuses. As this is Event Hubs, we see the Event Hub names we want to use (they will be auto-created). We also define the replication factor for the event hubs (topics). In Kafka, the default is 3, but Event Hubs works somewhat differently, so we set the replication factor to 1.</li>

<li><p>This is the security/authentication configuration for the Kafka Connect worker process for connecting to the bootstrap server. Outlined in red, we see how we pass in the JAAS configuration. This is required as Kafka uses JAAS (Java Authentication and Authorization Service) for SASL. The JAAS configuration is based on the Event Hubs namespace configuration string and looks like so:</p>

<pre><code class="language-bash">CONNECT_SASL_JAAS_CONFIG: \ 
     &quot;org.apache.kafka.common.security.plain.PlainLoginModule \ 
      required username=\&quot;$$ConnectionString\&quot; \ 
      password=\&quot;Endpoint=sb://dbzeventhubs.servicebus.windows.net/; \
              SharedAccessKeyName=KafkaConnect; \ 
              SharedAccessKey=&lt;secret-key&gt;;&quot;\&quot;;&quot;
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>JAAS Configuration</em></p>

<p>Notice in <em>Figure 2</em> how we set the username to <code>$$ConnectionString</code> instead of the &ldquo;normal&rdquo; <code>$ConnectionString</code> as user name for Event Hubs. Using single <code>$</code> in <code>docker-compose</code> implies variable substitution, so therefore we &ldquo;escape&rdquo; by using <code>$$</code>.</p></li>

<li><p>This is the security/authentication configuration for the connector to connect to the bootstrap server. In this case the bootstrap server is the same for the worker process and the connector, so the JAAS configuration is the same as in <em>Code Snippet 2</em>.</p></li>

<li><p>We install/deploy Debezium&rsquo;s SQL Server connector using <code>confluent-hub install</code>.</p></li>
</ol>

<p>Two things to keep in mind in the Kafka Connect configuration are:</p>

<ul>
<li>Use <code>$$ConnectionString</code> instead of <code>$ConnectionString</code> as the user name when connecting to Event Hubs. Read <a href="/2022/01/02/how-to-use-kafka-client-with-azure-event-hubs/">this post</a> to see why we use <code>$ConnectionString</code> at all when connecting to Event Hubs.</li>
<li>Set the authentication/security for the connector as well, not only the Kafka Connect worker process.</li>
</ul>

<p>We have now reached more or less where we finished the <a href="/2022/01/10/how-to-stream-data-to-event-hubs-from-databases-using-kafka-connect--debezium-in-docker---i/">previous post</a>, let us continue.</p>

<h2 id="enable-cdc">Enable CDC</h2>

<p>When streaming data using Debezium, CDC (Change Data Capture) must be enabled. So let us enable CDC:</p>

<pre><code class="language-sql">USE DebeziumTest;
GO
-- before we enable CDC ensure the SQL Server Agent is started
-- we need first to enable CDC on the database
EXEC sys.sp_cdc_enable_db;

-- then we can enable CDC on the table
EXEC sys.sp_cdc_enable_table @source_schema = N'dbo',
                               @source_name   = N'tb_CDCTab1',
                               @role_name = NULL,
                               @supports_net_changes = 0;
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Enabling Database and Table for CDC</em></p>

<p>We see in <em>Code Snippet 3</em> how we:</p>

<ul>
<li>Enable CDC on the database.</li>
<li>After enabling CDC on the database, we enable it for the table(s) from which we want to capture changes.</li>
</ul>

<p>Please remember that the SQL Server Agent needs to be started before enabling CDC. Having enabled CDC, we can now look at the connector.</p>

<h2 id="debezium-connector">Debezium Connector</h2>

<p>Above, in <em>Figure 1</em>, we see how we install/deploy the SQL Server connector, and in the last post, we saw how the connector was loaded in the Kafka Connect worker process after we did <code>docker-compose up -d</code>.</p>

<p>The connector is loaded, but it is not doing anything. To enable the connector, we configure it using a JSON file, which we then <code>POST</code> to a Kafka Connect endpoint. To <code>POST</code> the file, you can use your favorite tool, Postman, <code>curl</code>, etc. I tend to like Postman, so that is what I use later on.</p>

<p>Before we go any further, even though this post looks in somewhat detail at configuring Debezium, it looks at it from the perspective of configuring it to be able to communicate with Event Hubs. So, if you want/need more information about Debezium configuration in general, look <a href="https://debezium.io/documentation/reference/stable/connectors/sqlserver.html">here</a>.</p>

<h4 id="debezium-event-hubs-topics">Debezium Event Hubs (Topics)</h4>

<p>During the configuration of the Debezium connector, two Debezium specific event hubs are created, regardless of the tables we are interested in:</p>

<ul>
<li>Event hub for schema changes. The connector writes schema change events to this event hub whenever a schema change happens for a captured table. The name of the event hub is the name in the <code>database.server.name</code> configuration property in the configuration file.</li>
<li>Event hub for database history. Schema changes are written to the schema change event hub, and also to this database history event hub. You set the name of this event hub in the <code>database.history.kafka.topic</code> configuration property in the configuration file.</li>
</ul>

<p>Let us look at database history configuration.</p>

<h4 id="database-history">Database History</h4>

<p>The database history requires some specific configuration. This caused me issues when I tried Debezium to Event Hubs, so I thought I better cover the database history configuration in a bit more detail.</p>

<blockquote>
<p><strong>NOTE:</strong> The database history properties are not SQL Server specific, but every Debezium connector requires them, except for the connectors for PostgreSQL, Cassandra, and Vitess.</p>
</blockquote>

<p>So, I mentioned above about the database history event hub and how the connector writes schema changes to that event hub. We define the event hub name in the <code>database.history.kafka.topic</code> configuration property. We also need to specify the endpoint where the event hub should be created/exists. We do that via the <code>database.history.kafka.bootstrap.servers</code> property. Keep in mind that the database history endpoint should be the same as the Kafka Connect process endpoint. That&rsquo;s what all documentation says anyway, and I have not tried anything differently.</p>

<blockquote>
<p><strong>NOTE:</strong> The timeout for creating the database history topic is very short, so when you connect to the cloud, whether it is Confluent Cloud or Azure Event Hubs, you should create the event hub (topic) manually. I.e. creating it before <code>POST</code>:ing the connector configuration. Oh, and when you create it, you have to create it with a partition count of 1.</p>
</blockquote>

<p>Taking the above into consideration, the database history configuration looks something like so:</p>

<pre><code class="language-bash"># more properties above

&quot;database.history.kafka.bootstrap.servers&quot;: &quot;dbzeventhubs.servicebus.windows.net:9093&quot;,
&quot;database.history.kafka.topic&quot;: &quot;dbzdbhistory&quot;,

# more properties below
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Database History Configuration</em></p>

<p>In <em>Code Snippet 4</em>, we see the database history configuration properties, and we see that <code>database.history.kafka.bootstrap.servers</code> has the same value as in <em>Figure 1</em>, point two. The event hub name is set to <code>dbzdbhistory</code>. As mentioned before, we should manually create that event hub to avoid timeout errors.</p>

<p>This looks good! But wait, there is more - and this was one of the things I completely missed when I initially tested Debezium with Event Hubs. What I missed was that if your Kafka cluster/Event Hubs is secured, you must add the security properties prefixed with <code>database.history.consumer.*</code> and <code>database.history.producer.*</code> to the connector configuration:</p>

<pre><code class="language-bash"># more properties above
# consumer
&quot;database.history.consumer.security.protocol&quot;:&quot;SASL_SSL&quot;,
&quot;database.history.consumer.sasl.mechanism&quot;:&quot;PLAIN&quot;,
&quot;database.history.consumer.sasl.jaas.config&quot;: &quot;&lt;JAAS-config-string&gt;;&quot;,
# producer
&quot;database.history.producer.security.protocol&quot;:&quot;SASL_SSL&quot;,
&quot;database.history.producer.sasl.mechanism&quot;:&quot;PLAIN&quot;,
&quot;database.history.producer.sasl.jaas.config&quot;:&quot;&lt;JAAS-config-string&gt;;&quot;
</code></pre>

<p><strong>Code Snippet 5:</strong> <em>Database History Security Configuration</em></p>

<p>In <em>Code Snippet 5</em>, we see how the database history security is set up like the security in <em>Figure 1</em>. One difference is that we set it up for both <code>consumer</code> and <code>producer</code>, as the connector will both write to and read from the topic. The JAAS configuration string looks like what we see in  <em>Code Snippet 2</em>; apart from that the user name is <code>$ConnectionString</code>. I.e. one dollar sign.</p>

<h4 id="configuration-file">Configuration File</h4>

<p>Having covered some of the essential parts of the connector configuration, let us look at what the complete configuration file looks like:</p>

<p><img src="/images/posts/dbz-evthub-dbz-connector-config.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Debezium Configuration File</em></p>

<p>Outlined in yellow in <em>Figure 2</em> are the configuration properties mostly related to the source database. Some properties to look at are:</p>

<ul>
<li><code>connector.class</code>: this defines what Debezium connector to use.</li>
<li><code>database.dbname</code>: the database from which we capture events.</li>
<li><code>database.server.name</code>: logical name that identifies the SQL Server instance (it can be an arbitrary string). An event hub with this name will be created to capture schema changes (as mentioned above). This name is also used as a prefix for all event hub names created by the connector for tables from which we capture changes.</li>
<li><code>table.include.list</code>: a comma-separated list of fully-qualified table names for tables from which we capture changes.</li>
</ul>

<p>The outlined in-red portion in <em>Figure 2</em> is required for <code>database.history.*</code>. Since I covered quite a lot about database history above, I will not go further into it.</p>

<h2 id="configure-the-connector">Configure the Connector</h2>

<p>We are now ready to &ldquo;spin up&rdquo; Kafka Connect, followed by configuring and enabling the connector.</p>

<blockquote>
<p><strong>NOTE:</strong> When I am testing things out, I ALWAYS tear down my environment before each run. I.e., delete the connector if it is created, take down Docker, and delete all relevant topics.</p>
</blockquote>

<p>Let&rsquo;s do this! We start with &ldquo;spinning up&rdquo; Kafka Connect like we did in the <a href="/2022/01/10/how-to-stream-data-to-event-hubs-from-databases-using-kafka-connect--debezium-in-docker---i/">last post</a>: <code>docker-compose up -d</code>. We wait a minute or two, and then we do the same checks as in the <a href="/2022/01/10/how-to-stream-data-to-event-hubs-from-databases-using-kafka-connect--debezium-in-docker---i/">previous post</a>:</p>

<ul>
<li>In the Azure portal, ensure that the Kafka Connect internal topics have been created. In my case, <code>offsets</code>, <code>status</code>, and <code>configs</code>.</li>
<li>Look and see that the connector is loaded: <code>GET http://127.0.0.1:8083/connector-plugins</code>.</li>
</ul>

<p>I don&rsquo;t know about you, but in my environment, it looks good. We are now ready to configure the connector. Before we do that, ensure that the database and table are CDC enabled and that you have manually created the database history event hub.</p>

<p>Now let&rsquo;s do it. Let us <code>POST</code> the configuration:</p>

<p><img src="/images/posts/dbz-evthub2-post-connector.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>POST Configuration</em></p>

<p>In <em>Figure 3</em>, we see our <code>POST</code> call (outlined in yellow) to the <code>connectors</code> endpoint (outlined in blue), and we also see part of the configuration file. When configuring a connector, we give it a name, and in my case, I named it <code>sql-server</code> (imaginative - I know). After we have <code>POST</code>:ed we see if it worked:</p>

<p><img src="/images/posts/dbz-evthub2-connector-status.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Connector Status</em></p>

<p>We see in <em>Figure 4</em> how we do a <code>GET</code> call against the <code>status</code> endpoint with the connector&rsquo;s name as part of the path. According to what is outlined in blue, all looks OK.</p>

<blockquote>
<p><strong>NOTE:</strong> When checking the status of a newly created connector, it is a good practice to wait a little while (10 - 20) seconds right after creation before checking the status. This is to give the connector some time to &ldquo;spin up&rdquo;. Alternatively, run the status check a couple of times.</p>
</blockquote>

<p>The other thing we can do to ensure all is OK is to look in the portal and see what event hubs we have:</p>

<p><img src="/images/posts/dbz-evthub2-topics-1.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Created Event Hubs</em></p>

<p>At first glance at <em>Figure 5</em> all looks OK. We see the connector&rsquo;s event hub for scheme changes (<code>debeziumtestserver</code>). So far, so good. But what about the event hub for the table we want to capture changes from: <code>dbo.tb_CDCTab1</code>? Where is that event hub? The answer to that is that the event hubs for capture table events are not created until an event happens:</p>

<pre><code class="language-sql">USE DebeziumTest;
GO

INSERT INTO dbo.tb_CDCTab1(Col1, Col2)
VALUES(1, 'Hello Number 1')
</code></pre>

<p><strong>Code Snippet 6:</strong> <em>Ingest Data</em></p>

<p>After executing the code in <em>Code Snippet 6</em>, you refresh the event hubs in the portal, and you see this:</p>

<p><img src="/images/posts/dbz-evthub2-table-topic.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>Event Hub for Table</em></p>

<p>After doing an insert in the table, we see in <em>Figure 6</em> an event hub created for that table. We assume that events have been published to the event hub. To further &ldquo;prove&rdquo; that, we look at that particular event hub&rsquo;s overview page in the portal and its stats:</p>

<p><img src="/images/posts/dbz-evthub2-table-events.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>Event Hub Stats</em></p>

<p>The stats for incoming messages are outlined in red in <em>Figure 7</em>, and we see how one message has arrived. It works - yay!</p>

<h2 id="summary">Summary</h2>

<p>In the <a href="/2022/01/10/how-to-stream-data-to-event-hubs-from-databases-using-kafka-connect--debezium-in-docker---i/">previous post</a> and this, we set out to see if we can use Debezium and Kafka Connect to stream data from databases to Event Hubs. We have now seen it is possible!</p>

<p>Things to keep in mind:</p>

<ul>
<li>When connecting to Event Hubs, we use the connection string from a SAS policy.</li>
<li>We use JAAS configuration to set the user name and password. For Event Hubs the username is the literal string <code>$ConnectionString</code>, and the password is the SAS policy&rsquo;s connection string.</li>
<li>If we use <code>docker-compose</code>, we set the user name in the JAAS configuration to <code>$$ConnectionString</code> to avoid variable substitution.</li>
<li>When configuring the security for Kafka Connect, you do it both for the Kafka Connect worker process and the connector.</li>
<li>Almost all Debezium connectors require a database history event hub (topic).</li>
<li>Since the timeout for automatic creation of the event hub is very short, the event hub should be created manually before configuring the connector.</li>
<li>We need to set security for the database history endpoint and do it for both consumer and producer (<code>database.history.consumer.*</code>, <code>database.history.producer.*</code>).</li>
</ul>

<p>Lastly, I would not have been able to get this to work if it hadn&rsquo;t been for the blog post:</p>

<ul>
<li><a href="https://rmoff.net/2019/10/16/using-kafka-connect-and-debezium-with-confluent-cloud/"><strong>Using Kafka Connect and Debezium with Confluent Cloud</strong></a>. This post by by Kafka guru <a href="https://twitter.com/rmoff">Robin Moffat</a> gave me the necessary pointers for the connector configuration - especially the security configuration for the database history event hub.</li>
</ul>

<p>As I said, <a href="https://twitter.com/rmoff">Robin Moffat</a> is a Kafka Guru, and if you are into Kafka, then you <strong>MUST</strong> read his <a href="https://rmoff.net/">blog</a>.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or [ping][ma] me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to Stream Data to Event Hubs from Databases Using Kafka Connect &amp; Debezium in Docker - I]]></title>
    <link href="https://nielsberglund.com/2022/01/10/how-to-stream-data-to-event-hubs-from-databases-using-kafka-connect--debezium-in-docker---i/" rel="alternate" type="text/html"/>
    <updated>2022-01-10T19:05:33+02:00</updated>
    <id>https://nielsberglund.com/2022/01/10/how-to-stream-data-to-event-hubs-from-databases-using-kafka-connect--debezium-in-docker---i/</id>
    <content type="html"><![CDATA[<p>This post is the first of two looking at if and how we can stream data to Event Hubs from Debezium. Initially I had planned only one post covering this, but it turned out that the post would be too long, so therefore I split it in two.</p>

<p>The second post in the series is:</p>

<ul>
<li><a href="/2022/01/14/how-to-stream-data-to-event-hubs-from-databases-using-kafka-connect--debezium-in-docker---ii/"><strong>How to Stream Data to Event Hubs from Databases Using Kafka Connect &amp; Debezium in Docker - II</strong></a>. Here we look at the Debezium connector configuration needed if we want to stream data to Event Hubs.</li>
</ul>

<p>It started with the post, <a href="/2022/01/02/how-to-use-kafka-client-with-azure-event-hubs/"><strong>How to Use Kafka Client with Azure Event Hubs</strong></a>. In that post, I looked at how the Kafka client can publish messages to - not only - <strong>Apache Kafka</strong> but also <strong>Azure Event Hubs</strong>. In the post, I said something like:</p>

<p><em>An interesting point here is that it is not only your Kafka applications that can publish to Event Hubs but any application that uses Kafka Client 1.0+, like Kafka Connect connectors!</em></p>

<p>Obviously, for you who know me, I said that without having tested it properly, but: <em>how hard can it be? What could possibly go wrong?</em>. Well, I was called upon it by a guy who had read the post. He told me he had tried what I said at one time or the other, and it hadn&rsquo;t worked.</p>

<p>In this post (the first), we look at configuring Kafka Connect to connect to Event Hubs.</p>

<p></p>

<h2 id="background">Background</h2>

<p>Let us look at some of the moving parts of this.</p>

<h4 id="azure-event-hubs">Azure Event Hubs</h4>

<p>Azure Event Hubs is a big data streaming platform and event ingestion service. It is a fully managed Platform-as-a-Service (PaaS) with little configuration or management overhead, very much like Apache Kafka in <strong>Confluent Cloud</strong>.</p>

<p>The concepts are fairly similar between Event Hubs and Kafka, especially if we look at Apache Kafka in Confluent Cloud. There are however a couple of differences in terminology:</p>

<ul>
<li>In Kafka, we create/have a <em>cluster</em>, whereas, in Event Hubs, we have a <em>namespace</em>.</li>
<li>When we publish messages to Kafka, we publish to a <em>Topic</em>, where the topic is part of a cluster. In Event Hubs we publish to an <em>Event Hub</em> in an Event Hubs namespace. Even though the names (topics, Event Hub) are different, the underlying concepts are the same. I.e. both have partitions, and messages are located based on offsets in a partition.</li>
</ul>

<p>An Event Hubs namespace exposes API endpoints to where clients can connect. One such endpoint is the Kafka client protocol endpoint (protocol version 1.0 and above) which is exposed on port <code>9093</code> of the host of the namespace.</p>

<p>The previously mentioned <a href="/2022/01/02/how-to-use-kafka-client-with-azure-event-hubs/">post</a> discusses this in more detail.</p>

<h4 id="kafka-connect">Kafka Connect</h4>

<p>Kafka Connect is a JVM process (worker process) running separately from a Kafka broker. It is used for streaming data between Apache Kafka and other systems in a scalable and reliable way. Kafka Connect moves the data using <strong>connectors</strong>, where a connector is a <code>.jar</code> file, and the connector is loaded by the Kafka Connect process. Basically the worker acts as a host for one or more connectors. The connectors come in two flavours:</p>

<ul>
<li>Source connectors, which understand how to interact with the source system, publish records to Kafka topics (Kafka acts as a sink).</li>
<li>Sink connectors propagate records from Kafka topics to other systems.</li>
</ul>

<p>Connectors are Kafka specific, but since Event Hubs exposes the Kafka client endpoint, we can use (or at least supposedly can) connectors that use Kafka as a sink.</p>

<h4 id="debezium">Debezium</h4>

<p>Debezium is an open-source distributed platform for change data capture (CDC). It captures changes in your database(s) and publishes those changes to topics in Kafka.</p>

<p>Debezium has Kafka Connect connectors for many source systems; Oracle, PostgresSQL, SQL Server, etc., and in this post, we use the Debezium SQL Server connector. As with other Kafka Connect connectors, the Debezium connectors are deployed to Kafka Connect.</p>

<p>The post <a href="/2021/08/07/how-to-deploy-the-debezium-sql-server-connector-to-docker/"><strong>How to Deploy the Debezium SQL Server Connector to Docker</strong></a> looks at Kafka Connect, Debezium, and SQL Server in more detail.</p>

<p>Having had some background information, let&rsquo;s see what you need if you want to follow along.</p>

<h2 id="pre-reqs">Pre-reqs</h2>

<p>The pre-reqs are the same (with a couple of additions) as in the <a href="/2021/08/07/how-to-deploy-the-debezium-sql-server-connector-to-docker/"><strong>How to Deploy the Debezium SQL Server Connector to Docker</strong></a> post, so look at that post to find out what you need. The additions are:</p>

<ul>
<li>As you&rsquo;ll be working with Event Hubs, you need an Azure account. If you don&rsquo;t have an Azure subscription, sign up for a <a href="/2022/01/02/how-to-use-kafka-client-with-azure-event-hubs/">free account</a>.</li>
<li>An Event Hubs namespace to where the SQL data will be streamed.</li>
</ul>

<p>If you don&rsquo;t have an Event Hubs namespace and are unclear on how to create one, the Microsoft article <a href="https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-create"><strong>Quickstart: Create an event hub using Azure portal</strong></a> covers it in detail. While creating the namespace, ensure you create it under the <em>Standard</em> pricing tier (or higher), as <em>Basic</em> does not support Kafka.</p>

<p>The Event Hubs namespace I use in this post looks like so:</p>

<p><img src="/images/posts/dbz-evthub-namespace-1.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Namespace</em></p>

<p>In <em>Figure 1</em>, we see how the namespace is called <code>dbzeventhubs</code> (outlined in red) and that we don&rsquo;t have any Event Hub (topic) yet.</p>

<p>After you have downloaded and set up the pre-reqs as per the above <a href="/2021/08/07/how-to-deploy-the-debezium-sql-server-connector-to-docker/">post</a>, as well as the Event Hubs namespace, we are ready to go. We  should have:</p>

<ul>
<li>A SQL Server database: <code>DebeziumTest</code> (or whatever you named it).</li>
<li>A table in the database: <code>dbo.tb_CDCTab1</code>.</li>
</ul>

<p>We also have an Event Hubs namespace. I named it <code>dbzeventhubs</code> as in <em>Figure 1</em>.</p>

<h2 id="event-hubs-security-authentication">Event Hubs Security &amp; Authentication</h2>

<p>In the <a href="/2022/01/02/how-to-use-kafka-client-with-azure-event-hubs/">Kafka Client and Event Hubs</a> post, I discussed how we need to configure Event Hubs security to allow Kafka clients to interact with Event Hubs. We did it by creating a Shared Access Signature (SAS) policy. We then used the policy&rsquo;s connection string as the <code>SASL</code> authentication password.</p>

<p>In this post, we do the same, with one difference. In the <a href="/2022/01/02/how-to-use-kafka-client-with-azure-event-hubs/">previous post</a>, we created the policy for the Event Hub (topic), whereas now we do it for the namespace. This is because both Kafka Connect and Debezium need permissions on the namespace level (create event hubs, etc.).</p>

<p>For this post, I created the policy in the same way as I did in the <a href="/2022/01/02/how-to-use-kafka-client-with-azure-event-hubs/">Kafka Client and Event Hubs</a> post, and during creation, I saw something like so:</p>

<p><img src="/images/posts/dbz-evthub-sas-create.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Create Policy</em></p>

<p>In <em>Figure 2</em>, we see how I created a policy called <code>KafkaConnect</code> and how the policy has <code>Manage</code> permissions. The <code>Manage</code> permission allows the policy to manage the topology of the namespace, i.e. adding deleting entities.</p>

<p>Having created the policy, you copy one of the policy&rsquo;s connection strings as that is what we use for the Kafka client configuration. My connection string looks like so:</p>

<pre><code class="language-bash">Endpoint=sb://dbzeventhubs.servicebus.windows.net/; SharedAccessKeyName=KafkaConnect; \
SharedAccessKey=&lt;secret-key&gt;
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>SAS Policy Connection String</em></p>

<p>The string we see in <em>Code Snippet 1</em> acts as the password for authentication. When setting up Kafka Connect, we need to define the <code>bootstrap.servers</code> (the server(s) to connect to). We get that value from the <code>Endpoint</code> field, <code>dbzeventhubs.servicebus.windows.net</code>. We append it with the port, <code>9093</code>, for the Event Hubs Kafka API endpoint.</p>

<blockquote>
<p><strong>NOTE:</strong> I am aware that I have &ldquo;glossed&rdquo; over the details of SAS policies. Please look at the <a href="/2022/01/02/how-to-use-kafka-client-with-azure-event-hubs/">Kafka Client and Event Hubs</a> post if you need more information.</p>
</blockquote>

<p>Cool, we now have all we need to configure Kafka Connect in Docker.</p>

<h2 id="kafka-connect-docker">Kafka Connect &amp; Docker</h2>

<p>As mentioned before, we want to run Kafka Connect and the connector locally in Docker. We do it by using a <code>docker-compose.yml</code> file, similar to what we did in the post <a href="/2021/08/07/how-to-deploy-the-debezium-sql-server-connector-to-docker/"><strong>How to Deploy the Debezium SQL Server Connector to Docker</strong></a>. The difference here is that we only run Kafka Connect and the connector, no Kafka broker, etc.</p>

<p>This part, configuring Kafka Connect: <em>how hard can it be? What could possibly go wrong?</em></p>

<p>Needless to say, it was not as easy as I thought.</p>

<h2 id="docker-compose">Docker Compose</h2>

<p>As mentioned before, we use a <code>docker-compose.yml</code> and to make things a bit more readable, I have divided the file into three parts: <em>basics</em>, <em>security</em>, and <em>connector</em>. The three parts are represented by the figures below.</p>

<h4 id="basics">Basics</h4>

<p>The <em>basics</em> part looks like so:</p>

<p><img src="/images/posts/dbz-evthub-docker-compose-1.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Docker Compose - I</em></p>

<p>In <em>Figure 3</em>, you see the start of the <code>docker-compose.yml</code>. If you have done any work using Docker Compose before, nothing there should come as a surprise. Let&rsquo;s have a look at the outlined areas:</p>

<ul>
<li>Yellow: this is the image we use. As you see, I am using the Kafka Connect base image, which contains the bare minimum for Kafka Connect.</li>
<li>Red: defines the Kafka endpoint for the worker process. This is the Kafka endpoint of the Event Hubs namespace. You get the endpoint from the SAS policy&rsquo;s connection string.</li>
<li>Green: Kafka Connect uses topics to store connectors config, offsets, and statuses. As this is Event Hubs, we see the Event Hub names we want to use (they will be auto-created). We also define the replication factor for the event hubs (topics). In Kafka, the default is 3, but Event Hubs works somewhat differently, so we set the replication factor to 1.</li>
</ul>

<p>Let us go to the security part, where it got a bit interesting for me.</p>

<h4 id="security">Security</h4>

<p>In the <a href="/2022/01/02/how-to-use-kafka-client-with-azure-event-hubs/">Kafka Client and Event Hubs</a> post, I discussed using the <code>SASL_SSL</code> security protocol with <code>PLAIN</code> as the mechanism. Using <code>PLAIN</code> gives us a username/password authentication mechanism. I also mentioned how the password should be set to SAS policy&rsquo;s connection string value and the username to the &ldquo;magic&rdquo; string <code>$ConnectionString</code> (notice the dollar sign). Applying that to this post and the SAS policy we created above, the username password &ldquo;combo&rdquo; would look something like this:</p>

<pre><code class="language-python">'sasl.username': &quot;$ConnectionString&quot;,
'sasl.password': &quot;Endpoint=sb://dbzeventhubs.servicebus.windows.net/; \
                  SharedAccessKeyName=KafkaConnect; SharedAccessKey=&lt;secret-key&gt;;&quot;
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>User Name &amp; Password</em></p>

<p>In <em>Code Snippet 2</em> we see how I have taken the SAS policy&rsquo;s connection string and used that for the <code>sasl.password</code> value.</p>

<p>Kafka (brokers, Connect, etc.) uses JAAS (Java Authentication and Authorization Service) for SASL configuration. So when we set up security for Kafka Connect, we must provide JAAS configurations for this, where part of the configuration is username and password. In addition to the JAAS configuration, we need the security protocol and mechanism. With this in mind, connecting to Event Hubs would look like so:</p>

<pre><code class="language-bash">security.protocol=SASL_SSL
sasl.mechanism=PLAIN
sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule \
                   required username=&quot;$ConnectionString&quot; \ 
                   password=&quot;&lt;policy-connection-string&gt;&quot;;
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>JAAS Config</em></p>

<p>We see in <em>Code Snippet 3</em> how we first set the security protocol and the mechanism, followed by the JAAS configuration. The <code>org.apache.kafka.common.security.plain.PlainLoginModule</code> in the JAAS configuration defines the class handling logins using the <code>PLAIN</code> mechanism.</p>

<p>Wow, this was quite a detour; let us try to get back to track and talk about configuring this in a compose file. When configuring security in the compose file, we need to remember that we need to configure the security for the worker process and the connector, where the connector can be consumer, publisher or both.</p>

<blockquote>
<p><strong>NOTE:</strong> In certain circumstances, you do not need to configure the connectors security in the compose file, as you can override it in the connector&rsquo;s configuration. Since I had quite a lot of problems with the security configuration, I did it in the compose file.</p>
</blockquote>

<p>OK, so with all of the above in mind, the security configuration should look something like so:</p>

<pre><code class="language-bash"># connect worker
CONNECT_SECURITY_PROTOCOL: SASL_SSL
CONNECT_SASL_MECHANISM: PLAIN
CONNECT_SASL_JAAS_CONFIG: \ 
         &quot;org.apache.kafka.common.security.plain.PlainLoginModule \ 
          required username=\&quot;$ConnectionString\&quot; \ 
                   password=\&quot;&lt;connection-string-from-policy&gt;\&quot;;&quot;
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Security Config Compose</em></p>

<p>In <em>Code Snippet 4</em>, we see the security configuration for the Kafka Connect worker process. This looks good; just be aware of the <code>\</code> as line continuations and being used for escaping double quotes <code>&quot;</code> inside the JAAS configuration.</p>

<p>I thought this looked good. However, when I tried to &ldquo;spin up&rdquo; the Kafka Connect process, I got strange errors saying the <code>ConnectionString</code> (notice without <code>$</code>) was blank. This was followed by the log file reporting authentication issues.</p>

<p>After a lot of head-scratching, I finally figured out that the problem was <code>$ConnectionString</code>, more specifically the <code>$</code> sign. The dollar sign indicates variable substitution in <code>docker-compose</code>, and when the file is parsed, there is no variable named <code>$ConnectionString</code>. Having finally figured out the issue, it was pretty simple to fix by using <code>$$</code>, which says I actually want to use <code>$</code> as a literal sign.</p>

<p>After all this the security part of the compose file looks like this:</p>

<p><img src="/images/posts/dbz-evthub-docker-compose-2.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Docker Compose - II</em></p>

<p>We see in <em>Figure 4</em> how we configure security for the worker process <code>CONNECT_xxx</code> (outlined in red) and the connector, which acts as a producer: <code>CONNECT_PRODUCER_xxx</code> (outlined in yellow). As discussed earlier, <code>username</code> is set to <code>$$ConnectionString</code>.</p>

<h4 id="connector">Connector</h4>

<p>The connector is the last piece of the <code>docker-compose.yml</code> file, and I looked in the <a href="/2021/08/07/how-to-deploy-the-debezium-sql-server-connector-to-docker/"><strong>How to Deploy the Debezium SQL Server Connector to Docker</strong></a> post at various ways of running the connector in Kafka Connect. In this post, I use the ability to in a <code>docker-compose.yml</code> file to execute arbitrary commands, using the <code>command</code> option:</p>

<p><img src="/images/posts/dbz-evthub-docker-compose-3.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Docker Compose - III</em></p>

<p>In the <code>command</code> option (outlined in blue) in <em>Figure 5</em> we install the Debezium SQL Server connector.</p>

<blockquote>
<p>*NOTE:** If you wonder about <code>confluent-hub</code>, then read more about it <a href="https://docs.confluent.io/home/connect/confluent-hub/">here</a>.</p>
</blockquote>

<p>When you have come this far, you can do a test run. Before you run this, make sure your Event Hubs namespace does not have any event hubs (topics), or at least none with the same names as your storage topics.</p>

<h2 id="test-run">Test Run</h2>

<p>The test run is to ensure that the Docker container &ldquo;comes up&rdquo; properly and the connector is loaded into the Kafka Connect worker.</p>

<p>So:</p>

<ul>
<li>Ensure that Docker is running on your box</li>
<li>Start the container with: <code>docker-compose up -d</code>.</li>
</ul>

<p>To check what is happening, you can use the Docker Desktop app. When you open it up:</p>

<p><img src="/images/posts/dbz-evthub-docker-app-1.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>Docker Desktop App - I</em></p>

<p>When the container starts up, you see what is in <em>Figure 6</em>. More specifically, if you click on the <em>Container / Apps</em> outlined in red, you see the running containers. In our case, it is the <code>dbz-eventhub-cont</code> outlined in red. If you want to drill down further, you click on the container (outlined in red):</p>

<p><img src="/images/posts/dbz-evthub-docker-app-log.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>Docker Desktop App - II</em></p>

<p>Having clicked on the container, you now see the log for the container as in <em>Figure 7</em>. This is a great help when trying to figure out issues.</p>

<p>OK, so looking at the logs, we do not see anything strange, so let us check two more things: that event hubs have been created, and the connector has loaded.</p>

<h4 id="event-hubs-topics">Event Hubs (topics)</h4>

<p>In the Azure portal, browse to your namespace and click on the Event Hubs menu:</p>

<p><img src="/images/posts/dbz-evthub-topics-1.png" alt="" /></p>

<p><strong>Figure 8:</strong> <em>Created Event Hubs</em></p>

<p>In <em>Figure 8</em>, we see that the event hubs we defined for configs, offsets and status have all been created when the container started. Looking good so far.</p>

<h4 id="connector-1">Connector</h4>

<p>The last thing we want to check is whether the connector is loaded. Kafka Connect exposes a REST API allowing us to configure/manage/etc. our connectors. To check whether the SQL Server connector is loaded, we use a <code>GET</code> call:</p>

<pre><code class="language-bash">GET http://127.0.0.1:8083/connector-plugins
</code></pre>

<p><strong>Code Snippet 5:</strong> <em>GET Connectors</em></p>

<p>In <em>Code Snippet 5</em> we see how we execute a <code>GET</code> call into <code>localhost</code> (as we host Kafka Connect on our box):</p>

<p><img src="/images/posts/dbz-evthub-installed-connectors.png" alt="" /></p>

<p><strong>Figure 9:</strong> <em>GET Installed Connectors</em></p>

<p>The result of the call in <em>Code Snippet 5</em> is what we see in <em>Figure 9</em>: we see &ldquo;our&rdquo; SQL Server connector together with three other connectors. These other three connectors are all part of the base image we use.</p>

<p>The container is up and running, the event hubs for the Kafka Connect worker has been successfully created, and the connector has loaded! What remains to be done is configuring the connector to capture data from our table. I leave that to the next post, so let us just sum up what we have done so far.</p>

<h2 id="summary">Summary</h2>

<p>We set out to prove/disprove the ability to stream data from Debezium to Azure Event Hubs. We still haven&rsquo;t proven that it is possible, but we have come a bit on the way.</p>

<p>The main takeaways from this post are:</p>

<ul>
<li>When configuring the security, you do it both for the Kafka Connect worker process and the connector.</li>
<li>When setting up a username/password, you use a JAAS configuration.</li>
<li>When using SASL with Event Hubs, the username is hard-coded to <code>$ConnectionString</code>. Using <code>docker-compose.yml</code> you need to use two dollar signs: <code>$$ConnectionString</code>. You can read more about it <a href="https://docs.docker.com/compose/environment-variables/#substitute-environment-variables-in-compose-files">here</a>.</li>
</ul>

<p>In the <a href="/2022/01/14/how-to-stream-data-to-event-hubs-from-databases-using-kafka-connect--debezium-in-docker---ii/">next post</a>, we look at configuring the connector, and we will see whether Debezium to Event Hubs actually works.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or [ping][ma] me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 1, 2022]]></title>
    <link href="https://nielsberglund.com/2022/01/09/interesting-stuff---week-1-2022/" rel="alternate" type="text/html"/>
    <updated>2022-01-09T10:55:27+02:00</updated>
    <id>https://nielsberglund.com/2022/01/09/interesting-stuff---week-1-2022/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="databases">Databases</h2>

<ul>
<li><a href="https://ajay-yadav.medium.com/database-storage-engines-de757b03bd44">Database storage engines</a>. This is an awesome post, looking at various database storage engines!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/what-we-learned-building-confluent-cloud/">Building Confluent Cloud â€“ Here&rsquo;s What We&rsquo;ve Learned</a>. I was pointed to this post by a smart guy on the Confluent Cloud Slack channel - thanks, Ryan! The post explores the current architecture of Confluent Cloud, how the experience with the product has benefitted both Apache Kafka and Kafka in Confluent Cloud. Finally, the post lists some of the lessons learned.</li>
<li><a href="https://www.kai-waehner.de/blog/2022/01/04/when-not-to-use-apache-kafka/">When NOT to use Apache Kafka?</a>. So, <a href="https://twitter.com/kaiwaehner">Kai Waehner</a> is a Global Technology Advisor at Confluent and evangelises Apache Kafka and Confluent Cloud. Therefore, it is very cool to see this post, where he looks at scenarios where Kafka may not fit.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>A while ago, I wrote the post <a href="/2022/01/02/how-to-use-kafka-client-with-azure-event-hubs/"><strong>How to Use Kafka Client with Azure Event Hubs</strong></a>, where I looked at how to use the Kafka client to publish messages to - not only - <strong>Apache Kafka</strong> but also <strong>Azure Event Hubs</strong>. In my post, I said something like:</p>

<p><em>An interesting point here is that it is not only your Kafka applications that can publish to Event Hubs but any application that uses Kafka Client 1.0+, like Kafka Connect connectors!</em></p>

<p>Obviously (if you know me), I said that without having tested it properly, but: <em>how hard can it be? What could possibly go wrong?</em>. Well, I was called upon it by a guy who had read the post. He told me he had tried what I said at one time or the other, and it hadn&rsquo;t worked. Therefore I started writing a post looking at using Debezium and Kafka Connect to publish events to Event Hubs. The jury is still out whether you can do it or not. So, that is what I am doing right now.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Christmas, New Year 2021]]></title>
    <link href="https://nielsberglund.com/2022/01/02/interesting-stuff---christmas-new-year-2021/" rel="alternate" type="text/html"/>
    <updated>2022-01-02T12:36:02+02:00</updated>
    <id>https://nielsberglund.com/2022/01/02/interesting-stuff---christmas-new-year-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This is the &ldquo;roundup&rdquo; of the posts that have been most interesting to me over the Christmas and New Year period 2021. Oh, and welcome to 2022! I wonder what this year will have in store for us - after the last two years I will not even think about what will happen!</p>

<p></p>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://www.starburst.io/info/data-mesh-in-practice-ebook/">Data Mesh in Practice</a>. In my mind, 2021 was the year of the Data Mesh. Many people started talking about the Data Mesh and what it was. Fewer people actually discussed the practical steps to implement a Data Mesh. That is until the end of the year when the book linked to was published. The book is for you who wonders how to implement a Data Mesh and contains strategic guidance around implementing a Data Mesh. I, for one, find the book very valuable!</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://aaronstannard.com/opentelemetry-dotnet6/">An Overview of Distributed Tracing with OpenTelemetry in .NET 6</a>. OpenTelemetry is a collection of tools, APIs, and SDKs to help you analyze your software&rsquo;s performance and behaviour. This post, as the title implies, explores OpenTelemetry tracing in .NET 6</li>
</ul>

<h2 id="azure-data-explorer">Azure Data Explorer</h2>

<ul>
<li><a href="https://techcommunity.microsoft.com/t5/azure-data-explorer-blog/automatically-stop-unused-azure-data-explorer-clusters/ba-p/3047042">Automatically stop unused Azure Data Explorer Clusters</a>. The linked-to post announces an Azure Data Explorer cluster feature that I have wished for, how ADX clusters can automatically stop when not in use. This will really reduce costs!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/how-to-build-a-data-mesh-using-event-streams/">The Definitive Guide to Building a Data Mesh with Event Streams</a>. Hmm, maybe I was wrong when I, above, said there were not much around implementing a Data Mesh. This blog post looks at the practical steps of implementing a Data Mesh based on Kafka and streaming data. Very interesting!</li>
<li><a href="https://www.infoq.com/articles/microservices-inside-out/">Turning Microservices Inside-Out</a>. This <a href="https://www.infoq.com/">InfoQ</a> article provides a lot of good insight into designing and building for emitting data from microservices APIs.</li>
<li><a href="https://twitter.com/tlberglund">I Interviewed Nearly 200 Apache Kafka Experts and I Learned These 10 Things</a>. This post by <a href="https://twitter.com/tlberglund">Tim Berglund</a> (no, we&rsquo;re not related) is a top 10 of podcasts Tim has conducted with Kafka experts. There is some very cool stuff in there!</li>
<li><a href="https://www.confluent.io/blog/from-apache-kafka-to-confluent-cloud-optimizing-for-speed-scale-storage/">Speed, Scale, Storage: Our Journey from Apache Kafka to Performance in Confluent Cloud</a>. In this post, the authors share their experience optimizing Apache Kafka for Confluent Cloud. There are quite a few &ldquo;tips and tricks&rdquo; that all of us can use! Awesome!</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>I have had a severe bout of writer block in the last few months! It is not that I don&rsquo;t have anything to write about; I just can&rsquo;t get the words out there.</p>

<p>So, therefore I am pleased to say that I earlier today (Sunday, Jan 2, 2022) published a new post:</p>

<ul>
<li><a href="/2022/01/02/how-to-use-kafka-client-with-azure-event-hubs/"><strong>How to Use Kafka Client with Azure Event Hubs</strong></a>. As the title implies, I look at how one can use the Kafka client to publish to Event Hubs in the post.</li>
</ul>

<p>Hopefully, having managed to get that post out, I may be able to do others as well.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to Use Kafka Client with Azure Event Hubs]]></title>
    <link href="https://nielsberglund.com/2022/01/02/how-to-use-kafka-client-with-azure-event-hubs/" rel="alternate" type="text/html"/>
    <updated>2022-01-02T09:48:17+02:00</updated>
    <id>https://nielsberglund.com/2022/01/02/how-to-use-kafka-client-with-azure-event-hubs/</id>
    <content type="html"><![CDATA[<p>This blog post came by, by accident, lol. A couple of weeks ago, I started to prepare for a webinar: <a href="https://www.meetup.com/SQLCape-Meetup/events/282241220/"><strong>Analyze Billions of Rows of Data in Real-Time Using Azure Data Explorer</strong></a>. One of the demos in that webinar is about ingesting data from Apache Kafka into <strong>Azure Data Explorer</strong>. When prepping, I noticed that my Confluent Cloud Kafka cluster didn&rsquo;t exist anymore, so I had to come up with a workaround. That workaround was to use <strong>Azure Event Hubs</strong> instead of Kafka.</p>

<p>Since I already had the code to publish data to Kafka and knew that you could use the Kafka client to publish to Event Hubs, I thought I&rsquo;d test it out. I did run into some minor snags along the way, so I thought I&rsquo;d write a blog post about it. Then, at least, I have something to go back to. This post also looks at how to set up an Event Hubs cluster.</p>

<p></p>

<h2 id="background">Background</h2>

<p>Azure Event Hubs is similar to Apache Kafka in that it is a big data streaming platform and event ingestion service. It is a fully managed Platform-as-a-Service (PaaS) with little configuration or management overhead, very much like Apache Kafka in <strong>Confluent Cloud</strong>. The one difference between Azure Event Hubs and Apache Kafka is that Event Hubs does not have an on-prem solution.</p>

<h4 id="kafka-vs-event-hubs-concepts">Kafka vs Event Hubs Concepts</h4>

<p>Event Hubs and Kafka are pretty similar, as I mentioned above. Let us compare the concepts of the two:</p>

<table>
<thead>
<tr>
<th>Kafka</th>
<th>Event Hubs</th>
</tr>
</thead>

<tbody>
<tr>
<td>Cluster</td>
<td>Namespace</td>
</tr>

<tr>
<td>Topic</td>
<td>EventHub</td>
</tr>

<tr>
<td>Partition</td>
<td>Partition</td>
</tr>

<tr>
<td>Consumer Group</td>
<td>Consumer Group</td>
</tr>

<tr>
<td>Offset</td>
<td>Offset</td>
</tr>
</tbody>
</table>

<p><strong>Table 1:</strong> <em>Kafka vs Event Hubs Concepts</em></p>

<p>As we see in <em>Table 1</em>, there is not much difference between Kafka and Event Hubs. The one difference worth noting is the Event Hubs namespace instead of the Kafka cluster.</p>

<h4 id="namespace">Namespace</h4>

<p>An Event Hubs namespace is a dedicated scoping container for event hubs, where an event hub as mentioned above is the equivalent to a Kafka topic. We can see it as a management container for individual event hubs (topics), and it provides a range of access control and network integration management features.</p>

<p>For this post, the important part is that the namespace provides IP endpoints allowing us to publish to the namespace and its individual event hubs (topics).</p>

<h4 id="event-hubs-kafka-endpoint">Event Hubs Kafka Endpoint</h4>

<p>One of the endpoints the namespace provides is an endpoint compatible with the Apache Kafka producer and consumer APIs at version 1.0 and above.</p>

<p>So if your application uses a Kafka client version 1.0+, you can use the Event Hubs Kafka endpoint from your applications without code changes, apart from configuration, compared to your existing Kafka setup.</p>

<blockquote>
<p><strong>NOTE:</strong> When I above say you only need to change the configuration, I assume that the Kafka topics have corresponding EventHubs (remember Kafka topic = Event Hubs EventHub).</p>
</blockquote>

<p>An interesting point here is that it is not only your Kafka applications that can ~consume from~ publish to Event Hubs but any application that uses Kafka Client 1.0+, like Kafka Connect connectors!</p>

<h2 id="pre-reqs">Pre-Reqs</h2>

<p>This section is here if you want to follow along. This is what you need:</p>

<ul>
<li>An Azure account. If you don&rsquo;t have an Azure subscription, sign up for a <a href="https://azure.microsoft.com/en-us/free/">free account</a>.</li>
<li>Your favourite development language. In this post, I use Python.</li>
<li>Kafka client for your favourite language as per above. <a href="https://docs.confluent.io/platform/current/clients/index.htm">Here</a> you find a list of clients.</li>
<li>Application publishing to Kafka. The idea is that you can switch from publishing to Kafka to publish to Event Hubs in this application.</li>
</ul>

<p>Ensure the client is installed. In Python, I do it using <code>pip</code>, like so:</p>

<pre><code class="language-python">pip install confluent-kafka
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Install the Client</em></p>

<p>After having run the code in <em>Code Snippet 1</em>, or similar code for your language, you are good to go.</p>

<h4 id="kafka-application">Kafka Application</h4>

<p>Above I mentioned about an application publishing to Kafka being optional. Let us here take a look at a very simple example.</p>

<p>In this example, I have an Azure Confluent Cloud cluster looking like so:</p>

<p><img src="/images/posts/evthub-adx-confluent-cloud.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Confluent Cloud Cluster</em></p>

<p>In <em>Figure 1</em>, we see my Confluent Cloud cluster named <code>kafkaeventhubs</code> and how that cluster has one topic - <code>testtopic</code> - with four partitions.</p>

<blockquote>
<p><strong>NOTE:</strong> To see how to run Confluent Cloud in Azure, see my post <a href="/2021/08/14/run-confluent-cloud--serverless-apache-kafka-on-azure/"><strong>Run Confluent Cloud &amp; Serverless Apache Kafka on Azure</strong></a>.</p>
</blockquote>

<p>I have a very basic Python application publishing to the <code>testtopic</code> looking like so:</p>

<p><img src="/images/posts/evthub-adx-kafka-app.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Python Publish App</em></p>

<p>As we see in <em>Figure 2</em> the app is not doing anything advanced. It publishes a comma-delimited message containing the random key value, &ldquo;Hello World&rdquo;, and a timestamp.</p>

<p>What is interesting in the code is the configuration string (assigned to the variable <code>conf</code>) in lines 14 - 19, and specifically the following properties:</p>

<ul>
<li><code>bootstrap.servers</code>: The cluster endpoint.</li>
<li><code>security.protocol</code>: The protocol used to communicate with brokers. In this case, we use <code>SASL_SSL</code>, which uses SASL for authentication and SSL for encryption.</li>
<li><code>sasl.mechanism</code>: This indicates how we authenticate. By setting it to <code>PLAIN</code>, we use a &ldquo;simple&rdquo; username/password based authentication mechanism.</li>
<li><code>sasl.username</code>: The username to use. In Confluent Cloud, we use an API key mechanism, where the <code>username</code> is the API key, and the password is the API key&rsquo;s secret.</li>
<li><code>sasl.password</code>: As per above, this is the API key&rsquo;s secret.</li>
</ul>

<p>Why I say these are interesting is these are the ones that are in play if we want to publish to Event Hubs.</p>

<p>Having seen the pre-reqs lets us create a namespace in Event Hubs.</p>

<h2 id="create-event-hubs-namespace">Create Event Hubs Namespace</h2>

<p>There are multiple ways we can create an Event Hubs resource, where the Azure Portal is one of them.</p>

<p>Instead of doing a step-by-step explanation of creating an Event Hubs namespace, I suggest you read the Microsoft article <a href="https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-create"><strong>Quickstart: Create an event hub using Azure portal</strong></a>. While you are doing that, I will create an Event Hubs test namespace for this post.</p>

<blockquote>
<p><strong>NOTE:</strong> When you create the namespace, you have to ensure you create it under the <em>Standard</em> pricing tier (or higher), as <em>Basic</em> does not support Kafka.</p>
</blockquote>

<p>We are all back? Cool, I ended up with this:</p>

<p><img src="/images/posts/evthub-adx-topics-1.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Event Hubs Namespace with EventHub</em></p>

<p>As you see in <em>Figure 3</em> I ended up with an Event Hubs namespace called <code>kafkaeventhubs</code> and an Event Hub named <code>testtopic</code>, the same as I have in Confluent Cloud.</p>

<h2 id="event-hubs-security-authentication">Event Hubs Security &amp; Authentication</h2>

<p>So now we have all we need to replace Kafka with Event Hubs - almost. We still need to see how to configure the Event Hubs security and authentication.</p>

<p>When using Event Hubs, all data in transit is TLS encrypted, and we can satisfy that by using <code>SASL_SSL</code>. This is exactly as in the Kafka code. Using <code>SASL_SSL</code> we have basically two options for authentication: OAuth 2.0 or Shared Access Signature (SAS). In this post, I use SAS, which matches what I do using Kafka.</p>

<h4 id="create-shared-access-signature">Create Shared Access Signature</h4>

<p>In Event Hubs, we can create a SAS for either a namespace or an individual EventHub in a namespace. Let us create a SAS for our <code>testtopic</code> Event Hub.</p>

<p>If you click into the <code>testtopic</code> Event Hub we see in <em>Figure 3</em> and look at the left-hand side; you see something like so:</p>

<p><img src="/images/posts/evthub-adx-topic-sas.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Event Hub Shared Access Policies</em></p>

<p>We see under <em>Settings</em> in <em>Figure 4</em> <em>Shared access policies</em> (outlined in red). When we click on it:</p>

<p><img src="/images/posts/evthub-adx-topic-sas-2.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Shared Access Policies</em></p>

<p>We are being told that we don&rsquo;t have any policies set up, as outlined in yellow in <em>Figure 5</em>. Cool, let us create a policy. We do that by clicking on the <em>+ Add</em> button, which is outlined in red in <em>Figure 5</em>:</p>

<p><img src="/images/posts/evthub-adx-topic-sas-create.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>Add Policy</em></p>

<p>By clicking the <em>+ Add</em> button, we get a dialog <em>Add SAS Policy</em> as shown in <em>Figure 6</em>. We give it a name and the claims (what it allows), and then we click the <em>Create</em> button at the bottom of the dialog (not shown in <em>Figure 6</em>):</p>

<p><img src="/images/posts/evthub-adx-topic-sas-created.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>Policy Created</em></p>

<p>In <em>Figure 7</em>, we see the generated policy and the claims. Clicking on the policy, we get yet another dialog:</p>

<p><img src="/images/posts/evthub-adx-topic-sas-policy-keys.png" alt="" /></p>

<p><strong>Figure 8:</strong> <em>Policy Keys &amp; Connection Strings</em></p>

<p>The policy we created consists of two keys and corresponding connection strings as in <em>Figure 8</em>. The reason for having two is that in a production environment, you may want to cycle and regenerate the keys, so while you regenerate one, you can use the other.</p>

<p>Copy one of the connection strings as that is what we use for the Kafka client configuration:</p>

<p><img src="/images/posts/evthub-adx-topic-sas-policy-connstring.png" alt="" /></p>

<p><strong>Figure 9:</strong> <em>Connection String</em></p>

<p>In <em>Figure 9</em>, we see one of my connection strings. I have outlined in red the endpoint URL, and this is the one we use for the <code>bootstrap.servers</code> property in the configuration.</p>

<p>Now we should have everything we need to use the Kafka client to publish to the EventHub.</p>

<h2 id="configure-kafka-client">Configure Kafka Client</h2>

<p>Above I listed the configuration properties used when I connect to Confluent Cloud. Let us see what they should be when publishing to Event Hubs:</p>

<h4 id="bootstrap-servers">Bootstrap Servers</h4>

<p>The <code>bootstrap.servers</code> property defines the endpoint(s) where the client connects to. In <em>Figure 9</em>, I outlined the endpoint URL and said it would be used for <code>bootstrap.servers</code>. At the beginning of this post, I mentioned how Event Hubs exposes a Kafka endpoint. It does that on port <code>9093</code>. So:</p>

<pre><code class="language-python">`&quot;bootstrap.servers&quot;: &quot;kafkaeventhubs.servicebus.windows.net:9093&quot;`
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Bootstrap Servers</em></p>

<p>Onto the security and authorization part.</p>

<h4 id="security-protocol-sasl-mechanisms">Security Protocol` &amp; SASL Mechanisms</h4>

<p>As we use Shared Access Signature, We do not need to change <code>security.protocol</code> or <code>sasl.mechanism</code>; we keep them as <code>SASL_SSL</code> and <code>PLAIN</code>, respectively..</p>

<h4 id="username-password">Username &amp; Password</h4>

<p>At the very beginning of this post, I mentioned how I ran into some snags when trying to use the Kafka client to publish to Event Hubs. This is the part that caused me issues.</p>

<p>Reading documentation and blog posts when trying to connect to Event Hubs, I concluded that <code>sasl.password</code> should be set to the whole connection string you get from the SAS policy. OK, that&rsquo;s cool - but what about the user name?</p>

<p>Posts and docs talk about using <code>$ConnectionString</code>, but <code>$ConnectionString</code> looked like a variable to me, and I could not see where it was set. It finally dawned upon me that the user name property should literally be set to <code>$ConnectionString</code> - duh. So:</p>

<pre><code class="language-python">'sasl.username': &quot;$ConnectionString&quot;,
'sasl.password': &quot;Endpoint= sb://kafkaeventhubs.servicebus.windows.net/; \
                      SharedAccessKeyName=publishconsumepolicy; \
                      SharedAccessKey=&lt;secret-key&gt;, 
                      EntityPath=testtopic&quot;

</code></pre>

<p><strong>Code Snippet 3:</strong> <em>User Name &amp; Password</em></p>

<p>In <em>Code Snippet 3</em>, we see how <code>sasl.password</code> is set to the SAS policy&rsquo;s connection string and <code>sasl.username</code> to <code>$ConnectionString</code>.</p>

<p>The complete configuration required to connect to and publish to Event Hubs looks like so:</p>

<pre><code class="language-python">conf = {'bootstrap.servers': 'kafkaeventhubs.servicebus.windows.net:9093',
         'security.protocol': 'SASL_SSL',
         'sasl.mechanisms': 'PLAIN',
         'sasl.username': &quot;$ConnectionString&quot;,
         'sasl.password': &quot;Endpoint= \
                      sb://kafkaeventhubs.servicebus.windows.net/; \
                      SharedAccessKeyName=publishconsumepolicy; \
                      SharedAccessKey=&lt;secret-key&gt;; \
                      EntityPath=testtopic&quot;,
         'client.id': socket.gethostname()}

</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Event Hubs Configuration</em></p>

<p>You can now replace lines 14 - 19 that we see in <em>Figure 2</em> with what we have in <em>Code Snippet 4</em>, and you are good to go!</p>

<h2 id="publish-to-event-hubs">Publish to Event Hubs</h2>

<p>After editing the <code>conf</code> variable, you can run the application. Let it publish some messages and then check what you see in the overview for the <code>testtopic</code> Event Hub:</p>

<p><img src="/images/posts/evthub-adx-publish.png" alt="" /></p>

<p><strong>Figure 10:</strong> <em>Publish Events</em></p>

<p>There is no straightforward way to see if messages come into the Event Hub, so we look at the request stats. In <em>Figure 10</em>, we see how events have come into the Event Hub. Yay - it seems like it works!</p>

<p>I said above that there is no straightforward way to see if messages/events arrive into the Event Hub. Well, when we now know how to connect and publish to Event Hubs using the Kafka client, we could easily create a consuming application. However, I leave that up to you.</p>

<h2 id="summary">Summary</h2>

<p>In this post, we have seen how to use the Kafka client to connect and publish messages to an Azure Event Hub.</p>

<p>We compared the terminology of Kafka with Event Hubs, and saw that it is more or less the same. The two major differences are:</p>

<ol>
<li>In Kafka, we talk about clusters, whereas in Event Hubs, we have namespaces.</li>
<li>A topic in Kafka is called an Event Hub in Event Hubs.</li>
</ol>

<p>We use port <code>9093</code> on the Event Hubs endpoint to connect the Kafka client. When using <code>SASL_SSL</code> and the <code>PLAIN</code> <code>sasl.mechanism</code>, the user name we use is<code>$ConnectionString</code>, and the password is the entire connection string from the Event Hub&rsquo;s Shared Access Signature policy.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or [ping][ma] me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 50, 2021]]></title>
    <link href="https://nielsberglund.com/2021/12/12/interesting-stuff---week-50-2021/" rel="alternate" type="text/html"/>
    <updated>2021-12-12T10:37:02+02:00</updated>
    <id>https://nielsberglund.com/2021/12/12/interesting-stuff---week-50-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<blockquote>
<p><strong>NOTE:</strong> <em>It is now coming up on Christmas and New Year, and I will take a break with these posts and come back at the beginning of next year.</em></p>
</blockquote>

<p></p>

<h2 id="sql-server">SQL Server</h2>

<ul>
<li><a href="http://sql-sasquatch.blogspot.com/2021/12/sqlserver-column-store-object-pool.html">#SQLServer Column Store Object Pool &ndash; the Houdini Memory Broker Clerk AND Perfmon [\SQLServer:Buffer Manager\Target pages]</a>. In this post by Mr SQL Server NUMA, <a href="https://twitter.com/sqL_handLe">Lonny</a>, he &ldquo;spelunks&rdquo; around in SQL Server Buffer Pool. If you are interested in the &ldquo;innards&rdquo; of SQL Server, you need to read this post. Actually, you need to read everything Lonny <a href="http://sql-sasquatch.blogspot.com/">posts</a>.</li>
</ul>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://engineering.linkedin.com/blog/2021/evolving-linkedin-s-analytics-tech-stack">Evolving LinkedIn&rsquo;s analytics tech stack</a>. This is a fascinating post looking at lessons learned from LinkedIn&rsquo;s data platform migration. This post is a goldmine of information for anyone migrating from &ldquo;legacy&rdquo; data architecture to a modern one.</li>
<li><a href="https://databricks.com/blog/2021/12/06/deploying-dbt-on-databricks-just-got-even-simpler.html">Deploying dbt on Databricks Just Got Even Simpler</a>. Those interested in Big Data have probably heard about <a href="https://www.getdbt.com/"><strong>dbt</strong></a>, the open-source tool that allows you to build data pipelines using simple SQL. The post I link to announces the <strong>dbt-databricks</strong> adapter, which integrates dbt with the Databricks Lakehouse Platform. Cool stuff!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.infoq.com/news/2021/12/huyen-realtime-ml/">Chip Huyen on Streaming-First Infrastructure for Real-Time ML</a>. Even though you may do real-time ML predictions, you probably update your models manually. This <a href="https://www.infoq.com/">InfoQ</a> article looks at a QCon presentation where the presenter looked at, among other things, how a streaming-first infrastructure can help you do ML in real-time, both online prediction and continual learning.</li>
<li><a href="https://www.kai-waehner.de/blog/2021/12/08/apache-kafka-for-conversational-ai-nlp-chatbot/">Apache Kafka for Conversational AI, NLP and Chatbot</a>. The post looks at how event streaming with Apache Kafka is used in conjunction with Machine Learning platforms for reliable real-time conversational AI, NLP, and chatbots. The post looks at examples from the carmaker BMW, the online travel and booking portal Expedia, and Tinder&rsquo;s dating app. Very cool!</li>
<li><a href="https://www.confluent.io/blog/serverless-event-stream-processing/">Serverless Stream Processing with Apache Kafka, AWS Lambda, and ksqlDB</a>. This blog post defines what &ldquo;serverless stream processing&rdquo; is. Apart from just discussing concepts and implementations, it describes arguably the most essential pattern for building event streaming applications using ksqlDB. Read It!</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>The year is coming to a close, and as for presentations, webinars, etc., I have two left:</p>

<p><img src="/images/posts/sql-cape-adx.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>SQL Cape - Azure Data Explorer</em></p>

<p>On Tuesday (Dec. 14), I deliver the last <strong>Azure Data Explorer</strong> presentation for this year:</p>

<ul>
<li><a href="https://www.meetup.com/SQLCape-Meetup/events/282241220/"><strong>Analyze Billions of Rows of Data in Real-Time Using Azure Data Explorer</strong></a>. The presentation is a virtual event hosted by my mate <a href="https://www.linkedin.com/in/jodyrobertssql/"><strong>Jody Roberts</strong></a>. If you are interested in ADX, please <a href="https://www.meetup.com/SQLCape-Meetup/events/282241220/">sign up</a> (it is <strong>FREE</strong>) and come and join the fun. Any time Jody and I get together, regardless if it is IRL or a virtual event like this, some fun stuff happens!</li>
</ul>

<p>The second event is also virtual:</p>

<p><img src="/images/posts/tech-fun.jpg" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Tech Fun Space</em></p>

<p>The event takes place Thursday, Dec. 23. It is not a webinar but an event for the Global Data Community to get together to welcome 2022. The organiser is my good friend <a href="https://www.linkedin.com/in/jeandjoseph/"><strong>Jean Joseph</strong></a>. Read more about it (this event is also <strong>FREE</strong>) and <a href="https://www.eventbrite.com/e/tech-fun-space-welcome-2022-tickets-215046428657">sign up here</a>.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>

<p>Oh, and if I don&rsquo;t see you virtually or IRL before the holidays: <strong>Happy Holidays</strong>!</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 49, 2021]]></title>
    <link href="https://nielsberglund.com/2021/12/05/interesting-stuff---week-49-2021/" rel="alternate" type="text/html"/>
    <updated>2021-12-05T08:51:58+02:00</updated>
    <id>https://nielsberglund.com/2021/12/05/interesting-stuff---week-49-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://databricks.com/blog/2021/12/01/the-foundation-of-your-lakehouse-starts-with-delta-lake.html">The Foundation of Your Lakehouse Starts With Delta Lake</a>. The Databricks Delta Lake has continuously evolved during the last few years, and in May 2021, Delta Lake 1.0 was announced. The evolution of Delta Lake doesn&rsquo;t stop with the 1.0 release, and this blog post reviews the major features released so far and provides an overview of the upcoming roadmap.</li>
<li><a href="https://www.theseattledataguy.com/what-is-trino-and-why-is-it-great-at-processing-big-data/">What Is Trino And Why Is It Great At Processing Big Data</a>. Trino is an open-source distributed SQL query engine for ad-hoc and batch ETL queries against multiple types of data sources. It previously went under the name of Presto, but due to various reasons, it had to change its name. The post linked to looks at Trino and covers its positives and negatives. At <a href="/derivco">Derivco</a> we have contemplated using Trino. Let us see what the future brings.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://developer.confluent.io/podcast/ksqldb-fundamentals-how-apache-kafka-sql-and-ksqldb-work-together-ft-simon-aubury">ksqlDB Fundamentals: How Apache Kafka, SQL, and ksqlDB Work Together ft. Simon Aubury</a>. This link is to a podcast where <a href="https://twitter.com/tlberglund">Tim Berglund</a> talks to <a href="https://twitter.com/simonaubury">Simon Aubury</a> about everything ksqlDB. They cover basic ksqlDB, plus they look at how to use ksqlDB to find out which aeroplane wakes Simon&rsquo;s cat each morning. Very interesting!</li>
<li><a href="https://www.infoq.com/presentations/raft-kafka-api/">Co-Designing Raft + Thread-per-Core Execution Model for the Kafka-API</a>. This <a href="https://www.infoq.com/">InfoQ</a> presentation looks at, as the title says, codesign in Raft on a thread per core model for the Kafka API. This presentation is a must-see if you are interested in building low-latency software.</li>
<li><a href="https://www.confluent.io/blog/guide-to-stream-processing-and-ksqldb-fundamentals/">A Guide to Stream Processing and ksqlDB Fundamentals</a>. ksqlDB allows you to build applications that react to events as they happen and to take advantage of real-time data. Even though you use familiar SQL syntax when building your ksqlDB application, you might want some help. This post talks about the <strong>ksqlDB 101</strong> course on Confluent Developer, which offers both lectures and hands-on exercises.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>SQLBITS 2022 - The Greatest Data Show - is just around the corner, and I am happy to announce that I am doing a full-day training session:</p>

<p><img src="/images/posts/sqlbits-precon-adx.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>SQLBITS 2022 - A Day of Azure Data Explorer</em></p>

<p>Yes, I am doing a whole day of <strong>Azure Data Explorer</strong>. Read more at: <a href="https://sqlbits.com/information/event22/A_Day_of_Azure_Data_Explorer/trainingdetails"><strong>A Day of Azure Data Explorer</strong></a>.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
</feed>

