<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Niels Berglund</title>
  <link href="https://nielsberglund.com/atom.xml" rel="self"/>
  <link href="https://nielsberglund.com"/>
  <updated>2020-03-22T09:38:19+02:00</updated>
  <id>https://nielsberglund.com/</id>
  <generator uri="http://gohugo.io/">Hugo</generator>

  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 12, 2020]]></title>
    <link href="https://nielsberglund.com/2020/03/22/interesting-stuff---week-12-2020/" rel="alternate" type="text/html"/>
    <updated>2020-03-22T09:38:19+02:00</updated>
    <id>https://nielsberglund.com/2020/03/22/interesting-stuff---week-12-2020/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="misc">Misc.</h2>

<ul>
<li><a href="https://medium.com/better-programming/six-rules-for-good-git-hygiene-5006cf9e9e2">Six Rules for Good Git Hygiene</a>. My Git knowledge is minimal, so this post about how to be a team player with commits, pushes, and pulls was really useful for me.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://medium.com/better-programming/how-to-use-stateful-operations-in-kafka-streams-1cff4da41329">How to Use Stateful Operations in Kafka Streams</a>. The post linked to here explores stateful operations in the Kafka Streams DSL API. It focuses on aggregation operations such as <code>aggregate</code>, <code>count</code>, and <code>reduce</code> along with a discussion of related concepts. The post is the second in a series about Kafka Streams API, and the first post is <a href="https://medium.com/better-programming/learn-stream-processing-with-kafka-streams-stateless-operations-2111080e6c53">here</a>.</li>
<li><a href="https://medium.com/better-programming/testing-kafka-streams-applications-1c5cb14c5376">How to Test Kafka Streams Applications</a>. This post is the third in the series mentioned above, and the post explores a few examples of how to use the testing utilities provided by Kafka streams to validate topologies based on the Kafka Streams DSL API.</li>
<li><a href="https://www.confluent.io/blog/ksqldb-0-8-0-feature-updates/">Announcing ksqlDB 0.8.0</a>. As the title of the post says, the post covers the new version of ksqlDB. There are quite a few new features that I cannot wait to test!</li>
<li><a href="https://github.com/confluentinc/ksql/blob/7bf5c120c7128d4825f74fc7e691ef2c68c6911b/design-proposals/klip-15-new-api-and-client.md">KLIP 15 - ksqlDB Client and New Server API</a>. From reading the announcement above about ksqlDB 0.8.0, I came across the page linked to here. Getting new ksqlDB client and server APIs to make stream developing easier is a game changer! I, for one, am eager to hear more about this. Oh, and <a href="https://groups.google.com/forum/#!topic/ksql-dev/yxcRlsOsNmo">here</a> is a lengthy Google Group discussion around this.</li>
</ul>

<blockquote>
<p><strong>NOTE:</strong> KLIP stands for &ldquo;Kafka Language Improval Proposal&rdquo;</p>
</blockquote>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>What a weird week this has been all thanks to the Coronavirus! On Sunday, (Mar 15), the president of South Africa - Cyril Ramaphosa - declared the Coronavirus a national disaster and more or less put the country in lock-down.</p>

<p>This resulted in that on Monday morning at <a href="/derivco">Derivco</a>, everyone that had remote working capabilities was sent home. For the remainder of staff, plans were made how to get them up-and-running remotely. At the end of the week most of the company, (~1800 people), worked from home. Way to go <a href="/derivco">Derivco</a>!</p>

<p>So since Monday afternoon, I have been working from home, and even though it is a bit strange initially, the whole experience has been overall positive.</p>

<p>You may remember how I in <a href="/2020/03/15/interesting-stuff---week-11-2020/">last weeks roundup</a> wrote that I am &ldquo;blogifying&rdquo; one of the conference talks I do about SQL Server 2019 Big Data Cluster: <em>A Lap Around SQL Server 2019 Big Data Cluster</em>. Working from home has not given me any more time with the post, but I am nearly there, and it should be out in the next week.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me. Most importantly stay safe out there!</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 11, 2020]]></title>
    <link href="https://nielsberglund.com/2020/03/15/interesting-stuff---week-11-2020/" rel="alternate" type="text/html"/>
    <updated>2020-03-15T09:37:09+02:00</updated>
    <id>https://nielsberglund.com/2020/03/15/interesting-stuff---week-11-2020/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://itnext.io/scalable-microservice-demo-k8s-istio-kafka-344a2610eba3">Scalable Microservice Demo K8s Istio Kafka</a>. In this post, the author shows a very &ldquo;cool&rdquo; example of how one can auto-scale applications in Kubernetes, using a Horizontal Pod Autoscaler, (HPA), and Istio.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://towardsdatascience.com/deploying-ml-models-in-distributed-real-time-data-streaming-applications-217954a0b423">Deploying ML Models in Distributed Real-time Data Streaming Applications</a>. This post explores the various strategies to deploy ML models in Apache Flink/Spark or other realtime data streaming applications.</li>
<li><a href="https://www.confluent.io/blog/learn-stream-processing-with-kafka-tutorials/">Sharpening your Stream Processing Skills with Kafka Tutorials</a>. In the post linked to, the author discusses the various ways we can process streams in the Kafka eco-system and points us to the <a href="https://kafka-tutorials.confluent.io/">Kafka Tutorials</a> site, where we can learn about various aspects of building applications with Kafka.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>The last few weeks, I have been somewhat lazy, (OK then, very lazy), and I have not posted much about SQL Server 2019 Big Data Cluster or SQL Server Extensibility framework.</p>

<p>That is changing now. I am working on a post about SQL Server 2019 BDC, which is based on the conference talks I have given about the BDC.</p>

<p>I am also doing some cool &ldquo;stuff&rdquo; regarding the Extensibility Framework, but at this point, I am not allowed to say what it is. But if you are interested, watch this space!</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 10, 2020]]></title>
    <link href="https://nielsberglund.com/2020/03/08/interesting-stuff---week-10-2020/" rel="alternate" type="text/html"/>
    <updated>2020-03-08T12:44:22+02:00</updated>
    <id>https://nielsberglund.com/2020/03/08/interesting-stuff---week-10-2020/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://medium.com/okteto/how-to-develop-and-debug-asp-net-core-applications-in-kubernetes-e2d1fe62068f">How to Develop and Debug ASP.NET Core Applications in Kubernetes</a>. This is an interesting post looking at developing ASP.NET Core applications for Kubernetes using a tool/framework called <a href="https://github.com/okteto/okteto">Okteto</a>.</li>
</ul>

<h2 id="misc">Misc.</h2>

<ul>
<li><a href="https://www.confluent.io/blog/choosing-between-mock-api-and-real-backend/">Mock APIs vs. Real Backends â€“ Getting the Best of Both Worlds</a>. The post linked to here from the Confluent discusses whether to mock or not and if you can do both.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://netflixtechblog.com/how-netflix-uses-druid-for-real-time-insights-to-ensure-a-high-quality-experience-19e1e8568d06">How Netflix uses Druid for Real-time Insights to Ensure a High-Quality Experience</a>. A few years ago we looked at Druid in <a href="/derivco">Derivco</a> and evaluated if we could use it. Nothing came out of it, but from reading this post, maybe we should look again.</li>
<li><a href="https://towardsdatascience.com/scalable-efficient-big-data-analytics-machine-learning-pipeline-architecture-on-cloud-4d59efc092b5">Architecture for High-Throughput Low-Latency Big Data Pipeline on Cloud</a>. Don&rsquo;t let the title of this post fool you; this is not cloud specific. The post discusses tools and techniques for building data pipelines for Big Data.</li>
<li><a href="https://www.infoq.com/presentations/kafka-zookeeper/">Kafka Needs No Keeper</a>. In this <a href="https://www.infoq.com/">InfoQ</a> presentation, the presenter talks about the ongoing effort to replace the use of Zookeeper in Kafka: the reasons for why it should be done and how it will work.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 9, 2020]]></title>
    <link href="https://nielsberglund.com/2020/03/01/interesting-stuff---week-9-2020/" rel="alternate" type="text/html"/>
    <updated>2020-03-01T07:53:45+02:00</updated>
    <id>https://nielsberglund.com/2020/03/01/interesting-stuff---week-9-2020/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="azure">Azure</h2>

<ul>
<li><a href="https://www.paraesthesia.com/archive/2020/02/25/tips-for-custom-azure-devops-build-tasks/">Tips for Creating Custom Azure DevOps Build Tasks</a>. At <a href="/derivco">Derivco</a> we have started using Azure DevOps in earnest, so this post by Travis comes in real handy.</li>
</ul>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/data-mesh-paradigm/">Data Mesh Paradigm Shift in Data Platform Architecture</a>. In my weekly roundup for <a href="/2020/02/02/interesting-stuff---week-5-2020/">week 5</a>, I mentioned a post about the state of today&rsquo;s <a href="https://martinfowler.com/articles/data-monolith-to-mesh.html">data architecture</a>. The <a href="https://www.infoq.com/">InfoQ</a> presentation I link to here, is done by the same person that wrote the post, and the presentation is essentially the content of the blog post, (or the other way around :)).</li>
</ul>

<h2 id="machine-learning-data-science">Machine Learning / Data Science</h2>

<ul>
<li><a href="https://towardsdatascience.com/how-to-embed-a-spark-ml-model-as-a-kafka-real-time-streaming-application-for-production-deployment-933aecb79f3f">How to embed a Spark ML Model as a Kafka Real-Time Streaming Application for Production Deployment</a>. This is a very interesting post. As the title says, it covers the use of Spark ML together with Kafka, and how a streaming application can make ML predictions in real-time.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://medium.com/trendyol-tech/how-to-implement-retry-logic-with-spring-kafka-710b51501ce2">How to implement retry logic with Spring Kafka</a>. An informative post with ideas on how to implement retry logic, (exactly as the title says).</li>
<li><a href="https://www.confluent.io/blog/configure-kafka-to-minimize-latency/">99th Percentile Latency at Scale with Apache Kafka</a>. The post linked to here is a must-read for you who want to get the best performance out of your Kafka clusters. The post discusses how to configure Kafka to minimize latency.</li>
<li><a href="https://www.confluent.io/blog/confluent-developer-offers-kafka-tutorials-resources-guides/">Introducing Confluent Developer</a>. In this post, Confluent&rsquo;s director of developer relations, <a href="https://twitter.com/tlberglund">Tim Berglund</a>, introduces the goto place for everything Kafka - <a href="https://developer.confluent.io/">Confluent Developer</a>. I&rsquo;ve had a look over the weekend, and it is a treasure trove of material to go through if you are into Kafka.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>The title in this section is not entirely correct as this is not so much about what I am doing right now as it is what I did the week just passed.</p>

<p>Anyway, the week just passed, I did two webinars for <a href="https://www.dataplatformgeeks.com/"><strong>DataPlatformGeeks</strong></a>, (DPG):</p>

<ul>
<li><a href="https://www.dataplatformgeeks.com/videos/data-virtualization-in-sql-server-2019-big-data-cluster-by-niels-berglund-recorded-webinar/">Data Virtualization in SQL Server 2019 Big Data Cluster</a>. Where we look at how we do data virtualization in SQL Server 2019 Big Data Cluster.</li>
<li><a href="https://www.dataplatformgeeks.com/videos/deep-dives-into-the-storage-and-data-pools-in-sql-server-2019-big-data-cluster-by-niels-berglund-recorded-webinar/">Deep Dives into the Storage and Data Pools in SQL Server 2019 Big Data Cluster</a>. A closer look at the storage and data pools in SQL Server 2019 Big Data Cluster.</li>
</ul>

<p>I recommend all of you to <a href="https://www.dataplatformgeeks.com/registration">register</a> with DPG, as they have a plethora of free learning resources! Oh, and they also run a yearly conference; <a href="https://www.dps10.com/">Data Platform Summit</a>. I hope to be able to deliver a couple of sessions at the conference this year.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 8, 2020]]></title>
    <link href="https://nielsberglund.com/2020/02/23/interesting-stuff---week-8-2020/" rel="alternate" type="text/html"/>
    <updated>2020-02-23T06:46:33+02:00</updated>
    <id>https://nielsberglund.com/2020/02/23/interesting-stuff---week-8-2020/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/articles/anatomy-cascading-failure">Anatomy of Cascading Failure</a>. In the <a href="https://www.infoq.com/">InfoQ</a> article linked to here, the author discusses what a cascading failure is, and some anti-patterns around cascading failures. If you work with distributed systems, then you should really read this article.</li>
</ul>

<h1 id="streaming">Streaming</h1>

<ul>
<li><a href="https://medium.com/@dev.anowak/monolith-to-event-driven-microservices-with-apache-kafka-6e4abe171cbb">Monolith to Event-Driven Microservices with Apache Kafka</a>. The post in the link talks about the advantages of event-driven systems, and how one can gradually evolve to an event-driven system from a monolith.</li>
<li><a href="https://www.infoq.com/articles/building-scalable-iot-application-with-apache-kafka/">The Kongo Problem: Building a Scalable IoT Application with Apache Kafka</a>. This <a href="https://www.infoq.com/">InfoQ</a> article discusses scalability and Kafka. The article is based on a series of blog posts, where the first post is <a href="https://www.instaclustr.com/instaclustr-kongo-iot-logistics-streaming-demo-application/">here</a>.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>Back in December, I did two webinars for <a href="https://www.dataplatformgeeks.com/">DataPlatformGeeks</a>, and you can read more about it <a href="/2019/12/08/interesting-stuff---week-49-2019/">here</a>.</p>

<p>Next week I am doing two more:</p>

<p><strong>February 25</strong></p>

<ul>
<li><a href="https://www.dataplatformgeeks.com/events/dpg-webinar-25th-february-2020-0300-pm-0400-pm-ist-niels-berglund-data-virtualization-in-sql-server-2019-big-data-cluster/">Data Virtualization in SQL Server 2019 Big Data Cluster</a>. Where we look at how we do data virtualization in SQL Server 2019 Big Data Cluster.</li>
</ul>

<p><strong>February 27</strong></p>

<ul>
<li><a href="https://www.dataplatformgeeks.com/events/dpg-webinar-27th-february-2020-0300-pm-0400-pm-ist-niels-berglund-deep-dives-into-the-storage-and-data-pools-in-sql-server-2019-big-data-cluster/">Deep Dives into the Storage and Data Pools in SQL Server 2019 Big Data Cluster</a>. A closer look at the storage and data pools in SQL Server 2019 Big Data Cluster.</li>
</ul>

<p>The webinars are free, so if you have time, register and join the fun! Notice that the times in the links above are Indian Standard Time, so 3 pm IST is 09:30 am, UTC.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 7, 2020]]></title>
    <link href="https://nielsberglund.com/2020/02/16/interesting-stuff---week-7-2020/" rel="alternate" type="text/html"/>
    <updated>2020-02-16T07:30:30+02:00</updated>
    <id>https://nielsberglund.com/2020/02/16/interesting-stuff---week-7-2020/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="misc">Misc.</h2>

<ul>
<li><a href="https://www.infoq.com/news/2020/02/Microsoft-Application-Inspector">Microsoft Releases Application Inspector, a Tool for Examining Code Security</a>. This <a href="https://www.infoq.com/">InfoQ</a> article discusses <strong>Microsoft Application Inspector</strong> a source code analyzer with some interesting features. The application not only detects &ldquo;poor&rdquo; programming practices, but it also surfaces and reports interesting characteristics the code that would be difficult/time-consuming to identify.</li>
</ul>

<h2 id="data-science-machine-learning">Data Science / Machine Learning</h2>

<ul>
<li><a href="https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/">ZeRO &amp; DeepSpeed: New system optimizations enable training models with over 100 billion parameters</a>. The blog post linked to here looks at ZeRO, ( Zero Redundancy Optimizer), which is a memory optimization technology for large-scale distributed deep learning. From the post: <em>ZeRO can train deep learning models with 100 billion parameters on the current generation of GPU clusters at three to five times the throughput of the current best system.</em> Wowza! Those are large numbers!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://medium.com/swlh/domain-events-versus-change-data-capture-e426772f76e5">Domain Events versus Change Data Capture</a>. The topics of change data capture and event-based systems come more and more up in various discussions. In the linked post, the author looks at these topics and explains the differences as well as when to use what.</li>
<li><a href="https://www.confluent.io/blog/ksqldb-0-7-0-feature-updates/">Announcing ksqlDB 0.7.0</a>. In <a href="/2020/02/09/interesting-stuff---week-6-2020/">last weeks roundup</a> I covered a <a href="https://rmoff.net/2020/02/07/primitive-keys-in-ksqldb/">post</a> by <a href="https://twitter.com/rmoff">Robin Moffat</a> where he looked at a new feature in the upcoming ksqlDB 0.7 release. The post linked to here is the official announcement of ksqlDB 0.7. There are quite a few new interesting features in the release, and us at <a href="/derivco">Derivco</a> will definitely look at the release.</li>
<li><a href="https://www.confluent.io/blog/elasticsearch-ksqldb-integration-for-data-enrichment-and-analytics/">Integrating Elasticsearch and ksqlDB for Powerful Data Enrichment and Analytics</a>. In the post linked to here, the author looks at how we can use both Elasticsearch and ksqlDB to get more intelligence from your streaming data.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 6, 2020]]></title>
    <link href="https://nielsberglund.com/2020/02/09/interesting-stuff---week-6-2020/" rel="alternate" type="text/html"/>
    <updated>2020-02-09T06:06:11+02:00</updated>
    <id>https://nielsberglund.com/2020/02/09/interesting-stuff---week-6-2020/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="azure-sql-server">Azure SQL Server</h2>

<ul>
<li><a href="https://techcommunity.microsoft.com/t5/azure-sql-database/autoscaling-azure-sql-hyperscale/ba-p/1149025">Autoscaling Azure SQL Hyperscale</a>. Azure SQL Hyperscale is the latest architectural model fro Azure SQL Database. It allows for a complete separation of compute nodes and storage nodes, which makes it possible to independently scale each service, making Hyperscale more flexible and elastic. The article linked to here describes how it is possible to implement a solution to automatically scale Azure SQL Hyperscale database up or down, to dynamically and automatically adapt to different workload levels.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/build-materialized-cache-with-ksqldb/">Building a Materialized Cache with ksqlDB</a>. This post is excellent, and the title says it all: how to create materialized caches using Kafka and ksqlDB! At <a href="/derivco">Derivco</a> we are looking at this right now, so the post came in the right time.</li>
<li><a href="https://rmoff.net/2020/02/07/primitive-keys-in-ksqldb/">Primitive Keys in ksqlDB</a>. In this post <a href="https://twitter.com/rmoff">Robin Moffat</a> looks at a new feature in ksqlDB 0.7: support for message keys as primitive data types other than just strings. This new feature is very welcome, especially for us who stream database data into ksqlDB where the keys are integers.</li>
<li><a href="https://netflixtechblog.com/keystone-real-time-stream-processing-platform-a3ee651812a">Keystone Real-time Stream Processing Platform</a>. The article linked to here is from the <a href="https://netflixtechblog.com/">Netflix Tech Blog</a>, and it discusses one of Netflix&rsquo;s system for real-time stream processing. As with the post mentioned above about materialized caches this post gives us at <a href="/derivco">Derivco</a> some food for thoughts.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 5, 2020]]></title>
    <link href="https://nielsberglund.com/2020/02/02/interesting-stuff---week-5-2020/" rel="alternate" type="text/html"/>
    <updated>2020-02-02T09:48:52+02:00</updated>
    <id>https://nielsberglund.com/2020/02/02/interesting-stuff---week-5-2020/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="data-science-machine-learning">Data Science / Machine Learning</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/ci-cd-ml">CI/CD for Machine Learning</a>. The presentation this links to is an <a href="https://www.infoq.com/">InfoQ</a> presentation where the presenter discusses the challenges with CI/CD for machine learning and shows how a CI/CD pipeline for Machine Learning can greatly improve both productivity and reliability.</li>
</ul>

<h2 id="sql-server-2019">SQL Server 2019</h2>

<ul>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2020/01/29/the-ultimate-performance-for-your-big-data-with-sql-server-2019-big-data-clusters/">The ultimate performance for your big data with SQL Server 2019 Big Data Clusters</a>. This post summarizes a Microsoft <a href="https://download.microsoft.com/download/e/2/3/e231a918-97cf-4acc-81d8-5188967fd3da/Joint_Microsoft_SQL_Server_2019_Big_Data_Cluster_Case_Study_White_Paper.pdf">white paper</a> discussing the performance of SQL Server 2019 Big Data Cluster. After I read the post, I went back and looked at the white paper. The Big Data Cluster offers quite impressive performance, I must say!</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://docs.microsoft.com/en-us/azure/architecture/reference-architectures/microservices/aks">Microservices architecture on Azure Kubernetes Service (AKS)</a>. The link here is to a Microsoft document covering a reference architecture for microservices applications running on Azure Kubernetes Service. I found the document quite interesting, and I hope to be able to do some POC&rsquo;s around this shortly.</li>
</ul>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://martinfowler.com/articles/data-monolith-to-mesh.html">How to Move Beyond a Monolithic Data Lake to a Distributed Data Mesh</a>. This is a very interesting post, looking at the state of today&rsquo;s enterprise data architecture. It is a must-read for anyone interested in the subject.</li>
<li><a href="https://databricks.com/blog/2020/01/30/what-is-a-data-lakehouse.html">What is a Lakehouse?</a>. The post linked to here is similar to the one above in that it looks beyond data lakes. From the post: &ldquo;<em>The lakehouse is a new data management paradigm that radically simplifies enterprise data infrastructure and accelerates innovation in an age when machine learning is poised to disrupt every industry.</em>&rdquo;.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/streaming-machine-learning-with-tiered-storage/">Streaming Machine Learning with Tiered Storage and Without a Data Lake</a>. Once again, a post which discusses data lakes, or rather the lack thereof. This post introduces a new feature in Kafka: the ability to add external storage to a Kafka broker. A very interesting topic, (pun intended), and this definitely moves Kafka towards being a complete data store. My only concern when thinking about this is how to query the data from Kafka? I guess time will tell.</li>
<li><a href="https://engineeringblog.yelp.com/2020/01/streams-and-monk-how-yelp-approaches-kafka-in-2020.html">Streams and Monk â€“ How Yelp is Approaching Kafka in 2020</a>. This is a very interesting post, in that it describes how Yelp moves towards data as a service using Kafka and some internal applications. I will recommend this post to the people at <a href="/derivco">Derivco</a> working with Kafka.</li>
</ul>

<h2 id="microsoft-ignite-the-tour-johannesburg">Microsoft Ignite The Tour | Johannesburg</h2>

<p>I just came back from the Johannesburg leg of Microsoft Ignite The Tour.</p>

<p>I want to thank the ones of you that came to my sessions, you guys rocked!</p>

<p>At the moment I am cleaning up my presentation decks and the demo code. I&rsquo;ll publish it for download in a couple of days time.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 4, 2020]]></title>
    <link href="https://nielsberglund.com/2020/01/26/interesting-stuff---week-4-2020/" rel="alternate" type="text/html"/>
    <updated>2020-01-26T07:55:14+02:00</updated>
    <id>https://nielsberglund.com/2020/01/26/interesting-stuff---week-4-2020/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://medium.com/swlh/getting-started-with-istio-524628c025">Getting Started With Istio</a>. This post by <a href="https://twitter.com/IsTheArchitect">Emil</a> is an excellent writeup of <a href="https://cloud.google.com/istio/">Istio</a>, the open source service-mesh for distributed micro-services. I found the article very informative, as all the other articles <a href="https://twitter.com/IsTheArchitect">Emil</a> publishes.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.ververica.com/blog/how-sk-telecom-democratizes-streaming-data-with-flow-and-flink">How SK telecom democratizes streaming data with FLOW and Flink</a>. The post linked to here is how SK Telecom, the South Korean telecommunications company, has built a graphical frontend tool for creating Flink queries through drag and drop. The tool looks amazing, and hopefully, SK Telecom open sources it at some stage.</li>
<li><a href="https://www.confluent.io/blog/how-kafka-is-used-by-netflix/">Featuring Apache Kafka in the Netflix Studio and Finance World</a>. This is a very interesting post about how Netflix uses Kafka as its platform of choice when working with events.</li>
<li><a href="https://medium.com/pinterest-engineering/using-graph-algorithms-to-optimize-kafka-operations-part-1-abbabd606a25">Using graph algorithms to optimize Kafka operations, Part 1</a>. In this post, the Logging Platform team at Pinterest discusses how they use graph algorithms to handle their Kafka clusters and handle problems.</li>
<li><a href="https://eng.uber.com/kappa-architecture-data-stream-processing/">Designing a Production-Ready Kappa Architecture for Timely Data Stream Processing</a>. A couple of Years ago <a href="https://twitter.com/jaykreps">Jay Kreps</a> who is CEO of Confluent and one of the original developers of Kafka, suggested an alternative to the Lambda architecture, and he called it <a href="https://www.oreilly.com/radar/questioning-the-lambda-architecture/">Kappa</a>. The post linked to discusses how Uber implements Kappa in production. A very interesting read!</li>
</ul>

<h2 id="microsoft-ignite-the-tour-johannesburg">Microsoft Ignite The Tour | Johannesburg</h2>

<p>Microsoft&rsquo;s <a href="https://news.microsoft.com/ignite2019/">Ignite conference</a> took place in November last year, and now, (as they did last year), Microsoft takes Ignite on the road: <a href="https://www.microsoft.com/en-za/ignite-the-tour/">Microsoft Ignite The Tour</a>.</p>

<p>The tour comes to <a href="https://www.microsoft.com/en-za/ignite-the-tour/johannesburg">Johannesburg January 30 - 31</a>, and I am lucky enough to present at the event. I am doing three presentations, and also some community &ldquo;booth duty&rdquo;:</p>

<ul>
<li><a href="https://johannesburg.myignitetour.techcommunity.microsoft.com/sessions/91139?source=sessions">A Lap Around SQL Server Big Data Cluster</a></li>
<li><a href="https://johannesburg.myignitetour.techcommunity.microsoft.com/sessions/91141?source=sessions">Simplify and Scale Your Data Pipelines with Azure Delta Lake</a></li>
<li><a href="https://johannesburg.myignitetour.techcommunity.microsoft.com/sessions/91140?source=sessions">Improve Customer Lifetime Value using Azure Databricks &amp; Delta Lake</a></li>
</ul>

<p>The conference is free of charge, so <a href="https://register.msignite-the-tour.microsoft.com/johannesburg">register</a> now and come and say Hi!</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 3, 2020]]></title>
    <link href="https://nielsberglund.com/2020/01/19/interesting-stuff---week-3-2020/" rel="alternate" type="text/html"/>
    <updated>2020-01-19T08:44:17+02:00</updated>
    <id>https://nielsberglund.com/2020/01/19/interesting-stuff---week-3-2020/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="misc">Misc.</h2>

<ul>
<li><a href="http://blogs.tedneward.com/post/2020-tech-predictions/">2020 Tech Predictions</a>. As it is the beginning of a new year, (and a decade at that), you see prediction posts being published. Here is a predictions post for 2020 from an old colleague from the Developmentor days, Ted Neward.</li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://www.microsoft.com/en-us/research/blog/are-all-samples-created-equal-boosting-generative-models-via-importance-weighting/">Are all samples created equal?: Boosting generative models via importance weighting</a>. This is a very interesting post discussing how to correct the imperfections of generative models. Please go and read the post to see what techniques they use.</li>
<li><a href="https://www.linkedin.com/pulse/data-literacy-10-minutes-buck-woody/">Data Literacy in 10 Minutes</a>. In last weeks roundup, I mentioned that <a href="https://twitter.com/BuckWoodyMSFT">Buck Woody</a> just started a blog post series about data literacy. Well, the link here is to the whole series posted as one single post on LinkedIn.</li>
</ul>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://medium.com/young-coder/the-reunification-of-net-5-5902744df9fe">The Reunification of .NET 5</a>. The post linked to here discusses how Microsoft tries to merge .NET Framework with .NET Core, in order to have one single .NET offering.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://yokota.blog/2020/01/13/building-a-graph-database-using-kafka/">BUILDING A GRAPH DATABASE USING KAFKA</a>. Another excellent blog post about Kafka by <a href="https://www.linkedin.com/in/robert-yokota-477108/">Robert Yokota</a>. In this post, he looks at how you can create a Kafka based graph database. Very exciting! Oh, and if you are interested in distributed systems, and/or, Kafka, his <a href="https://yokota.blog/">blog</a> should be in your RSS feed.</li>
<li><a href="https://www.infoq.com/podcasts/change-data-capture-debezium/">Gunnar Morling on Change Data Capture and Debezium</a>. This is an <a href="https://www.infoq.com/">InfoQ</a> podcast. The podcast discusses Debezium, the open-source distributed platform for change data capture (CDC).</li>
<li><a href="https://www.confluent.io/blog/kafka-streams-tables-part-1-event-streaming/">Streams and Tables in Apache Kafka: A Primer</a>. The post linked to here is the first in a series about Kafka, streams and tables. The series should be mandatory reading for anyone working with Kafka! I am making sure that anyone at <a href="/derivco">Derivco</a> working with Kafka reads this series!</li>
</ul>

<h2 id="microsoft-ignite-the-tour-johannesburg">Microsoft Ignite The Tour | Johannesburg</h2>

<p>Microsoft&rsquo;s <a href="https://news.microsoft.com/ignite2019/">Ignite conference</a> took place in November last year, and now, (as they did last year), Microsoft takes Ignite on the road: <a href="https://www.microsoft.com/en-za/ignite-the-tour/">Microsoft Ignite The Tour</a>.</p>

<p>The tour comes to <a href="https://www.microsoft.com/en-za/ignite-the-tour/johannesburg">Johannesburg January 30 - 31</a>, and I am lucky enough to present at the event. I am doing three presentations, and also some community &ldquo;booth duty&rdquo;:</p>

<ul>
<li><a href="https://johannesburg.myignitetour.techcommunity.microsoft.com/sessions/91139?source=sessions">A Lap Around SQL Server Big Data Cluster</a></li>
<li><a href="https://johannesburg.myignitetour.techcommunity.microsoft.com/sessions/91141?source=sessions">Simplify and Scale Your Data Pipelines with Azure Delta Lake</a></li>
<li><a href="https://johannesburg.myignitetour.techcommunity.microsoft.com/sessions/91140?source=sessions">Improve Customer Lifetime Value using Azure Databricks &amp; Delta Lake</a></li>
</ul>

<p>The conference is free of charge, so <a href="https://register.msignite-the-tour.microsoft.com/johannesburg">register</a> now and come and say Hi!</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 2, 2020]]></title>
    <link href="https://nielsberglund.com/2020/01/12/interesting-stuff---week-2-2020/" rel="alternate" type="text/html"/>
    <updated>2020-01-12T08:04:28+02:00</updated>
    <id>https://nielsberglund.com/2020/01/12/interesting-stuff---week-2-2020/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="data-science-machine-learning">Data Science / Machine Learning</h2>

<ul>
<li><a href="https://eng.uber.com/manifold-open-source/">Open Sourcing Manifold, a Visual Debugging Tool for Machine Learning</a>. An interesting blog post, discussing Uber&rsquo;s visual debugging tool for machine learning. A couple of guys at <a href="/derivco">Derivco</a> has started looking at it and is quite impressed.</li>
<li><a href="https://buckwoody.wordpress.com/2020/01/06/data-literacy/">Data Literacy</a>. In this blog post, <a href="https://twitter.com/BuckWoodyMSFT">Buck Woody</a>, (&ldquo;Mr Data Science&rdquo; at Microsoft), announces a series of posts covering Data Literacy. If you are into data, read the posts - it is well worth it!</li>
</ul>

<h2 id="vs-code">VS Code</h2>

<ul>
<li><a href="https://medium.com/swlh/create-a-reproducible-dev-environment-with-vs-code-fd89285644da">Create a Reproducible Dev Environment with VS Code</a>. This blog post covers how you can create dev environments quickly using VS Code and Docker. Seeing how I struggle with setting up dev environments, this post comes in really handy!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://rmoff.net/2020/01/08/streaming-messages-from-rabbitmq-into-kafka-with-kafka-connect/">Streaming messages from RabbitMQ into Kafka with Kafka Connect</a>. This is an interesting post by <a href="https://twitter.com/rmoff">Robin Moffat</a>, where he looks at getting messages from RabbitMQ in Kafka. Seeing that we, (<a href="/derivco">Derivco</a>), are starting our Kafka journey and we have quite a lot of Rabbit instances this post comes in very handy!</li>
<li><a href="https://rmoff.net/2020/01/09/exploring-ksqldb-window-start-time/">Exploring ksqlDB window start time</a>. This is another interesting post by <a href="https://twitter.com/rmoff">Robin</a>. In this post, he covers how time windows behave in ksqlDB.</li>
</ul>

<h2 id="microsoft-ignite-the-tour-johannesburg">Microsoft Ignite The Tour | Johannesburg</h2>

<p>Microsoft&rsquo;s <a href="https://news.microsoft.com/ignite2019/">Ignite conference</a> took place in November last year, and now, (as they did last year), Microsoft takes Ignite on the road: <a href="https://www.microsoft.com/en-za/ignite-the-tour/">Microsoft Ignite The Tour</a>.</p>

<p>The tour comes to <a href="https://www.microsoft.com/en-za/ignite-the-tour/johannesburg">Johannesburg January 30 - 31</a>, and I am lucky enough to present at the event. I am doing three presentations, and also some community &ldquo;booth duty&rdquo;:</p>

<ul>
<li><a href="https://johannesburg.myignitetour.techcommunity.microsoft.com/sessions/91139?source=sessions">A Lap Around SQL Server Big Data Cluster</a></li>
<li><a href="https://johannesburg.myignitetour.techcommunity.microsoft.com/sessions/91141?source=sessions">Simplify and Scale Your Data Pipelines with Azure Delta Lake</a></li>
<li><a href="https://johannesburg.myignitetour.techcommunity.microsoft.com/sessions/91140?source=sessions">Improve Customer Lifetime Value using Azure Databricks &amp; Delta Lake</a></li>
</ul>

<p>The conference is free of charge, so <a href="https://register.msignite-the-tour.microsoft.com/johannesburg">register</a> now and come and say Hi!</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Christmas, New Year, Week 1, 2020]]></title>
    <link href="https://nielsberglund.com/2020/01/05/interesting-stuff---christmas-new-year-week-1-2020/" rel="alternate" type="text/html"/>
    <updated>2020-01-05T09:24:49+02:00</updated>
    <id>https://nielsberglund.com/2020/01/05/interesting-stuff---christmas-new-year-week-1-2020/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This is the &ldquo;roundup&rdquo; of the posts that have been most interesting to me over the Christmas and New Year period 2019, and the first week of 2020.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://medium.com/swlh/optimizing-garbage-collection-in-a-high-load-net-web-service-3bb620b444a7">Optimizing garbage collection in a high load .NET service</a>. The blog post I link to is an awesome post about how to track down memory issues which impacts the performance of your .NET application. If you write .NET applications, please go ahead and read this post.</li>
</ul>

<h2 id="data-databases">Data &amp; Databases</h2>

<ul>
<li><a href="https://eng.uber.com/uber-data-platform-2019/">Uberâ€™s Data Platform in 2019: Transforming Information to Intelligence</a>. This is a very interesting post, discussing how Uber is handling data and the infrastructure for data. One of the things I found interesting was how they use data science for infrastructure.</li>
<li><a href="https://medium.com/netflix-techblog/dblog-a-generic-change-data-capture-framework-69351fb9099b">DBLog: A Generic Change-Data-Capture Framework</a>. This post is about <strong>DBLog</strong>, which is Netflix &ldquo;homegrown&rdquo; Change Data Capture, (CDC), framework. What is interesting is that <strong>DBLog</strong> is developed as a framework, and you can &ldquo;plug in&rdquo; different data sources. I am looking forward to when it becomes open source!</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.youtube.com/playlist?list=PLLasX02E8BPCrIhFrc_ZiINhbRkYMKdPT">Kubernetes Video Series</a>. What I link to here is not a blog post, but a playlist of Kubernetes videos ranging from the very basic to more advanced.</li>
<li><a href="https://bravenewgeek.com/microservice-observability-part-2-evolutionary-patterns-for-solving-observability-problems/">Microservice Observability, Part 2: Evolutionary Patterns for Solving Observability Problems</a>. This is the second post in a series, covering microservices and observability. If you develop microservices, do yourself a favor and read this series. It is well worth it!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/apache-kafka-2-4-latest-version-updates/">What&rsquo;s New in Apache Kafka 2.4</a>. Apache Kafka 2.4 was released a couple of weeks ago, and this post lists new and improved functionality in the release. There are quite a few &ldquo;juicy&rdquo; things in there!</li>
<li><a href="https://www.confluent.io/blog/test-kafka-streams-with-topologytestdriver/">Testing Kafka Streams Using TestInputTopic and TestOutputTopic</a>. Above I mentioned about new features in Apache Kafka 2.4, and this post looks at one of the new features. The feature is the ability to easier test Kafka Streams via a couple of new classes: <code>TestInputTopic</code> and <code>TestOutputTopic</code>.</li>
<li><a href="https://www.infoq.com/presentations/microservices-streams-state-scalability/">Beyond Microservices: Streams, State and Scalability</a>. This is an <a href="https://www.infoq.com/">InfoQ</a> presentation where the presenter discusses how microservices evolved in the last few years.</li>
<li><a href="https://www.confluent.io/blog/stream-processing-twitter-data-with-ksqldb/">Exploring ksqlDB with Twitter Data</a>. A while ago Confluent released <strong>ksqlDB</strong>, the evolution of KSQL. In this post. <a href="https://twitter.com/rmoff">Robin Moffat</a> looks at using <strong>ksqlDB</strong> to connect to a live Twitter feed and do &ldquo;cool stuff&rdquo;.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[More PolyBase Issues in SQL Server 2019 Developers Edition]]></title>
    <link href="https://nielsberglund.com/2019/12/30/more-polybase-issues-in-sql-server-2019-developers-edition/" rel="alternate" type="text/html"/>
    <updated>2019-12-30T18:26:55+02:00</updated>
    <id>https://nielsberglund.com/2019/12/30/more-polybase-issues-in-sql-server-2019-developers-edition/</id>
    <content type="html"><![CDATA[<p>A couple of weeks ago, I published the post <a href="/2019/11/20/fix-polybase-in-sql-server-2019-developers-edition/"><strong>Fix PolyBase in SQL Server 2019 Developers Edition</strong></a>. In the post, I discussed how to fix an issue in SQL Server 2019 Developers edition, where the PolyBase services do not start up after installation.</p>

<p>The fix in the post works fine, i.e. the services start after the fix. However some days after I published the post there was a question on a SQL Server 2019 forum, where the poster says he cannot create an external file format on the SQL Server 2019 Developer Edition. Sure enough, when I tried to do the same on my &ldquo;fixed&rdquo; instance, I could not either.</p>

<p>So in this post, I look at how to fix that second issue.</p>

<p></p>

<h2 id="demo-data-code">Demo Data / Code</h2>

<p>The following code is for you who want to follow along and try it out for yourselves:</p>

<pre><code class="language-sql">CREATE DATABASE MyOraSourceDB;
GO

USE MyOraSourceDB;
GO

CREATE TABLE emp
(  
  EMPNO int primary key, 
  ENAME nvarchar(50), 
  JOB nvarchar(50), 
  MGR int, 
  HIREDATE date, 
  SAL int, 
  COMM int, 
  DEPTNO int
);

INSERT INTO emp (EMPNO,ENAME,JOB,MGR,HIREDATE,SAL,COMM,DEPTNO)
VALUES (7369,'SMITH','CLERK',7902, '1980-12-17',800,null,20)
, (7521,'WARD','SALESMAN',7698,'1981-02-20',1250,500,30)
,(7499,'ALLEN','SALESMAN',7698,'1981-02-22',1600,300,30)
,(7566,'JONES','MANAGER',7839,'1981-04-02',2975,null,20)
,(7654,'MARTIN','SALESMAN',7698,'1981-09-28',1250,1400,30)
,(7698,'BLAKE','MANAGER',7839,'1981-05-01',2850,null,30)
,(7782,'CLARK','MANAGER',7839,'1981-06-09',2450,null,10)
,(7788,'SCOTT','ANALYST',7566,'1987-04-19',3000,null,20)
,(7839,'KING','PRESIDENT',null,'1981-11-17',5000,null,10)
,(7844,'TURNER','SALESMAN',7698,'1981-09-08',1500,0,30)
,(7876,'ADAMS','CLERK',7788,'1987-05-23',1100,null,20)
,(7900,'JAMES','CLERK',7698,'1981-12-03',950,null,30)
,(7902,'FORD','ANALYST',7566,'1981-12-03',3000,null,20)
,(7934,'MILLER','CLERK',7782,'1982-01-23',1300,null,10);
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Create Source Database</em></p>

<p>The code in <em>Code Snippet 1</em> creates a database and a table with some data. The database is what we use as the source database. Typically for demo purposes, I use an Oracle database as the source, but for this, we use a local SQL Server database.</p>

<p>We also need a database which we work from, i.e. the target database:</p>

<pre><code class="language-sql">CREATE DATABASE MyTargetDB;
GO
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Create Target Database</em></p>

<p>In <em>Code Snippet 2</em> we see how I create the database we will work in. For now, we only create the database, and later we add &ldquo;stuff&rdquo;.</p>

<h2 id="external-table">External Table</h2>

<p>PolyBase enables a database in a SQL Server instance to process Transact-SQL queries that read data from external data sources. The data in the remote data source is exposed via external table(s).</p>

<p>In our case, we emulate a remote Oracle database through our <code>MyOraSourceDB</code> database, and the data source is the <code>emp</code> table. What we want to do is to query the &ldquo;remote&rdquo; <code>emp</code> table from inside the <code>MyTargetDB</code>. To do this, we create an external table targeting the <code>emp</code> table in the <code>MyOraSourceDB</code> database.</p>

<p>The signature for <code>CREATE EXTERNAL TABLE</code> looks like so:</p>

<pre><code class="language-sql">CREATE EXTERNAL TABLE { database_name.schema_name.table_name | 
                        schema_name.table_name | 
                        table_name }
    ( &lt;column_definition&gt; [ ,...n ] )
    WITH (
        LOCATION = 'folder_or_filepath',
        DATA_SOURCE = external_data_source_name
        [ , FILE_FORMAT = external_file_format_name ]
        [ , &lt;reject_options&gt; [ ,...n ] ]
    )
[;]
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Create External Table</em></p>

<p>We see in <em>Code Snippet 3</em> the main portion of <code>CREATE EXTERNAL TABLE</code>. Notice the <code>DATA_SOURCE</code>, and <code>FILE_FORMAT</code> arguments.</p>

<h4 id="data-source">DATA_SOURCE</h4>

<p>Specifies the name of the external data source that contains the location of the external data. External data sources can be:</p>

<ul>
<li>Hadoop, (SQL Server 2016+).</li>
<li>Azure Blob Storage, (SQL Server 2016+).</li>
<li>SQL Server, (SQL Server 2019+).</li>
<li>Oracle, (SQL Server 2019+).</li>
<li>Teradata, (SQL Server 2019+).</li>
<li>Mongo DB, (SQL Server 2019+).</li>
<li>Cosmos DB, (SQL Server 2019+).</li>
<li>Generic ODBC sources, (only in SQL Server 2019 Windows).</li>
</ul>

<p>The signature of <code>CREATE EXTERNAL DATA SOURCE</code> is as follows:</p>

<pre><code class="language-sql">CREATE EXTERNAL DATA SOURCE &lt;data_source_name&gt;  
WITH
(    
  LOCATION                  = '&lt;prefix&gt;://&lt;path&gt;[:&lt;port&gt;]'
  [,   CONNECTION_OPTIONS        = '&lt;name_value_pairs&gt;']
  [,   CREDENTIAL                = &lt;credential_name&gt; ]
  [,   PUSHDOWN                  = ON | OFF]
  [,   TYPE                      = HADOOP | BLOB_STORAGE ]
  [,   RESOURCE_MANAGER_LOCATION = '&lt;resource_manager&gt;[:&lt;port&gt;]'
)
[;]
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Create External Data Source - I</em></p>

<p>In <em>Code Snippet 4</em> we see that most of <code>Create External Data Source</code> arguments are optional. In our code further below, we use <code>LOCATION</code> and <code>CREDENTIAL</code>.</p>

<h4 id="file-format">FILE_FORMAT</h4>

<p>The <code>CREATE EXTERNAL FILE FORMAT</code> specifies the layout of the data referenced by an external table. You use this for external tables, not referencing relational data.</p>

<p>The command differs depending on the actual remote file format. The code the poster used in the forum, mentioned above, looked like so:</p>

<pre><code class="language-sql">CREATE EXTERNAL FILE FORMAT csv_file
WITH 
(
  FORMAT_TYPE = DELIMITEDTEXT,
  FORMAT_OPTIONS
  (
    FIELD_TERMINATOR = ',',
    STRING_DELIMITER = '&quot;'
  )
);
</code></pre>

<p><strong>Code Snippet 5:</strong> <em>Create External File Format</em></p>

<p>As we see in <em>Code Snippet 5</em>, the poster wanted to define the format for <code>.csv</code> files. He:</p>

<ul>
<li>Named the format: <code>csv_file</code>.</li>
<li>Defined the type: <code>DELIMITEDTEXT</code>.</li>
<li>Defined what terminates a field, and what delimits a string.</li>
</ul>

<p>A bit further down we see what happened when the poster wanted to execute the statement.</p>

<h2 id="the-issue-s">The Issue(s)</h2>

<p>Let us look at the issue which occurred for the forum post as well for me. When we execute the code in <em>Code Snippet 5</em> we see the following:</p>

<p><img src="/images/posts/sql-pb-issues2-syntax1.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Incorrect Syntax</em></p>

<p>What we see in <em>Figure 1</em>, <em>incorrect syntax exception</em>, is strange, as I have executed the same code in a SQL Server 2019 Big Data Cluster, (BDC), without any issues, and the forum poster executed the same in SQL Server 2019 Enterprise Edition also without any issues.</p>

<p>Ok, but what about creating an external table against a relational data source - where we do not need to define an external file format? Let us go back to the databases we created above, and - in the <code>MyTargetDB</code> - create an external table against the <code>emp</code> table in <code>MyOraSourceDB</code>. We start with the external data source:</p>

<pre><code class="language-sql">CREATE MASTER KEY ENCRYPTION BY PASSWORD = '&lt;some-secret-password&gt;';
GO

CREATEÂ DATABASEÂ SCOPEDÂ CREDENTIALÂ [oracleCred]
WITHÂ IDENTITYÂ =Â N'sa',Â SECRETÂ =Â N'&lt;some-other-secret-password&gt;';

CREATE EXTERNAL DATA SOURCE OraConnection
WITH 
( 
  LOCATION = 'sqlserver://localhost',
  CREDENTIAL = [oracleCred]
);
</code></pre>

<p><strong>Code Snippet 6:</strong> <em>Create External Data Source - II</em></p>

<p>You may wonder why we, in <em>Code Snippet 6</em>, create a master key and a scoped credential? Well, we create the credential as the &ldquo;remote&rdquo; data source, in this case, the <code>MyOraSourceDB</code> requires authentication. The master key is required to protect the scoped credential.</p>

<blockquote>
<p><strong>NOTE:</strong> If the database in question already have a master key, you do not need to create a new.</p>
</blockquote>

<p>As for creating the data source we:</p>

<ul>
<li>Give it a name.</li>
<li>Sets the location. The <code>sqlserver</code> prefix indicates this is a SQL Server instance.</li>
<li>Assign the credential created above.</li>
</ul>

<blockquote>
<p><strong>NOTE:</strong> If the remote data source had been an Oracle server, the <code>LOCATION</code> would be like: `oracle://[IP-address]&lsquo;.</p>
</blockquote>

<p>All this seems correct, however, when we execute the code in <em>Code Snippet 6</em> we get:</p>

<p><img src="/images/posts/sql-pb-issues2-type-generic.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Type Generic not Supported</em></p>

<p>The exception we see in <em>Figure 2</em> is, once again, strange, as when I run the same code in SQL Server 2019 BDC it works fine. What is going on here?</p>

<h2 id="solution">Solution</h2>

<p>Above we have seen two instances of where we received exceptions for operations in SQL Server 2019 Developers Edition that works fine in other editions, (BDC, Enterprise, etc.).</p>

<p>As Developers edition is equivalent to Enterprise edition, PolyBase should just work. When I looked into these issues, the only explanation I could come up with was that some settings differ during setup between Developer and Enterprise Edition, (and BDC for that matter).</p>

<p>So what I did was that I connected to my BDC, and looked at the configuration:</p>

<pre><code class="language-sql">SELECT name, value, value_in_use, description 
FROM sys.configurations
where name like '%polybase%'
</code></pre>

<p><strong>Code Snippet 7:</strong> <em>Get Config Detail for PolyBase</em></p>

<p>When I executed the code on my BDC instance the result looked like so:</p>

<p><img src="/images/posts/sql-pb-issues2-bdc-pb-config.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>BDC PolyBase Config</em></p>

<p>We see in <em>Figure 3</em> how all PolyBase related configuration settings are enabled. Now, what if we run the code in <em>Code Snippet 7</em> on my local Developer edition:</p>

<p><img src="/images/posts/sql-pb-issues2-bdc-pb-config2.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Local Developer Edition PolyBase Config</em></p>

<p>Oh, notice in <em>Figure 4</em>, outlined in red, how PolyBase is <strong>NOT</strong> enabled. That certainly looks like a smoking gun. So, let us enable PolyBase:</p>

<pre><code class="language-sql">EXEC sp_configure 'polybase enabled', 1
RECONFIGURE WITH OVERRIDE;
</code></pre>

<p><strong>Code Snippet 8:</strong> <em>Enable PolyBase</em></p>

<p>After I execute the code in <em>Code Snippet 8</em> and run <code>EXEC sp_configure 'polybase enabled'</code> I see:</p>

<p><img src="/images/posts/sql-pb-issues2-pb-enabled.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Local Developer Edition PolyBase Config</em></p>

<p>Yay, seeing <em>Figure 5</em>, it looks like PolyBase is enabled. To try it out, run the code in <em>Code Snippet 5</em>. When I run it I get:</p>

<p><img src="/images/posts/sql-pb-issues2-success1.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>Command Completed Successfully</em></p>

<p>As we see in <em>Figure 6</em>, it looks like all is OK.</p>

<p>Let us now go to the code for the external data source and see what happens when we try to create it. So I execute the <code>CREATE EXTERNAL DATA_SOURCE</code> part of <em>Code Snippet 6</em>, and that succeeds as well.</p>

<p>Cool, so we have the external data source. Now it is time for the external table:</p>

<pre><code class="language-sql">CREATE EXTERNAL TABLE [dbo].[emp_ora]
(
  [EMPNO] int NOT NULL,
  [ENAME] NVARCHAR(50),
  [JOB] NVARCHAR(50),
  [MGR] int,
  [HIREDATE] DATE,
  [SAL] int,
  [COMM] int,
  [DEPTNO] int
)
WITH (LOCATION = N'[MyOraSourceDB].[dbo].[emp]', 
      DATA_SOURCE = [OraConnection]);
</code></pre>

<p><strong>Code Snippet 9:</strong> <em>External Table</em></p>

<p>In <em>Code Snippet 9</em> we see how we use more or less the same DDL as for the original, remote, table. The only difference is that we define a location which is <code>database.schema.tablename</code>, as well as the data source. After we execute the code in <em>Code Snippet 9</em>, we can query the table: <code>SELECT * FROM dbo.emp_ora</code>:</p>

<p><img src="/images/posts/sql-pb-issues2-query.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>Query Result</em></p>

<p>We see in <em>Figure 7</em> the first 5 rows in the external table. However, the data is not persisted in the external table; it is loaded from the remote table. We can prove that by executing: <code>EXEC sp_spaceused 'dbo.emp_ora'</code>:</p>

<p><img src="/images/posts/sql-pb-issues2-space.png" alt="" /></p>

<p><strong>Figure 8:</strong> <em>Space Used</em></p>

<p>As we see in <em>Figure 8</em> the table has no rows!</p>

<h2 id="summary">Summary</h2>

<p>In the <a href="/2019/11/20/fix-polybase-in-sql-server-2019-developers-edition/"><strong>Fix PolyBase in SQL Server 2019 Developers Edition</strong></a> post we saw how to fix the issue when the PolyBase services do not start up after installation of SQL Server 2019 Developers Edition.</p>

<p>In this post, we looked at how to fix the errors we see in <em>Figure 1</em> and <em>Figure 2</em> when we try to either create an external file format or an external data source.</p>

<p>The solution is simple: <code>EXEC sp_configure 'polybase enabled', 1</code> followed by <code>RECONFIGURE WITH OVERRIDE</code>.</p>

<p>Yes, the fix is simple, but I cannot help but think that the error messages could be more descriptive.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to Deploy SQL Server 2019 Big Data Cluster Using Azure Data Studio]]></title>
    <link href="https://nielsberglund.com/2019/12/23/how-to-deploy-sql-server-2019-big-data-cluster-using-azure-data-studio/" rel="alternate" type="text/html"/>
    <updated>2019-12-23T19:35:36+02:00</updated>
    <id>https://nielsberglund.com/2019/12/23/how-to-deploy-sql-server-2019-big-data-cluster-using-azure-data-studio/</id>
    <content type="html"><![CDATA[<p>For you who follows my sporadic posts, you may wonder why I have yet another post, (YAP), covering how to deploy a <strong>SQL Server 2019 Big Data Cluster</strong>, (BDC), using <strong>Azure Data Studio</strong>, (ADS).</p>

<p>The answer to that is that the version of BDC I deployed in <a href="/2019/09/11/install-sql-server-2019-big-data-cluster-using-azure-data-studio/">this</a> post was a pre-release, and since then BDC has gone GA, (General Availability), and there are certain differences in deployment process between RC1, and RTM.</p>

<p>So in this post let us look at how to deploy BDC RTM to Azure Kubernetes Service using ADS.</p>

<p></p>

<h2 id="pre-reqs">Pre-Reqs</h2>

<p>To deploy using <strong>Azure Data Studio</strong>, (ADS), you need ADS (duh - see below), but you also need some other things.</p>

<blockquote>
<p><strong>NOTE:</strong> If you wonder where you have seen this pre-req section before, the answer is that it is almost identical to the pre-req section in my <a href="/2019/09/11/install-sql-server-2019-big-data-cluster-using-azure-data-studio/">previous install post</a>.</p>
</blockquote>

<h4 id="azure-subscription">Azure Subscription</h4>

<p>As this post covers how to deploy a <strong>SQL Server 2019 Big Data Cluster</strong> to <strong>Azure Kubernetes Service</strong>, (AKS), you need an Azure subscription. If you do not have one, you can sign up for a free trial subscription <a href="https://azure.microsoft.com/en-us/free/">here</a>.</p>

<h4 id="python">Python</h4>

<p>Well, Python is not a tool as such, but you need Python installed on the machine you install from, as the ADS deployment runs some Python scrips. You need Python3, and on my machine, I have Python 3.7.3. Ensure that Python is on the <code>PATH</code>.</p>

<h4 id="azdata">azdata</h4>

<p><code>azdata</code> is a Python command-line tool enabling cluster administrators to bootstrap and manages the big data cluster via REST APIs. It replaces <code>mssqlctl</code>, which was the previous command-line tool for deploying a BDC.</p>

<p>There are a couple of steps to install <code>azdata</code>:</p>

<ul>
<li>If you have <code>mssqlctl</code> installed you need to uninstall it:</li>
</ul>

<pre><code class="language-bash">$ pip3 uninstall -r https://private-repo.microsoft.com/ \
                           python/ctp3.1/mssqlctl/requirements.txt
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Uninstall <code>mssqlctl</code></em></p>

<p>In <em>Code Snippet 1</em> above I have inserted a line continuation (<code>\</code>) to make the code fit the page.</p>

<ul>
<li>If you have deployed CTP 3.2, or any later CTP&rsquo;s of the BDC, (including RC1), then you need to uninstall the corresponding version of <code>azdata</code>:</li>
</ul>

<pre><code class="language-bash">pip3 uninstall -r https://azdatacli.blob.core.windows.net/ \
                  python/azdata/&lt;version-indicator&gt;/requirements.txt
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Uninstall <code>azdata</code></em></p>

<p>In <em>Code Snippet 2</em> you see how the uninstall command indicates which version of <code>azdata</code> to uninstall via <code>&lt;version-indicator&gt;</code>. The value of the `<version-indicator> is as follows:</p>

<ul>
<li>CTP 3.2: <code>2019-ctp3.2</code>.</li>

<li><p>RC1: <code>2019-rc1</code>.</p></li>

<li><p>With the above in mind, the command I used to uninstall the <code>RC1</code> version of <code>azdata</code> looks like so:</p></li>
</ul>

<pre><code class="language-bash">pip3 uninstall -r https://azdatacli.blob.core.windows.net/ \
                  python/azdata/2019-rc1/requirements.txt
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Uninstall RC1 <code>azdata</code></em></p>

<p>In <em>Code Snippet 3</em> we see how I have replaced <code>&lt;version-indicator&gt;</code> with <code>2019-rc1</code>. When you run the command, you have to confirm that you want to remove some installed components:</p>

<p><img src="/images/posts/inst-bdcrc1-uninst-azdata.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Confirm Uninstall <code>azdata</code></em></p>

<p>Just click <code>y</code> when asked to proceed.</p>

<blockquote>
<p><strong>NOTE:</strong> The biggest issue causing errors in a BDC deployment, by far, is using an older version of <code>azdata</code>. So please, do not be &ldquo;that guy&rdquo; (or girl) - make sure you uninstall <code>azdata</code> if you have an earlier version. In fact, before a deployment, always uninstall <code>azdata</code> followed by an install, (see below).</p>
</blockquote>

<ul>
<li>You need the latest version of the Python <code>requests</code> package installed:</li>
</ul>

<pre><code class="language-bash">$ pip3 install -U requests
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Install/Upgrade <code>requests</code></em></p>

<p>When you have executed the code in <em>Code Snippet 4</em> you can install <code>azdata</code>:</p>

<pre><code class="language-bash">$ pip3 install -r https://aka.ms/azdata
</code></pre>

<p><strong>Code Snippet 5:</strong> <em>Installing <code>azdata</code></em></p>

<p>After executing the code in <em>Code Snippet 5</em> you install the other tools needed.</p>

<h4 id="kubectl">kubectl</h4>

<p>The <code>kubectl</code> tool is a Kubernetes command-line tool, and it allows you to run commands against Kubernetes clusters. You use <code>kubectl</code> to deploy applications, inspect and manage cluster resources, and view logs.</p>

<p>You can install <code>kubectl</code> in different ways, and I installed it from <a href="https://chocolatey.org/packages/kubernetes-cli">Chocolatey</a>: <code>choco install kubernetes-cli</code>.</p>

<h4 id="azure-cli">Azure CLI</h4>

<p>The Azure CLI is Microsoft&rsquo;s cross-platform command-line experience for managing Azure resources, and you install it on your local machine. You find installation links for Azure CLI <a href="https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest">here</a>.</p>

<h2 id="azure-data-studio">Azure Data Studio</h2>

<p>Since this post is about installing and deploying a BDC using <strong>Azure Data Studio</strong>, you also need ADS. Pre-releases of the BDC required special builds of ADS for deployment, (the Insiders builds), and the BDC RC1 release required the ADS RC build.</p>

<p>This is not the case anymore; any ADS build from 1.13.0 is sufficient for deployment. You find installation links for ADS <a href="https://docs.microsoft.com/en-us/sql/azure-data-studio/download?view=sql-server-ver15">here</a>.</p>

<h4 id="azure-data-studio-notebooks">Azure Data Studio Notebooks</h4>

<p>You deploy the BDC using ADS deployment <em>Notebooks</em>. You may ask yourself what an <strong>Azure Data Studio Notebook</strong> is? Well, Notebooks come from the Data Science world where a Notebook can contain live code, equations, visualizations and narrative text. It is a tool for teaching or sharing information between people. A notebook makes it easy to link lots of docs and code together.</p>

<p>When Microsoft developed ADS, they embedded the <a href="https://jupyter.org/">Jupyter</a> service in ADS, which enables ADS to run Notebooks. When you talk about Notebooks, you also talk about <em>Kernels</em>. A <em>Kernel</em> is the programming language you write and execute code in, in the <em>Notebook</em>:</p>

<p><img src="/images/posts/inst-bdcrc1-ads-notebook-kernels.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Notebook Kernels</em></p>

<p>The drop-down you see in <em>Figure 2</em> shows the <em>Kernels</em> ADS supports. When you deploy, you use the <em>Python 3</em> kernel.</p>

<p>Code in Notebooks exists in cells, and to run the code you execute the cell.</p>

<p>If you have not used Python Notebooks before in ADS, you need to configure Python for use with Notebooks. You enter <strong>Ctrl+Shift+P</strong> to open the command palette, and you search for <em>Configure Python</em>:</p>

<p><img src="/images/posts/inst-bdcrc1-ads-configure-notebooks.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Configure Notebooks</em></p>

<p>In <em>Figure 3</em> you see the command palette, and you choose <em>Configure Python for Notebooks</em>, and follow the instructions.</p>

<p>When you have configured Python for the notebooks, you are ready to deploy the BDC.</p>

<h2 id="azure-data-studio-deployment-wizard">Azure Data Studio Deployment Wizard</h2>

<p>When you use ADS to to a deployment you use a deployment wizard who guides you through the steps necessary for the deployment. To start up the wizard you can use the <em>Command Palette</em>, (as per above), and you enter <em>dep</em> in the textbox:</p>

<p><img src="/images/posts/ads-install-bdc-install3.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Command Palette Deploy SQL Server</em></p>

<p>We see in <em>Figure 1</em> the command palette and how I entered <em>dep</em> in the text box, which then shows available commands. Among the commands we see <em>Deployment: Deploy SQL Server &hellip;</em>, (outlined in red).</p>

<p>Before we go any further I want to mention that we do not necessarily need to use the command palette to get to the <em>Deploy &hellip;</em> command:</p>

<p><img src="/images/posts/ads-install-bdc-install2.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Deploy via Connections</em></p>

<p>In <em>Figure 2</em> we see the <em>Deploy SQL Server &hellip;</em> command, (outlined in red), as we saw in <em>Figure 1</em>. In this case, we got to the command by:</p>

<ul>
<li>Clicking on the first icon from the top, (outlined in yellow), in the activity bar. The activity bar is the leftmost panel in ADS.</li>
<li>Click on the ellipsis, (outlined in blue), in the <em>CONNECTIONS</em> panel.</li>
</ul>

<p>So, we get to the <em>Deploy</em> command either through the command palette or the <em>CONNECTIONS</em> panel.</p>

<p>To start the deployment wizard, we click on the <em>Deploy</em> command as we see in <em>Figure 1</em>/<em>Figure 2</em>:</p>

<p><img src="/images/posts/ads-install-bdc-install4.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Deployment Options</em></p>

<p>We see in <em>Figure 3</em> how the deployment wizard supports different SQL Server deployment options:</p>

<ul>
<li>SQL on Windows.</li>
<li>SQL Container.</li>
<li>SQL Server Big Data Cluster.</li>
</ul>

<p>The default deployment option, as we see in <em>Figure 3</em>, is to deploy SQL Server as a container image. We also see that ADS checks whether we have the necessary tools installed for the deployment in question. In this case, I do not have <code>docker</code> installed, so there is an error.</p>

<p>Fortunately, I do not want to deploy a container but the SQL Server Big Data Cluster, so I click on the BDC option, (outlined in red):</p>

<p><img src="/images/posts/ads-install-bdc-install6.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>BDC Deployment Options</em></p>

<p>Clicking the BDC option we see something like in <em>Figure 4</em>: the <em>Select the deployment options</em> dialog. The dialog gives us the options for the BDC deployment:</p>

<ul>
<li>Version - at this stage only SQL Server 2019.</li>
<li>Deployment target - where we want to deploy the BDC to. In <em>Figure 4</em> we see how I chose to deploy to a new Azure Kubernetes Service cluster.</li>
</ul>

<p>We also see in <em>Figure 4</em> how ADS ensure that we have the required tools installed.</p>

<p>When deploying a BDC to AKS, we have to do some configuration before the actual deployment can happen, and the configuration consists of five steps:</p>

<ul>
<li>Configuration template / profile.</li>
<li>Azure settings.</li>
<li>BDC and Docker settings.</li>
<li>Service settings.</li>
<li>Summary of settings.</li>
</ul>

<h4 id="configuration-template">Configuration Template</h4>

<p>After we have chosen the SQL Server version and deployment target in <em>Figure 4</em>, we click <strong>Select</strong> and we get the dialog for the configuration template:</p>

<p><img src="/images/posts/ads-install-bdc-install7.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Configuration Template</em></p>

<p>The dialog we see in <em>Figure 5</em> allows us to choose a deployment profile. The profile defines things like how many instances we want of the various BDC components, and storage requirements. These settings can be changed later during the deployment process.</p>

<p>I chose the <code>aks-dev-test</code> profile with default values,</p>

<h4 id="azure-settings">Azure Settings</h4>

<p>Going on from the configuration template we see:</p>

<p><img src="/images/posts/ads-install-bdc-install8.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>Azure Settings</em></p>

<p>As we see in <em>Figure 6</em>, we now are at the settings for the Azure Kubernetes Service cluster.</p>

<p>All of the settings are relatively self-explanatory, and if you are unsure about any of the settings, you can refer back to my <a href="/2019/09/11/install-sql-server-2019-big-data-cluster-using-azure-data-studio/">previous post</a> about deploying a BDC via ADS.</p>

<p>It is worth noting that I have changed the <em>VM size</em>, and <em>VM count</em> from its default of <code>Standard_E4s_v3</code>, and <code>5</code> to <code>Standard_B8ms</code> and <code>3</code>. Reason for this is that having fewer nodes cuts down on install time.</p>

<p>The thing to bear in mind here is that a BDC deployment requires at a minimum around 24 hard disks altogether in your cluster, and each VM has a set number of disks. In my case, each <code>Standard_B8ms</code> VM has 16 disks so I should be good (3 * 16).</p>

<h4 id="cluster-settings">Cluster Settings</h4>

<p>Moving on from Azure settings:</p>

<p><img src="/images/posts/ads-install-bdc-install9.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>BDC Settings</em></p>

<p>The dialog we see in <em>Figure 7</em> is for configuring settings for the SQL Server BDC. As we see, there are two sections:</p>

<ul>
<li>Cluster settings.</li>
<li>Docker settings.</li>
</ul>

<p>When I set up a BDC, I do not use the default value for cluster name, and as you see in the figure I named the cluster: <code>sqlbdc-cluster</code>. Make sure you remember the password as you need it later.</p>

<p>At the moment, the only authentication mode supported is <em>Basic</em>, so we do not need to do anything there.</p>

<p>For the Docker settings, I go with the default values, and no user-name or password is required.</p>

<h4 id="service-settings">Service Settings</h4>

<p>Having done the cluster settings we are now almost finished, and we get to settings for the various BDC services:</p>

<p><img src="/images/posts/ads-install-bdc-install10.png" alt="" /></p>

<p><strong>Figure 8:</strong> <em>Service Settings</em></p>

<p>In service settings, as we see in <em>Figure 8</em>, we define how many instances of various services we want, endpoints for services, and also settings for storage.</p>

<p>When I deploy a BDC, I do not change any of these settings.</p>

<h4 id="settings-summary">Settings Summary</h4>

<p>We go on from the service settings:</p>

<p><img src="/images/posts/ads-install-bdc-install11.png" alt="" /></p>

<p><strong>Figure 9:</strong> <em>Settings Summary</em></p>

<p>The last step is not so much of a step where we do things, but - as we see in <em>Figure 9</em> - it is a summary of the settings we have defined in the previous steps.</p>

<p>In this final step, (before actual deployment), we can:</p>

<ul>
<li>Save the settings from previous steps to config files.</li>
<li>Go back and change settings.</li>
<li>Cancel out.</li>
<li>Script the settings to a notebook.</li>
</ul>

<p>The last option in the list above is what we choose when we deploy.</p>

<h2 id="deployment-notebook">Deployment Notebook</h2>

<p>When we click on <strong>Script to Notebook</strong> a Notebook opens:</p>

<p><img src="/images/posts/ads-install-bdc-install12.png" alt="" /></p>

<p><strong>Figure 10:</strong> <em>Deploy Notebook</em></p>

<p>We see in <em>Figure 10</em> the notebook that has been scripted for us based on the settings we defined in the steps above. Since we said we wanted to deploy to a new Azure Kubernetes Service Cluster, the Notebook creates a new AKS cluster for us together with deploying the BDC.</p>

<p>When you scroll through the notebook, you see the various stages of the deployment and what it does in each stage:</p>

<ul>
<li>Check dependencies.</li>
<li>Required information.</li>
<li>Azure settings.</li>
<li>Default settings.</li>
<li>Login to Azure.</li>
<li>Set active Azure subscription.</li>
<li>Create Azure resource group.</li>
<li>Create AKS cluster.</li>
<li>Set the new AKS cluster as current context.</li>
<li>Create a deployment configuration file.</li>
<li>Create SQL Server 2019 big data cluster.</li>
<li>Login to SQL Server 2019 big data cluster.</li>
<li>Show SQL Server 2019 big data cluster endpoints.</li>
<li>Connect to master SQL Server instance in Azure Data Studio.</li>
</ul>

<p>An example of the Notebook is below:</p>

<p><img src="/images/posts/ads-install-bdc-install13.png" alt="" /></p>

<p><strong>Figure 11:</strong> <em>Notebook Cells</em></p>

<p>In <em>Figure 11</em> we see some cells with code, and above the cells describing text.</p>

<p>To do the deployment, you can now either run each cell independently by clicking on the cell and hit F5 or click on the <strong>Run Cells</strong> command at the top of the notebook, (outlined in red in <em>Figure 10</em>). In either case, you see what command the cell executes as well as the outcome:</p>

<p><img src="/images/posts/inst-bdcrc1-cell-output.png" alt="" /></p>

<p><strong>Figure 12:</strong> <em>Cell Output</em></p>

<p>What you see in <em>Figure 12</em> is the output from creating the Azure resource group.</p>

<p>Be aware that the deployment takes a while, and especially the stage <em>Create SQL Server 2019 big data cluster</em>. Unfortunately, the Notebook does not give you much information where you are in the deployment, but you can use <code>kubectl</code> from the command line to get some feel for where you are in the process:</p>

<p><img src="/images/posts/ads-install-bdc-install16-get-pods.png" alt="" /></p>

<p><strong>Figure 13:</strong> <em>Get Pods - I</em></p>

<p>We see in <em>Figure 13</em> how I have executed <code>kubectl get pods -n sqlbdc-cluster</code> early in the deployment process.</p>

<blockquote>
<p><strong>NOTE:</strong> For the <code>-n</code> flag in the command I use the name of the BDC cluster I assigned in step 3, (<em>Cluster Settings</em>), above.</p>
</blockquote>

<p>We see that the deployment is busy deploying two pods related to the controller service. If I run the same command a bit later, I see:</p>

<p><img src="/images/posts/ads-install-bdc-install16-get-pods2.png" alt="" /></p>

<p><strong>Figure 14:</strong> <em>Get Pods - II</em></p>

<p>Now we see in <em>Figure 14</em> how more pods are deployed, and some of them also have a state of running, amongst them the control pods. As the control pods have a state of running, the controller service should now be up and running.</p>

<h4 id="controller-service">Controller Service</h4>

<p>The controller service is, as the name implies, what controls the BDC, and it is the controller service which interacts with the Kubernetes cluster. When deploying a BDC, the controller service is always deployed first so it can co-ordinate deployments with the Kubernetes service.</p>

<p>The controller service exposes an endpoint with which we can monitor the BDC. While the deployment is in process we can get to the IP address for the endpoint via a <code>kubectl</code> command:</p>

<pre><code class="language-bash">kubectl get svc -n sqlbdc-cluster
</code></pre>

<p><strong>Code Snippet 6:</strong> <em>Retrieve Endpoints</em></p>

<p>In <em>Code Snippet 6</em> we see how I call <code>kubectl get svc</code> with the name of the BDC cluster. The command lists all services in the specified namespace, (the <code>-n</code> flag), together with information about the services. Part of the information is the exposed IP address, (if any), of the service.</p>

<p>When I run the code in <em>Code Snippet 6</em> I see the following:</p>

<p><img src="/images/posts/ads-install-bdc-get-svc.png" alt="" /></p>

<p><strong>Figure 15:</strong> <em>Services</em></p>

<p>We see in <em>Figure 15</em> the controller service and its external IP address and port, (outlined in red). With this in hand, we can now use ADS to connect to the controller:</p>

<p><img src="/images/posts/ads-install-bdc-controller1.png" alt="" /></p>

<p><strong>Figure 16:</strong> <em>Add Big Data Cluster Controller</em></p>

<p>To add a BDC controller we expand the <em>SQL SERVER BIG DATA CLUSTERS</em> panel in ADS as we see in <em>Figure 16</em>, and click on the <code>+</code> sign. That gives us a connection dialog:</p>

<p><img src="/images/posts/ads-install-bdc-add-controller.png" alt="" /></p>

<p><strong>Figure 17:</strong> <em>Connect to BDC Controller</em></p>

<p>In the connection dialog we see in <em>Figure 17</em> we fill in the IP address and port we retrieved when we executed <em>Code Snippet 6</em>. The user name and password are the ones we defined in <em>Cluster Settings</em>, (<em>Figure 7</em>). We then click <strong>Add</strong> and the controller appears in the <em>SQL SERVER BIG DATA CLUSTERS</em> panel. Right-clicking on the controller we see:</p>

<p><img src="/images/posts/ads-install-bdc-controller-manage.png" alt="" /></p>

<p><strong>Figure 18:</strong> <em>Controller Manage</em></p>

<p>We see in <em>Figure 18</em> how we get a menu where we choose <strong>Manage</strong>:</p>

<p><img src="/images/posts/ads-install-bdc-controller2.png" alt="" /></p>

<p><strong>Figure 19:</strong> <em>BDC Dashboard</em></p>

<p>When we click <strong>Manage</strong> as in <em>Figure 18</em> we get a BDC dashboard as we see in <em>Figure 19</em>, and the dashboard gives us an overview of the cluster. Since we have not finished deploying yet, we see that quite a few services are in an unhealthy state. By refreshing now and then we see how the services move from unhealthy to healthy.</p>

<h4 id="deployment-finished">Deployment Finished</h4>

<p>Eventually, the deployment finishes, and we get an output from the cell <em>Create SQL Server 2019 Big Data Cluster</em>:</p>

<p><img src="/images/posts/ads-install-bdc-install17-finished.png" alt="" /></p>

<p><strong>Figure 20:</strong> <em>Deployment Finished</em></p>

<p>In <em>Figure 20</em> we see the output after a successful deployment. We can now go back to the controller service and see what it reports:</p>

<p><img src="/images/posts/ads-install-bdc-controller3.png" alt="" /></p>

<p><strong>Figure 21:</strong> <em>Healthy Cluster</em></p>

<p>As we see in <em>Figure 21</em> all services are green, yay! What we also see in <em>Figure 21</em> are service endpoints. The BDC exposes external endpoints for various services, and those are the ones you see in <em>Figure 21</em>. It is beyond the scope of this post to discuss what all those endpoints are, but the one outlined in red is the endpoint for the SQL Server master instance.</p>

<h2 id="connect-to-cluster">Connect to Cluster</h2>

<p>To connect to the SQL Server master instance, we use the endpoint outlined in red in <em>Figure 21</em>. We can either try and connect via clicking on the link for the endpoint or via the <strong>New Connection</strong> icon in the servers panel:</p>

<p><img src="/images/posts/ads-install-bdc-connect-servers.png" alt="" /></p>

<p><strong>Figure 22:</strong> <em>New Connection</em></p>

<p>Clicking on the <strong>New Connection</strong> icon as we see in <em>Figure 22</em> brings up a <em>Connection</em> dialog:</p>

<p><img src="/images/posts/ads-install-bdc-connect-admin.png" alt="" /></p>

<p><strong>Figure 23:</strong> <em>Connect to SQL Server</em></p>

<p>There are two things to notice in <em>Figure 23</em>:</p>

<ul>
<li>The Server IP includes a port number.</li>
<li>User name is not <code>sa</code> but the user name we defined in <em>Cluster Settings</em>, (<em>Figure 7</em>).</li>
</ul>

<p>The reason for the port number is that the default port number for SQL Server: <code>1433</code> is used within the BDC, and for external use <code>31433</code> is used as default. You can set the port number to something other than the default in the <em>Service Settings</em> step, (<em>Figure 8</em>).</p>

<p>Why we use <code>admin</code> as user name, and not <code>sa</code> - which every SQL DBA/Developer worth his/her salt loves - is that <code>sa</code> is by default disabled.</p>

<p>You would have noticed that if you tried to connect via the endpoint link, as that uses <code>sa</code> as user name. To enable <code>sa</code> we log in as <code>admin</code>, and then run the following code:</p>

<pre><code class="language-sql">USE master;
GO

ALTER LOGIN sa WITH PASSWORD=N'&lt;some-secret-pwd&gt;''
GO

ALTER LOGIN sa ENABLE;
GO
</code></pre>

<p><strong>Code Snippet 7:</strong> <em>Enable <code>sa</code></em></p>

<p>In <em>Code Snippet 7</em> we see how we set a super-secret password for <code>sa</code>, and then we enable the account.</p>

<p>We now have a fully functional BDC!</p>

<h2 id="summary">Summary</h2>

<p>In this post you saw how you can deploy a *<em>SQL Server 2019 Big Data Cluster</em> using <strong>Azure Data Studio</strong> and notebooks.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 50 &amp; Year End, 2019]]></title>
    <link href="https://nielsberglund.com/2019/12/15/interesting-stuff---week-50--year-end-2019/" rel="alternate" type="text/html"/>
    <updated>2019-12-15T07:21:12+02:00</updated>
    <id>https://nielsberglund.com/2019/12/15/interesting-stuff---week-50--year-end-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<blockquote>
<p><strong>NOTE:</strong> <em>It is now coming up on Christmas and New Year, and I will take a break with these posts and come back in the beginning of next year.</em></p>
</blockquote>

<p></p>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://medium.com/adaltas/should-you-move-your-big-data-and-data-lake-to-the-cloud-c26f6e1975c0">Should you move your Big Data and Data Lake to the Cloud</a>. The post linked to here discusses the pros and cons of running your data lake(s) in the &ldquo;cloud&rdquo;. Interesting read!</li>
</ul>

<h2 id="sql-server-2019">SQL Server 2019</h2>

<ul>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2019/12/11/tools-and-commands-for-running-sql-server-2019-on-linux/">Tools and commands for running SQL Server 2019 on Linux</a>. This post is about running Microsoft SQL Server 2019 on Linux. It provides necessary information that is good to know before upgrading or migrating SQL Server onto Linux.</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://medium.com/swlh/running-a-kubernetes-lab-for-windows-users-7ba2400216ad?">Creating a Kubernetes lab for Windows Users</a>. This is a &ldquo;cool&rdquo; post discussing how to set up Kubernetes on Windows using Minikube.</li>
<li><a href="https://www.infoq.com/presentations/building-microservices-events-functions/">Monoliths, Microservices, Events, Functions: What It Takes to Go Through the Transformation</a>. This is an <a href="https://www.infoq.com/">InfoQ</a> presentation discussing lessons learned building apps in a distributed environment.</li>
<li><a href="https://www.infoq.com/presentations/spring-reactor-microservices/">Building Reactive Pipelines: How to Go from Scalable Apps to (Ridiculously) Scalable Systems</a>. The <a href="https://www.infoq.com/">InfoQ</a> presentation linked to here discusses and demos reactive and highly scalable microservices built with Project Reactor using RabbitMQ, Apache Kafka, and Spring Cloud Stream.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/kafka-connect-tutorial-transfer-avro-schemas-across-schema-registry-clusters/">Transferring Avro Schemas Across Schema Registries with Kafka Connect</a>. The tutorial linked to here discusses how to replicate Avro schemas across independent Kafka Schema Registry clusters without overwriting any schemas. This is being made possible through the help of a custom Kafka Connect Single Message Transform (SMT). Very interesting read!</li>
<li><a href="https://towardsdatascience.com/kafdrop-e869e5490d62">Kafdrop: An Open-Source Kafka UI</a>. This post by <a href="https://twitter.com/IsTheArchitect">Emil</a> looks at an open-source Kafka Web UI: <strong>Kafdrop</strong>. Kafdrop looks very promising, and I&rsquo;ll definitely check it out. As a side note; you should really <a href="https://medium.com/@ekoutanov">follow Emil</a>, he posts awesome articles around event-driven architecture and microservices.</li>
<li><a href="https://charlla.com/kafka-donuts-on-ksqldb-2/">Kafka Donuts - on KSQLDB - #2</a>. The second post by my friend and colleague <a href="https://twitter.com/charllamprecht">Charl</a> about donuts and <strong>ksqlDB</strong>. You find the first post <a href="https://charlla.com/building-a-donut-shop-on-ksqldb/">here</a>.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 49, 2019]]></title>
    <link href="https://nielsberglund.com/2019/12/08/interesting-stuff---week-49-2019/" rel="alternate" type="text/html"/>
    <updated>2019-12-08T08:02:31+02:00</updated>
    <id>https://nielsberglund.com/2019/12/08/interesting-stuff---week-49-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://blog.acolyer.org/2019/12/02/efficient-lock-free-durable-sets/">Efficient lock-free durable sets</a>. The post linked to is a dissection by <a href="https://twitter.com/adriancolyer">Adrian</a> of a white paper discussing a new approach for durable concurrent sets and the use of this approach to build the most efficient durable hash tables available today.</li>
<li><a href="https://www.infoq.com/presentations/roadmap-chaos-experimentation/">A Roadmap Towards Chaos Engineering</a>. This presentation linked to here is an <a href="https://www.infoq.com/">InfoQ</a> presentation presenting a roadmap for Chaos Experimentation that can be applicable to any organization.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="http://tutorials.jenkov.com/data-streaming/stream-processing-api-designs.html">Stream Processing API Designs</a>. The post linked to is part of a series about data streaming. The post discusses different API designs for stream processing. It is a very interesting read!</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>The week just passed, I did two webinars for <a href="https://www.dataplatformgeeks.com/"><strong>DataPlatformGeeks</strong></a>, (DPG):</p>

<ul>
<li><a href="https://www.dataplatformgeeks.com/videos/a-lap-around-sql-server-2019-big-data-cluster-by-niels-berglund/">A Lap Around SQL Server 2019 Big Data Cluster</a>. An overview of SQL Server 2019 Big Data Cluster.</li>
<li><a href="https://www.dataplatformgeeks.com/videos/azure-data-studio-and-sql-server-2019-big-data-cluster-by-niels-berglund/">Azure Data Studio and SQL Server 2019 Big Data Cluster</a>. We look at using Azure Data Studio to deploy and manage a SQL Server 2019 Big Data Cluster.</li>
</ul>

<p>It was interesting doing webinars, but it is quite different than from doing conference talks. I had some technical difficulties in the first webinar, but as they say - &ldquo;You Live and Learn&rdquo;. Despite the technical difficulties, I certainly plan on doing more webinars.</p>

<p>I recommend all of you to <a href="https://www.dataplatformgeeks.com/registration">register</a> with DPG, as they have a plethora of free learning resources!</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 48, 2019]]></title>
    <link href="https://nielsberglund.com/2019/12/01/interesting-stuff---week-48-2019/" rel="alternate" type="text/html"/>
    <updated>2019-12-01T08:03:26+02:00</updated>
    <id>https://nielsberglund.com/2019/12/01/interesting-stuff---week-48-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="azure">Azure</h2>

<ul>
<li><a href="https://www.youtube.com/watch?v=ddo3e0IxAAA">Azure Arc for data services, including SQL and PostgreSQL</a>. At <a href="https://www.microsoft.com/en-us/ignite">Microsoft Ignite</a> earlier this year, Microsoft introduced <a href="https://azure.microsoft.com/en-us/services/azure-arc/">Azure Arc</a>, a service which enables deployment of Azure services anywhere and extends Azure management to any infrastructure. In the video linked to here <a href="https://twitter.com/radtravis">Travis Wright</a> takes a deep-dive into Azure Data Services as part of Azure Arc. He shows how you can provision and manage SQL Server or PostgreSQL databases regardless if they are on-prem, in Azure, or in any other data centers. Very, very Powerful!</li>
</ul>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://towardsdatascience.com/big-data-from-b-to-a-the-hadoop-distributed-filesystem-hdfs-992612cbf8aa">Big Data From B to A: The Hadoop Distributed Filesystem â€” HDFS</a>. If you work with or are interested in Big Data, you have most likely heard of Hadoop File System, (HDFS). The blog post linked to here discusses HDFS and looks at the design, architecture, and the data flow.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://towardsdatascience.com/https-medium-com-ekoutanov-introduction-to-event-streaming-with-kafka-and-kafdrop-303d5d0ceeec">Introduction to Event Streaming with Kafka and Kafdrop</a>. This post I link to here is an awesome introduction to Kafka and event streaming. I recommend the post highly!<br /></li>
<li><a href="https://charlla.com/building-a-donut-shop-on-ksqldb/">Kafka Donuts - on KSQLDB - #1</a>. In last weeks <a href="/2019/11/24/interesting-stuff---week-47-2019/">roundup</a> I covered the <a href="https://www.dataplatformgeeks.com/">introduction of <strong>ksqlDB</strong></a>. My good friend, and colleague, <a href="https://twitter.com/charllamprecht">Charl Lamprecht</a> who loves Kafka almost as much as he loves doughnuts did a dive into how <strong>ksqlDB</strong> works and the post I link to here is the result. Please go ahead and read it - it gives you a good insight into <strong>ksqlDB</strong>!</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>This coming week I am delivering two webinars for <a href="https://www.dataplatformgeeks.com/">DataPlatformGeeks</a>:</p>

<p><strong>3rd December:</strong></p>

<ul>
<li><a href="https://www.dataplatformgeeks.com/events/dpg-webinar-03rd-december-2019-0300-pm-0400-pm-ist-niels-berglund-a-lap-around-sql-server-2019-big-data-cluster/">A Lap Around SQL Server 2019 Big Data Cluster</a>. An overview of SQL Server 2019 Big Data Cluster.</li>
</ul>

<p><strong>5th December:</strong></p>

<ul>
<li><a href="https://www.dataplatformgeeks.com/events/dpg-webinar-05th-december-2019-0300-pm-0400-pm-ist-niels-berglund-azure-data-studio-and-sql-server-2019-big-data-cluster/">Azure Data Studio and SQL Server 2019 Big Data Cluster</a>. We look at using Azure Data Studio to deploy and manage a SQL Server 2019 Big Data Cluster.</li>
</ul>

<p>The webinars are free, so if you have time, register and join the fun! Notice that the times in the links above are Indian Standard Time, so 3 pm IST is 09:30, (am), UTC.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 47, 2019]]></title>
    <link href="https://nielsberglund.com/2019/11/24/interesting-stuff---week-47-2019/" rel="alternate" type="text/html"/>
    <updated>2019-11-24T08:37:03+02:00</updated>
    <id>https://nielsberglund.com/2019/11/24/interesting-stuff---week-47-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/conde-nast-chaos-resilience/">The Future of Chaos Engineering: In Pursuit of the Unknown Unknowns</a>. This is an <a href="https://www.infoq.com/">InfoQ</a> presentation where the presenter discusses how chaos and resilience practices must evolve to keep pace with the challenges of growing complexity.</li>
</ul>

<h2 id="sql-server-2019">SQL Server 2019</h2>

<ul>
<li><a href="/2019/11/20/fix-polybase-in-sql-server-2019-developers-edition/">Fix PolyBase in SQL Server 2019 Developers Edition</a>. This post is by yours truly, discussing how to fix an issue with PolyBase not starting up correctly after installing SQL Server 2019 Developers Edition.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://medium.com/trending-information-technologies/stream-processing-17384a23111f">Real-World Data Stream Processing</a>. In this post, the author discusses using streaming technologies with Kafka, Spark, and Cassandra to gain insights on data.</li>
<li><a href="https://www.confluent.io/blog/intro-to-ksqldb-sql-database-streaming/">Introducing ksqlDB</a>. This post discusses <a href="https://ksqldb.io/"><strong>ksqlDB</strong></a>, the latest release of KSQL. There are essentially two new features in the release, but they are so far-reaching that Confluent decided to rename KSQL to <strong>ksqlDB</strong>. I will keep an eye on what Confluent is doing in this space - I think we will see loads of interesting &ldquo;stuff&rdquo; happening!</li>
<li><a href="https://www.confluent.io/blog/kafka-streams-vs-ksqldb-compared/">Kafka Streams and ksqlDB Compared â€“ How to Choose</a>. So, KSQL and <strong>ksqlDB</strong>, (as per above), uses the Kafka Streams API, (KStreams), under the covers. This blog-post discusses when to use KStreams vs <strong>ksqlDB</strong>.</li>
<li><a href="https://www.youtube.com/watch?v=D5QMqapzX8o">ksqlDB Demo | The Event Streaming Database in Action</a>. Here is a <strong>ksqlDB</strong> demo video to round off the <strong>ksqlDB</strong> coverage with.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>The first week of December, I am delivering two webinars for <a href="https://www.dataplatformgeeks.com/">DataPlatformGeeks</a>:</p>

<p><strong>3rd December:</strong></p>

<ul>
<li><a href="https://www.dataplatformgeeks.com/events/dpg-webinar-03rd-december-2019-0300-pm-0400-pm-ist-niels-berglund-a-lap-around-sql-server-2019-big-data-cluster/">A Lap Around SQL Server 2019 Big Data Cluster</a>. An overview of SQL Server 2019 Big Data Cluster.</li>
</ul>

<p><strong>5th December:</strong></p>

<ul>
<li><a href="https://www.dataplatformgeeks.com/events/dpg-webinar-05th-december-2019-0300-pm-0400-pm-ist-niels-berglund-azure-data-studio-and-sql-server-2019-big-data-cluster/">Azure Data Studio and SQL Server 2019 Big Data Cluster</a>. We look at using Azure Data Studio to deploy and manage a SQL Server 2019 Big Data Cluster.</li>
</ul>

<p>The webinars are free, so if you have time, register and join the fun! Notice that the times in the links above are Indian Standard Time, so 3 pm IST is 09:30, (am), UTC.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Fix PolyBase in SQL Server 2019 Developers Edition]]></title>
    <link href="https://nielsberglund.com/2019/11/20/fix-polybase-in-sql-server-2019-developers-edition/" rel="alternate" type="text/html"/>
    <updated>2019-11-20T05:52:17+02:00</updated>
    <id>https://nielsberglund.com/2019/11/20/fix-polybase-in-sql-server-2019-developers-edition/</id>
    <content type="html"><![CDATA[<p>At <a href="https://www.microsoft.com/en-us/ignite">MS Ignite</a> in Orlando November 4 - 8, 2019, Microsoft announced the general availability of SQL Server 2019. At the same time, the SQL Server 2019 Developers Edition appeared as an MSDN download, and of course, I downloaded it and installed it on my dev box.</p>

<p>After the installation, I noticed that PolyBase did not start up correctly, and I saw dump files all over the place. After some investigation, I figured out what the issue was, and this blog post describes the fix.</p>

<p></p>

<h2 id="polybase-intro">PolyBase Intro</h2>

<p>Before we go into the issue and solution, let us do a quick introduction of PolyBase and also look at the installation of PolyBase.</p>

<p>PolyBase enables you to, in your SQL Server instance, process Transact-SQL queries that read data from external data sources. Microsoft introduced PolyBase in SQL Server 2016, and in that version of PolyBase the only data-sources you could process data from were: Hadoop and Azure Blob Storage. That changed in SQL Server 2019 where PolyBase now supports:</p>

<ul>
<li>Hadoop.</li>
<li>Azure Blob Storage.</li>
<li>SQL Server.</li>
<li>Oracle.</li>
<li>Teradata.</li>
<li>MongoDB</li>
<li>ODBC Generic Types, (only PolyBase in SQL Server 2019 on Windows).</li>
</ul>

<h4 id="installation">Installation</h4>

<p>You either install PolyBase as part of a regular installation of a SQL Server instance, or you add it after installation. In both cases, you select it during feature selection:</p>

<p><img src="/images/posts/sql-2k19-polybase-issue-install.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>PolyBase Feature Selection</em></p>

<p>In <em>Figure 1</em> we see how I chose both the <em>PolyBase Query Service</em> as well as the <em>Java connector for HDFS data sources</em>. After clicking on from there passing <em>Feature Rules</em>, and <em>Instance Configuration</em> we get to <em>PolyBase Configuration</em>:</p>

<p><img src="/images/posts/sql-2k19-polybase-issue-install2.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>PolyBase Configuration</em></p>

<p>As we see from <em>Figure 2</em> I choose a standalone PolyBase-enabled instance.</p>

<p>I click on from there and eventually the installation finishes:</p>

<p><img src="/images/posts/sql-2k19-polybase-issue-install3.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Installation Complete</em></p>

<p>We see in <em>Figure 3</em> how all components are green which indicates that the installation has succeeded. Cool!</p>

<h2 id="problem">Problem</h2>

<p>But has the installation succeeded? What usually I do after a SQL installation is to go to <em>Services</em> and change properties for the various SQL services, (automatic startup to manual startup, etc.). When I do this, I see:</p>

<p><img src="/images/posts/sql-2k19-polybase-issue-services.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Services</em></p>

<p>What we see in <em>Figure 4</em> is a view of services a couple of minutes after the installation. We see that the SQL Server service, (<code>MSSQLSERVER</code>), is in <code>Running</code> state. However, the two PolyBase services are still in a <code>Starting</code> state. Something must have gone wrong, but what!</p>

<blockquote>
<p><strong>NOTE:</strong> Depending on how long after installation you look at services you may see the PolyBase Engine in a stopped state, but the Data Movement still in a <code>Starting</code> state.</p>
</blockquote>

<p>Let us see if we can try and figure out what is wrong. We start by going to the SQL Server log files directory, which is at: <code>&lt;path_to_sql_instance&gt;\MSSQL\Log</code>. In there you find a <code>Polybase</code> directory with a <code>dump</code> subdirectory:</p>

<p><img src="/images/posts/sql-2k19-polybase-issue-dump-dir.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>PolyBase Dump Directory</em></p>

<p>In <em>Figure 5</em> we see how the <code>dump</code> directory contains a <code>.dmp</code> file. Maybe we can find any clues if we open it in <strong>WinDbg</strong>:</p>

<pre><code class="language-bash">Comment: 'Stack Trace'
Comment: 
  '&lt;Identity&gt;
     &lt;Element key=&quot;CodePackageName&quot; val=&quot;DwDms.Code&quot;/&gt;
     [snip]
     &lt;Element key=&quot;ExceptionMessage&quot; val=&quot;A network-related or 
      instance-specific error occurred while establishing a connection 
      to SQL Server. The server was not found or was not accessible. 
      Verify that the instance name is correct and that SQL Server is 
      configured to allow remote connections. 
      (provider: TCP Provider, error: 0 - The remote computer refused 
       the network connection.)&quot;/&gt;
     &lt;Element key=&quot;ExceptionType&quot; val=&quot;System.Data.SqlClient.SqlException&quot;/&gt;
     [snip]
     &lt;/Identity&gt;'
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Exception in File</em></p>

<p>Hmm, in <em>Code Snippet 1</em> we see an excerpt of the top of the file, (I have &ldquo;snipped&rdquo; out parts for brevity), and we see an error about how PolyBase cannot connect to SQL Server via the TCP provider.</p>

<h2 id="fix">Fix</h2>

<p>The error we see in <em>Code Snippet 1</em> is a big clue since TCP is not an enabled network protocol by default for SQL Server Developer edition.</p>

<p>Let us enable TCP/IP as a network protocol for SQL Server. We enable network protocols using the <em>SQL Server Configuration Manager</em>. Open <em>SQL Server Configuration Manager</em> either via the Windows <strong>Start</strong> menu or from <em>Computer Manager</em>:</p>

<p><img src="/images/posts/sql-2k19-polybase-issue-compmngr.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>SQL Server Configuration Manager</em></p>

<p>In <em>Figure 6</em> we see, outlined in blue, <em>SQL Server Configuration Manager</em>, and how we have expanded <em>SQL Server Network Configuration</em>, (outlined in red). On the right-hand side in <em>Figure 6</em> we see how the TCP/IP protocol is indeed disabled.</p>

<p>To enable we right-click on the TCP/IP protocol and in the menu choose <strong>Enable</strong>. When we do that a <em>Warning</em> dialog &ldquo;pops&rdquo; up:</p>

<p><img src="/images/posts/sql-2k19-polybase-issue-enable-warning.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>Warning Dialog</em></p>

<p>So, we see in <em>Figure 7</em> that any changes will not happen until the SQL Server instance is restarted. Cool, let us restart the service. We go back to <em>Services</em> as in <em>Figure 4</em>, and we right-click on the SQL Server service, and choose <strong>Restart</strong>:</p>

<p><img src="/images/posts/sql-2k19-polybase-issue-stop-service.png" alt="" /></p>

<p><strong>Figure 8:</strong> <em>Stop Services</em></p>

<p>When we try to restart we get the warning we see in <em>Figure 8</em>, telling us we need to also restart the <em>PolyBase Data Movement</em> service, and the <em>Launchpad</em> service.</p>

<blockquote>
<p><strong>NOTE:</strong> The <em>Launchpad</em> service is part of SQL Server Machine Learning Services.</p>
</blockquote>

<p>I click <strong>Yes</strong> as in <em>Figure 8</em>, and almost immediately I see:</p>

<p><img src="/images/posts/sql-2k19-polybase-issue-stop-control.png" alt="" /></p>

<p><strong>Figure 9:</strong> <em>Can Not Stop Service</em></p>

<p>The problem that manifests itself by what we see in <em>Figure 9</em> is that the <em>PolyBase Data Movement</em> service is in a <code>Starting</code> state, and cannot be stopped.</p>

<p>What do we do now? Well, we can try and restart the machine, but that seems like overkill, (and it may not work). So let us try to use a command from the command prompt, where we can force the termination of a service: <code>taskkill</code>.</p>

<p>To use the <code>taskkill</code> command, we need to know either the process-id, (<code>PID</code>), of the process we want to terminate, or the process name. To get the <code>PID</code> we open <em>Task Manager</em> and under the services tab we find the SQL Server service:</p>

<p><img src="/images/posts/sql-2k19-polybase-issue-task-manager.png" alt="" /></p>

<p><strong>Figure 10:</strong> <em>Task Manager</em></p>

<p>We see in <em>Figure 10</em> the <code>PID</code>, (highlighted in yellow), which we now use to terminate the SQL Server process:</p>

<p><img src="/images/posts/sql-2k19-polybase-issue-task-kill.png" alt="" /></p>

<p><strong>Figure 11:</strong> <em>Task Manager</em></p>

<p>As we see in <em>Figure 11</em> I called <code>taskkill</code> from command prompt which I opened as administrator. I used the process-id from <em>Figure 10</em>, and I used the <code>/F</code> flag to indicate I wanted to force the termination. The command executed successfully, and as we see in <em>Figure 11</em> the process has been terminated.</p>

<p>What is left to do now is to start the SQL Server process, followed by the PolyBase Data Movement service and the PolyBase Engine service. The PolyBase Data Movement service may still be in the state of `Starting, but by right-clicking on the service and choose <strong>Start</strong> it will start:</p>

<p><img src="/images/posts/sql-2k19-polybase-issue-services-running.png" alt="" /></p>

<p><strong>Figure 12:</strong> <em>Services Running</em></p>

<p>We see in <em>Figure 12</em> how the services are up and running.</p>

<h2 id="summary">Summary</h2>

<p>In this post, we saw how we fix the issue where the PolyBase services do not start on a SQL Server 2019 Developer Edition.</p>

<p>We said it was due to that TCP/IP is not by default an enabled SQL Server network protocol for the Developer edition. Therefore the PolyBase services cannot connect to the SQL Server instance at startup. We fixed it by:</p>

<ul>
<li>Enable the TCP/IP protocol under <em>SQL Server Network Configuration</em> in <em>SQL Server Configuration Manager</em>.</li>
<li>Retrieve the process-id, (<code>PID</code>), for the SQL Server service.</li>
<li>Forcing the process to be terminated by the <code>taskkill /PID &lt;process-id&gt; /F</code> command.</li>
<li>Restart the SQL Server service, followed by the PolyBase services.</li>
</ul>

<p>Another way to do this is to install SQL Server without the PolyBase features. After installation you enable the TCP/IP protocol, restart SQL Server, and then install the PolyBase features.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 46, 2019]]></title>
    <link href="https://nielsberglund.com/2019/11/17/interesting-stuff---week-46-2019/" rel="alternate" type="text/html"/>
    <updated>2019-11-17T07:47:05+02:00</updated>
    <id>https://nielsberglund.com/2019/11/17/interesting-stuff---week-46-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://devblogs.microsoft.com/dotnet/building-modern-cloud-applications-using-pulumi-and-net-core/">Building Modern Cloud Applications using Pulumi and .NET Core</a>. As you probably know, <a href="https://www.pulumi.com/">Pulumi</a> is an open source infrastructure as code tool which makes it easy to create, deploy, and manage cloud applications, using your favorite development languages. The post I link to announces .NET Core support for Pulumi.</li>
</ul>

<h2 id="data-science-machine-learning">Data Science / Machine Learning</h2>

<ul>
<li><a href="https://blog.revolutionanalytics.com/2019/11/azure-ai-and-machine-learning-talk-series.html">Azure AI and Machine Learning talk series</a>. This is a post by <a href="https://twitter.com/revodavid">David</a>, where lists the talks he and his team did at Microsoft Ignite. These talks are also presented worldwide at <a href="https://www.microsoft.com/en-us/ignite-the-tour/"><strong>Microsoft Ignite The Tour</strong></a> during the next six months.<br /></li>
</ul>

<h2 id="sql-server-2019-big-data-cluster">SQL Server 2019 Big Data Cluster</h2>

<ul>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2019/11/11/build-an-intelligent-analytics-platform-with-sql-server-2019-big-data-clusters/">Build an intelligent analytics platform with SQL Server 2019 Big Data Clusters</a>. This post gives a high-level view of one can create a powerful analytics platform on top of SQL Server 2019 Big Data Cluster.</li>
</ul>

<h2 id="microsoft-ignite-the-tour">Microsoft Ignite The Tour</h2>

<ul>
<li><p><a href="https://www.microsoft.com/en-za/ignite-the-tour/johannesburg">Microsoft Ignite The Tour Johannesburg</a>. I mentioned <strong>Microsoft Ignite the Tour</strong> above, and this is for my readers in South Africa. <strong>Microsoft Ignite the Tour</strong> visits Johannesburg, January 30 - 31, 2020, so register now to not miss out on the latest in cloud technologies and developer tools. Oh, and by the way, I deliver three talks in Johannesburg:</p>

<ul>
<li><strong>A Lap Around SQL Server Big Data Cluster</strong>.</li>
<li><strong>Improve Customer Lifetime Value using Azure Databricks &amp; Delta Lake</strong>.</li>
<li><strong>Simplify and Scale Your Data Pipelines with Azure Delta Lake</strong>.</li>
</ul></li>
</ul>

<p>So <a href="https://www.microsoft.com/en-za/ignite-the-tour/johannesburg">register</a> now, and come and say Hi!</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
</feed>

