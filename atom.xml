<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Niels Berglund</title>
  <link href="https://nielsberglund.com/atom.xml" rel="self"/>
  <link href="https://nielsberglund.com"/>
  <updated>2022-02-13T17:27:47+02:00</updated>
  <id>https://nielsberglund.com/</id>
  <generator uri="http://gohugo.io/">Hugo</generator>

  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 6, 2022]]></title>
    <link href="https://nielsberglund.com/2022/02/13/interesting-stuff---week-6-2022/" rel="alternate" type="text/html"/>
    <updated>2022-02-13T17:27:47+02:00</updated>
    <id>https://nielsberglund.com/2022/02/13/interesting-stuff---week-6-2022/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://databricks.com/blog/2022/02/10/databricks-delta-live-tables-announces-support-for-simplified-change-data-capture.html">Databricks Delta Live Tables Announces Support for Simplified Change Data Capture</a>. I don&rsquo;t know what category this post falls under, but Big Data sounds OK:ish, so that&rsquo;s where it ends up. Anyway, the post looks at <a href="https://databricks.com/product/delta-live-tables">Delta Live</a> table&rsquo;s support for CDC. Cool stuff!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://databricks.com/blog/2022/02/07/structured-streaming-a-year-in-review.html">Structured Streaming: A Year in Review</a>. This post gives an overview of all the new Structured Streaming features developed in 2021 for Databricks and Apache Spark. As you see, it is quite a lot!</li>
<li><a href="https://www.confluent.io/blog/intro-to-ksqldb-sql-database-streaming/">Introducing ksqlDB</a>. The post linked to is from back in November 2019, and I must have missed it then. I came across it as I was &ldquo;researching&rdquo; ksqlDB (read browsing). The post introduces pull queries and Kafka Connect connector management from inside ksqlDB. As cool as that is, that was not what caught my eye. No, what I found interesting was the section around ksqlDB&rsquo;s internal architecture, very cool!</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 5, 2022]]></title>
    <link href="https://nielsberglund.com/2022/02/06/interesting-stuff---week-5-2022/" rel="alternate" type="text/html"/>
    <updated>2022-02-06T12:51:07+02:00</updated>
    <id>https://nielsberglund.com/2022/02/06/interesting-stuff---week-5-2022/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="cloud">Cloud</h2>

<ul>
<li><a href="https://www.tbray.org/ongoing/When/202x/2022/01/30/Cloud-Lock-In">Lock-in and Multi-Cloud</a>. This is an excellent post by <a href="https://en.wikipedia.org/wiki/Tim_Bray">Tim Bray</a>, where he looks at various options for &ldquo;going to the cloud&rdquo;. As the post title implies, he also looks at the perception of lock-in and the fear thereof. As I mentioned in the beginning, this is an excellent post. Thanks to this post, I also got to an old post (from 2003) of his: <a href="https://www.tbray.org/ongoing/When/200x/2003/03/10/GigaTeraPeta"><em>Half a Billion Bibles</em></a>, where he puts data sizes in perspective.</li>
</ul>

<h2 id="data-architecture">Data Architecture</h2>

<ul>
<li><a href="https://databricks.com/blog/2022/01/31/make-your-data-lakehouse-run-faster-with-delta-lake-1-1.html">Make Your Data Lakehouse Run, Faster With Delta Lake 1.1</a>. The 1.1 release of Databricks&rsquo; Delta Lake has some significant performance improvements. This post goes over the major changes and notable features in this release. There is some very cool stuff in there!</li>
<li><a href="https://towardsdatascience.com/data-mesh-pattern-deep-dive-event-streaming-backbone-99a5bb2a7cbf">Data Mesh Patterns: Event Streaming Backbone</a>. This post is the second in a series of articles on Foundational Data Mesh Patterns, and it discusses the <em>Event Streaming Backbone</em> pattern. The post is very interesting, and if you are interested in Data Mesh and/or Event Driven architectures you should read it.</li>
</ul>

<h2 id="azure-data-explorer">Azure Data Explorer</h2>

<ul>
<li><a href="https://techcommunity.microsoft.com/t5/azure-data-explorer-blog/kibana-dashboards-and-visualizations-on-top-of-azure-data/ba-p/3062838">Kibana dashboards and visualizations on top of Azure Data Explorer are now supported with K2Bridge</a>. If you are a Kibana user, this post is for you! It discusses how you can now easily migrate to Azure Data Explorer (ADX) while keeping Kibana as your visualization tool, alongside the other Azure Data Explorer experiences and the powerful KQL language.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/streaming-etl-sfdc-data-for-real-time-customer-analytics/">Streaming ETL SFDC Data for Real-Time Customer Analytics</a>. Confluent relies heavily on Salesforce data for marketing and other purposes, where the Salesforce data is loaded into Google Big Query. This blog post shares how Confluent leverages Confluent Cloud connectors, Schema Registry, ksqlDB, and Kafka Streams (KStreams) to build a streaming ETL pipeline to send Salesforce data to BigQuery. It is cool to see how Confluent &ldquo;eats their own dog-food&rdquo;!</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>It is getting closer:</p>


<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="//www.youtube.com/embed/wQwQIy_wwes" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"></iframe>
</div>


<p>The &ldquo;trailer&rdquo; above is my attempt to &ldquo;shameless self-promotion&rdquo; of my one day <a href="https://arcade.sqlbits.com/sessions/"><strong>Azure Data Explorer</strong> training class</a> at <strong>SQLBits 2022</strong> in London next month. There are still some seats left so, if you are interested, go ahead and <a href="https://register.sqlbits.com/event/36db4730-583c-49c1-adcf-f76432bb6580/summary"><strong>REGISTER</strong></a>!</p>

<p>Speaking of registering:</p>

<p><img src="/images/posts/stream-processing-kafka.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Stream Processing with Apache Kafka and .NET</em></p>

<p>This coming Wednesday (February 9), I present <a href="https://www.meetup.com/NET-to-the-Core/events/283278548"><strong>Stream Processing with Apache Kafka and .NET</strong></a> at the <strong>.NET to the Core</strong> meetup user group. Register for free <a href="https://www.meetup.com/NET-to-the-Core/events/283278548">here</a>.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 4, 2022]]></title>
    <link href="https://nielsberglund.com/2022/01/30/interesting-stuff---week-4-2022/" rel="alternate" type="text/html"/>
    <updated>2022-01-30T12:46:36+02:00</updated>
    <id>https://nielsberglund.com/2022/01/30/interesting-stuff---week-4-2022/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="misc">Misc.</h2>

<ul>
<li><a href="https://netflixtechblog.com/fixing-performance-regressions-before-they-happen-eab2602b86fe">Fixing Performance Regressions Before they Happen</a>. This <a href="https://netflixtechblog.com/">Netflix</a> blog post looks at one of the strategies they use to quickly and easily detect performance anomalies before they are released — and often before they are even committed to the codebase.</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://hongilkwon.medium.com/when-to-use-two-phase-commit-in-distributed-transaction-f1296b8c23fd">When to Use Two Phase Commit in Distributed Transaction</a>. Ah, one of my favorite topics - distributed transactions and 2 phase commit! This post looks at helping you understand the pros and cons of the 2PC algorithm and build capability to thoroughly assess the trade-off between different algorithms for your future system designs.</li>
<li><a href="http://muratbuffalo.blogspot.com/2022/01/decoupled-transactions-low-tail-latency.html">Decoupled Transactions: Low Tail Latency Online Transactions Atop Jittery Servers (CIDR 2022)</a>. In <a href="/2022/01/23/interesting-stuff---week-3-2022/">last weeks roundup</a>, I mentioned the CIDR conference and how the talks were available on YouTube. The post linked to by <a href="https://twitter.com/muratdemirbas">Murat</a> looks at a White Paper submitted to the conference by Pat Helland. If you are not interested in reading the post (or the paper), Pat&rsquo;s talk based on the white paper is now up on YouTube: <a href="https://youtu.be/72UZ8DxPa8o">Decoupled Transactions: Low Tail Latency Online Transactions Atop Jittery Servers</a>. The video is very, very &ldquo;watch worthy&rdquo;!</li>
</ul>

<h2 id="azure-data-explorer">Azure Data Explorer</h2>

<ul>
<li><a href="https://techcommunity.microsoft.com/t5/azure-data-explorer-blog/adx-dashboards-january-2022-updates/ba-p/3074426">ADX dashboards January 2022 updates</a>. As the title implies, this post looks at new functionality introduced for dashboards in <strong>Azure Data Explorer</strong>. Hmm, the ability to export dashboards, very cool!</li>
</ul>

<h2 id="data-architecture">Data Architecture</h2>

<ul>
<li><a href="https://www.datanami.com/2022/01/20/all-eyes-on-snowflake-and-databricks-in-2022/">All Eyes on Snowflake and Databricks in 2022</a>. It&rsquo;s hard to overstate Snowflake and Databricks&rsquo; impact on the data industry for customers, partners, and competitors. This blog post looks at both Databricks and Snowflake and tries to determine what 2022 has in store.</li>
<li><a href="https://medium.com/towards-data-science/modern-data-stack-which-place-for-spark-8e10365a8772">Modern Data Stack: Which Place for Spark ?</a>. This very interesting post looks at why you rarely see any mentions of Apache Spark in articles about the Modern Data Stack.</li>
<li><a href="https://medium.com/event-driven-utopia/building-reference-architectures-for-user-facing-analytics-dc11c7c89df3">Building Reference Architectures for User-Facing Analytics</a>. User-facing analytics can be challenging to implement and, more specifically, user-facing analytics <strong>at scale</strong>. The post, written by <a href="https://twitter.com/dunithd">Dunith Dhanushka</a>, discusses several blueprints for building scalable analytics infrastructure to deliver user-facing analytics. This is a <strong>MUST</strong> read post if you are interested in real-time user-facing analytics.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/apache-kafka-3-1-version-features-and-updates/">What&rsquo;s New in Apache Kafka 3.1.0</a>. This post highlights some of the more prominent features in the newly released 3.1.0 version of Apache Kafka.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>In the coming days and weeks, I will be prepping for quite a few conference talks:</p>

<p><img src="/images/posts/stream-processing-kafka.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Stream Processing with Apache Kafka and .NET</em></p>

<ul>
<li><a href="https://www.meetup.com/NET-to-the-Core/events/283278548"><strong>Stream Processing with Apache Kafka and .NET</strong></a>. This is a Meetup user group talk, where I discuss Kafka, .NET and stream-processing.</li>
</ul>

<p><img src="/images/posts/sqlbits-precon-adx.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>SQLBITS 2022 - A Day of Azure Data Explorer</em></p>

<ul>
<li><a href="https://arcade.sqlbits.com/sessions/"><strong>A Day of Azure Data Explorer</strong></a>. This is a one-day training class about Azure Data Explorer at SQLBits 2022. I am presenting live (woohoo), but you can also attend virtually.</li>
</ul>

<p><img src="/images/posts/ksqldb-streaming-db.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>ksqlDB - The Real-Time Streaming Database</em></p>

<ul>
<li><a href="https://arcade.sqlbits.com/sessions/"><strong>ksqlDB - The Real-Time Streaming Database</strong></a>. An SQLBits 2022 conference talk, where I - in this demo filled session - look at using ksqlDB to gain real-time insights into streaming data. We look at push/pull queries, User Defined Functions, tables, streams, and <em>other cool stuff</em>.</li>
</ul>

<p><img src="/images/posts/analyze-billions-adx-sqlbits.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Analyze Billions of Rows of Data in Real-Time Using Azure Data Explorer</em></p>

<ul>
<li><a href="https://arcade.sqlbits.com/sessions/"><strong>Analyze Billions of Rows of Data in Real-Time Using Azure Data Explorer</strong></a>. This is my second conference talk at SQLBits 2022 (not counting the training day). In this session (full of demos), we get an overview of Azure Data Explorer. We look at how to ingest data and query in real-time against billions of rows with sub-second latency.</li>
</ul>

<p>For the SQLBits related &ldquo;stuff&rdquo; (training day and conference), you register <a href="https://register.sqlbits.com/event/36db4730-583c-49c1-adcf-f76432bb6580/summary">here</a>.</p>

<p>I hope to see you either at the <a href="https://www.meetup.com/NET-to-the-Core/events/283278548"><strong>.NET to the Core</strong></a> session or at <a href="https://register.sqlbits.com/event/36db4730-583c-49c1-adcf-f76432bb6580/summary"><strong>SQLBits</strong></a>!</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 3, 2022]]></title>
    <link href="https://nielsberglund.com/2022/01/23/interesting-stuff---week-3-2022/" rel="alternate" type="text/html"/>
    <updated>2022-01-23T08:40:56+02:00</updated>
    <id>https://nielsberglund.com/2022/01/23/interesting-stuff---week-3-2022/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="databases">Databases</h2>

<ul>
<li><a href="http://cidrdb.org/cidr2022/program.html">CIDR 2022 Conference</a>. The Conference on Innovative Data Systems Research (CIDR) is a systems-oriented conference emphasizing the systems architecture perspective. CIDR 2022 was held a couple of weeks ago, and the link is to the table of content, which has links to the given talks. There are some very interesting presentations there.</li>
<li><a href="https://starrocks.medium.com/trip-com-starrocks-efficiently-supports-high-concurrent-queries-dramatically-reduces-labor-and-1e1921dd6bf8">Trip.com：StarRocks efficiently supports high concurrent queries, dramatically reduces labor and hardware cost</a>. The last few years have seen an emergence of real-time distributed OLAP datastore&rsquo;s; Druid, Apache Pinot, Clickhouse, etc. The link here is a post about StarRocks, a fairly new entry into the market. The post compares StarRocks with ClickHouse, and StarRocks comes out quite favourable.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/real-time-data-integrations-at-scale-with-confluent-q1-22-launch/">Announcing the Confluent Q1 &lsquo;22 Launch</a>. This blog post announces new features for Confluent Cloud being launched in Q1 2022. One new &ldquo;feature&rdquo; that I like is the new Confluent CLI <code>confluent v2</code>. With <code>confluent v2</code>, I no longer need separate CLI&rsquo;s for on-prem and cloud!</li>
<li><a href="https://www.confluent.io/blog/streaming-data-pipeline-with-apache-kafka-and-ksqldb/">Introduction to Streaming Data Pipelines with Apache Kafka and ksqlDB</a>. This blog post describes the elements involved in setting up a Kafka-based data pipeline: connecting data entities together; streaming data from source(s) into the middle of the pipeline; filtering, joining, and enriching the data with ksqlDB. Cool stuff!</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>If you read my <a href="/2021/12/05/interesting-stuff---week-49-2021/">roundup from week 49 last year</a>, you do know:</p>

<p><img src="/images/posts/sqlbits-precon-adx.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>SQLBITS 2022 - A Day of Azure Data Explorer</em></p>

<p>Yes, I am presenting a one-day training class about Azure Data Explorer at SQLBits 2022. Referring back to what I am doing, I did a promo video for my class last evening, so go to <a href="https://youtu.be/wQwQIy_wwes"><strong>A Day of Azure Data Explorer</strong></a> and have a look. Then <a href="https://register.sqlbits.com/event/36db4730-583c-49c1-adcf-f76432bb6580/summary"><strong>register</strong></a> for <a href="https://register.sqlbits.com/event/36db4730-583c-49c1-adcf-f76432bb6580/summary"><strong>SQLBits 2022</strong></a>.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 2, 2022]]></title>
    <link href="https://nielsberglund.com/2022/01/16/interesting-stuff---week-2-2022/" rel="alternate" type="text/html"/>
    <updated>2022-01-16T08:56:07+02:00</updated>
    <id>https://nielsberglund.com/2022/01/16/interesting-stuff---week-2-2022/</id>
    <content type="html"><![CDATA[<p><img src="/images/posts/monitoring-1.jpg" alt="" /></p>

<p><em>Photo by <a href="https://unsplash.com/@dawson2406?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">Stephen Dawson</a> on <a href="https://unsplash.com/s/photos/technical?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">Unsplash</a></em></p>

<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="data-data-architecture">Data &amp; Data Architecture</h2>

<ul>
<li><a href="https://medium.com/event-driven-utopia/the-state-of-data-infrastructure-landscape-in-2022-and-beyond-c57b9f85505c">The State of Data Infrastructure Landscape in 2022 and Beyond</a>. This post looks at the evolution of the data infrastructure landscape over the last decade. It then goes on to look at key trends and what can be expected from now and onwards.</li>
<li><a href="https://www.infoq.com/articles/next-evolution-of-database-sharding-architecture/">The Next Evolution of the Database Sharding Architecture</a>. In this <a href="https://www.infoq.com/">InfoQ</a> article, the author discusses the data sharding architecture patterns in a distributed database system. She explains how the Apache ShardingSphere project solves the data sharding challenges. Also discussed are two practical examples of how to create a distributed database and an encrypted table with DistSQL. Very, very interesting!</li>
</ul>

<h2 id="azure-data-explorer">Azure Data Explorer</h2>

<ul>
<li><a href="https://www.meetup.com/data-platform-downunder-meetup-group/events/283207013/">Big Data Analytics using Azure Data Explorer</a>. Learn from the masters of Azure Data Explorer. This link is to register for an Azure Data Explorer talk by the Azure Data Explorer gurus <a href="https://www.linkedin.com/in/minni-walia-17968521/">Minni Walia</a> and Uri Barash<a href="https://www.linkedin.com/in/uri-barash-7820594/">5</a>. Read the abstract on the <a href="https://www.meetup.com/data-platform-downunder-meetup-group/events/283207013/">sign-up page</a> to see all the goodies you&rsquo;ll hear about. See you there!</li>
<li><a href="https://techcommunity.microsoft.com/t5/azure-data-explorer-blog/azure-data-explorer-offers-on-amd-skus/ba-p/3033197">Azure Data Explorer offers on AMD SKUs</a>. The post linked to covers &ldquo;what it says on the tin&rdquo;; it talks about how we can now run Azure Data Explorer on AMD SKUs. This is cool as we can now gain higher performance while keeping the costs low.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://databricks.com/blog/2022/01/13/confluent-streaming-for-databricks-build-scalable-real-time-applications-on-the-lakehouse.html">Confluent Streaming for Databricks: Build Scalable Real-time Applications on the Lakehouse</a>. This post is about the fully managed Confluent connector against Databricks Delta Lake: the ability to ingest directly into Delta Lake tables from Kafka topics. I got really excited when I saw this post because this would solve some issues for us: us as in <a href="/derivco">Derivco</a>. Unfortunately, we cannot use it yet, as it is AWS only - for now. Regardless of that, this is really cool!</li>
<li><a href="https://www.confluent.io/blog/ksqldb-0-23-1-features-updates/">Announcing ksqlDB 0.23.1</a>. A new version of ksqlDB is out in the wild! Some of the new exciting features are: perform pull queries on streams, access topic partition and offset through pseudo-columns, and use grace periods when joining streams. All very cool!</li>
<li><a href="https://medium.com/paypal-tech/kafka-consumer-benchmarking-c726fbe4000">Scaling Kafka Consumer for Billions of Events</a>. This post provides a &ldquo;ton&rdquo; of helpful information about configuring Kafka consumers for optimal throughput. I have made this post a mandatory read for my developers that write Kafka applications.</li>
<li><a href="https://developer.confluent.io/ksqldb-recipes/">Transform your Kafka data into real-time insights</a>. The post looks at ksqlDB recipes for the most popular stream processing use cases. Each recipe provides a set of ksqlDB queries you can run to process real-time data streams and take immediate action.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>In last week&rsquo;s roundup, I mentioned how I had started writing a post using Debezium and Kafka Connect to publish events to Event Hubs. The one blog post turned into two, and I published both during last week:</p>

<ul>
<li><a href="/2022/01/10/how-to-stream-data-to-event-hubs-from-databases-using-kafka-connect--debezium-in-docker---i/">How to Stream Data to Event Hubs from Databases Using Kafka Connect &amp; Debezium in Docker - I</a>. In this post I looked at the configuration of Kafka Connect in <code>docker-compose.yml</code> to enable connection to Event Hubs.</li>
<li><a href="/2022/01/14/how-to-stream-data-to-event-hubs-from-databases-using-kafka-connect--debezium-in-docker---ii/">How to Stream Data to Event Hubs from Databases Using Kafka Connect &amp; Debezium in Docker - II</a>. This, the second post, concluded the &ldquo;adventure&rdquo; of streaming data to Eent Hubs using Debezium and Kafka Connect. More specifically, I looked at the configuration of the Debezium connector and the various properties required to push data to Event Hubs.</li>
</ul>

<p>When I started with the two posts above I was not 100% sure it would work (publishing to Event Hubs using Kafka Connect). Fortunately it turned out that, yes - you can use Kafka Connect to publish to Event Hubs.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to Stream Data to Event Hubs from Databases Using Kafka Connect &amp; Debezium in Docker - II]]></title>
    <link href="https://nielsberglund.com/2022/01/14/how-to-stream-data-to-event-hubs-from-databases-using-kafka-connect--debezium-in-docker---ii/" rel="alternate" type="text/html"/>
    <updated>2022-01-14T05:25:03+02:00</updated>
    <id>https://nielsberglund.com/2022/01/14/how-to-stream-data-to-event-hubs-from-databases-using-kafka-connect--debezium-in-docker---ii/</id>
    <content type="html"><![CDATA[<p>In a two-post series, this second post looks at streaming data from a database to <strong>Azure Event Hubs</strong> using Kafka Connect and Debezium, where Kafka Connect and Debezium run in Docker.</p>

<p>The first post:</p>

<ul>
<li><a href="/2022/01/10/how-to-stream-data-to-event-hubs-from-databases-using-kafka-connect--debezium-in-docker---i/"><strong>How to Stream Data to Event Hubs from Databases Using Kafka Connect &amp; Debezium in Docker - I</strong></a>. This post mainly looks at the configuration of Kafka Connect&rsquo;s <code>docker-compose.yml</code> file to allow us to connect to Event Hubs.</li>
</ul>

<p>This series came about as I in the post <a href="/2022/01/02/how-to-use-kafka-client-with-azure-event-hubs/"><strong>How to Use Kafka Client with Azure Event Hubs</strong></a>, somewhat foolishly said:</p>

<p><em>An interesting point here is that it is not only your Kafka applications that can publish to Event Hubs but any application that uses Kafka Client 1.0+, like Kafka Connect connectors!</em></p>

<p>I wrote the above without testing it myself, so when I was called out on it, I started researching (read &ldquo;Googling&rdquo;) to see if it was possible. The result of the &ldquo;Googling&rdquo; didn&rsquo;t give a 100% answer, so I decided to try it out, and this series is the result.</p>

<p>In the <a href="/2022/01/10/how-to-stream-data-to-event-hubs-from-databases-using-kafka-connect--debezium-in-docker---i/">first post</a>, - as mentioned - we configured Kafka Connect to connect into Event Hubs. In this post, we look at configuring the Debezium connector.</p>

<p></p>

<h2 id="pre-reqs">Pre-reqs</h2>

<p>The pre-reqs are the same as in <a href="/2022/01/10/how-to-stream-data-to-event-hubs-from-databases-using-kafka-connect--debezium-in-docker---i/"><strong>How to Stream Data to Event Hubs from Databases Using Kafka Connect &amp; Debezium in Docker - I</strong></a> and if you followed along in that post you should now have:</p>

<ul>
<li>A SQL Server database: <code>DebeziumTest</code> (or whatever you named it).</li>
<li>A table in the database: <code>dbo.tb_CDCTab1</code>.</li>
</ul>

<p>In addition to the above, you should also have an Event Hubs namespace. In the namespace you should have created a SAS policy connection string, where the connection string looks like so:</p>

<pre><code class="language-bash">Endpoint=sb://dbzeventhubs.servicebus.windows.net/; SharedAccessKeyName=KafkaConnect; \
SharedAccessKey=&lt;secret-key&gt;
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>SAS Policy Connection String</em></p>

<p>In <em>Code Snippet 1</em>, we see the policy connection string I created in the previous post.</p>

<h2 id="recap">Recap</h2>

<p>Before diving into configuring Debezium, let us recap what we did in the <a href="/2022/01/10/how-to-stream-data-to-event-hubs-from-databases-using-kafka-connect--debezium-in-docker---i/">previous post</a>, where we configured the <code>docker-compose.yml</code> file we use for Kafka Connect. In the last post, I divided the file into three parts to look at the details. Here I show the whole file with pointers to interesting areas:</p>

<p><img src="/images/posts/dbz-evthub-docker-compose-complete-1.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Docker Compose File</em></p>

<p>The numbered/outlined areas in <em>Figure 1</em> refer to:</p>

<ol>
<li>The image we use. Here, we use the Kafka Connect base image, which contains the bare minimum for Kafka Connect.</li>
<li>Defines the Kafka endpoint for the worker process. This is the Kafka endpoint of the Event Hubs namespace for Event Hubs. You get the endpoint from the SAS policy&rsquo;s connection string, and you append it with port, <code>9093</code>, which is the Event Hubs Kafka API endpoint.</li>
<li>Kafka Connect uses topics to store connectors config, offsets, and statuses. As this is Event Hubs, we see the Event Hub names we want to use (they will be auto-created). We also define the replication factor for the event hubs (topics). In Kafka, the default is 3, but Event Hubs works somewhat differently, so we set the replication factor to 1.</li>

<li><p>This is the security/authentication configuration for the Kafka Connect worker process for connecting to the bootstrap server. Outlined in red, we see how we pass in the JAAS configuration. This is required as Kafka uses JAAS (Java Authentication and Authorization Service) for SASL. The JAAS configuration is based on the Event Hubs namespace configuration string and looks like so:</p>

<pre><code class="language-bash">CONNECT_SASL_JAAS_CONFIG: \ 
     &quot;org.apache.kafka.common.security.plain.PlainLoginModule \ 
      required username=\&quot;$$ConnectionString\&quot; \ 
      password=\&quot;Endpoint=sb://dbzeventhubs.servicebus.windows.net/; \
              SharedAccessKeyName=KafkaConnect; \ 
              SharedAccessKey=&lt;secret-key&gt;;&quot;\&quot;;&quot;
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>JAAS Configuration</em></p>

<p>Notice in <em>Figure 2</em> how we set the username to <code>$$ConnectionString</code> instead of the &ldquo;normal&rdquo; <code>$ConnectionString</code> as user name for Event Hubs. Using single <code>$</code> in <code>docker-compose</code> implies variable substitution, so therefore we &ldquo;escape&rdquo; by using <code>$$</code>.</p></li>

<li><p>This is the security/authentication configuration for the connector to connect to the bootstrap server. In this case the bootstrap server is the same for the worker process and the connector, so the JAAS configuration is the same as in <em>Code Snippet 2</em>.</p></li>

<li><p>We install/deploy Debezium&rsquo;s SQL Server connector using <code>confluent-hub install</code>.</p></li>
</ol>

<p>Two things to keep in mind in the Kafka Connect configuration are:</p>

<ul>
<li>Use <code>$$ConnectionString</code> instead of <code>$ConnectionString</code> as the user name when connecting to Event Hubs. Read <a href="/2022/01/02/how-to-use-kafka-client-with-azure-event-hubs/">this post</a> to see why we use <code>$ConnectionString</code> at all when connecting to Event Hubs.</li>
<li>Set the authentication/security for the connector as well, not only the Kafka Connect worker process.</li>
</ul>

<p>We have now reached more or less where we finished the <a href="/2022/01/10/how-to-stream-data-to-event-hubs-from-databases-using-kafka-connect--debezium-in-docker---i/">previous post</a>, let us continue.</p>

<h2 id="enable-cdc">Enable CDC</h2>

<p>When streaming data using Debezium, CDC (Change Data Capture) must be enabled. So let us enable CDC:</p>

<pre><code class="language-sql">USE DebeziumTest;
GO
-- before we enable CDC ensure the SQL Server Agent is started
-- we need first to enable CDC on the database
EXEC sys.sp_cdc_enable_db;

-- then we can enable CDC on the table
EXEC sys.sp_cdc_enable_table @source_schema = N'dbo',
                               @source_name   = N'tb_CDCTab1',
                               @role_name = NULL,
                               @supports_net_changes = 0;
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Enabling Database and Table for CDC</em></p>

<p>We see in <em>Code Snippet 3</em> how we:</p>

<ul>
<li>Enable CDC on the database.</li>
<li>After enabling CDC on the database, we enable it for the table(s) from which we want to capture changes.</li>
</ul>

<p>Please remember that the SQL Server Agent needs to be started before enabling CDC. Having enabled CDC, we can now look at the connector.</p>

<h2 id="debezium-connector">Debezium Connector</h2>

<p>Above, in <em>Figure 1</em>, we see how we install/deploy the SQL Server connector, and in the last post, we saw how the connector was loaded in the Kafka Connect worker process after we did <code>docker-compose up -d</code>.</p>

<p>The connector is loaded, but it is not doing anything. To enable the connector, we configure it using a JSON file, which we then <code>POST</code> to a Kafka Connect endpoint. To <code>POST</code> the file, you can use your favorite tool, Postman, <code>curl</code>, etc. I tend to like Postman, so that is what I use later on.</p>

<p>Before we go any further, even though this post looks in somewhat detail at configuring Debezium, it looks at it from the perspective of configuring it to be able to communicate with Event Hubs. So, if you want/need more information about Debezium configuration in general, look <a href="https://debezium.io/documentation/reference/stable/connectors/sqlserver.html">here</a>.</p>

<h4 id="debezium-event-hubs-topics">Debezium Event Hubs (Topics)</h4>

<p>During the configuration of the Debezium connector, two Debezium specific event hubs are created, regardless of the tables we are interested in:</p>

<ul>
<li>Event hub for schema changes. The connector writes schema change events to this event hub whenever a schema change happens for a captured table. The name of the event hub is the name in the <code>database.server.name</code> configuration property in the configuration file.</li>
<li>Event hub for database history. Schema changes are written to the schema change event hub, and also to this database history event hub. You set the name of this event hub in the <code>database.history.kafka.topic</code> configuration property in the configuration file.</li>
</ul>

<p>Let us look at database history configuration.</p>

<h4 id="database-history">Database History</h4>

<p>The database history requires some specific configuration. This caused me issues when I tried Debezium to Event Hubs, so I thought I better cover the database history configuration in a bit more detail.</p>

<blockquote>
<p><strong>NOTE:</strong> The database history properties are not SQL Server specific, but every Debezium connector requires them, except for the connectors for PostgreSQL, Cassandra, and Vitess.</p>
</blockquote>

<p>So, I mentioned above about the database history event hub and how the connector writes schema changes to that event hub. We define the event hub name in the <code>database.history.kafka.topic</code> configuration property. We also need to specify the endpoint where the event hub should be created/exists. We do that via the <code>database.history.kafka.bootstrap.servers</code> property. Keep in mind that the database history endpoint should be the same as the Kafka Connect process endpoint. That&rsquo;s what all documentation says anyway, and I have not tried anything differently.</p>

<blockquote>
<p><strong>NOTE:</strong> The timeout for creating the database history topic is very short, so when you connect to the cloud, whether it is Confluent Cloud or Azure Event Hubs, you should create the event hub (topic) manually. I.e. creating it before <code>POST</code>:ing the connector configuration. Oh, and when you create it, you have to create it with a partition count of 1.</p>
</blockquote>

<p>Taking the above into consideration, the database history configuration looks something like so:</p>

<pre><code class="language-bash"># more properties above

&quot;database.history.kafka.bootstrap.servers&quot;: &quot;dbzeventhubs.servicebus.windows.net:9093&quot;,
&quot;database.history.kafka.topic&quot;: &quot;dbzdbhistory&quot;,

# more properties below
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Database History Configuration</em></p>

<p>In <em>Code Snippet 4</em>, we see the database history configuration properties, and we see that <code>database.history.kafka.bootstrap.servers</code> has the same value as in <em>Figure 1</em>, point two. The event hub name is set to <code>dbzdbhistory</code>. As mentioned before, we should manually create that event hub to avoid timeout errors.</p>

<p>This looks good! But wait, there is more - and this was one of the things I completely missed when I initially tested Debezium with Event Hubs. What I missed was that if your Kafka cluster/Event Hubs is secured, you must add the security properties prefixed with <code>database.history.consumer.*</code> and <code>database.history.producer.*</code> to the connector configuration:</p>

<pre><code class="language-bash"># more properties above
# consumer
&quot;database.history.consumer.security.protocol&quot;:&quot;SASL_SSL&quot;,
&quot;database.history.consumer.sasl.mechanism&quot;:&quot;PLAIN&quot;,
&quot;database.history.consumer.sasl.jaas.config&quot;: &quot;&lt;JAAS-config-string&gt;;&quot;,
# producer
&quot;database.history.producer.security.protocol&quot;:&quot;SASL_SSL&quot;,
&quot;database.history.producer.sasl.mechanism&quot;:&quot;PLAIN&quot;,
&quot;database.history.producer.sasl.jaas.config&quot;:&quot;&lt;JAAS-config-string&gt;;&quot;
</code></pre>

<p><strong>Code Snippet 5:</strong> <em>Database History Security Configuration</em></p>

<p>In <em>Code Snippet 5</em>, we see how the database history security is set up like the security in <em>Figure 1</em>. One difference is that we set it up for both <code>consumer</code> and <code>producer</code>, as the connector will both write to and read from the topic. The JAAS configuration string looks like what we see in  <em>Code Snippet 2</em>; apart from that the user name is <code>$ConnectionString</code>. I.e. one dollar sign.</p>

<h4 id="configuration-file">Configuration File</h4>

<p>Having covered some of the essential parts of the connector configuration, let us look at what the complete configuration file looks like:</p>

<p><img src="/images/posts/dbz-evthub-dbz-connector-config.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Debezium Configuration File</em></p>

<p>Outlined in yellow in <em>Figure 2</em> are the configuration properties mostly related to the source database. Some properties to look at are:</p>

<ul>
<li><code>connector.class</code>: this defines what Debezium connector to use.</li>
<li><code>database.dbname</code>: the database from which we capture events.</li>
<li><code>database.server.name</code>: logical name that identifies the SQL Server instance (it can be an arbitrary string). An event hub with this name will be created to capture schema changes (as mentioned above). This name is also used as a prefix for all event hub names created by the connector for tables from which we capture changes.</li>
<li><code>table.include.list</code>: a comma-separated list of fully-qualified table names for tables from which we capture changes.</li>
</ul>

<p>The outlined in-red portion in <em>Figure 2</em> is required for <code>database.history.*</code>. Since I covered quite a lot about database history above, I will not go further into it.</p>

<h2 id="configure-the-connector">Configure the Connector</h2>

<p>We are now ready to &ldquo;spin up&rdquo; Kafka Connect, followed by configuring and enabling the connector.</p>

<blockquote>
<p><strong>NOTE:</strong> When I am testing things out, I ALWAYS tear down my environment before each run. I.e., delete the connector if it is created, take down Docker, and delete all relevant topics.</p>
</blockquote>

<p>Let&rsquo;s do this! We start with &ldquo;spinning up&rdquo; Kafka Connect like we did in the <a href="/2022/01/10/how-to-stream-data-to-event-hubs-from-databases-using-kafka-connect--debezium-in-docker---i/">last post</a>: <code>docker-compose up -d</code>. We wait a minute or two, and then we do the same checks as in the <a href="/2022/01/10/how-to-stream-data-to-event-hubs-from-databases-using-kafka-connect--debezium-in-docker---i/">previous post</a>:</p>

<ul>
<li>In the Azure portal, ensure that the Kafka Connect internal topics have been created. In my case, <code>offsets</code>, <code>status</code>, and <code>configs</code>.</li>
<li>Look and see that the connector is loaded: <code>GET http://127.0.0.1:8083/connector-plugins</code>.</li>
</ul>

<p>I don&rsquo;t know about you, but in my environment, it looks good. We are now ready to configure the connector. Before we do that, ensure that the database and table are CDC enabled and that you have manually created the database history event hub.</p>

<p>Now let&rsquo;s do it. Let us <code>POST</code> the configuration:</p>

<p><img src="/images/posts/dbz-evthub2-post-connector.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>POST Configuration</em></p>

<p>In <em>Figure 3</em>, we see our <code>POST</code> call (outlined in yellow) to the <code>connectors</code> endpoint (outlined in blue), and we also see part of the configuration file. When configuring a connector, we give it a name, and in my case, I named it <code>sql-server</code> (imaginative - I know). After we have <code>POST</code>:ed we see if it worked:</p>

<p><img src="/images/posts/dbz-evthub2-connector-status.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Connector Status</em></p>

<p>We see in <em>Figure 4</em> how we do a <code>GET</code> call against the <code>status</code> endpoint with the connector&rsquo;s name as part of the path. According to what is outlined in blue, all looks OK.</p>

<blockquote>
<p><strong>NOTE:</strong> When checking the status of a newly created connector, it is a good practice to wait a little while (10 - 20) seconds right after creation before checking the status. This is to give the connector some time to &ldquo;spin up&rdquo;. Alternatively, run the status check a couple of times.</p>
</blockquote>

<p>The other thing we can do to ensure all is OK is to look in the portal and see what event hubs we have:</p>

<p><img src="/images/posts/dbz-evthub2-topics-1.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Created Event Hubs</em></p>

<p>At first glance at <em>Figure 5</em> all looks OK. We see the connector&rsquo;s event hub for scheme changes (<code>debeziumtestserver</code>). So far, so good. But what about the event hub for the table we want to capture changes from: <code>dbo.tb_CDCTab1</code>? Where is that event hub? The answer to that is that the event hubs for capture table events are not created until an event happens:</p>

<pre><code class="language-sql">USE DebeziumTest;
GO

INSERT INTO dbo.tb_CDCTab1(Col1, Col2)
VALUES(1, 'Hello Number 1')
</code></pre>

<p><strong>Code Snippet 6:</strong> <em>Ingest Data</em></p>

<p>After executing the code in <em>Code Snippet 6</em>, you refresh the event hubs in the portal, and you see this:</p>

<p><img src="/images/posts/dbz-evthub2-table-topic.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>Event Hub for Table</em></p>

<p>After doing an insert in the table, we see in <em>Figure 6</em> an event hub created for that table. We assume that events have been published to the event hub. To further &ldquo;prove&rdquo; that, we look at that particular event hub&rsquo;s overview page in the portal and its stats:</p>

<p><img src="/images/posts/dbz-evthub2-table-events.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>Event Hub Stats</em></p>

<p>The stats for incoming messages are outlined in red in <em>Figure 7</em>, and we see how one message has arrived. It works - yay!</p>

<h2 id="summary">Summary</h2>

<p>In the <a href="/2022/01/10/how-to-stream-data-to-event-hubs-from-databases-using-kafka-connect--debezium-in-docker---i/">previous post</a> and this, we set out to see if we can use Debezium and Kafka Connect to stream data from databases to Event Hubs. We have now seen it is possible!</p>

<p>Things to keep in mind:</p>

<ul>
<li>When connecting to Event Hubs, we use the connection string from a SAS policy.</li>
<li>We use JAAS configuration to set the user name and password. For Event Hubs the username is the literal string <code>$ConnectionString</code>, and the password is the SAS policy&rsquo;s connection string.</li>
<li>If we use <code>docker-compose</code>, we set the user name in the JAAS configuration to <code>$$ConnectionString</code> to avoid variable substitution.</li>
<li>When configuring the security for Kafka Connect, you do it both for the Kafka Connect worker process and the connector.</li>
<li>Almost all Debezium connectors require a database history event hub (topic).</li>
<li>Since the timeout for automatic creation of the event hub is very short, the event hub should be created manually before configuring the connector.</li>
<li>We need to set security for the database history endpoint and do it for both consumer and producer (<code>database.history.consumer.*</code>, <code>database.history.producer.*</code>).</li>
</ul>

<p>Lastly, I would not have been able to get this to work if it hadn&rsquo;t been for the blog post:</p>

<ul>
<li><a href="https://rmoff.net/2019/10/16/using-kafka-connect-and-debezium-with-confluent-cloud/"><strong>Using Kafka Connect and Debezium with Confluent Cloud</strong></a>. This post by by Kafka guru <a href="https://twitter.com/rmoff">Robin Moffat</a> gave me the necessary pointers for the connector configuration - especially the security configuration for the database history event hub.</li>
</ul>

<p>As I said, <a href="https://twitter.com/rmoff">Robin Moffat</a> is a Kafka Guru, and if you are into Kafka, then you <strong>MUST</strong> read his <a href="https://rmoff.net/">blog</a>.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or [ping][ma] me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to Stream Data to Event Hubs from Databases Using Kafka Connect &amp; Debezium in Docker - I]]></title>
    <link href="https://nielsberglund.com/2022/01/10/how-to-stream-data-to-event-hubs-from-databases-using-kafka-connect--debezium-in-docker---i/" rel="alternate" type="text/html"/>
    <updated>2022-01-10T19:05:33+02:00</updated>
    <id>https://nielsberglund.com/2022/01/10/how-to-stream-data-to-event-hubs-from-databases-using-kafka-connect--debezium-in-docker---i/</id>
    <content type="html"><![CDATA[<p>This post is the first of two looking at if and how we can stream data to Event Hubs from Debezium. Initially I had planned only one post covering this, but it turned out that the post would be too long, so therefore I split it in two.</p>

<p>The second post in the series is:</p>

<ul>
<li><a href="/2022/01/14/how-to-stream-data-to-event-hubs-from-databases-using-kafka-connect--debezium-in-docker---ii/"><strong>How to Stream Data to Event Hubs from Databases Using Kafka Connect &amp; Debezium in Docker - II</strong></a>. Here we look at the Debezium connector configuration needed if we want to stream data to Event Hubs.</li>
</ul>

<p>It started with the post, <a href="/2022/01/02/how-to-use-kafka-client-with-azure-event-hubs/"><strong>How to Use Kafka Client with Azure Event Hubs</strong></a>. In that post, I looked at how the Kafka client can publish messages to - not only - <strong>Apache Kafka</strong> but also <strong>Azure Event Hubs</strong>. In the post, I said something like:</p>

<p><em>An interesting point here is that it is not only your Kafka applications that can publish to Event Hubs but any application that uses Kafka Client 1.0+, like Kafka Connect connectors!</em></p>

<p>Obviously, for you who know me, I said that without having tested it properly, but: <em>how hard can it be? What could possibly go wrong?</em>. Well, I was called upon it by a guy who had read the post. He told me he had tried what I said at one time or the other, and it hadn&rsquo;t worked.</p>

<p>In this post (the first), we look at configuring Kafka Connect to connect to Event Hubs.</p>

<p></p>

<h2 id="background">Background</h2>

<p>Let us look at some of the moving parts of this.</p>

<h4 id="azure-event-hubs">Azure Event Hubs</h4>

<p>Azure Event Hubs is a big data streaming platform and event ingestion service. It is a fully managed Platform-as-a-Service (PaaS) with little configuration or management overhead, very much like Apache Kafka in <strong>Confluent Cloud</strong>.</p>

<p>The concepts are fairly similar between Event Hubs and Kafka, especially if we look at Apache Kafka in Confluent Cloud. There are however a couple of differences in terminology:</p>

<ul>
<li>In Kafka, we create/have a <em>cluster</em>, whereas, in Event Hubs, we have a <em>namespace</em>.</li>
<li>When we publish messages to Kafka, we publish to a <em>Topic</em>, where the topic is part of a cluster. In Event Hubs we publish to an <em>Event Hub</em> in an Event Hubs namespace. Even though the names (topics, Event Hub) are different, the underlying concepts are the same. I.e. both have partitions, and messages are located based on offsets in a partition.</li>
</ul>

<p>An Event Hubs namespace exposes API endpoints to where clients can connect. One such endpoint is the Kafka client protocol endpoint (protocol version 1.0 and above) which is exposed on port <code>9093</code> of the host of the namespace.</p>

<p>The previously mentioned <a href="/2022/01/02/how-to-use-kafka-client-with-azure-event-hubs/">post</a> discusses this in more detail.</p>

<h4 id="kafka-connect">Kafka Connect</h4>

<p>Kafka Connect is a JVM process (worker process) running separately from a Kafka broker. It is used for streaming data between Apache Kafka and other systems in a scalable and reliable way. Kafka Connect moves the data using <strong>connectors</strong>, where a connector is a <code>.jar</code> file, and the connector is loaded by the Kafka Connect process. Basically the worker acts as a host for one or more connectors. The connectors come in two flavours:</p>

<ul>
<li>Source connectors, which understand how to interact with the source system, publish records to Kafka topics (Kafka acts as a sink).</li>
<li>Sink connectors propagate records from Kafka topics to other systems.</li>
</ul>

<p>Connectors are Kafka specific, but since Event Hubs exposes the Kafka client endpoint, we can use (or at least supposedly can) connectors that use Kafka as a sink.</p>

<h4 id="debezium">Debezium</h4>

<p>Debezium is an open-source distributed platform for change data capture (CDC). It captures changes in your database(s) and publishes those changes to topics in Kafka.</p>

<p>Debezium has Kafka Connect connectors for many source systems; Oracle, PostgresSQL, SQL Server, etc., and in this post, we use the Debezium SQL Server connector. As with other Kafka Connect connectors, the Debezium connectors are deployed to Kafka Connect.</p>

<p>The post <a href="/2021/08/07/how-to-deploy-the-debezium-sql-server-connector-to-docker/"><strong>How to Deploy the Debezium SQL Server Connector to Docker</strong></a> looks at Kafka Connect, Debezium, and SQL Server in more detail.</p>

<p>Having had some background information, let&rsquo;s see what you need if you want to follow along.</p>

<h2 id="pre-reqs">Pre-reqs</h2>

<p>The pre-reqs are the same (with a couple of additions) as in the <a href="/2021/08/07/how-to-deploy-the-debezium-sql-server-connector-to-docker/"><strong>How to Deploy the Debezium SQL Server Connector to Docker</strong></a> post, so look at that post to find out what you need. The additions are:</p>

<ul>
<li>As you&rsquo;ll be working with Event Hubs, you need an Azure account. If you don&rsquo;t have an Azure subscription, sign up for a <a href="/2022/01/02/how-to-use-kafka-client-with-azure-event-hubs/">free account</a>.</li>
<li>An Event Hubs namespace to where the SQL data will be streamed.</li>
</ul>

<p>If you don&rsquo;t have an Event Hubs namespace and are unclear on how to create one, the Microsoft article <a href="https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-create"><strong>Quickstart: Create an event hub using Azure portal</strong></a> covers it in detail. While creating the namespace, ensure you create it under the <em>Standard</em> pricing tier (or higher), as <em>Basic</em> does not support Kafka.</p>

<p>The Event Hubs namespace I use in this post looks like so:</p>

<p><img src="/images/posts/dbz-evthub-namespace-1.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Namespace</em></p>

<p>In <em>Figure 1</em>, we see how the namespace is called <code>dbzeventhubs</code> (outlined in red) and that we don&rsquo;t have any Event Hub (topic) yet.</p>

<p>After you have downloaded and set up the pre-reqs as per the above <a href="/2021/08/07/how-to-deploy-the-debezium-sql-server-connector-to-docker/">post</a>, as well as the Event Hubs namespace, we are ready to go. We  should have:</p>

<ul>
<li>A SQL Server database: <code>DebeziumTest</code> (or whatever you named it).</li>
<li>A table in the database: <code>dbo.tb_CDCTab1</code>.</li>
</ul>

<p>We also have an Event Hubs namespace. I named it <code>dbzeventhubs</code> as in <em>Figure 1</em>.</p>

<h2 id="event-hubs-security-authentication">Event Hubs Security &amp; Authentication</h2>

<p>In the <a href="/2022/01/02/how-to-use-kafka-client-with-azure-event-hubs/">Kafka Client and Event Hubs</a> post, I discussed how we need to configure Event Hubs security to allow Kafka clients to interact with Event Hubs. We did it by creating a Shared Access Signature (SAS) policy. We then used the policy&rsquo;s connection string as the <code>SASL</code> authentication password.</p>

<p>In this post, we do the same, with one difference. In the <a href="/2022/01/02/how-to-use-kafka-client-with-azure-event-hubs/">previous post</a>, we created the policy for the Event Hub (topic), whereas now we do it for the namespace. This is because both Kafka Connect and Debezium need permissions on the namespace level (create event hubs, etc.).</p>

<p>For this post, I created the policy in the same way as I did in the <a href="/2022/01/02/how-to-use-kafka-client-with-azure-event-hubs/">Kafka Client and Event Hubs</a> post, and during creation, I saw something like so:</p>

<p><img src="/images/posts/dbz-evthub-sas-create.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Create Policy</em></p>

<p>In <em>Figure 2</em>, we see how I created a policy called <code>KafkaConnect</code> and how the policy has <code>Manage</code> permissions. The <code>Manage</code> permission allows the policy to manage the topology of the namespace, i.e. adding deleting entities.</p>

<p>Having created the policy, you copy one of the policy&rsquo;s connection strings as that is what we use for the Kafka client configuration. My connection string looks like so:</p>

<pre><code class="language-bash">Endpoint=sb://dbzeventhubs.servicebus.windows.net/; SharedAccessKeyName=KafkaConnect; \
SharedAccessKey=&lt;secret-key&gt;
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>SAS Policy Connection String</em></p>

<p>The string we see in <em>Code Snippet 1</em> acts as the password for authentication. When setting up Kafka Connect, we need to define the <code>bootstrap.servers</code> (the server(s) to connect to). We get that value from the <code>Endpoint</code> field, <code>dbzeventhubs.servicebus.windows.net</code>. We append it with the port, <code>9093</code>, for the Event Hubs Kafka API endpoint.</p>

<blockquote>
<p><strong>NOTE:</strong> I am aware that I have &ldquo;glossed&rdquo; over the details of SAS policies. Please look at the <a href="/2022/01/02/how-to-use-kafka-client-with-azure-event-hubs/">Kafka Client and Event Hubs</a> post if you need more information.</p>
</blockquote>

<p>Cool, we now have all we need to configure Kafka Connect in Docker.</p>

<h2 id="kafka-connect-docker">Kafka Connect &amp; Docker</h2>

<p>As mentioned before, we want to run Kafka Connect and the connector locally in Docker. We do it by using a <code>docker-compose.yml</code> file, similar to what we did in the post <a href="/2021/08/07/how-to-deploy-the-debezium-sql-server-connector-to-docker/"><strong>How to Deploy the Debezium SQL Server Connector to Docker</strong></a>. The difference here is that we only run Kafka Connect and the connector, no Kafka broker, etc.</p>

<p>This part, configuring Kafka Connect: <em>how hard can it be? What could possibly go wrong?</em></p>

<p>Needless to say, it was not as easy as I thought.</p>

<h2 id="docker-compose">Docker Compose</h2>

<p>As mentioned before, we use a <code>docker-compose.yml</code> and to make things a bit more readable, I have divided the file into three parts: <em>basics</em>, <em>security</em>, and <em>connector</em>. The three parts are represented by the figures below.</p>

<h4 id="basics">Basics</h4>

<p>The <em>basics</em> part looks like so:</p>

<p><img src="/images/posts/dbz-evthub-docker-compose-1.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Docker Compose - I</em></p>

<p>In <em>Figure 3</em>, you see the start of the <code>docker-compose.yml</code>. If you have done any work using Docker Compose before, nothing there should come as a surprise. Let&rsquo;s have a look at the outlined areas:</p>

<ul>
<li>Yellow: this is the image we use. As you see, I am using the Kafka Connect base image, which contains the bare minimum for Kafka Connect.</li>
<li>Red: defines the Kafka endpoint for the worker process. This is the Kafka endpoint of the Event Hubs namespace. You get the endpoint from the SAS policy&rsquo;s connection string.</li>
<li>Green: Kafka Connect uses topics to store connectors config, offsets, and statuses. As this is Event Hubs, we see the Event Hub names we want to use (they will be auto-created). We also define the replication factor for the event hubs (topics). In Kafka, the default is 3, but Event Hubs works somewhat differently, so we set the replication factor to 1.</li>
</ul>

<p>Let us go to the security part, where it got a bit interesting for me.</p>

<h4 id="security">Security</h4>

<p>In the <a href="/2022/01/02/how-to-use-kafka-client-with-azure-event-hubs/">Kafka Client and Event Hubs</a> post, I discussed using the <code>SASL_SSL</code> security protocol with <code>PLAIN</code> as the mechanism. Using <code>PLAIN</code> gives us a username/password authentication mechanism. I also mentioned how the password should be set to SAS policy&rsquo;s connection string value and the username to the &ldquo;magic&rdquo; string <code>$ConnectionString</code> (notice the dollar sign). Applying that to this post and the SAS policy we created above, the username password &ldquo;combo&rdquo; would look something like this:</p>

<pre><code class="language-python">'sasl.username': &quot;$ConnectionString&quot;,
'sasl.password': &quot;Endpoint=sb://dbzeventhubs.servicebus.windows.net/; \
                  SharedAccessKeyName=KafkaConnect; SharedAccessKey=&lt;secret-key&gt;;&quot;
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>User Name &amp; Password</em></p>

<p>In <em>Code Snippet 2</em> we see how I have taken the SAS policy&rsquo;s connection string and used that for the <code>sasl.password</code> value.</p>

<p>Kafka (brokers, Connect, etc.) uses JAAS (Java Authentication and Authorization Service) for SASL configuration. So when we set up security for Kafka Connect, we must provide JAAS configurations for this, where part of the configuration is username and password. In addition to the JAAS configuration, we need the security protocol and mechanism. With this in mind, connecting to Event Hubs would look like so:</p>

<pre><code class="language-bash">security.protocol=SASL_SSL
sasl.mechanism=PLAIN
sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule \
                   required username=&quot;$ConnectionString&quot; \ 
                   password=&quot;&lt;policy-connection-string&gt;&quot;;
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>JAAS Config</em></p>

<p>We see in <em>Code Snippet 3</em> how we first set the security protocol and the mechanism, followed by the JAAS configuration. The <code>org.apache.kafka.common.security.plain.PlainLoginModule</code> in the JAAS configuration defines the class handling logins using the <code>PLAIN</code> mechanism.</p>

<p>Wow, this was quite a detour; let us try to get back to track and talk about configuring this in a compose file. When configuring security in the compose file, we need to remember that we need to configure the security for the worker process and the connector, where the connector can be consumer, publisher or both.</p>

<blockquote>
<p><strong>NOTE:</strong> In certain circumstances, you do not need to configure the connectors security in the compose file, as you can override it in the connector&rsquo;s configuration. Since I had quite a lot of problems with the security configuration, I did it in the compose file.</p>
</blockquote>

<p>OK, so with all of the above in mind, the security configuration should look something like so:</p>

<pre><code class="language-bash"># connect worker
CONNECT_SECURITY_PROTOCOL: SASL_SSL
CONNECT_SASL_MECHANISM: PLAIN
CONNECT_SASL_JAAS_CONFIG: \ 
         &quot;org.apache.kafka.common.security.plain.PlainLoginModule \ 
          required username=\&quot;$ConnectionString\&quot; \ 
                   password=\&quot;&lt;connection-string-from-policy&gt;\&quot;;&quot;
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Security Config Compose</em></p>

<p>In <em>Code Snippet 4</em>, we see the security configuration for the Kafka Connect worker process. This looks good; just be aware of the <code>\</code> as line continuations and being used for escaping double quotes <code>&quot;</code> inside the JAAS configuration.</p>

<p>I thought this looked good. However, when I tried to &ldquo;spin up&rdquo; the Kafka Connect process, I got strange errors saying the <code>ConnectionString</code> (notice without <code>$</code>) was blank. This was followed by the log file reporting authentication issues.</p>

<p>After a lot of head-scratching, I finally figured out that the problem was <code>$ConnectionString</code>, more specifically the <code>$</code> sign. The dollar sign indicates variable substitution in <code>docker-compose</code>, and when the file is parsed, there is no variable named <code>$ConnectionString</code>. Having finally figured out the issue, it was pretty simple to fix by using <code>$$</code>, which says I actually want to use <code>$</code> as a literal sign.</p>

<p>After all this the security part of the compose file looks like this:</p>

<p><img src="/images/posts/dbz-evthub-docker-compose-2.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Docker Compose - II</em></p>

<p>We see in <em>Figure 4</em> how we configure security for the worker process <code>CONNECT_xxx</code> (outlined in red) and the connector, which acts as a producer: <code>CONNECT_PRODUCER_xxx</code> (outlined in yellow). As discussed earlier, <code>username</code> is set to <code>$$ConnectionString</code>.</p>

<h4 id="connector">Connector</h4>

<p>The connector is the last piece of the <code>docker-compose.yml</code> file, and I looked in the <a href="/2021/08/07/how-to-deploy-the-debezium-sql-server-connector-to-docker/"><strong>How to Deploy the Debezium SQL Server Connector to Docker</strong></a> post at various ways of running the connector in Kafka Connect. In this post, I use the ability to in a <code>docker-compose.yml</code> file to execute arbitrary commands, using the <code>command</code> option:</p>

<p><img src="/images/posts/dbz-evthub-docker-compose-3.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Docker Compose - III</em></p>

<p>In the <code>command</code> option (outlined in blue) in <em>Figure 5</em> we install the Debezium SQL Server connector.</p>

<blockquote>
<p>*NOTE:** If you wonder about <code>confluent-hub</code>, then read more about it <a href="https://docs.confluent.io/home/connect/confluent-hub/">here</a>.</p>
</blockquote>

<p>When you have come this far, you can do a test run. Before you run this, make sure your Event Hubs namespace does not have any event hubs (topics), or at least none with the same names as your storage topics.</p>

<h2 id="test-run">Test Run</h2>

<p>The test run is to ensure that the Docker container &ldquo;comes up&rdquo; properly and the connector is loaded into the Kafka Connect worker.</p>

<p>So:</p>

<ul>
<li>Ensure that Docker is running on your box</li>
<li>Start the container with: <code>docker-compose up -d</code>.</li>
</ul>

<p>To check what is happening, you can use the Docker Desktop app. When you open it up:</p>

<p><img src="/images/posts/dbz-evthub-docker-app-1.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>Docker Desktop App - I</em></p>

<p>When the container starts up, you see what is in <em>Figure 6</em>. More specifically, if you click on the <em>Container / Apps</em> outlined in red, you see the running containers. In our case, it is the <code>dbz-eventhub-cont</code> outlined in red. If you want to drill down further, you click on the container (outlined in red):</p>

<p><img src="/images/posts/dbz-evthub-docker-app-log.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>Docker Desktop App - II</em></p>

<p>Having clicked on the container, you now see the log for the container as in <em>Figure 7</em>. This is a great help when trying to figure out issues.</p>

<p>OK, so looking at the logs, we do not see anything strange, so let us check two more things: that event hubs have been created, and the connector has loaded.</p>

<h4 id="event-hubs-topics">Event Hubs (topics)</h4>

<p>In the Azure portal, browse to your namespace and click on the Event Hubs menu:</p>

<p><img src="/images/posts/dbz-evthub-topics-1.png" alt="" /></p>

<p><strong>Figure 8:</strong> <em>Created Event Hubs</em></p>

<p>In <em>Figure 8</em>, we see that the event hubs we defined for configs, offsets and status have all been created when the container started. Looking good so far.</p>

<h4 id="connector-1">Connector</h4>

<p>The last thing we want to check is whether the connector is loaded. Kafka Connect exposes a REST API allowing us to configure/manage/etc. our connectors. To check whether the SQL Server connector is loaded, we use a <code>GET</code> call:</p>

<pre><code class="language-bash">GET http://127.0.0.1:8083/connector-plugins
</code></pre>

<p><strong>Code Snippet 5:</strong> <em>GET Connectors</em></p>

<p>In <em>Code Snippet 5</em> we see how we execute a <code>GET</code> call into <code>localhost</code> (as we host Kafka Connect on our box):</p>

<p><img src="/images/posts/dbz-evthub-installed-connectors.png" alt="" /></p>

<p><strong>Figure 9:</strong> <em>GET Installed Connectors</em></p>

<p>The result of the call in <em>Code Snippet 5</em> is what we see in <em>Figure 9</em>: we see &ldquo;our&rdquo; SQL Server connector together with three other connectors. These other three connectors are all part of the base image we use.</p>

<p>The container is up and running, the event hubs for the Kafka Connect worker has been successfully created, and the connector has loaded! What remains to be done is configuring the connector to capture data from our table. I leave that to the next post, so let us just sum up what we have done so far.</p>

<h2 id="summary">Summary</h2>

<p>We set out to prove/disprove the ability to stream data from Debezium to Azure Event Hubs. We still haven&rsquo;t proven that it is possible, but we have come a bit on the way.</p>

<p>The main takeaways from this post are:</p>

<ul>
<li>When configuring the security, you do it both for the Kafka Connect worker process and the connector.</li>
<li>When setting up a username/password, you use a JAAS configuration.</li>
<li>When using SASL with Event Hubs, the username is hard-coded to <code>$ConnectionString</code>. Using <code>docker-compose.yml</code> you need to use two dollar signs: <code>$$ConnectionString</code>. You can read more about it <a href="https://docs.docker.com/compose/environment-variables/#substitute-environment-variables-in-compose-files">here</a>.</li>
</ul>

<p>In the <a href="/2022/01/14/how-to-stream-data-to-event-hubs-from-databases-using-kafka-connect--debezium-in-docker---ii/">next post</a>, we look at configuring the connector, and we will see whether Debezium to Event Hubs actually works.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or [ping][ma] me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 1, 2022]]></title>
    <link href="https://nielsberglund.com/2022/01/09/interesting-stuff---week-1-2022/" rel="alternate" type="text/html"/>
    <updated>2022-01-09T10:55:27+02:00</updated>
    <id>https://nielsberglund.com/2022/01/09/interesting-stuff---week-1-2022/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="databases">Databases</h2>

<ul>
<li><a href="https://ajay-yadav.medium.com/database-storage-engines-de757b03bd44">Database storage engines</a>. This is an awesome post, looking at various database storage engines!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/what-we-learned-building-confluent-cloud/">Building Confluent Cloud – Here&rsquo;s What We&rsquo;ve Learned</a>. I was pointed to this post by a smart guy on the Confluent Cloud Slack channel - thanks, Ryan! The post explores the current architecture of Confluent Cloud, how the experience with the product has benefitted both Apache Kafka and Kafka in Confluent Cloud. Finally, the post lists some of the lessons learned.</li>
<li><a href="https://www.kai-waehner.de/blog/2022/01/04/when-not-to-use-apache-kafka/">When NOT to use Apache Kafka?</a>. So, <a href="https://twitter.com/kaiwaehner">Kai Waehner</a> is a Global Technology Advisor at Confluent and evangelises Apache Kafka and Confluent Cloud. Therefore, it is very cool to see this post, where he looks at scenarios where Kafka may not fit.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>A while ago, I wrote the post <a href="/2022/01/02/how-to-use-kafka-client-with-azure-event-hubs/"><strong>How to Use Kafka Client with Azure Event Hubs</strong></a>, where I looked at how to use the Kafka client to publish messages to - not only - <strong>Apache Kafka</strong> but also <strong>Azure Event Hubs</strong>. In my post, I said something like:</p>

<p><em>An interesting point here is that it is not only your Kafka applications that can publish to Event Hubs but any application that uses Kafka Client 1.0+, like Kafka Connect connectors!</em></p>

<p>Obviously (if you know me), I said that without having tested it properly, but: <em>how hard can it be? What could possibly go wrong?</em>. Well, I was called upon it by a guy who had read the post. He told me he had tried what I said at one time or the other, and it hadn&rsquo;t worked. Therefore I started writing a post looking at using Debezium and Kafka Connect to publish events to Event Hubs. The jury is still out whether you can do it or not. So, that is what I am doing right now.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Christmas, New Year 2021]]></title>
    <link href="https://nielsberglund.com/2022/01/02/interesting-stuff---christmas-new-year-2021/" rel="alternate" type="text/html"/>
    <updated>2022-01-02T12:36:02+02:00</updated>
    <id>https://nielsberglund.com/2022/01/02/interesting-stuff---christmas-new-year-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This is the &ldquo;roundup&rdquo; of the posts that have been most interesting to me over the Christmas and New Year period 2021. Oh, and welcome to 2022! I wonder what this year will have in store for us - after the last two years I will not even think about what will happen!</p>

<p></p>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://www.starburst.io/info/data-mesh-in-practice-ebook/">Data Mesh in Practice</a>. In my mind, 2021 was the year of the Data Mesh. Many people started talking about the Data Mesh and what it was. Fewer people actually discussed the practical steps to implement a Data Mesh. That is until the end of the year when the book linked to was published. The book is for you who wonders how to implement a Data Mesh and contains strategic guidance around implementing a Data Mesh. I, for one, find the book very valuable!</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://aaronstannard.com/opentelemetry-dotnet6/">An Overview of Distributed Tracing with OpenTelemetry in .NET 6</a>. OpenTelemetry is a collection of tools, APIs, and SDKs to help you analyze your software&rsquo;s performance and behaviour. This post, as the title implies, explores OpenTelemetry tracing in .NET 6</li>
</ul>

<h2 id="azure-data-explorer">Azure Data Explorer</h2>

<ul>
<li><a href="https://techcommunity.microsoft.com/t5/azure-data-explorer-blog/automatically-stop-unused-azure-data-explorer-clusters/ba-p/3047042">Automatically stop unused Azure Data Explorer Clusters</a>. The linked-to post announces an Azure Data Explorer cluster feature that I have wished for, how ADX clusters can automatically stop when not in use. This will really reduce costs!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/how-to-build-a-data-mesh-using-event-streams/">The Definitive Guide to Building a Data Mesh with Event Streams</a>. Hmm, maybe I was wrong when I, above, said there were not much around implementing a Data Mesh. This blog post looks at the practical steps of implementing a Data Mesh based on Kafka and streaming data. Very interesting!</li>
<li><a href="https://www.infoq.com/articles/microservices-inside-out/">Turning Microservices Inside-Out</a>. This <a href="https://www.infoq.com/">InfoQ</a> article provides a lot of good insight into designing and building for emitting data from microservices APIs.</li>
<li><a href="https://twitter.com/tlberglund">I Interviewed Nearly 200 Apache Kafka Experts and I Learned These 10 Things</a>. This post by <a href="https://twitter.com/tlberglund">Tim Berglund</a> (no, we&rsquo;re not related) is a top 10 of podcasts Tim has conducted with Kafka experts. There is some very cool stuff in there!</li>
<li><a href="https://www.confluent.io/blog/from-apache-kafka-to-confluent-cloud-optimizing-for-speed-scale-storage/">Speed, Scale, Storage: Our Journey from Apache Kafka to Performance in Confluent Cloud</a>. In this post, the authors share their experience optimizing Apache Kafka for Confluent Cloud. There are quite a few &ldquo;tips and tricks&rdquo; that all of us can use! Awesome!</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>I have had a severe bout of writer block in the last few months! It is not that I don&rsquo;t have anything to write about; I just can&rsquo;t get the words out there.</p>

<p>So, therefore I am pleased to say that I earlier today (Sunday, Jan 2, 2022) published a new post:</p>

<ul>
<li><a href="/2022/01/02/how-to-use-kafka-client-with-azure-event-hubs/"><strong>How to Use Kafka Client with Azure Event Hubs</strong></a>. As the title implies, I look at how one can use the Kafka client to publish to Event Hubs in the post.</li>
</ul>

<p>Hopefully, having managed to get that post out, I may be able to do others as well.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to Use Kafka Client with Azure Event Hubs]]></title>
    <link href="https://nielsberglund.com/2022/01/02/how-to-use-kafka-client-with-azure-event-hubs/" rel="alternate" type="text/html"/>
    <updated>2022-01-02T09:48:17+02:00</updated>
    <id>https://nielsberglund.com/2022/01/02/how-to-use-kafka-client-with-azure-event-hubs/</id>
    <content type="html"><![CDATA[<p>This blog post came by, by accident, lol. A couple of weeks ago, I started to prepare for a webinar: <a href="https://www.meetup.com/SQLCape-Meetup/events/282241220/"><strong>Analyze Billions of Rows of Data in Real-Time Using Azure Data Explorer</strong></a>. One of the demos in that webinar is about ingesting data from Apache Kafka into <strong>Azure Data Explorer</strong>. When prepping, I noticed that my Confluent Cloud Kafka cluster didn&rsquo;t exist anymore, so I had to come up with a workaround. That workaround was to use <strong>Azure Event Hubs</strong> instead of Kafka.</p>

<p>Since I already had the code to publish data to Kafka and knew that you could use the Kafka client to publish to Event Hubs, I thought I&rsquo;d test it out. I did run into some minor snags along the way, so I thought I&rsquo;d write a blog post about it. Then, at least, I have something to go back to. This post also looks at how to set up an Event Hubs cluster.</p>

<p></p>

<h2 id="background">Background</h2>

<p>Azure Event Hubs is similar to Apache Kafka in that it is a big data streaming platform and event ingestion service. It is a fully managed Platform-as-a-Service (PaaS) with little configuration or management overhead, very much like Apache Kafka in <strong>Confluent Cloud</strong>. The one difference between Azure Event Hubs and Apache Kafka is that Event Hubs does not have an on-prem solution.</p>

<h4 id="kafka-vs-event-hubs-concepts">Kafka vs Event Hubs Concepts</h4>

<p>Event Hubs and Kafka are pretty similar, as I mentioned above. Let us compare the concepts of the two:</p>

<table>
<thead>
<tr>
<th>Kafka</th>
<th>Event Hubs</th>
</tr>
</thead>

<tbody>
<tr>
<td>Cluster</td>
<td>Namespace</td>
</tr>

<tr>
<td>Topic</td>
<td>EventHub</td>
</tr>

<tr>
<td>Partition</td>
<td>Partition</td>
</tr>

<tr>
<td>Consumer Group</td>
<td>Consumer Group</td>
</tr>

<tr>
<td>Offset</td>
<td>Offset</td>
</tr>
</tbody>
</table>

<p><strong>Table 1:</strong> <em>Kafka vs Event Hubs Concepts</em></p>

<p>As we see in <em>Table 1</em>, there is not much difference between Kafka and Event Hubs. The one difference worth noting is the Event Hubs namespace instead of the Kafka cluster.</p>

<h4 id="namespace">Namespace</h4>

<p>An Event Hubs namespace is a dedicated scoping container for event hubs, where an event hub as mentioned above is the equivalent to a Kafka topic. We can see it as a management container for individual event hubs (topics), and it provides a range of access control and network integration management features.</p>

<p>For this post, the important part is that the namespace provides IP endpoints allowing us to publish to the namespace and its individual event hubs (topics).</p>

<h4 id="event-hubs-kafka-endpoint">Event Hubs Kafka Endpoint</h4>

<p>One of the endpoints the namespace provides is an endpoint compatible with the Apache Kafka producer and consumer APIs at version 1.0 and above.</p>

<p>So if your application uses a Kafka client version 1.0+, you can use the Event Hubs Kafka endpoint from your applications without code changes, apart from configuration, compared to your existing Kafka setup.</p>

<blockquote>
<p><strong>NOTE:</strong> When I above say you only need to change the configuration, I assume that the Kafka topics have corresponding EventHubs (remember Kafka topic = Event Hubs EventHub).</p>
</blockquote>

<p>An interesting point here is that it is not only your Kafka applications that can ~consume from~ publish to Event Hubs but any application that uses Kafka Client 1.0+, like Kafka Connect connectors!</p>

<h2 id="pre-reqs">Pre-Reqs</h2>

<p>This section is here if you want to follow along. This is what you need:</p>

<ul>
<li>An Azure account. If you don&rsquo;t have an Azure subscription, sign up for a <a href="https://azure.microsoft.com/en-us/free/">free account</a>.</li>
<li>Your favourite development language. In this post, I use Python.</li>
<li>Kafka client for your favourite language as per above. <a href="https://docs.confluent.io/platform/current/clients/index.htm">Here</a> you find a list of clients.</li>
<li>Application publishing to Kafka. The idea is that you can switch from publishing to Kafka to publish to Event Hubs in this application.</li>
</ul>

<p>Ensure the client is installed. In Python, I do it using <code>pip</code>, like so:</p>

<pre><code class="language-python">pip install confluent-kafka
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Install the Client</em></p>

<p>After having run the code in <em>Code Snippet 1</em>, or similar code for your language, you are good to go.</p>

<h4 id="kafka-application">Kafka Application</h4>

<p>Above I mentioned about an application publishing to Kafka being optional. Let us here take a look at a very simple example.</p>

<p>In this example, I have an Azure Confluent Cloud cluster looking like so:</p>

<p><img src="/images/posts/evthub-adx-confluent-cloud.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Confluent Cloud Cluster</em></p>

<p>In <em>Figure 1</em>, we see my Confluent Cloud cluster named <code>kafkaeventhubs</code> and how that cluster has one topic - <code>testtopic</code> - with four partitions.</p>

<blockquote>
<p><strong>NOTE:</strong> To see how to run Confluent Cloud in Azure, see my post <a href="/2021/08/14/run-confluent-cloud--serverless-apache-kafka-on-azure/"><strong>Run Confluent Cloud &amp; Serverless Apache Kafka on Azure</strong></a>.</p>
</blockquote>

<p>I have a very basic Python application publishing to the <code>testtopic</code> looking like so:</p>

<p><img src="/images/posts/evthub-adx-kafka-app.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Python Publish App</em></p>

<p>As we see in <em>Figure 2</em> the app is not doing anything advanced. It publishes a comma-delimited message containing the random key value, &ldquo;Hello World&rdquo;, and a timestamp.</p>

<p>What is interesting in the code is the configuration string (assigned to the variable <code>conf</code>) in lines 14 - 19, and specifically the following properties:</p>

<ul>
<li><code>bootstrap.servers</code>: The cluster endpoint.</li>
<li><code>security.protocol</code>: The protocol used to communicate with brokers. In this case, we use <code>SASL_SSL</code>, which uses SASL for authentication and SSL for encryption.</li>
<li><code>sasl.mechanism</code>: This indicates how we authenticate. By setting it to <code>PLAIN</code>, we use a &ldquo;simple&rdquo; username/password based authentication mechanism.</li>
<li><code>sasl.username</code>: The username to use. In Confluent Cloud, we use an API key mechanism, where the <code>username</code> is the API key, and the password is the API key&rsquo;s secret.</li>
<li><code>sasl.password</code>: As per above, this is the API key&rsquo;s secret.</li>
</ul>

<p>Why I say these are interesting is these are the ones that are in play if we want to publish to Event Hubs.</p>

<p>Having seen the pre-reqs lets us create a namespace in Event Hubs.</p>

<h2 id="create-event-hubs-namespace">Create Event Hubs Namespace</h2>

<p>There are multiple ways we can create an Event Hubs resource, where the Azure Portal is one of them.</p>

<p>Instead of doing a step-by-step explanation of creating an Event Hubs namespace, I suggest you read the Microsoft article <a href="https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-create"><strong>Quickstart: Create an event hub using Azure portal</strong></a>. While you are doing that, I will create an Event Hubs test namespace for this post.</p>

<blockquote>
<p><strong>NOTE:</strong> When you create the namespace, you have to ensure you create it under the <em>Standard</em> pricing tier (or higher), as <em>Basic</em> does not support Kafka.</p>
</blockquote>

<p>We are all back? Cool, I ended up with this:</p>

<p><img src="/images/posts/evthub-adx-topics-1.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Event Hubs Namespace with EventHub</em></p>

<p>As you see in <em>Figure 3</em> I ended up with an Event Hubs namespace called <code>kafkaeventhubs</code> and an Event Hub named <code>testtopic</code>, the same as I have in Confluent Cloud.</p>

<h2 id="event-hubs-security-authentication">Event Hubs Security &amp; Authentication</h2>

<p>So now we have all we need to replace Kafka with Event Hubs - almost. We still need to see how to configure the Event Hubs security and authentication.</p>

<p>When using Event Hubs, all data in transit is TLS encrypted, and we can satisfy that by using <code>SASL_SSL</code>. This is exactly as in the Kafka code. Using <code>SASL_SSL</code> we have basically two options for authentication: OAuth 2.0 or Shared Access Signature (SAS). In this post, I use SAS, which matches what I do using Kafka.</p>

<h4 id="create-shared-access-signature">Create Shared Access Signature</h4>

<p>In Event Hubs, we can create a SAS for either a namespace or an individual EventHub in a namespace. Let us create a SAS for our <code>testtopic</code> Event Hub.</p>

<p>If you click into the <code>testtopic</code> Event Hub we see in <em>Figure 3</em> and look at the left-hand side; you see something like so:</p>

<p><img src="/images/posts/evthub-adx-topic-sas.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Event Hub Shared Access Policies</em></p>

<p>We see under <em>Settings</em> in <em>Figure 4</em> <em>Shared access policies</em> (outlined in red). When we click on it:</p>

<p><img src="/images/posts/evthub-adx-topic-sas-2.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Shared Access Policies</em></p>

<p>We are being told that we don&rsquo;t have any policies set up, as outlined in yellow in <em>Figure 5</em>. Cool, let us create a policy. We do that by clicking on the <em>+ Add</em> button, which is outlined in red in <em>Figure 5</em>:</p>

<p><img src="/images/posts/evthub-adx-topic-sas-create.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>Add Policy</em></p>

<p>By clicking the <em>+ Add</em> button, we get a dialog <em>Add SAS Policy</em> as shown in <em>Figure 6</em>. We give it a name and the claims (what it allows), and then we click the <em>Create</em> button at the bottom of the dialog (not shown in <em>Figure 6</em>):</p>

<p><img src="/images/posts/evthub-adx-topic-sas-created.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>Policy Created</em></p>

<p>In <em>Figure 7</em>, we see the generated policy and the claims. Clicking on the policy, we get yet another dialog:</p>

<p><img src="/images/posts/evthub-adx-topic-sas-policy-keys.png" alt="" /></p>

<p><strong>Figure 8:</strong> <em>Policy Keys &amp; Connection Strings</em></p>

<p>The policy we created consists of two keys and corresponding connection strings as in <em>Figure 8</em>. The reason for having two is that in a production environment, you may want to cycle and regenerate the keys, so while you regenerate one, you can use the other.</p>

<p>Copy one of the connection strings as that is what we use for the Kafka client configuration:</p>

<p><img src="/images/posts/evthub-adx-topic-sas-policy-connstring.png" alt="" /></p>

<p><strong>Figure 9:</strong> <em>Connection String</em></p>

<p>In <em>Figure 9</em>, we see one of my connection strings. I have outlined in red the endpoint URL, and this is the one we use for the <code>bootstrap.servers</code> property in the configuration.</p>

<p>Now we should have everything we need to use the Kafka client to publish to the EventHub.</p>

<h2 id="configure-kafka-client">Configure Kafka Client</h2>

<p>Above I listed the configuration properties used when I connect to Confluent Cloud. Let us see what they should be when publishing to Event Hubs:</p>

<h4 id="bootstrap-servers">Bootstrap Servers</h4>

<p>The <code>bootstrap.servers</code> property defines the endpoint(s) where the client connects to. In <em>Figure 9</em>, I outlined the endpoint URL and said it would be used for <code>bootstrap.servers</code>. At the beginning of this post, I mentioned how Event Hubs exposes a Kafka endpoint. It does that on port <code>9093</code>. So:</p>

<pre><code class="language-python">`&quot;bootstrap.servers&quot;: &quot;kafkaeventhubs.servicebus.windows.net:9093&quot;`
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Bootstrap Servers</em></p>

<p>Onto the security and authorization part.</p>

<h4 id="security-protocol-sasl-mechanisms">Security Protocol` &amp; SASL Mechanisms</h4>

<p>As we use Shared Access Signature, We do not need to change <code>security.protocol</code> or <code>sasl.mechanism</code>; we keep them as <code>SASL_SSL</code> and <code>PLAIN</code>, respectively..</p>

<h4 id="username-password">Username &amp; Password</h4>

<p>At the very beginning of this post, I mentioned how I ran into some snags when trying to use the Kafka client to publish to Event Hubs. This is the part that caused me issues.</p>

<p>Reading documentation and blog posts when trying to connect to Event Hubs, I concluded that <code>sasl.password</code> should be set to the whole connection string you get from the SAS policy. OK, that&rsquo;s cool - but what about the user name?</p>

<p>Posts and docs talk about using <code>$ConnectionString</code>, but <code>$ConnectionString</code> looked like a variable to me, and I could not see where it was set. It finally dawned upon me that the user name property should literally be set to <code>$ConnectionString</code> - duh. So:</p>

<pre><code class="language-python">'sasl.username': &quot;$ConnectionString&quot;,
'sasl.password': &quot;Endpoint= sb://kafkaeventhubs.servicebus.windows.net/; \
                      SharedAccessKeyName=publishconsumepolicy; \
                      SharedAccessKey=&lt;secret-key&gt;, 
                      EntityPath=testtopic&quot;

</code></pre>

<p><strong>Code Snippet 3:</strong> <em>User Name &amp; Password</em></p>

<p>In <em>Code Snippet 3</em>, we see how <code>sasl.password</code> is set to the SAS policy&rsquo;s connection string and <code>sasl.username</code> to <code>$ConnectionString</code>.</p>

<p>The complete configuration required to connect to and publish to Event Hubs looks like so:</p>

<pre><code class="language-python">conf = {'bootstrap.servers': 'kafkaeventhubs.servicebus.windows.net:9093',
         'security.protocol': 'SASL_SSL',
         'sasl.mechanisms': 'PLAIN',
         'sasl.username': &quot;$ConnectionString&quot;,
         'sasl.password': &quot;Endpoint= \
                      sb://kafkaeventhubs.servicebus.windows.net/; \
                      SharedAccessKeyName=publishconsumepolicy; \
                      SharedAccessKey=&lt;secret-key&gt;; \
                      EntityPath=testtopic&quot;,
         'client.id': socket.gethostname()}

</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Event Hubs Configuration</em></p>

<p>You can now replace lines 14 - 19 that we see in <em>Figure 2</em> with what we have in <em>Code Snippet 4</em>, and you are good to go!</p>

<h2 id="publish-to-event-hubs">Publish to Event Hubs</h2>

<p>After editing the <code>conf</code> variable, you can run the application. Let it publish some messages and then check what you see in the overview for the <code>testtopic</code> Event Hub:</p>

<p><img src="/images/posts/evthub-adx-publish.png" alt="" /></p>

<p><strong>Figure 10:</strong> <em>Publish Events</em></p>

<p>There is no straightforward way to see if messages come into the Event Hub, so we look at the request stats. In <em>Figure 10</em>, we see how events have come into the Event Hub. Yay - it seems like it works!</p>

<p>I said above that there is no straightforward way to see if messages/events arrive into the Event Hub. Well, when we now know how to connect and publish to Event Hubs using the Kafka client, we could easily create a consuming application. However, I leave that up to you.</p>

<h2 id="summary">Summary</h2>

<p>In this post, we have seen how to use the Kafka client to connect and publish messages to an Azure Event Hub.</p>

<p>We compared the terminology of Kafka with Event Hubs, and saw that it is more or less the same. The two major differences are:</p>

<ol>
<li>In Kafka, we talk about clusters, whereas in Event Hubs, we have namespaces.</li>
<li>A topic in Kafka is called an Event Hub in Event Hubs.</li>
</ol>

<p>We use port <code>9093</code> on the Event Hubs endpoint to connect the Kafka client. When using <code>SASL_SSL</code> and the <code>PLAIN</code> <code>sasl.mechanism</code>, the user name we use is<code>$ConnectionString</code>, and the password is the entire connection string from the Event Hub&rsquo;s Shared Access Signature policy.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or [ping][ma] me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 50, 2021]]></title>
    <link href="https://nielsberglund.com/2021/12/12/interesting-stuff---week-50-2021/" rel="alternate" type="text/html"/>
    <updated>2021-12-12T10:37:02+02:00</updated>
    <id>https://nielsberglund.com/2021/12/12/interesting-stuff---week-50-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<blockquote>
<p><strong>NOTE:</strong> <em>It is now coming up on Christmas and New Year, and I will take a break with these posts and come back at the beginning of next year.</em></p>
</blockquote>

<p></p>

<h2 id="sql-server">SQL Server</h2>

<ul>
<li><a href="http://sql-sasquatch.blogspot.com/2021/12/sqlserver-column-store-object-pool.html">#SQLServer Column Store Object Pool &ndash; the Houdini Memory Broker Clerk AND Perfmon [\SQLServer:Buffer Manager\Target pages]</a>. In this post by Mr SQL Server NUMA, <a href="https://twitter.com/sqL_handLe">Lonny</a>, he &ldquo;spelunks&rdquo; around in SQL Server Buffer Pool. If you are interested in the &ldquo;innards&rdquo; of SQL Server, you need to read this post. Actually, you need to read everything Lonny <a href="http://sql-sasquatch.blogspot.com/">posts</a>.</li>
</ul>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://engineering.linkedin.com/blog/2021/evolving-linkedin-s-analytics-tech-stack">Evolving LinkedIn&rsquo;s analytics tech stack</a>. This is a fascinating post looking at lessons learned from LinkedIn&rsquo;s data platform migration. This post is a goldmine of information for anyone migrating from &ldquo;legacy&rdquo; data architecture to a modern one.</li>
<li><a href="https://databricks.com/blog/2021/12/06/deploying-dbt-on-databricks-just-got-even-simpler.html">Deploying dbt on Databricks Just Got Even Simpler</a>. Those interested in Big Data have probably heard about <a href="https://www.getdbt.com/"><strong>dbt</strong></a>, the open-source tool that allows you to build data pipelines using simple SQL. The post I link to announces the <strong>dbt-databricks</strong> adapter, which integrates dbt with the Databricks Lakehouse Platform. Cool stuff!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.infoq.com/news/2021/12/huyen-realtime-ml/">Chip Huyen on Streaming-First Infrastructure for Real-Time ML</a>. Even though you may do real-time ML predictions, you probably update your models manually. This <a href="https://www.infoq.com/">InfoQ</a> article looks at a QCon presentation where the presenter looked at, among other things, how a streaming-first infrastructure can help you do ML in real-time, both online prediction and continual learning.</li>
<li><a href="https://www.kai-waehner.de/blog/2021/12/08/apache-kafka-for-conversational-ai-nlp-chatbot/">Apache Kafka for Conversational AI, NLP and Chatbot</a>. The post looks at how event streaming with Apache Kafka is used in conjunction with Machine Learning platforms for reliable real-time conversational AI, NLP, and chatbots. The post looks at examples from the carmaker BMW, the online travel and booking portal Expedia, and Tinder&rsquo;s dating app. Very cool!</li>
<li><a href="https://www.confluent.io/blog/serverless-event-stream-processing/">Serverless Stream Processing with Apache Kafka, AWS Lambda, and ksqlDB</a>. This blog post defines what &ldquo;serverless stream processing&rdquo; is. Apart from just discussing concepts and implementations, it describes arguably the most essential pattern for building event streaming applications using ksqlDB. Read It!</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>The year is coming to a close, and as for presentations, webinars, etc., I have two left:</p>

<p><img src="/images/posts/sql-cape-adx.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>SQL Cape - Azure Data Explorer</em></p>

<p>On Tuesday (Dec. 14), I deliver the last <strong>Azure Data Explorer</strong> presentation for this year:</p>

<ul>
<li><a href="https://www.meetup.com/SQLCape-Meetup/events/282241220/"><strong>Analyze Billions of Rows of Data in Real-Time Using Azure Data Explorer</strong></a>. The presentation is a virtual event hosted by my mate <a href="https://www.linkedin.com/in/jodyrobertssql/"><strong>Jody Roberts</strong></a>. If you are interested in ADX, please <a href="https://www.meetup.com/SQLCape-Meetup/events/282241220/">sign up</a> (it is <strong>FREE</strong>) and come and join the fun. Any time Jody and I get together, regardless if it is IRL or a virtual event like this, some fun stuff happens!</li>
</ul>

<p>The second event is also virtual:</p>

<p><img src="/images/posts/tech-fun.jpg" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Tech Fun Space</em></p>

<p>The event takes place Thursday, Dec. 23. It is not a webinar but an event for the Global Data Community to get together to welcome 2022. The organiser is my good friend <a href="https://www.linkedin.com/in/jeandjoseph/"><strong>Jean Joseph</strong></a>. Read more about it (this event is also <strong>FREE</strong>) and <a href="https://www.eventbrite.com/e/tech-fun-space-welcome-2022-tickets-215046428657">sign up here</a>.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>

<p>Oh, and if I don&rsquo;t see you virtually or IRL before the holidays: <strong>Happy Holidays</strong>!</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 49, 2021]]></title>
    <link href="https://nielsberglund.com/2021/12/05/interesting-stuff---week-49-2021/" rel="alternate" type="text/html"/>
    <updated>2021-12-05T08:51:58+02:00</updated>
    <id>https://nielsberglund.com/2021/12/05/interesting-stuff---week-49-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://databricks.com/blog/2021/12/01/the-foundation-of-your-lakehouse-starts-with-delta-lake.html">The Foundation of Your Lakehouse Starts With Delta Lake</a>. The Databricks Delta Lake has continuously evolved during the last few years, and in May 2021, Delta Lake 1.0 was announced. The evolution of Delta Lake doesn&rsquo;t stop with the 1.0 release, and this blog post reviews the major features released so far and provides an overview of the upcoming roadmap.</li>
<li><a href="https://www.theseattledataguy.com/what-is-trino-and-why-is-it-great-at-processing-big-data/">What Is Trino And Why Is It Great At Processing Big Data</a>. Trino is an open-source distributed SQL query engine for ad-hoc and batch ETL queries against multiple types of data sources. It previously went under the name of Presto, but due to various reasons, it had to change its name. The post linked to looks at Trino and covers its positives and negatives. At <a href="/derivco">Derivco</a> we have contemplated using Trino. Let us see what the future brings.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://developer.confluent.io/podcast/ksqldb-fundamentals-how-apache-kafka-sql-and-ksqldb-work-together-ft-simon-aubury">ksqlDB Fundamentals: How Apache Kafka, SQL, and ksqlDB Work Together ft. Simon Aubury</a>. This link is to a podcast where <a href="https://twitter.com/tlberglund">Tim Berglund</a> talks to <a href="https://twitter.com/simonaubury">Simon Aubury</a> about everything ksqlDB. They cover basic ksqlDB, plus they look at how to use ksqlDB to find out which aeroplane wakes Simon&rsquo;s cat each morning. Very interesting!</li>
<li><a href="https://www.infoq.com/presentations/raft-kafka-api/">Co-Designing Raft + Thread-per-Core Execution Model for the Kafka-API</a>. This <a href="https://www.infoq.com/">InfoQ</a> presentation looks at, as the title says, codesign in Raft on a thread per core model for the Kafka API. This presentation is a must-see if you are interested in building low-latency software.</li>
<li><a href="https://www.confluent.io/blog/guide-to-stream-processing-and-ksqldb-fundamentals/">A Guide to Stream Processing and ksqlDB Fundamentals</a>. ksqlDB allows you to build applications that react to events as they happen and to take advantage of real-time data. Even though you use familiar SQL syntax when building your ksqlDB application, you might want some help. This post talks about the <strong>ksqlDB 101</strong> course on Confluent Developer, which offers both lectures and hands-on exercises.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>SQLBITS 2022 - The Greatest Data Show - is just around the corner, and I am happy to announce that I am doing a full-day training session:</p>

<p><img src="/images/posts/sqlbits-precon-adx.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>SQLBITS 2022 - A Day of Azure Data Explorer</em></p>

<p>Yes, I am doing a whole day of <strong>Azure Data Explorer</strong>. Read more at: <a href="https://sqlbits.com/information/event22/A_Day_of_Azure_Data_Explorer/trainingdetails"><strong>A Day of Azure Data Explorer</strong></a>.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 48, 2021]]></title>
    <link href="https://nielsberglund.com/2021/11/28/interesting-stuff---week-48-2021/" rel="alternate" type="text/html"/>
    <updated>2021-11-28T08:38:06+02:00</updated>
    <id>https://nielsberglund.com/2021/11/28/interesting-stuff---week-48-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://blogvisionarios.com/e-learning/articulos-data/databricks-photon-vs-azure-synapse/">Databricks Photon vs Azure Synapse</a>. This post is in Spanish, but as I know that the readers of my blog (all two of you) are fluent in Spanish, so that shouldn&rsquo;t be a problem. Or - you could do what I did, translate it to English. The translation didn&rsquo;t come out half bad, and the post compares the performance of Databricks and Azure Synapse. Interesting read!</li>
</ul>

<h2 id="azure">Azure</h2>

<ul>
<li><a href="https://blog.ediri.io/podtato-head-pulumi-and-azure-container-apps">Podtato-head, Pulumi and Azure Container Apps</a>. This post caught my eye as it mentions <a href="https://www.pulumi.com/"><strong>Pulumi</strong></a>. I am interested in Pulumi not so much for it being a cloud engineering platform, but for <a href="https://twitter.com/funcOfJoe"><strong>Joe Duffy</strong></a> being one of the founders. Joe is a very smart guy, and I keep an eye out for things he is doing. Anyway, I link to this post because of the reference to <strong>Azure Container Apps</strong>. ACA looks extremely cool, and I must look at it further.</li>
</ul>

<h2 id="azure-data-explorer">Azure Data Explorer</h2>

<ul>
<li><a href="https://techcommunity.microsoft.com/t5/azure-data-explorer-blog/using-azure-data-explorer-timeseries-capabilities-in-power-bi/ba-p/2727977">Using Azure Data Explorer timeseries capabilities in Power BI</a>. The more I see of <strong>Azure Data Explorer</strong> (ADX), the more impressed I become. Take this post, for example, which looks at using PowerBI to hook into the time series capabilities of ADX. Soo cool!</li>
<li><a href="https://medium.com/@ravikanthmusti/azure-data-explorer-for-real-time-alerts-in-healthcare-480158bcf5d3">Azure Data Explorer for Real time alerts in Healthcare</a>. One more post which makes me realise how awesome <strong>Azure Data Explorer</strong> is. This post looks at how you can &ldquo;hook-up&rdquo; <a href="https://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-overview"><strong>Azure Logic Apps</strong></a> with ADX.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-overview">Consuming over 1 billion Kafka messages per day at Ifood</a>. Well, the title says it all! In the post, the author looks at the journey he had towards being able to consume a &ldquo;shed-load&rdquo; (&ldquo;shed-load&rdquo; is a technical term) of messages per day. The post has quite a few &ldquo;tips and tricks&rdquo;, so you should definitely read this post if you are a high volume Kafka consumer.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>As with last weeks roundup, this is not so much of what I am doing as of what I did:</p>

<p><img src="/images/posts/analyze-billions-adx.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Future Data Driven</em></p>

<p>And also, like last week, it is a recording for a presentation I did a while back. The presentation is <a href="https://youtu.be/AUXzlBZtebg"><strong>Analyze Billions of Rows of Data in Real-Time Using Azure Data Explorer</strong></a>, and the conference was <strong>Future Data Driven</strong>.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 47, 2021]]></title>
    <link href="https://nielsberglund.com/2021/11/21/interesting-stuff---week-47-2021/" rel="alternate" type="text/html"/>
    <updated>2021-11-21T09:27:19+02:00</updated>
    <id>https://nielsberglund.com/2021/11/21/interesting-stuff---week-47-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="machine-learning-ai">Machine Learning / AI</h2>

<ul>
<li><a href="https://www.microsoft.com/en-us/research/blog/synapseml-a-simple-multilingual-and-massively-parallel-machine-learning-library/">SynapseML: A simple, multilingual, and massively parallel machine learning library</a>. This post introduces <strong>SynapseML</strong>. SynapseML was previously known as MMLSpark and is an open-source library that simplifies the creation of massively scalable machine learning (ML) pipelines. SynapseML unifies several ML frameworks and new Microsoft algorithms in a single, scalable API usable across Python, R, Scala, and Java.</li>
</ul>

<h2 id="databases">Databases</h2>

<ul>
<li><a href="http://muratbuffalo.blogspot.com/2021/11/whats-really-new-with-newsql.html">What&rsquo;s Really New with NewSQL?</a>. In this post, <a href="https://twitter.com/muratdemirbas">Murat</a> looks at the evolution of NoSQL into NewSQL and what NewSQL is. Very informative; I liked the post a lot.</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://databricks.com/blog/2021/11/19/ray-on-databricks.html">Ray on Databricks</a>. No, this is not a post where someone named <strong>Ray</strong> talks about Databricks. Ray is an open-source project that makes it simple to scale any compute-intensive Python workload. Running Ray on top of an Apache Spark cluster creates the ability to distribute the internal code of PySpark UDFs and Python code that used to be only run on the driver node. But hang on a sec; Spark is a distributed framework. Why would I want to run another distributed framework on top of Spark? Well, read the post and find out.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.kai-waehner.de/blog/2021/11/14/streaming-data-exchange-data-mesh-apache-kafka-in-motion/">Streaming Data Exchange with Kafka and a Data Mesh in Motion</a>. In quite a few roundups, I have linked to posts about <strong>Data Mesh</strong>. In even more roundups, I have linked to Kafka material and posts about streaming data. The post linked to looks at the principle behind the Data Mesh and why we need multiple technologies to build a Data Mesh. The post dives into why Kafka is a good solution for the foundation of a Data Mesh.</li>
<li><a href="https://www.confluent.io/blog/ksqldb-0-22-new-features-major-upgrades/">Announcing ksqlDB 0.22.0</a>. I guess the post title says it all: it looks at some of the new features of <strong>ksqlDB</strong> 0.22. And some very cool new features they are as well! Please read the post to find out more!</li>
<li><a href="https://www.confluent.io/blog/push-queries-v2-with-ksqldb-scalable-sql-query-subscriptions/">How to Efficiently Subscribe to a SQL Query for Changes</a>. This post looks at one of the new features in <strong>ksqlDB</strong> 0.22; enhancements to push queries and increased scalability of said queries. Very, very cool!</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>It is not so much of what I am doing as of what did I do:</p>

<p><img src="/images/posts/improve-clv-2.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Cloud Data Driven</em></p>

<p>I did a presentation a little while back at <a href="https://www.linkedin.com/company/clouddatadriven/"><strong>Cloud Data Driven</strong></a>, where I looked at <strong>Customer Lifetime Value</strong> (CLV), and how you can use Azure Databricks to calculate the CLV. As I said, it was a week or two ago, why i mention it now is because the recording of the webinar is up on YouTube. So, if you are interested - go and have a look at <a href="https://youtu.be/e6MJ4DRCgj8"><strong>Improve Customer Lifetime Value using Azure Databricks Delta Lake</strong></a>.</p>

<p>While you are at it, register with <a href="https://www.meetup.com/cloud-data-driven"><strong>Cloud Data Driven</strong></a>&rsquo;s Meetup group. The group is awesome if you are interested in everything data!</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 46, 2021]]></title>
    <link href="https://nielsberglund.com/2021/11/14/interesting-stuff---week-46-2021/" rel="alternate" type="text/html"/>
    <updated>2021-11-14T08:50:09+02:00</updated>
    <id>https://nielsberglund.com/2021/11/14/interesting-stuff---week-46-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://azure.microsoft.com/en-us/services/chaos-studio/">Azure Chaos Studio</a>. At <a href="https://myignite.microsoft.com/home">Microsoft Ignite</a>, a week or two ago, Microsoft announced the public preview of <strong>Azure Chaos Studio</strong>. Azure Chaos Studio is a fully-managed experimentation service to help customers track, measure, and mitigate faults with controlled chaos engineering to improve the resilience of their cloud applications. This looks very interesting, and we will definitely have a look at it.</li>
</ul>

<h2 id="azure-data-explorer">Azure Data Explorer</h2>

<ul>
<li><a href="https://docs.microsoft.com/en-us/azure/architecture/example-scenario/security/security-log-retention-azure-data-explorer">Long-term security log retention with Azure Data Explorer</a>. Having access to long-term security logs is essential. Querying long-term logs is critical for identifying the impact of threats and investigating illicit access attempts. This post outlines a solution for long-term retention of security logs where Azure Data Explorer is at the core of the architecture.</li>
<li><a href="https://techcommunity.microsoft.com/t5/azure-data-explorer/query-past-data-with-hot-windows/ba-p/2848114">Query past data with hot windows</a>. Azure Data Explorer has the notion of hot and cold data. Hot data is stored on SSD&rsquo;s on cluster nodes, whereas cold data is stored in Azure Blob Storage. Hot data offers the best query performance: an order of magnitude more performant than cold data. Sometimes you may want to query the hot data together with some of the cold. This post looks recently added functionality to Azure Data Explorer, creating a time window in the past which we want to be part of the hot data: Hot Window.<br /></li>
<li><a href="https://techcommunity.microsoft.com/t5/azure-data-explorer/train-your-model-on-spark-databricks-score-it-on-adx/ba-p/2098522">Train your Model on Spark/Databricks, score it on ADX</a>. Recently, I have been doing conference talks around Azure Databricks and Apache Spark and Azure Data Explorer. How cool would it be if you could combine the two?! The post linked to does just that. It looks at training and creating Machine Learning models using Azure Databricks and Spark and then using those models from Azure Data Explorer. Very cool! Oh, BTW - with Azure Data Explorer Pool&rsquo;s being made available in Azure Synapse, you no longer need Azure Databricks. You can do the same thing with Azure Synapse Analytics. The <a href="https://techcommunity.microsoft.com/t5/azure-data-explorer/query-past-data-with-hot-windows/ba-p/2848114"><strong>Azure Synapse Analytics - Operationalize your Spark ML model into Data Explorer pool for scoring</strong></a> post looks at that.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/introducing-confluent-platform-7-0/">Building Real-Time Hybrid Architectures with Cluster Linking and Confluent Platform 7.0</a>. Confluent recently released <a href="https://www.confluent.io/product/confluent-platform/"><strong>Confluent Platform 7.0</strong></a>, and this post looks at one of the new features in detail, the ability to directly connect clusters and mirror topics from one cluster to another: <a href="https://docs.confluent.io/platform/current/multi-dc-deployments/cluster-linking/index.html"><strong>Cluster Linking</strong></a>. This is something that we at <a href="/derivco">Derivco</a> are really interested in.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 45, 2021]]></title>
    <link href="https://nielsberglund.com/2021/11/07/interesting-stuff---week-45-2021/" rel="alternate" type="text/html"/>
    <updated>2021-11-07T08:13:38+02:00</updated>
    <id>https://nielsberglund.com/2021/11/07/interesting-stuff---week-45-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="sql-server">SQL Server</h2>

<ul>
<li><a href="https://techcommunity.microsoft.com/t5/sql-server/announcing-the-sql-server-2022-early-adoption-program/ba-p/2910617">Announcing the SQL Server 2022 Early Adoption Program</a>. The <a href="https://myignite.microsoft.com/home">Microsoft Ignite</a> conference was held during the week just gone by. As expected, there were quite a few announcements around new and improved products. One such announcement was related to this post; the next version of SQL Server is in the works, and Microsoft has just opened the <strong>Early Adoption Program</strong> (EAP) for <strong>SQL Server 2022</strong>. If you are interested in shaping the next version of SQL Server, I suggest you sign up!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://medium.com/event-driven-utopia/architecting-a-kafka-centric-retail-analytics-platform-part-2-137bfac04001">Architecting a Kafka-centric Retail Analytics Platform — Part 2</a>. In <a href="/2021/10/31/interesting-stuff---week-44-2021/">last weeks roundup</a>, I linked to the first post in a series, of which this post is the second instalment. In this post, the author, <a href="https://twitter.com/dunithd">Dunith Dhanushka</a>, looks at data ingestion into Kafka in detail. As the series is about retail, the post looks at the retail data landscape, what data to capture, and how it can be ingested into Kafka using the Kafka ecosystem.</li>
<li><a href="https://www.confluent.io/blog/streaming-databases-vs-traditional-databases/">Readings in Streaming Database Systems</a>. This post is the introduction/prequel to a series looking at streaming databases: <strong>The Streaming Database Series</strong>. This post gives a high level overview of what&rsquo;s coming in the posts in the series.  It also provides an overview of streaming databases.</li>
<li><a href="https://www.confluent.io/blog/databases-meet-stream-processing-the-future-of-sql/">The Future of SQL: Databases Meet Stream Processing</a>. This post is the first in the <strong>The Streaming Database Series</strong> mentioned above. The post discusses why the database world needs enhancements to handle data both at rest and in transit. The enhancements looked at are the <code>STREAM</code> abstraction, new query types, and extended semantics for handling time.</li>
<li><a href="https://www.confluent.io/blog/streaming-database-design-principles-and-guarantees/">4 Key Design Principles and Guarantees of Streaming Databases</a>. The second in the series mentioned above, this post summarizes a few challenging design principles for modern streaming databases that act as a source of truth for stream data management and query processing systems. The post also presents ksqlDB&rsquo;s persistent log-based approach to following the design principles.</li>
<li><a href="https://www.confluent.io/blog/streaming-databases-cloud-data-in-motion-never-rests/">How Do You Change a Never-Ending Query?</a>. The post linked to is the third in the <strong>The Streaming Database Series</strong>. The post looks at how we can evolve queries in a streaming database and some of the pitfalls that may occur.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>Now is the conference season, and I am presenting at <a href="https://www.linkedin.com/company/clouddatadriven/"><strong>Cloud Data Driven</strong></a>:</p>

<p><img src="/images/posts/improve-clv.jpg" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Cloud Data Driven</em></p>

<p>I will be talking about how to calculate Customer Lifetime Value using Azure Databricks. If you are interested, the registration is <strong>FREE</strong>, so go ahead and <a href="https://www.eventbrite.com/e/improve-customer-lifetime-value-using-azure-databricks-delta-lake-tickets-189230642947"><strong>register</strong></a>.</p>

<p>As you see, the presentation is on Thursday, November 11. If you read the last week&rsquo;s <a href="/2021/10/31/interesting-stuff---week-44-2021/">roundup</a>, you may have noticed this:</p>

<p><img src="/images/posts/PASS-I-am-speaking-800.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>PASS Session</em></p>

<p>Yes, I am doing a live PASS Q&amp;A the same day. The PASS session id for my <a href="https://passdatacommunitysummit.com/sessions/265026"><strong>Analyze Billions of Rows of Data in Real-Time Using Azure Data Explorer</strong></a> presentation. Fortunately, the PASS session is 3:15 - 3:45 pm UTC, and my Cloud Data Driven presentation is at 4 pm UTC. Phew!</p>

<p>So here is an idea; get a double dose of Niels:</p>

<ul>
<li><a href="https://reg.passdatacommunitysummit.com/flow/redgate/summit2021/registrationopeningpage/page/introlanding">Register</a> (FREE) for PASS now, view the <a href="https://passdatacommunitysummit.com/sessions/265026"><strong>Analyze Billions of Rows of Data in Real-Time Using Azure Data Explorer</strong></a> recorded video and come and chat to me <a href="https://passdatacommunitysummit.com/sessions/265026">Thursday, Nov 11 15:15 - 15:45 UTC</a>.</li>
<li><a href="https://www.eventbrite.com/e/improve-customer-lifetime-value-using-azure-databricks-delta-lake-tickets-189230642947">Register</a> and attend my Cloud Data Driven presentation.</li>
</ul>

<p>Yay, Niels on Thursday from 3:15 UTC. What could be better than that? Actually, don&rsquo;t answer that question.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 44, 2021]]></title>
    <link href="https://nielsberglund.com/2021/10/31/interesting-stuff---week-44-2021/" rel="alternate" type="text/html"/>
    <updated>2021-10-31T08:53:44+02:00</updated>
    <id>https://nielsberglund.com/2021/10/31/interesting-stuff---week-44-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h1 id="machine-learning-data-science">Machine Learning / Data Science</h1>

<ul>
<li><a href="https://netflixtechblog.com/open-sourcing-a-monitoring-gui-for-metaflow-75ff465f0d60">Open-Sourcing a Monitoring GUI for Metaflow, Netflix&rsquo;s ML Platform</a>. This Netflix post looks at <strong>Metaflow GUI</strong>. This GUI for their open-sourced, <strong>Metaflow</strong> library allows data scientists to monitor their workflows in real-time, track experiments, and see detailed logs and results for every executed task. The GUI can be extended with plugins, allowing the community to build integrations to other systems, etc.</li>
<li><a href="https://databricks.com/blog/2021/10/28/moneyball-2-0-real-time-decision-making-with-mlbs-statcast-data.html">Moneyball 2.0: Real-time Decision Making With MLB&rsquo;s Statcast Data</a>. Back in 2003, <a href="https://en.wikipedia.org/wiki/Michael_Lewis">Michael Lewis</a> wrote the book <a href="https://en.wikipedia.org/wiki/Moneyball">Moneyball</a>. The book was about how a baseball manager used data analysis to identify undervalued players. The post here looks at how baseball teams today use streaming data and Databricks to do real-time analysis and decisions. Very interesting! Oh, BTW, Micheal Lewis is an excellent author, and the book mentioned is great!</li>
</ul>

<h2 id="azure-data-explorer">Azure Data Explorer</h2>

<ul>
<li><a href="https://dps10.com/how-to-do-real-time-analytics-using-apache-kafka-and-azure-data-explorer/">How to do Real-Time Analytics Using Apache Kafka and Azure Data Explorer</a>. In a couple of blog posts, I have mentioned how I did a session about Apache Kafka and Azure Data Explorer at the <strong>Data Platform Summit 2021</strong>. The recordings from the Summit has now been made available for FREE, and the link is to my session. Notice that you need to join Data Platform Geeks unless you are a member already, but it is free, and by joining, you get access to all recordings!</li>
<li><a href="/2021/10/27/how-to-ingest-into-azure-data-explorer-from-apache-kafka-using-kafka-connect/">How to Ingest Into Azure Data Explorer From Apache Kafka using Kafka Connect</a>. This post is also from &ldquo;yours truly&rdquo;. In the post we look at how to configure and set up Kafka Connect to allow ingestion into Azure Data Explorer.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://medium.com/event-driven-utopia/architecting-a-kafka-native-retail-analytics-platform-part-1-bf1eba42a371">Architecting a Kafka-centric Retail Analytics Platform — Part 1</a>. This post is the first of a series looking at building a Kafka-centric analytics platform that ingests and processes business data at scale. By the way, the author of the post, <a href="https://twitter.com/dunithd">Dunith Dhanushka</a>, is someone you should follow if you are interested in event driven architecture, streaming, Kafka, etc. He is excellent, and I am subscribing to his writings on Medium!</li>
<li><a href="https://www.confluent.io/blog/stream-governance-how-it-works/">Stream Governance – How it Works</a>. I have written previously about the new Stream Governance functionality Confluent introduced recently. This post is the first in a series about Stream Governance and how it works: <strong>Stream Governance – How it Works</strong>. This first post looks at some of the key features of Stream Governance. At <a href="/derivco"><strong>Derivco</strong></a>, we are highly interested in the topic of data governance. I will follow this closely!</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>Well, I have mentioned it before, but:</p>

<p><img src="/images/posts/PASS-I-am-speaking-800.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>PASS Session</em></p>

<p>Yeah, I am delivering <a href="https://passdatacommunitysummit.com/sessions/265026"><strong>Analyze Billions of Rows of Data in Real-Time Using Azure Data Explorer</strong></a>. The session is recorded and will be available for viewing from when the conference starts. Then <a href="https://event.passdatacommunitysummit.com/widget/redgate/summit2021/sessioncatalog/session/1629311226474001sKl1">Thursday, Nov 11 15:15 - 15:45 UTC</a>, (you have to <a href="https://reg.passdatacommunitysummit.com/flow/redgate/summit2021/registrationopeningpage/page/introlanding">register</a> to access this link), is a virtual live Q&amp;A with me where we discuss <strong>Azure Data Explorer</strong>.</p>

<p>So, <a href="https://reg.passdatacommunitysummit.com/flow/redgate/summit2021/registrationopeningpage/page/introlanding">register</a> now, view the recorded video and come and chat to me <a href="https://event.passdatacommunitysummit.com/widget/redgate/summit2021/sessioncatalog/session/1629311226474001sKl1">Thursday, Nov 11 15:15 - 15:45 UTC</a>. The registration is FREE, and besides me, you get to hear from the people that really know what they are talking about!</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to Ingest Into Azure Data Explorer From Apache Kafka using Kafka Connect]]></title>
    <link href="https://nielsberglund.com/2021/10/27/how-to-ingest-into-azure-data-explorer-from-apache-kafka-using-kafka-connect/" rel="alternate" type="text/html"/>
    <updated>2021-10-27T14:13:37+02:00</updated>
    <id>https://nielsberglund.com/2021/10/27/how-to-ingest-into-azure-data-explorer-from-apache-kafka-using-kafka-connect/</id>
    <content type="html"><![CDATA[<p>If you follow my blog, you probably know that I am a huge fan of <strong>Apache Kafka</strong> and event streaming/stream processing. Recently <strong>Azure Data Explorer</strong> (<strong>ADX</strong>) has caught my eye. In fact, in the last few weeks, I did two conference sessions about ADX. A month ago, I published a blog post related to Kafka and ADX: <a href="/2021/09/06/run-self-managed-kusto-kafka-connector-serverless-in-azure-container-instances/"><strong>Run Self-Managed Kusto Kafka Connector Serverless in Azure Container Instances</strong></a>.</p>

<p>As the title of that post implies, it looked at the ADX Kafka sink connector and how to run it in Azure. What the post did not look at was how to configure the connector and connect it to ADX. That is what we will do in this post (and maybe in a couple of more posts).</p>

<p></p>

<h2 id="pre-reqs">Pre-Reqs</h2>

<p>This section is here if you want to follow along. This is what you need:</p>

<ul>
<li>Docker desktop. If you are on Windows and don&rsquo;t have it, you download it from <a href="https://docs.docker.com/desktop/windows/install/">here</a>.</li>
<li>An Azure account. If you don&rsquo;t have an Azure subscription, sign up for a <a href="https://azure.microsoft.com/en-us/free/">free account</a>.</li>
<li>Azure CLI. The Azure CLI is a cross-platform command-line tool connecting to Azure and executing administrative commands on Azure resources. You find downloads and install instructions <a href="https://docs.microsoft.com/en-us/cli/azure/install-azure-cli">here</a>.</li>
<li>An Azure Data Explorer cluster and database. To see how to set up a cluster and a database, look here: <a href="https://docs.microsoft.com/en-us/azure/data-explorer/create-cluster-database-portal">Quickstart: Create an Azure Data Explorer cluster and database</a>.</li>
<li>Kafka cluster. You can either run the cluster &ldquo;on-prem&rdquo; or in the cloud. I wrote a blog post about how to run Confluent Platform using Docker <a href="/2019/06/18/confluent-platform--kafka-for-a-.net-developer-on-windows/">here</a> and running Confluent Cloud on Azure <a href="/2021/08/14/run-confluent-cloud--serverless-apache-kafka-on-azure/">here</a>. In this post, I use Confluent Cloud in Azure.</li>
<li>You need to download the Kusto connector from <a href="https://github.com/Azure/kafka-sink-azure-kusto/releases">here</a>. In the <a href="/2021/09/06/run-self-managed-kusto-kafka-connector-serverless-in-azure-container-instances/"><strong>Run Self-Managed Kusto Kafka Connector Serverless in Azure Container Instances</strong></a> post, I downloaded and used the 2.0 version. In this post, we will use the <a href="https://github.com/Azure/kafka-sink-azure-kusto/releases/download/v2.2.0/kafka-sink-azure-kusto-2.2.0-jar-with-dependencies.jar">2.2 version</a>.</li>
</ul>

<h4 id="confluent-cloud-cli">Confluent Cloud CLI</h4>

<p>We need a way to post messages/events to Kafka, and usually, I do it using a .NET Core application. Since in this post I only will send a few messages, I am going to use the <a href="https://docs.confluent.io/ccloud-cli/current/overview.html"><strong>Confluent Cloud CLI</strong></a> (ccloud) tool.</p>

<p>This tool is a must for anyone using Confluent Cloud as it enables developers to create, manage, and deploy their Confluent components. You find download and install instructions <a href="https://docs.confluent.io/ccloud-cli/current/install.html#tarball-or-zip-installation">here</a> and some examples of how to use it <a href="https://docs.confluent.io/platform/current/tutorials/examples/clients/docs/ccloud.html">here</a>.</p>

<p>If your Kafka is &ldquo;bare metal&rdquo;, or Docker based, there are commands based on script files for various operations. In this post, the following commands are what you need if you are not using Confluent Cloud:</p>

<ul>
<li><code>/kafka-topics</code>: handle topics; list, create, etc.</li>
<li><code>/kafka-console-producer</code>: publish messages to a topic.</li>
<li><code>/kafka-console-consumer</code>: consume messages from a topic.</li>
</ul>

<p>Having sorted out the pre-reqs, and some Kafka tools, let us move on.</p>

<h2 id="scene-setting">Scene Setting</h2>

<p>Before we go further, let&rsquo;s see what data we are working with. Since I am working at <a href="/derivco">Derivco</a>, an online gaming company, I guess it is only natural that my sample data is gameplay-related (fictitious gameplay that is). The idea is that, in this post, we have online players playing various types of games, and the games are submitting game events to a Kafka topic. The event looks something like so:</p>

<pre><code class="language-json">{
  &quot;playerId&quot;: Int32,
  &quot;gameId&quot;: Int32,
  &quot;win&quot;: Int64,
  &quot;score&quot;: Int32,
  &quot;eventTime&quot;: DateTime
}
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Event Schema</em></p>

<p>The fields we see in <em>Code Snippet 1</em> are:</p>

<ul>
<li><code>playerId</code>: the unique id of a player.</li>
<li><code>gameId</code>: an identifier for the specific game a player is playing.</li>
<li><code>win</code>: playing the game may result in a &ldquo;win&rdquo;, which is a unit of &ldquo;something&rdquo;.</li>
<li><code>score</code>: each interaction in a game by a player earns the player a score.</li>
<li><code>eventTime</code>: the time in UTC when the event happened.</li>
</ul>

<p>The event we see the schema for above is what is submitted to Kafka.</p>

<h4 id="create-topic">Create Topic</h4>

<p>Let us create the topic to which the events are submitted:</p>

<pre><code class="language-bash"># list existing topics
$ ccloud kafka topic list

# the above returns no topics
# create a topic for gameplay
$ ccloud kafka topic create gameplay --partitions 4

# check that we have a topic
$ ccloud kafka topic list
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Create Kafka Topic</em></p>

<p>In <em>Code Snippet 2</em> we see:</p>

<ul>
<li>before I create the topic, I check what topics exist in the cluster. As this is a brand new Confluent Cloud cluster, there were no topics.</li>
<li>I create the <code>gameplay</code> topic, explicitly setting the number of partitions to four. If I hadn&rsquo;t set the number of partitions, it would default to six.</li>
<li>to ensure that the topic has been created, I check for topics again, and yes - it is there.</li>
</ul>

<p>Let us publish an event to finish the <em>Setting the Scene</em> topic (did you see what I did there?).</p>

<h4 id="test-publish">Test Publish</h4>

<p>We start with setting up a listener in a terminal window, so we can see when data arrives in the topic:</p>

<pre><code class="language-bash"># setup a listener, when clicking enter it will start listening
ccloud kafka topic consume gameplay -b
Starting Kafka Consumer. Use Ctrl-C to exit.
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Consume from Topic</em></p>

<p>The code in <em>Code Snippet 3</em> sets up a listener. After clicking enter, you see the &ldquo;Starting Kafka Consumer &hellip;&rdquo; as in the code snippet, and the listener is now ready to receive messages.</p>

<p>Time to publish an event. Open another terminal window and, if you are on Windows, make sure the terminal is Windows command prompt - not PowerShell. For some reason, when I try to publish with <code>ccloud</code> using PowerShell, I get errors. Anyway, the code:</p>

<pre><code class="language-bash">$ ccloud kafka topic produce gameplay --parse-key --delimiter ,
33, {&quot;playerId&quot;:33, &quot;gameId&quot;:23, &quot;win&quot;:55, &quot;score&quot;:123, \ 
      &quot;eventTime&quot;:&quot;2021-10-24 04:14:39.572&quot;}
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Publish to Topic</em></p>

<p>In <em>Code Snippet 4</em>, we see how we use the <code>ccloud kafka topic produce</code> command to publish to our <code>gameplay</code> topic. The two flags we see are:</p>

<ul>
<li>The <code>--parse-key</code> flag indicates we supply a message key with the message.</li>
<li>The <code>--delimiter</code> flag defines the delimiter between the message key and message value.</li>
</ul>

<p>After hitting enter after the first line, we enter the message. In <em>Code Snippet 4</em>, we start with the message key <code>33</code>, followed by the message value. The message key, <code>33</code>, represents the <code>playerId</code> as we want to partition the Kafka topic on <code>playerId</code>.</p>

<blockquote>
<p><strong>NOTE:</strong> If you copy the code from <em>Code Snippet 4</em>, be aware of the line continuation character (<code>\</code>) in the message.</p>
</blockquote>

<p>To publish the message, you hit enter, and when you go to the consumer terminal window, you should now see the message.</p>

<p>We now have a topic, and we can produce to that topic. So now, let us go back to connecting up our Kafka topic with Azure Data Explorer.</p>

<h2 id="create-a-kusto-connect-image">Create a Kusto Connect Image</h2>

<p>As we know from my previous post and what I mentioned at the beginning of this post, we use a Kafka connector to connect Kafka to ADX, and in the <em>Pre-Reqs</em> section, I downloaded the connector.</p>

<p>Suppose I had had an on-prem Kafka Connect installation. In that case, I could have taken my downloaded Kusto connector, copied it into my Kafka connect box, restarted the Kafka Connect process, and all would be good.</p>

<blockquote>
<p><strong>NOTE:</strong> What I wrote above is a slight simplification; I would have needed to set some connect properties etc., as well. But from a high level, that&rsquo;s what I would have done.</p>
</blockquote>

<p>The point above is moot as I don&rsquo;t have a Kafka Connect installation, so I will run the Kusto connector from Docker instead. To do that, I need to create a Docker image of the connector.</p>

<h4 id="create-a-dockerfile">Create a Dockerfile</h4>

<p>We build the image from a <code>Dockerfile</code>, so we start with creating the file:</p>

<p><img src="/images/posts/kusto-conn-docker-file.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Dockerfile</em></p>

<p>We see in <em>Figure 1</em> above:</p>

<ul>
<li>As the connector needs to run in a Kafka Connect process, we use the <code>FROM</code> statement to bring in Kafka Connect&rsquo;s latest Confluent base image (<code>cp-server-connect-base:latest</code>).</li>
<li>We copy the <code>.jar</code> file of the downloaded connector to a location in Kafka Connect where the <code>.jar</code> can be loaded from.</li>
<li>As I want to connect to a Kafka cluster requiring authentication, I set some security settings.</li>
</ul>

<p>The last bullet point above is really &ldquo;glossing over&rdquo; what we are doing, so let me explain in a bit more detail. When we use a Kafka connector, the connector consumes from one or more Kafka topics or publishes to one or more Kafka topics. The necessary connection details to the Kafka instance is set up in the individual connector&rsquo;s configuration, which we&rsquo;ll see an example of later.</p>

<p>However, in the <a href="/2021/09/06/run-self-managed-kusto-kafka-connector-serverless-in-azure-container-instances/">previous post</a>, we said how Kafka Connect is a JVM process (a worker) that loads and runs individual connectors such as our Kusto connector. The worker process needs to store configurations of the respective connectors and their state, and it stores this in Kafka. It doesn&rsquo;t have to store it in the Kafka instance the connector(s) consumes from or publishes to - it can use a completely separate Kafka instance and potentially a separate instance per connector. So when we want to use a connector, we need to set that information before configuring the connector. We see that in <em>Figure 1</em> lines 5 - 10, we set the security information to connect to the Kafka cluster where we want to store state and configuration.</p>

<p>That is it. We now have created a Docker file.</p>

<h4 id="build-the-image">Build the Image</h4>

<p>Having the Docker file, we can now go ahead and build the image:</p>

<p><img src="/images/posts/kusto-conn-docker-build.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Docker Build</em></p>

<p>In <em>Figure 2</em>, we see how we use the <code>docker build</code> command to build an image with a given name. We also use the <code>-t</code> flag to <em>tag</em> the image. Looking at <em>Figure 2</em> it seems like the build has succeeded. Let us see if we have an image by running <code>docker images</code>:</p>

<p><img src="/images/posts/kusto-conn-docker-image.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Kusto Docker Image</em></p>

<p>It definitely looks like we are in business as we in <em>Figure 3</em> see the newly built image. The image is now available locally, but we can also push it to the likes of Dockerhub, or - as I did in the <a href="/2021/09/06/run-self-managed-kusto-kafka-connector-serverless-in-azure-container-instances/">previous post</a>  - to Azure Container Instances.</p>

<h2 id="run-the-connector">Run the Connector</h2>

<p>For now, we will not push the image anywhere but run it locally, using Docker Compose.</p>

<h4 id="docker-compose-configuration">Docker Compose Configuration</h4>

<p>In the <a href="/2021/09/06/run-self-managed-kusto-kafka-connector-serverless-in-azure-container-instances/">previous</a> post, we did a quick run of the connector using Docker Compose. Here we will use almost the same Docker Compose file, but look a little bit more in detail what it consists of:</p>

<p><img src="/images/posts/kusto-conn-docker-compose.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Docker Compose File</em></p>

<p>In <em>Figure 4</em>, we see the compose file - named <code>docker-compose.yml</code> - I created to &ldquo;spin up&rdquo; the connector. We see how I have in <em>Figure 4</em> added some numbered &ldquo;bullet&rdquo; points at the left. They indicate relevant &ldquo;stuff&rdquo; for the container so let us look at those and see what they refer to:</p>

<ol>
<li><code>image</code>: the image name and tag which we are using.</li>
<li><code>container_name</code>: arbitrary name of the container.</li>
<li><code>ports</code>: this tells the container to listen on this particular port, and how to map the internal IP to external.</li>
<li><code>CONNECT_BOOTSTRAP_SERVERS</code>: host:port pair for the initial connection to the Kafka cluster. You can define multiple servers with a comma-separated host:port pairs.</li>
<li><code>CONNECT_REST_ADVERTISED_HOST_NAME</code>: hostname for other workers to connect to when we run a distributed Kafka Connect cluster. The <a href="https://rmoff.net/2019/11/22/common-mistakes-made-when-configuring-multiple-kafka-connect-workers/"><strong>Common mistakes made when configuring multiple Kafka Connect workers</strong></a> post by Kafka guru Robin Moffat talks more about this.</li>
<li><code>CONNECT_REST_PORT</code>: port for the REST API endpoint. It is <code>8083</code> by default, so I would not have had to define it in my compose file. Make sure that whatever port number you use is reflected in the <code>ports</code> entry in point 3. You use this port to manage and administer your connector.</li>
<li><code>CONNECT_GROUP_ID</code>: a string that identifies the Kafka Connect cluster group this worker belongs to.</li>
<li><code>CONNECT_CONFIG_STORAGE_TOPIC</code>: name of the topic in which to store connector and task configuration data.</li>
<li><code>CONNECT_OFFSET_STORAGE_TOPIC</code>: name of the topic in which to store offset data for connectors.</li>
<li><code>CONNECT_STATUS_STORAGE_TOPIC</code>: name of the topic in which to store state for connectors.</li>
<li><code>CONNECT_KEY_CONVERTER</code>: what class to use for conversion of the message key. This can be overridden by the configuration of an individual connector.</li>
<li><code>CONNECT_VALUE_CONVERTER</code>: what class to use for conversion of the message value. This can be overridden by the configuration of an individual connector.</li>
</ol>

<p>Oh, one thing about the <code>...STORAGE_TOPIC</code>&rsquo;s; if you have multiple Kafka Connect workers in a Connect group (<code>CONNECT_GROUP_ID</code>), then those workers need to use the same topics in the same cluster.</p>

<h4 id="run-the-container">Run the Container</h4>

<p>When we have created the Docker Compose file as per above, we can run the container. However, before we do that, I would like you to check what topics are in the Kafka Cluster: <code>ccloud kafka topic list</code>. When I run that code, I only see the topic we created above.</p>

<p>Cool, let us now &ldquo;spin up&rdquo; the container:</p>

<p><img src="/images/posts/kusto-conn-docker-compose-up.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Run Docker Compose</em></p>

<p>We see in <em>Figure 5</em> how we start the container using the <code>docker-compose up -d</code> command. I execute the command in the same directory I have the <code>docker-compose.yml</code> file.</p>

<blockquote>
<p><strong>NOTE:</strong> The <code>docker-compose up</code> command expects a file named <code>docker-compose.yml</code>. If you want to use another name, you tell the command the file name using the <code>-f</code> flag.</p>
</blockquote>

<p>In <em>Figure 5</em>, we also see how after the command has been executed, Docker creates an internal network and starts the container.</p>

<p>Seeing what we see in <em>Figure 5</em>, the assumption is that all has gone OK. Let us confirm by looking for new topics in the Kafka cluster. I do the same as I did above executing <code>ccloud kafka topic list</code>:</p>

<p><img src="/images/posts/kusto-conn-kafka-topics-2-new.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>Kafka Topics</em></p>

<p>Yay, when looking at <em>Figure 6</em> we see three new topics. I.e. the topics we defined in the <code>docker-compose.yml</code> file to store status, config and offset. So, so far, so good. Now let us take it one step further.</p>

<p>Remember from above how I said that the <code>CONNECT_REST_PORT</code> in the <code>docker-compose.yml</code> file defines the port we use to administer and configure the connector. We do this by calling endpoints exposed by the Kafka Connect REST API.</p>

<p>Let us now use one of the endpoints to see that the Kafka worker is up and running and that our connector is available. The endpoint we use is the same we used in the [previous][post]: <code>GET /connector-plugins</code>:</p>

<p><img src="/images/posts/kusto-conn-get-connector-plugins.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>Connector Plugins</em></p>

<p>As I run the Docker container on my machine, we see in <em>Figure 7</em> (outlined in red) how I make a <code>GET</code> request against <code>localhost:8083</code> and the <code>connector-plugins</code> endpoint. We also see in <em>Figure 7</em> that the request is successful. It returns an array of connector plugins, of which our plugin (outlined in yellow) is one.</p>

<p>This is awesome; we have now built a Docker image of the Kusto sink connector and &ldquo;spun&rdquo; it up on our local machine. Basically, we are at the same point as at the end of the <a href="/2021/09/06/run-self-managed-kusto-kafka-connector-serverless-in-azure-container-instances/">previous post</a>, apart from the container is not in Azure Container Registry.</p>

<p>Now it is almost time to &ldquo;hook&rdquo; the connector up to Kafka and Azure Data Explorer. However, we need to do some configuration of the ADX cluster and the database before hooking up the connector.</p>

<h2 id="setup-adx-properties">Setup ADX &amp; Properties</h2>

<p>The configuration we need to do of ADX is:</p>

<ul>
<li>Creating an Azure Active Directory (AAD) security principal, which the connector uses to write to the ADX table(s).</li>
<li>Getting some properties of the ADX cluster, so the connector knows where to connect and write data to.</li>
</ul>

<p>We start with the Service Principal (SPN).</p>

<h4 id="adx-service-principal">ADX Service Principal</h4>

<p>To set up the SPN, we use the Azure CLI mentioned above in the <em>Pre-Reqs</em>. This is what you do:</p>

<ul>
<li>log in to Azure: <code>az login</code>. This returns a list of your subscriptions after successful login.</li>
<li>if you have more than one subscription, you set the subscription you want to use: <code>az account set --subscription your-sub-id</code>.</li>
</ul>

<p>Having done the above, you now create the service principal:</p>

<pre><code class="language-bash">az ad sp create-for-rbac -n &quot;kusto-kafka-nielsblog&quot;
</code></pre>

<p><strong>Code Snippet 5:</strong> <em>Create Service Principal</em></p>

<p>The code in <em>Code Snippet 5</em> creates - as mentioned before - a service principal configured to access Azure resources. The output, when executing the code, looks something like so:</p>

<p><img src="/images/posts/kusto-conn-spn.png" alt="" /></p>

<p><strong>Figure 8:</strong> <em>Service Principal Properties</em></p>

<p>In <em>Figure 8</em>, we see how the result from the code returns JSON with 5 fields/properties. Take note of <code>appId</code>, <code>password</code>, and <code>tenant</code> as we will use them later when we configure the connector.</p>

<p>Actually, we&rsquo;ll use <code>appId</code> and <code>tenant</code> right now, as we will add the created service principal to your ADX database.</p>

<p>The easiest way to add the created service principal is to do it from the <em>Query Explorer</em> window for ADX in the Azure portal:</p>

<p><img src="/images/posts/kusto-conn-adx-db-1.png" alt="" /></p>

<p><strong>Figure 9:</strong> <em>ADX Database</em></p>

<p>I have in <em>Figure 9</em> logged in to my Azure subscription and navigated to my ADX cluster (highlighted in yellow). In the cluster, I have one database (outlined in blue). The database is selected, and to get to the query explorer, I click on the <em>Query</em> button outlined in red. When the query explorer opens, I can create the service principal as an admin user in the database:</p>

<pre><code class="language-sql">.add database nielsblogpostsdb1 admins ('aadapp=The-AppId;The-Tenant') 'AAD App'
</code></pre>

<p><strong>Code Snippet 6</strong> <em>Create Service Principal Admin User in Database</em></p>

<p>The code in <em>Code Snippet 6</em> creates an admin user in the <code>nielsblogpostsdb1</code> database. As the user is a service principal, we identify the user with <code>aadapp=appId;tenant</code> from the result in <em>Figure 8</em>. We also give the &ldquo;user&rdquo; a name: in this case: <code>AAD App</code>.</p>

<h4 id="adx-properties">ADX Properties</h4>

<p>The final thing we need to do with ADX is to find the equivalent of a connection string, i.e. where the connector can find ADX and connect to. That is exposed as endpoints in the overview page for the ADX cluster:</p>

<p><img src="/images/posts/kusto-conn-cluster-props.png" alt="" /></p>

<p><strong>Figure 10:</strong> <em>Connection Strings</em></p>

<p>We are looking for two endpoints outlined in red in <em>Figure 10</em>. Those two endpoints represent:</p>

<ul>
<li><code>URI</code>: endpoint for querying the cluster.</li>
<li><code>Data Ingestion URI</code>: this is the endpoint for ingesting into the cluster.</li>
</ul>

<p>Of the two endpoints above, only the ingestion endpoint is required when configuring the connector. When you have taken note of those endpoints, we can go on.</p>

<h2 id="configure-the-connector">Configure the Connector</h2>

<p>To do the &ldquo;hook-up,&rdquo; the connector needs to be configured. From a high level, we need to configure:</p>

<ul>
<li>what topic(s) to read data from.</li>
<li>the target Azure Data Explorer cluster.</li>
<li>the target database and table(s) to ingest the data into.</li>
</ul>

<p>We already have an ADX cluster and a database, so let us create a table.</p>

<h4 id="create-table">Create Table</h4>

<p>We need to create a table to ingest the data from the Kafka <code>gameplay</code> topic. We do it from the <em>Query Explorer</em> window for ADX, as we saw in <em>Figure 9</em>. Open the editor and write the create table code:</p>

<p><img src="/images/posts/kusto-conn-query-editor-1.png" alt="" /></p>

<p><strong>Figure 11:</strong> <em>Query Explorer</em></p>

<p>When we look at the code in <em>Figure 11</em> we see how the statement to create a table looks slightly different from what you were to write in SQL. That is because this is not SQL, but KQL - Kusto Query Language. Expect a future post looking more in detail into KQL.</p>

<p>In <em>Figure 11</em>, we see how the table matches the event schema in <em>Code Snippet 1</em>, and we can now execute the code by hitting enter or clicking the <em>Run</em> button outlined in yellow. After executing the code in <em>Figure 11</em>, you can check that the table has been created by executing: <code>.show tables</code>.</p>

<p><img src="/images/posts/kusto-conn-show-tables.png" alt="" /></p>

<p><strong>Figure 12:</strong> <em>Show Tables</em></p>

<p>As we see in <em>Figure 12</em>, the table has been created. There is one more thing we need to do in the database related to the table. We need to create a mapping between the event data and the columns in the table.</p>

<h4 id="ingestion-mapping">Ingestion Mapping</h4>

<p>As the event lands in Kafka as JSON (or some other format), Azure Data Explorer needs to understand how the fields in the event map to the columns in the table. We do this by creating a table ingestion mapping in the database:</p>

<pre><code class="language-sql">.create table GamePlay ingestion json mapping 'gameplay_json_mapping' 
'[{&quot;column&quot;:&quot;PlayerID&quot;, &quot;Properties&quot;:{&quot;path&quot;:&quot;$.playerId&quot;}}, 
{&quot;column&quot;:&quot;GameID&quot;, &quot;Properties&quot;:{&quot;path&quot;:&quot;$.gameId&quot;}}, 
{&quot;column&quot;:&quot;Win&quot;, &quot;Properties&quot;:{&quot;path&quot;:&quot;$.win&quot;}}, 
{&quot;column&quot;:&quot;Score&quot;, &quot;Properties&quot;:{&quot;path&quot;:&quot;$.score&quot;}}, 
{&quot;column&quot;:&quot;EventTime&quot;, &quot;Properties&quot;:{&quot;path&quot;:&quot;$.eventTime&quot;}} ]'
</code></pre>

<p><strong>Code Snippet 7:</strong> <em>Ingestion Mapping</em></p>

<p>The code in <em>Code Snippet 7</em> shows how we create a JSON ingestion mapping, naming it <code>gameplay_json_mapping</code>. We further see how the columns are mapped against the event fields, where <code>$</code> represents the event&rsquo;s root. After running the code in <em>Code Snippet 7</em>, we can check that the mapping exists executing: <code>.show table GamePlay ingestion mappings</code>, which returns all mappings for that table. The page <a href="https://docs.microsoft.com/en-us/azure/data-explorer/kusto/management/show-ingestion-mapping-command">.show ingestion mapping</a> from ADX documentation covers ingestion mappings in more detail.</p>

<p>Above I said that we had to do one more thing in the database, which was the ingestion mapping we just saw. Actually, there is another thing to do. It is not 100% necessary, but it impacts the ingestion latency. We should create an ingestion policy.</p>

<h4 id="ingestion-policy">Ingestion Policy</h4>

<p>A future blog post, will talk in detail about ADX various ingestion options, and what an ingestion policy is. For now, let us just &ldquo;roll&rdquo; with this, and take it for what it is worth. So to create an ingestion batching policy you:</p>

<pre><code class="language-sql">.alter table GamePlay policy ingestionbatching 
@'{&quot;MaximumBatchingTimeSpan&quot;:&quot;00:00:10&quot;, &quot;MaximumNumberOfItems&quot;: 5, &quot;MaximumRawDataSizeMB&quot;: 100}'
</code></pre>

<p><strong>Code Snippet 8:</strong> <em>Ingestion Batching Policy</em></p>

<p>An ingestion batching policy determines when data aggregation should stop during data ingestion according to the specified settings. In <em>Code Snippet 8</em> we see the code to set the policy with our preferred settings:</p>

<ul>
<li><code>MaximumBatchingTimeSpan</code>: maximum time to close the aggregation and ingest the data. In our case, we set it to 10 seconds, which is the minimum value allowed by ADX. The default value is 5 minutes.</li>
<li><code>MaximumNumberOfItems</code>: when the maximum number is reached, the aggregation is closed and the data ingested. Default is 1000 items.</li>
<li><code>MaximumRawDataSizeMB</code>: when aggregation reaches the maximum size, it closes and ingests the data. Default is 1000 MB.</li>
</ul>

<p>After running the code in <em>Code Snippet 8</em>, we can check and see that the policy has been created: <code>.show table GamePlay policy ingestionbatching</code>.</p>

<blockquote>
<p><strong>NOTE:</strong> If we didn&rsquo;t set the policy as in <em>Code Snippet 8</em>, it would run with the default values, and we may have to wait a while to see data in the table.</p>
</blockquote>

<p>We should now be all set and ready to configure the connector.</p>

<h4 id="connector-configuration">Connector Configuration</h4>

<p>You configure the connector using a JSON file and <code>POST</code>:ing it either via <code>curl</code> or Postman. Personally, I prefer Postman, so that is what I will use later.</p>

<p>The configuration file I use looks like so:</p>

<p><img src="/images/posts/kusto-conn-connector-config-1-new.png" alt="" /></p>

<p><strong>Figure 13:</strong> <em>Connector Config</em></p>

<p>Some of the properties we see in <em>Figure 13</em> is heavily cut, but don&rsquo;t worry; below is an explanation of the most important ones. For an explanation of all of them, see <a href="https://github.com/Azure/kafka-sink-azure-kusto#5-sink-properties">5. Sink properties</a>.</p>

<p>The numbers in <em>Figure 13</em> and their related properties:</p>

<ol>
<li>The <code>topics</code> property points to the topic(s) the connector consumes. It is defined as a comma-delimited string.</li>
<li>In <code>kusto.ingestion.url</code>, you set the ingestion endpoint you &ldquo;grabbed&rdquo; in <em>Figure 10</em>.</li>
<li>The same goes for the <code>kusto.query.url</code>. Remember, this property is optional.</li>
<li>The <code>aad.auth.authority</code> refers to the <code>tenant</code> you received when creating the service principa</li>
<li>The <code>aad.auth.appid</code> is the <code>appId</code> property from the service principal.</li>
<li>The last property referring to authentication is the <code>aad.auth.appkey</code>; the <code>password</code> property for the service principal.</li>
<li>Let us come back to <code>kusto.tables.topics.mapping</code> below.</li>
<li>I have found that I need to set both <code>key.converter.schemas.enable</code> and <code>value.converter.schemas.enable</code> to <code>false</code>. I get some errors otherwise. I need to drill into that a bit deeper at a later stage.</li>
<li>These are the connection and authentication details for the Kafka broker you consume from.</li>
</ol>

<p>Above, I said we&rsquo;d come back to <code>kusto.tables.topics.mapping</code>, so let us do that.</p>

<p><strong>kusto.tables.topics.mapping</strong></p>

<p>As the name implies, this property defines the mapping between the topic and the table(s) in the database(s). My mapping looks like so:</p>

<pre><code class="language-json">[{'topic': 'gameplay','db': 'nielsblogpostsdb1', 'table': 'GamePlay',' \
   format': 'json', 'mapping':'gameplay_json_mapping', 'streaming': 'false'}]
</code></pre>

<p><strong>Code Snippet 9:</strong> <em>Tables Topic Mapping</em></p>

<p>The mapping is an array of topics and their respective mapping to databases and tables in the cluster. In my example, I have only one mapping, and in the mapping, I define:</p>

<ul>
<li>the topic to consume from.</li>
<li>the database and table to ingest into.</li>
<li>the format of the data.</li>
<li>the ingestion mapping that has been set up for the table. We see this in <em>Code Snippet 7</em>.</li>
<li>whether the ingestion is streaming or batch. Default is batch. In a future post, I will talk more about stream and batch ingestion.</li>
</ul>

<p>When we have created the config JSON, we are ready to create the connector on the Kafka Connect cluster:</p>

<p><img src="/images/posts/kusto-conn-connector-config-post.png" alt="" /></p>

<p><strong>Figure 14:</strong> <em>Create the Connector</em></p>

<p>In <em>Figure 14</em>, we see part of the configuration file in <em>Figure 13</em>, and how we <code>POST</code> it to Kafka Connect&rsquo;s <code>/connectors</code> endpoint. When running the code (doing the <code>POST</code>), we should get a <code>201</code> response back, indicating all is well.</p>

<p>I didn&rsquo;t mention the &lsquo;name&rsquo; property we see at line 2 in <em>Figure 13</em>. The name is an arbitrary string, and it comes in useful if you want to manage this particular connector. Things you can do are:</p>

<ul>
<li>pause the connector.</li>
<li>restart the connector.</li>
<li>delete the connector.</li>
<li>check the status of the connector.</li>
<li>etc.</li>
</ul>

<p>Each individual operation is against the <code>/connectors</code> endpoint and takes the connector name as an input parameter, potentially followed by a resource request. An example would be that when we have created the connector, we want to check that it actually is up and running:*</p>

<p><img src="/images/posts/kusto-conn-connector-status.png" alt="" /></p>

<p><strong>Figure 15:</strong> <em>Check Connector Status</em></p>

<p>What <em>Figure 15</em> shows is how we make a <code>GET</code> request (outlined in red) against the <code>/connectors</code> endpoint (highlighted in yellow), for the connectors name (highlighted in red), and we are asking for the <code>status</code> (highlighted in blue).</p>

<p>Executing that request gives us:</p>

<p><img src="/images/posts/kusto-conn-connector-status-result.png" alt="" /></p>

<p><strong>Figure 16:</strong> <em>Status Result</em></p>

<p>We see in <em>Figure 16</em> how everything looks OK! The connector is now configured and up and running, and we should now be able to test it out.</p>

<h2 id="testing-the-connector">Testing the Connector</h2>

<p>Suppose you haven&rsquo;t torn down the topic and recreated it. In that case, the easiest way to do a quick test is that after 10 seconds after the connector is up and running, execute the following query in the <em>Query Editor</em>: <code>GamePlay | count</code>. That on my box results in a value of 1, which is the event we published in <em>Code Snippet 4</em>. The event sits in the topic; the connector connects and starts consuming from the topic ingesting into the table.</p>

<p>Let&rsquo;s now publish some more events:</p>

<pre><code class="language-bash">$ ccloud kafka topic produce gameplay --parse-key --delimiter ,
34, {&quot;playerId&quot;:34, &quot;gameId&quot;:27, &quot;win&quot;:0, &quot;score&quot;:99, \ 
      &quot;eventTime&quot;:&quot;2021-10-27 07:56:39.572&quot;}
38, {&quot;playerId&quot;:38, &quot;gameId&quot;:27, &quot;win&quot;:0, &quot;score&quot;:99, \ 
      &quot;eventTime&quot;:&quot;2021-10-27 07:56:39.572&quot;}
45, {&quot;playerId&quot;:45, &quot;gameId&quot;:17, &quot;win&quot;:10, &quot;score&quot;:103, \ 
      &quot;eventTime&quot;:&quot;2021-10-27 07:56:39.572&quot;}       
</code></pre>

<p><strong>Code Snippet 10:</strong> <em>Publish More Events to Topic</em></p>

<p>In <em>Code Snippet 10</em>, we see how we publish more events. While you publish, execute the <code>GamePlay | count</code> call. You should see how the count increases, not as fast as you publish due to the ingestion batching, but with a slight latency. After a while, you should see the correct count, and if you execute: <code>GamePlay</code>, you should see something similar to this:</p>

<p><img src="/images/posts/kusto-conn-query-result.png" alt="" /></p>

<p><strong>Figure 17:</strong> <em>Query Result</em></p>

<p>From what we published in *Code Snippet 4, together with what we published here in <em>Code Snippet 10</em>, we see how 4 rows in total have been ingested into the <code>GamePlay</code> table. Success!</p>

<h2 id="summary">Summary</h2>

<p>Phew, this was quite a journey, but we made it. So, in this post we have looked at:</p>

<ul>
<li>Confluent Cloud CLI, and how we use it to create topics, publish, and consume messages.</li>
<li>How to create and configure a Docker image from the Kusto Sink Connector.</li>
<li>Set up a Docker Compose file to use the image above.</li>
<li>Create an Azure Active Directory Service Principal using Azure CLI (<code>az</code>).</li>
<li>Create an admin user in our ADX database from that Service Principal.</li>
<li>Define ingestion mapping and ingestion batch policy in the ADX database.</li>
<li>Created the connector configuration file and got an insight into some of its properties.</li>
<li>Created the connector from the connector configuration file.</li>
</ul>

<p>After the above, we published some messages to Kafka. We saw when publishing how the messages were ingested into the table. Notice that depending on the batching policy, you may see different results in the latency when ingesting.</p>

<p>In a future post, we will look more in detail into the different ingestion methods and how to configure them. Until then!</p>

<h2 id="finally">Finally</h2>

<p>If you have comments, questions etc., please comment on this post or [ping][ma] me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 43, 2021]]></title>
    <link href="https://nielsberglund.com/2021/10/24/interesting-stuff---week-43-2021/" rel="alternate" type="text/html"/>
    <updated>2021-10-24T12:07:06+02:00</updated>
    <id>https://nielsberglund.com/2021/10/24/interesting-stuff---week-43-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://aka.ms/adls/hitchhikersguide">The Hitchhiker&rsquo;s Guide to the Data Lake</a>. This post discusses considerations and best practices around how to effectively utilize <strong>Azure Data Lake Storage Gen2</strong> in large scale Big Data platform architectures. I found this post to be extremely useful!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://databricks.com/blog/2021/10/19/introducing-apache-spark-3-2.html">Introducing Apache Spark 3.2</a>. In last week&rsquo;s <a href="/2021/10/17/interesting-stuff---week-42-2021/">roundup</a>, I linked to a post about a new Window type coming in Apache Spark 3.2: the session window. The post linked to here looks at other new interesting features in the 3.2 release!</li>
<li><a href="https://stlplaces.com/blog/best-apache-kafka-books-in-year">Best Apache Kafka Books in 2021</a>. Well, not much to say really here. As the title says, the post lists the Kafka books the author likes best.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>Today and tomorrow, I am putting the finishing touches on my video recording for the <a href="https://passdatacommunitysummit.com/"><strong>PASS Data Community Summit 2021</strong></a>:</p>

<ul>
<li><a href="https://passdatacommunitysummit.com/sessions/265026"><strong>Analyze Billions of Rows of Data in Real-Time Using Azure Data Explorer</strong></a>. In this session, I look at how <strong>Azure Data Explorer</strong> enables us to do near real-time analysis of Big Data.</li>
</ul>

<p>If you are interested you can register <a href="https://reg.passdatacommunitysummit.com/flow/redgate/summit2021/registrationopeningpage/page/introlanding">here</a>. The conference sessions are free!</p>

<p>In addition to the above, I am also working on a blog post looking at ingesting data from Kafka into Azure Data Explorer. I&rsquo;ve been working on it for quite a while now. Hopefully, I&rsquo;ll have it done within a week or two.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 42, 2021]]></title>
    <link href="https://nielsberglund.com/2021/10/17/interesting-stuff---week-42-2021/" rel="alternate" type="text/html"/>
    <updated>2021-10-17T08:52:05+02:00</updated>
    <id>https://nielsberglund.com/2021/10/17/interesting-stuff---week-42-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="azure-data-explorer">Azure Data Explorer</h2>

<ul>
<li><a href="https://youtu.be/52dPYC1V5a0">Azure Data Explorer Shorts: Managed Ingestion</a>. An excellent short (~9 minutes) video explaining the ins and outs of data ingestion into <strong>Azure Data Explorer</strong>.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/how-baader-built-a-predictive-analytics-machine-learning-system-with-kafka-and-rstudio/">Apache Kafka and R: Real-Time Prediction and Model (Re)training</a>. This blog post looks at how KStreams, ksqlDB, and R can be used to create a data pipeline in which a machine learning model is applied to streaming data. The post also looks at how the model can be automatically retrained once the prediction results exceed a certain threshold. Very Cool!</li>
<li><a href="https://databricks.com/blog/2021/10/12/native-support-of-session-window-in-spark-structured-streaming.html">Native Support of Session Window in Spark Structured Streaming</a>. The post linked to, looks at a new window type in the upcoming Apache Spark 3.2 version. Before Spark 3.2, Spark supported tumbling and sliding windows. In the 3.2 version, the session window is introduced. The interesting thing with a session window is that it has a dynamic size of window length depending on the input.</li>
<li><a href="https://www.confluent.io/blog/new-confluent-cloud-connector-features-and-single-message-transforms/">Introducing Single Message Transforms and New Connector Features on Confluent Cloud</a>. Part of Confluent cloud is managed Kafka Connect connectors, and this post announces new features for most of the managed connectors. I am quite &ldquo;chuffed&rdquo; about seeing Single Message Transforms as one such new feature.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
</feed>

