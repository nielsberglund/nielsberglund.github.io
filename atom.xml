<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Niels Berglund</title>
  <link href="https://nielsberglund.com/atom.xml" rel="self"/>
  <link href="https://nielsberglund.com"/>
  <updated>2021-08-07T06:02:12+02:00</updated>
  <id>https://nielsberglund.com/</id>
  <generator uri="http://gohugo.io/">Hugo</generator>

  
  <entry>
    <title type="html"><![CDATA[How to Deploy the Debezium SQL Server Connector to Docker]]></title>
    <link href="https://nielsberglund.com/2021/08/07/how-to-deploy-the-debezium-sql-server-connector-to-docker/" rel="alternate" type="text/html"/>
    <updated>2021-08-07T06:02:12+02:00</updated>
    <id>https://nielsberglund.com/2021/08/07/how-to-deploy-the-debezium-sql-server-connector-to-docker/</id>
    <content type="html"><![CDATA[<p>I have been doing a couple of conference talks lately (virtual, of course) about streaming data from SQL Server to Kafka. The title of the presentation is <strong>Free Your SQL Server Data With Kafka</strong>.</p>

<p>In the presentation, I talk (and show) various ways of getting data from SQL Server to Kafka. One of the ways I cover is Microsoft CDC, together with Debezium.</p>

<p>When I do the presentation, I always have a SQL Server installed locally, and I run Kafka in Docker. Without fail, every time I set up the environment, I cannot remember how to deploy the Debezium SQL Server Connector into Docker. Therefore I decided to write this post to have something to go back to for next time.</p>

<p></p>

<blockquote>
<p><strong>NOTE:</strong> This post does not cover the intricacies of how to configure Debezium for SQL Server. I leave that for a future post.</p>
</blockquote>

<h2 id="background">Background</h2>

<p>Before diving into how to do this, let us look at the moving parts of this.</p>

<h4 id="kafka-connect">Kafka Connect</h4>

<p>Kafka Connect is a tool for streaming data between Apache Kafka and other systems in a scalable and reliable way. The way you move data between systems and Kafka is using connectors, and there are two flavors of connectors:</p>

<ul>
<li>Source connectors which understand how to interact with the source system send records into Kafka</li>
<li>Sink connectors that propagate records from Kafka topics to other systems.</li>
</ul>

<p>Kafka Connect is a JVM process, and it operates separately from the Kafka Broker. Connectors are <code>.jar</code> files loaded by the connect process. The diagram below shows a high-level overview of what it looks like:</p>

<p><img src="/images/posts/kafka-connect-1.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Kafka Connect Overview</em></p>

<p>In <em>Figure 1</em> we see, (from left to right):</p>

<ul>
<li>Source systems, i.e. systems we want to get data from. These systems can be databases, Hadoop, files, etc.</li>
<li>The Kafka Connect worker with source connectors. The connectors know how to interact with the source system, whether querying a database, using CDC, reading from a filesystem, etc. The connectors publish data to Kafka topics.</li>
<li>The Kafka broker(s). The broker(s) contain topics that are the &ldquo;sinks&rdquo; for the source connectors.</li>
<li>Kafka Connect worker with sink connectors. Source and sink connectors can be in the same Kafka Connect worker. The sink connectors know how to consume events from Kafka topics and ingest them into sink systems.</li>
<li>Sink systems. These are systems we ingest data into. As with source systems, these can be databases, Hadoop, files, etc.</li>
</ul>

<h4 id="debezium">Debezium</h4>

<p>Debezium is an open source distributed platform for change data capture, (I &ldquo;stole&rdquo; the preceding shamelessly from <a href="https://debezium.io/">here</a>). It captures changes in your database(s) and publishes those changes to topics in Kafka.</p>

<blockquote>
<p><strong>NOTE:</strong> Debezium <em>can</em> work without a Kafka cluster, in which case it is embedded in your application, and the application receives the change notifications. Read more about that <a href="https://debezium.io/documentation/reference/1.4/development/engine.html">here</a>.</p>
</blockquote>

<p>Debezium has Kafka Connect connectors for a multitude of source systems. When interacting with Kafka, the connector(s) is deployed to Kafka Connect.</p>

<p>With the above in mind, let us look at how this works with SQL Server.</p>

<h4 id="sql-server-debezium-and-kafka">SQL Server, Debezium, and Kafka</h4>

<p>As I mentioned at the beginning of this post, the aim is to get data out of some table(s) in a database(s) and stream it to a topic(s) in Kafka.</p>

<p>We do not necessarily need to use Debezium as there are other Kafka Connect connectors. We could, for example, use the Confluent SQL Server connector. However, as we want to stream the data in near real-time, with the least amount of work on our side, the Debezium connector is our choice. Coming back to <em>Figure 1</em> it would look something like so:</p>

<p><img src="/images/posts/kafka-connect-cdc.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Kafka Connect SQL Server &amp; Debezium</em></p>

<p>We see in <em>Figure 2</em> how the source system is SQL Server and how the source connector is the Debezium SQL Server connector. In the diagram we publish to one topic as we only retrieve data from one table. If we were to retrieve data from multiple tables, we&rsquo;d publish to multiple topics.</p>

<p>We have several sink connectors reading from our topic and ingest into various sink systems.</p>

<p>Ok, enough background; before we get into the &ldquo;nitty-gritty&rdquo;, let&rsquo;s see what you need if you want to follow along.</p>

<h2 id="pre-reqs-code">Pre-reqs &amp; Code</h2>

<p>There are not many pre-reqs, but here goes:</p>

<ul>
<li><strong>SQL Server</strong>: well, duh - as we want to set up CDC and Debezium to stream data from SQL Server, we would need SQL Server installed somewhere. I have SQL Server 2019 installed on my local dev machine.</li>
<li><strong>Docker Desktop</strong>: another duh - this post is all about how to set up Kafka Connect and Debezium in Docker, so yes - we need Docker Desktop.</li>
</ul>

<h4 id="test-code">Test Code</h4>

<p>I mentioned in the beginning that this post is not about configuring Debezium to read data from SQL Server, so I won&rsquo;t discuss CDC in any detail. However, we need something to test that what we are doing works, so here&rsquo;s some code to set up a database on SQL Server:</p>

<pre><code class="language-sql">USE master;
GO

--  to start from scratch drop the database if exists
IF EXISTS(SELECT * FROM sys.databases WHERE name = 'DebeziumTest')
BEGIN
  ALTER DATABASE DebeziumTest
  SET SINGLE_USER
  WITH ROLLBACK IMMEDIATE;

  DROP DATABASE DebeziumTest;
END
GO

-- create the database
CREATE DATABASE DebeziumTest;
GO

USE DebeziumTest;
GO

-- this statement just if we don't want to drop the db, 
-- but still start over with the table
-- DROP TABLE dbo.tb_CDCTab1;

-- table which we later will CDC enable
CREATE TABLE dbo.tb_CDCTab1 (RowID int identity primary key,
                      Col1 int,
                      Col2 nvarchar(25));
GO

</code></pre>

<p><strong>Code Snippet 1:</strong> <em>DB Objects Creation Script</em></p>

<p>The code in <em>Code Snippet 1</em> creates a database, <code>DebeziumTest</code> and a table, <code>dbo.tb_CDCTab1</code> in the database. Later in the post, we enable the table for CDC. Enabling it allows us to check that our Kafka Connect/Debezium &ldquo;stuff&rdquo; works as expected. So, if you want to follow along, run the script, and after you have run it, ensure you have the database and the table.</p>

<p>Now, let us get into what we are supposed to do; to set this up in Docker.</p>

<h2 id="docker">Docker</h2>

<p>Let us start with getting the necessary Docker images and compose files.</p>

<h4 id="docker-kafka-image">Docker Kafka Image</h4>

<p>There are quite a few Docker images, and Docker composes files around for setting up Kafka and Kafka Connect. The ones I usually use are from Confluent&rsquo;s <a href="https://github.com/confluentinc/cp-all-in-one/"><code>cp-all-in-one</code></a> repository.</p>

<p>Let us get started:</p>

<pre><code class="language-bash">mkdir kafka
cd kafka
git clone https://github.com/confluentinc/cp-all-in-one.git
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Clone the Repo</em></p>

<p>In <em>Code Snippet 2</em>, we create a directory for the repo files and clone the <a href="https://github.com/confluentinc/cp-all-in-one/"><code>cp-all-in-one</code></a>. After cloning, we have a directory named <code>cp-all-in-one</code> under the <code>kafka</code> directory. The <code>cp-all-in-one</code> directory looks like so:</p>

<p><img src="/images/posts/kafka-connect-docker-1.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Kafka Connect SQL Server &amp; Debezium</em></p>

<p>In <em>Figure 3</em>, we see that <code>cp-all-in-one</code> (outlined in red) has some sub-directories. These directories contain Docker Compose files for various setups of Kafka. We are interested in the directory outlined in blue: <code>cp-all-in-one</code>, (yeah I know - the same name as the parent directory).</p>

<p>A quick side note here about <code>cp-all-in-one</code>. This directory contain the image for Confluent Platform, which is the enterprise edition of Confluent. As with most enterprise editions this requires a licennse. However, with the introduction of Confluent Platform 5.2 &ldquo;back in the day&rdquo;, Confluent <a href="https://www.confluent.io/blog/introducing-confluent-platform-5-2">announced</a> that Confluent Platform is &ldquo;free forever&rdquo; on a single Kafka broker! In other words, it is like a &ldquo;Developer Edition&rdquo; of Confluent Platform. The benefit of using Confluent Platform is that you get ALL the goodies, including <strong>Control Center</strong>, which is the WEB UI for Confluent Platform.</p>

<p>When going into that directory, we see a <code>docker-compose.yml</code>. Opening it in an editor, we see the various images deployed as services when running the <code>docker-compose</code> command. From a Kafka Connect perspective, we are interested in the <code>connect</code> service (and image):</p>

<p><img src="/images/posts/kafka-connect-docker-connect-yml.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Docker Compose - Connect Service</em></p>

<p>What <em>Figure 3</em> shows is a &ldquo;condensed&rdquo; part of the <code>yml</code> for the Kafka Connect image. There are three outlined section in the figure:</p>

<ul>
<li>Outlined in green: <code>image</code>, the image the service is built on.</li>
<li>Yellow: <code>CONNECT_REST_PORT</code>, the port to access the service on.</li>
<li>Red: <code>CONNECT_PLUGIN_PATH</code>, the path from where the service loads plugins (connectors).</li>
</ul>

<p>We see in <em>Figure 3</em> how the image is <code>cp-server-connect-datagen</code>. That image is an image containing some &ldquo;base&rdquo; connectors and also tools for generating data. There are other Connect images, and we see one other later in this post. But for now, let us use this image.</p>

<h2 id="run-kafka-connect">Run Kafka &amp; Connect</h2>

<p>Having retrieved the required files as in <em>Code Snippet 2</em> it is time to &ldquo;spin up&rdquo; the Kafka cluster, including Kafka Connect. We do that by <code>cd</code>:ing into the directory where the <code>docker-compose.yml</code> file is and execute: <code>docker-compose up -d</code>.</p>

<p>When running the code you see how Docker is pulling images, starts up the various services, and finally:</p>

<p><img src="/images/posts/kafka-connect-docker-compose-up.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Kafka Started</em></p>

<p>We see in <em>Figure 4</em> how all services have started. Well, that is not exactly true - some services are still &ldquo;spinning&rdquo; up, but after a minute or two, you can browse to the Confluent Control Center and see your Kafka cluster in all its &ldquo;glory&rdquo;:</p>

<p><img src="/images/posts/kafka-connect-control-center.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Confluent Control Center</em></p>

<p>In <em>Figure 5</em>, we see the overview page for the Kafka cluster. Outlined in red at the top, we see how we access it from port 9021 on the box where Docker runs. Outlined in blue, we see that we have one Connect cluster.</p>

<h4 id="installed-connectors">Installed Connectors</h4>

<p>I mentioned above that the Connect image has some connectors installed by default. To see what connectors are pre-installed, we use the Kafka REST API:</p>

<pre><code class="language-bash">GET http://127.0.0.1:8083/connector-plugins
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>GET Connectors</em></p>

<p>We can use whatever tool we are comfortable with to call the REST API (<code>curl</code>, Postman, etc.). Personally, I prefer Postman, and in <em>Code Snippet 3</em> we see how we call into the <code>connector-plugins</code> endpoint and how we use the port I mentioned above: <code>8083</code>:</p>

<p><img src="/images/posts/kafka-connect-get-connectors.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>Installed Connectors</em></p>

<p>Executing the code in <em>Code Snippet 3</em> I get the result we see in <em>Figure 6</em>. We see various Connect connectors, but nothing for SQL Server. So, we need to install it somehow.</p>

<h2 id="installing-debezium-sql-server-connector">Installing Debezium SQL Server Connector</h2>

<p>How do we go about installing a connector into the Connect service? Above I mentioned how a connector is a <code>.jar</code> file, which the service (JVM process) loads. I also mentioned the <code>CONNECT_PLUGIN_PATH</code>, which indicates where the service loads connectors from. So, with that in mind, we can imagine the process to install a connector being:</p>

<ol>
<li>Get the specific connector from &ldquo;somewhere&rdquo;.</li>
<li>Copy it into the path where the service loads connectors from.</li>
<li>Restart the service.</li>
</ol>

<p>If we ran the Kafka cluster as a local install, installing a connector would be as easy as above, but what about installing it when running Kafka in Docker containers?</p>

<p>Well, we could do something similar:</p>

<ol>
<li>Download the connector we want to install (the file is most likely &ldquo;tar&rdquo;:ed).</li>
<li>&ldquo;spin up&rdquo; the Kafka cluster.</li>
<li>Use <code>docker cp</code> to copy the connector into the connect container.</li>
<li>Use <code>docker exec</code> to get to the bash shell in the connect container.</li>
<li>Un-tar the file to the plugin load path.</li>
<li>Back out from the container and <code>docker commit</code> the changes to a new image name.</li>
<li>Tear down the running Kafka cluster: <code>docker-compose down</code>.</li>
<li>Use that image in the docker compose file in place of the &ldquo;original&rdquo; connect image.</li>
</ol>

<p>Yes, we could do something like that, and that is how I did it initially (yeah I know - I am a &ldquo;noob&rdquo;, so sue me). There are however better ways of doing it:</p>

<ul>
<li>Confluent Hub</li>
<li>Create a new image from a <code>Dockerfile</code>.</li>
</ul>

<p>Let us look at the two options.</p>

<h4 id="confluent-hub">Confluent Hub</h4>

<p>Confluent is like the App Store (or NuGet), but for Kafka. The <a href="https://docs.confluent.io/home/connect/confluent-hub/">home page</a> expresses it a lot better than what I can do:</p>

<p><em>Confluent Hub is an online library of pre-packaged and ready-to-install extensions or add-ons for Confluent Platform and Apache Kafka. You can browse the large ecosystem of connectors, transforms, and converters to find the components that suit your needs and easily install them into your local Confluent Platform environment.</em></p>

<p>Using Confluent Hub, you can install Kafka Connect connectors, whether you do it locally or in Docker. This is made possible via the <a href="https://docs.confluent.io/home/connect/confluent-hub/client.html">Confluent Hub Client</a>. The client is part of the Confluent Platform and is located in the <code>/bin</code> directory. If you are not using the Confluent Platform, you can download and install the client locally. Since I am using Confluent Platform, all is good.</p>

<p>Right, so we want to install the Debezium SQL Server connector. Let us go to the hub and look for it. Browse to <a href="https://www.confluent.io/hub/">here</a>, and in the search box, enter &ldquo;SQL Server&rdquo;, followed by a carriage return:</p>

<p><img src="/images/posts/kafka-connect-confl-hub-dbz.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>Debezium SQL Server Connector</em></p>

<p>The result coming back from the search looks like what we see in <em>Figure 7</em>; one entry: <strong>Debezium SQL Server CDC Source Connector</strong>. When you click on the result, you end up at a page with some more information about the connector, and more importantly, the syntax of how to install the connector:</p>

<pre><code class="language-bash">confluent-hub install debezium/debezium-connector-sqlserver:1.6.0
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Install SQL Server Connector</em></p>

<p>Ok, cool - so we see the syntax to install a connector in <em>Code Snippet 4</em>, but this looks suspiciously like how we do it from a bash shell. How do I do it for a Docker container without resorting to the &ldquo;hack&rdquo; I mentioned at the beginning of this section?</p>

<p>Ah, that&rsquo;s where the &ldquo;magic&rdquo; of Docker compose files comes in. It turns out that when you define a container, you can also specify configuration options. One such option is the <code>command</code> option, which allows you to execute arbitrary commands.</p>

<p>Let us edit our <code>docker-compose.yml</code> file and add the command configuration:</p>

<pre><code class="language-bash">connect:
    image: cnfldemos/cp-server-connect-datagen:0.5.0-6.2.0
    
    [snip]

    environment:

      [snip]

      CONNECT_PLUGIN_PATH: &quot;/usr/share/java,/usr/share/confluent-hub-components&quot;
      CONNECT_LOG4J_LOGGERS: org.apache.zookeeper=ERROR,org.I0Itec.zkclient=ERROR,org.reflections=ERROR
    command: 
      - bash 
      - -c 
      - |
        echo &quot;Installing connector plugins&quot;
        confluent-hub install --no-prompt debezium/debezium-connector-sqlserver:latest
        #
        echo &quot;Launching Kafka Connect worker&quot;
        /etc/confluent/docker/run &amp; 
        #
        sleep infinity

</code></pre>

<p><strong>Code Snippet 5:</strong> <em>Installing Connector from Confluent Hub in Container</em></p>

<p>In <em>Code Snippet 5</em>, we see how we use the <code>docker-compose.yml</code> file from before (heavily &ldquo;snipped&rdquo;), but we have added the <code>command</code> configuration option.</p>

<p>In the code, we see how we:</p>

<ul>
<li>&ldquo;spin up&rdquo; the bash shell.</li>
<li>set some options, <code>-c</code> and <code>|</code>.</li>
<li>executing the <code>install</code> command.</li>
<li>making sure we start up the worker process</li>
<li>we finally do <code>sleep infinity</code> to keep the container alive.</li>
</ul>

<p>It is worth noting that instead of a version number of the connector, I say <code>latest</code> to always get the latest release. Having edited the <code>docker-compose.yml</code> file, we now start the cluster: <code>docker-compose up -d</code>.</p>

<blockquote>
<p><strong>NOTE:</strong> Above I say *&hellip; get the latest <strong>release</strong> &hellip;*. What to be aware of is that the Confluent Hub only contains released versions of connectors (AFAIK).</p>
</blockquote>

<p>Wait a little while for the cluster to start up, and then use Postman to retrieve the installed connectors as in <em>Code Snippet 3</em>. Executing the Postman <code>GET</code> command, we see:</p>

<p><img src="/images/posts/kafka-connect-dbz-sql-conn.png" alt="" /></p>

<p><strong>Figure 8:</strong> <em>Installed Debezium SQL Server Connector</em></p>

<p>Yay, the Debezium SQL Server Connector is now deployed to Kafka Connect, as we see in <em>Figure 8</em> (outlined in red). What you see outlined in yellow is the datagen connector which is one of the extra parts of the <code>cp-server-connect-datagen</code> image.</p>

<h4 id="dockerfile">Dockerfile</h4>

<p>Ok, so we have seen how we can install a connector using <code>confluent-hub install</code> in the <code>command</code> option in the <code>docker-compose.yml</code>. That is awesome; however, this requires you to have an internet connection every time you &ldquo;spin up&rdquo; the Kafka cluster.</p>

<p>So, what you can do instead is build your own connect worker image, include the connector(s) you want and use that image in your compose file. To do this we need a base image and a file that tells Docker what to do to build our own image. For the base image, we can definitely use the image we have seen so far, the <code>cp-server-connect-datagen</code> image, but in reality, you want an image containing the bare minimum as the base. For that, we use the <code>cp-server-connect-base</code> image.</p>

<p>I mentioned above that we need a file telling Docker what to do. This file is essentially a build file. It is common practice to name that file <code>Dockerfile</code>. To achieve what we did above in <em>Code Snippet 5</em>, we create an empty file, name it <code>Dockerfile</code> and add the following:</p>

<pre><code class="language-bash">FROM confluentinc/cp-server-connect-base:6.2.0

RUN confluent-hub install --no-prompt \
            debezium/debezium-connector-sqlserver:latest
</code></pre>

<p><strong>Code Snippet 6:</strong> <em>File to Create Image</em></p>

<p>In <em>Code Snippet 6</em>, we see how we first define what image to use, and then we tell Docker we want to run the command to add the connector. Having saved the <code>Dockerfile</code>, we now build the image:</p>

<pre><code class="language-bash">docker build . -t dbz-sql-conn
</code></pre>

<p><strong>Code Snippet 7:</strong> <em>Docker Build</em></p>

<p>The code in <em>Code Snippet 7</em> shows the syntax to build the image, where:</p>

<ul>
<li><code>docker build</code> is the build command.</li>
<li>the <code>.</code> tells docker to pick the file named <code>Dockerfile</code>.</li>
<li>the <code>-t dbz-sql-conn</code> tags the image with a name. Here we can also assign a version number.</li>
</ul>

<p>After having run the code in <em>Code Snippet 7</em>, you can execute <code>docker images</code>, and in the list of images being returned, you should now see the <code>dbz-sql-conn</code>. Notice how it automatically has been assigned the <code>latest</code> tag, as we did not give it a version.</p>

<p>If the Kafka cluster is still up bring it down. Edit the <code>docker-compose.yml</code> file and replace the <code>cnfldemos/cp-server-connect-datagen:0.5.0-6.2.0</code> image with the image we just built: <code>dbz-sql-conn</code>, and bring up the cluster again. We do as we did before; wait a while for the cluster to come up and then use Postman to see the installed connectors. If everything has gone according to plan you now - once again - see the SQL Server connector. This time though, you will not see the datagen connector as it is not part of the <code>cp-server-connect-base</code> image.</p>

<h4 id="pre-release-connector-versions">Pre-Release Connector Versions</h4>

<p>I mentioned previously that Confluent Hub contains released versions of connectors. What if you want to use an Alpha/Beta version of a connector? For example, the latest release of the Debezium SQL Server connector is version 1.6.0, but there is a version 1.7.0 in testing, and I would like to test that version.</p>

<p>First of all, how do you know what versions there are? Well, browse to <a href="https://debezium.io/releases/">Debezium Releases Overview</a>, where you see what version is in development and what version is released:</p>

<p><img src="/images/posts/kafka-connect-dbz-release-overview.png" alt="" /></p>

<p><strong>Figure 9:</strong> <em>Installed Debezium SQL Server Connector</em></p>

<p>Click on the <em>More Info</em> button, which you see outlined in red in <em>Figure 9</em>. That takes you to the <a href="https://debezium.io/releases/1.7/">Debezium Release Series</a> page, where you at the bottom of the page have a <em>Downloads</em> button. Click on that button and then right-click on the connector for the product you want (SQL Server Connector Plug-in), and copy the link. The link for the latest development release for the SQL Server Connector Plug-in at the time of me writing this post (August 2021) is:</p>

<pre><code class="language-bash">https://repo1.maven.org/maven2/io/debezium/ \
       debezium-connector-sqlserver/1.7.0.Alpha1/ \
       debezium-connector-sqlserver-1.7.0.Alpha1-plugin.tar.gz
</code></pre>

<p><strong>Code Snippet 8:</strong> <em>Link to Connector Plugin</em></p>

<p>When you look at <em>Code Snippet 8</em> beware of the line continuations <code>\</code>. As the connector is not available via Confluent Hub, how do we get it?</p>

<p>The answer to that is that nothing much changes. We can either install it via the <code>docker-compose.yml</code> file similar to what we did in <em>Code Snippet 5</em> or what we did in <em>Code Snippet 6</em>. The only difference is that we cannot use <code>confluent-hub install</code>, but we have to:</p>

<ul>
<li>download it using <code>wget</code>.</li>
<li>un-tar it into the plugin path.</li>
</ul>

<p>I will do it by building a new image, and the code in <code>Dockerfile</code> for this looks like so:</p>

<pre><code class="language-bash">FROM confluentinc/cp-server-connect-base:6.2.0

RUN wget https://repo1.maven.org/maven2/io/debezium/ \
        debezium-connector-sqlserver/1.7.0.Alpha1/ \
        debezium-connector-sqlserver-1.7.0.Alpha1-plugin.tar.gz \
    &amp;&amp; tar -xvf ./debezium-connector-sqlserver-1.7.0.Alpha1-plugin.tar.gz \
     -C /usr/share/java/
</code></pre>

<p><strong>Code Snippet 9:</strong> <em>Dockerfile Downloading Connector</em></p>

<p>As with <em>Code Snippet 8</em>, beware of the line continuations in <em>Code Snippet 9</em>. In <em>Code Snippet 9</em>, we see how we download it using <code>wget</code> and then un-tar it into the plugin path. Having edited and saved the <code>Dockerfile</code>, we build it like so:</p>

<pre><code class="language-bash">docker build . -t dbz-sql-conn:1.7.Alpha1
</code></pre>

<p><strong>Code Snippet 10:</strong> <em>Building Alpha Release</em></p>

<p>As we see in <em>Code Snippet 10</em>, I have given the image a tag of <code>1.7.Alpha1</code>. To ensure that it works we:</p>

<ul>
<li>replace the <code>dbz-sql-conn</code> image in the <code>docker-compose.yml</code> file with dbz-sql-conn:1.7.Alpha1</li>
<li>tear down the Kafka cluster</li>
<li>spin up the Kafka cluster again.</li>
</ul>

<p>When we use Postman to get installed connectors, we see the following:</p>

<p><img src="/images/posts/kafka-connect-dbz-sql-conn-alpha.png" alt="" /></p>

<p><strong>Figure 10:</strong> <em>Installed Debezium SQL Server Connector 1.7.0</em></p>

<p>We see in <em>Figure 10</em> how we indeed have installed a pre-release version of the connector - yay!</p>

<h2 id="test">Test</h2>

<p>So far, we have seen that the SQL Server connector is installed, but we have not made sure it does what it is supposed to do, i.e. retrieve data into a Kafka topic.</p>

<p>As I mentioned initially, this post is not about SQL Server CDC or configuring the Debezium connector. In any case, let us quickly go over how to test it works. We start with enabling SQL Server, and we use the database and table we created above:</p>

<pre><code class="language-sql">USE DebeziumTest;
GO
-- before we enable CDC ensure the SQL Server Agent is started
-- we need first to enable CDC on the database
EXEC sys.sp_cdc_enable_db;

-- then we can enable CDC on the table
EXEC sys.sp_cdc_enable_table @source_schema = N'dbo',
                               @source_name   = N'tb_CDCTab1',
                               @role_name = NULL,
                               @supports_net_changes = 0;
</code></pre>

<p><strong>Code Snippet 11:</strong> <em>Enabling Database and Table for CDC</em></p>

<p>The code comments in <em>Code Snippet 10</em> should be pretty self-explanatory. The only thing to think about is that the SQL Server Agent needs to be started for this to work.</p>

<p>Having enabled CDC, we now configure and create the connector instance. We do it using Postman, <code>POST</code>:ing to the <code>connectors</code> endpoint:</p>

<p><img src="/images/posts/kafka-connect-dbz-create-connector.png" alt="" /></p>

<p><strong>Figure 11:</strong> <em>Create Connector</em></p>

<p>In <em>Figure 11</em>, we see a straightforward connector configuration. You can find out more about the various configuration properties <a href="https://debezium.io/documentation/reference/connectors/sqlserver.html#sqlserver-example-configuration">here</a>. When looking at the Kafka topics in Control Center after sending the <code>POST</code> request, you see something like so:</p>

<p><img src="/images/posts/kafka-connect-dbz-topics.png" alt="" /></p>

<p><strong>Figure 12:</strong> <em>Debezium Topics</em></p>

<p>We see in <em>Figure 12</em> how two new topics were automatically created when we created the connector. These topics are Debezium specific topics, and you as a user would not do much with them. When you look at the topics in Kafka at this stage, you do not see any topic related to the table we want to stream data from. That changes as soon as you insert some data into the table:</p>

<pre><code class="language-sql">USE DebeziumTest;
GO

INSERT INTO dbo.tb_CDCTab1(Col1, Col2)
VALUES(1, 'Hello Number 1')
</code></pre>

<p><strong>Code Snippet 12:</strong> <em>Ingest Data</em></p>

<p>When you refresh the Topics page in Control Center after executing the code in <em>Code Snippet 12</em>, you see this:</p>

<p><img src="/images/posts/kafka-connect-dbz-table-topic.png" alt="" /></p>

<p><strong>Figure 13:</strong> <em>Table Topic</em></p>

<p>Cool, in <em>Figure 13</em>, outlined in red, we see a new topic. This is the topic for the table we want to stream data from into Kafka. If you now were to look at the messages in the topic, you would see the data we just inserted. I leave that for you, my dear readers, to do on your own. We have now confirmed that everything works!</p>

<h2 id="summary">Summary</h2>

<p>In this post, I describe what to do if you want to run the Debezium SQL Server connector in a Docker environment.</p>

<p>We started by looking at what Kafka Connect and Debezium is. We said that:</p>

<ul>
<li>Kafka Connect is a JVM process allowing us to stream data between Apache Kafka and other systems. It works by the use of connectors which are <code>.jar</code> files. There are two types of connectors:

<ul>
<li>source connectors that understand how to retrieve data from a system and publish it to Kafka.</li>
<li>sink connectors that read data from Kafka topics and ingests that data into target systems.</li>
</ul></li>
<li>Debezium is a distributed platform with connectors for a multitude of database systems. It uses the underlying system&rsquo;s CDC functionality to capture database changes and publish those changes to topics in Kafka.</li>
</ul>

<p>We then looked at how to deploy a Debezium connector, more specifically the SQL Server connector, to a Kafka Connect Docker container. We saw there are two main ways to get a connector into the Kafka Connect container, and both ways use the Confluent Hub client:</p>

<ul>
<li>In the <code>command</code> option for the Kafka Connect container, run the install command.</li>
<li>Create your own image, and in the <code>Dockerfile</code> file, run the install command.</li>
</ul>

<p>That was about it. Once again, this post was not about how CDC works or the various Debezium configuration options. That may be covered in a future post.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 31, 2021]]></title>
    <link href="https://nielsberglund.com/2021/08/01/interesting-stuff---week-31-2021/" rel="alternate" type="text/html"/>
    <updated>2021-08-01T07:40:28+02:00</updated>
    <id>https://nielsberglund.com/2021/08/01/interesting-stuff---week-31-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/articles/saga-orchestration-outbox/">Saga Orchestration for Microservices Using the Outbox Pattern</a>. The last few weeks, at <a href="/derivco">Derivco</a>, I have been ~playing around~ researching the use of CDC, Debezium and the outbox pattern (a blog post or two may come soon). I&rsquo;ve been looking at it in relation to publishing events from the database. It was then interesting to come across this post discussing CDC and Debezium and how these technologies combined can be used for implementing the SAGA pattern. Very cool!</li>
</ul>

<h2 id="data-architecture">Data Architecture</h2>

<ul>
<li><a href="https://netflixtechblog.com/data-movement-in-netflix-studio-via-data-mesh-3fddcceb1059">Data Movement in Netflix Studio via Data Mesh</a>. I have previously covered posts discussing Data Mesh. In this post, Netflix talks about their Data Mesh. Data Mesh, in this context, is a fully managed, streaming data pipeline product used for enabling Change Data Capture (CDC) use cases. The post is very informative, and there are quite a few concepts worth investigating!</li>
</ul>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://eng.uber.com/pinot-real-time-ingestion/">Pinot Real-Time Ingestion with Cloud Segment Storage</a>. This post, by Uber, discusses how Uber added a deep store to Pinot&rsquo;s real-time ingestion protocol.</li>
<li><a href="https://towardsdatascience.com/getting-started-with-azure-data-explorer-and-azure-synapse-analytics-for-big-data-processing-25500821e370">Getting started with Azure Data Explorer and Azure Synapse Analytics for Big Data processing</a>. Azure Data Explorer is a fully managed data analytics service that can handle large volumes of diverse data from any data source, such as websites, applications, IoT devices, and more. This post looks at leveraging integration between Azure Data Explorer and Azure Synapse for processing data with Apache Spark.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/designing-an-elastic-apache-kafka-for-the-cloud/">Making Apache Kafka Serverless: Lessons From Confluent Cloud</a>. From a developers perspective, serverless in the cloud is awesome and easy to use. However, the system designer and the engineer who has to design and implement a serverless system have challenges. This post starts with looking at the confluent cloud architecture and then dives into how some of the difficulties mentioned above have been overcome.</li>
<li><a href="https://www.confluent.io/blog/from-apache-kafka-to-confluent-cloud-optimizing-for-speed-scale-storage/">Speed, Scale, Storage: Our Journey from Apache Kafka to Performance in Confluent Cloud</a>. Hmm, Confluent Cloud seemed popular this week. This post looks at optimizing Apache Kafka for Confluent Cloud. Even if you are not interested in the cloud, the post is full of good advice and best practices. Oh, and I have to look at the test framework mentioned in the post: <a href="https://github.com/apache/kafka/blob/db3e5e2c0de367ffcfe4078359d6d208ba722581/TROGDOR.md">Trogdor</a>.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 30, 2021]]></title>
    <link href="https://nielsberglund.com/2021/07/25/interesting-stuff---week-30-2021/" rel="alternate" type="text/html"/>
    <updated>2021-07-25T09:21:37+02:00</updated>
    <id>https://nielsberglund.com/2021/07/25/interesting-stuff---week-30-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="data-architecture">Data Architecture</h2>

<ul>
<li><a href="https://medium.com/datahub-project/enabling-data-discovery-in-a-data-mesh-the-saxo-journey-451b06969c8f">Enabling Data Discovery in a Data Mesh: The Saxo Journey</a>. In the <a href="/2021/06/27/interesting-stuff---week-26-2021/">roundup for week 26</a>, I wrote about how Saxo Bank in Denmark has moved to a domain-driven data architecture with Kafka as an integral part and the lessons learned/best practices. This post is a follow-up, and in this post, Saxo Bank writes about its data infrastructure with an in-house central data management application and how Saxo Bank approaches and solves data inconsistency issues. As with the previous article, this is a must-read!<br /></li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://medium.com/event-driven-utopia/the-duality-of-streams-and-tables-why-it-matters-ed9bb17e7505">The Duality of Streams and Tables - Why It Matters?</a>. When you read articles, blogs about Kafka and stream processing, you may have come across the statement about the duality between streams and tables. When I have read about it, I must admit that I have not really &ldquo;grasped&rdquo; it, that is, until now. The post linked to explains in a way that even I can understand the concepts behind the stream-table duality. Good job!</li>
<li><a href="https://www.confluent.io/blog/ksqldb-0-19-adds-data-modeling-foreign-key-joins/">Announcing ksqlDB 0.19.0</a>. In the <a href="/2021/07/04/interesting-stuff---week-27-2021/">roundup three weeks ago</a> I wrote about the new functionality in KStreams - the foreign-key join, and how I looked forward to it appearing in ksqlDB soon. Well, it must be Christmas, as my wish has come through. The post I link to announces ksqlDB 0.19, and one of the new features in the release is the foreign-key join. How awesome is that?!</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 29, 2021]]></title>
    <link href="https://nielsberglund.com/2021/07/18/interesting-stuff---week-29-2021/" rel="alternate" type="text/html"/>
    <updated>2021-07-18T14:05:36+02:00</updated>
    <id>https://nielsberglund.com/2021/07/18/interesting-stuff---week-29-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p>Speaking about the week just ending - wow, what a week! As you may know or not, I live in Durban, South Africa. Durban is in the province of KZN (KwaZulu-Natal), home to Jacob Zuma, ex-president of SA. On July 8, Jacob Zuma was put in jail after being found guilty of contempt of court. The jailing of Jacob Zuma led to protests that initially were peaceful. However, this changed Sunday, July 11, when small scale rioting broke out in and around Durban. On Monday (July 12), it escalated to full-scale rioting, looting, and factories burning.</p>

<p>The police were not prepared for the scale of the riots and could not offer any form of protection and help, so the local communities took it upon themselves to protect the local areas. Roadblocks were put up to prevent rioters from entering the neighbourhoods. The roadblocks were manned <sup>24</sup>&frasl;<sub>7</sub> by people from the community, and we did nighttime patrols to show presence and act as a deterrent. So I have done night patrols from last Monday until Friday night.</p>

<p>What we did seem to have helped, and fortunately we had no issues in the area I live in. I must say however, that it is a bit frightening when you are out in the middle of the night, and you hear gunshots some hundred meters away!</p>

<p>That&rsquo;s enough of my &ldquo;adventures&rdquo;, let&rsquo;s get back to what this post is all about.</p>

<p></p>

<h2 id="sql-server">SQL Server</h2>

<ul>
<li><a href="https://azure.microsoft.com/en-us/blog/accelerate-big-data-analytics-with-the-spark-30-connector-for-sql-server-now-generally-available/">Accelerate big data analytics with the Spark 3.0 connector for SQL Server—now generally available</a>. The title of this post says it all; the Apache Spark 3.0 connector is now available. The connector is a high-performance connector that enables you to use transactional data in big data analytics, and persists results for ad-hoc queries or reporting.</li>
</ul>

<h2 id="big-data">Big data</h2>

<ul>
<li><a href="https://towardsdatascience.com/identity-keyrings-201d17295954">Identity keyrings</a>. In an enterprise various parts of the business may have different identifiers for the same entity, and correlating these different identities may be difficult. The post linked to looks at how we can use Azure Data Explorer to build an identity keyring that brings together the various identifier. This is a very interesting read, showing what Azure Data Explorer can do!</li>
<li><a href="https://medium.com/pinterest-engineering/interactive-querying-with-apache-spark-sql-at-pinterest-2a3eaf60ac1b">Interactive Querying with Apache Spark SQL at Pinterest</a>. This post looks at how Pinterest built a scalable, reliable, and efficient interactive querying platform that processes hundreds of petabytes of data daily with Apache Spark SQL. Very, very cool!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://medium.com/pinterest-engineering/unified-flink-source-at-pinterest-streaming-data-processing-c9d4e89f2ed6">Unified Flink Source at Pinterest: Streaming Data Processing</a>. The post here is another post by Pinterest. In the post they drill down and look at how they use Apache Spark for stream processing.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>Above I wrote about the problems we had in Durban last week. I wrote about doing night patrols. Well, it looks like we need to start them up again, as rumours are floating around about new attacks. So, back to night shifts, <em>sad-face</em>!</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 28, 2021]]></title>
    <link href="https://nielsberglund.com/2021/07/11/interesting-stuff---week-28-2021/" rel="alternate" type="text/html"/>
    <updated>2021-07-11T09:07:39+02:00</updated>
    <id>https://nielsberglund.com/2021/07/11/interesting-stuff---week-28-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="sql-server">SQL Server</h2>

<ul>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2021/07/08/whats-new-with-sql-server-big-data-clusters-cu11-release/">What&rsquo;s new with SQL Server Big Data Clusters—CU11 Release</a>. As the title implies, this blog post announces the latest release of SQL Server Big Data Cluster (BDC). Read the post to learn more about new security features as well as PolyBase enhancements.</li>
</ul>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://arxiv.org/pdf/2104.00087.pdf">Real-time Data Infrastructure at Uber</a>. For followers of my blog, it should not come as a surprise that I like both Kafka and Apache Pinot. The link here is to a white paper around Uber looking at how Uber uses, among other tech, Kafka and Pinot as a foundation for their real-time data infrastructure. It is an awesome and very informative read!</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>As I have mentioned previously, I am doing both a training class and a conference session at the <a href="https://dataplatformgeeks.com/dps2021/"><strong>2021 Data Platform Summit</strong></a>. Related to that, I had the opportunity to pen down a new article in the <a href="https://www.sqlservergeeks.com/magazine/"><strong>SQLServerGeeks Magazine</strong></a>, where I wrote about <strong>SQL Server 2019 Big Data Cluster</strong>. The magazine has good content overall for the SQL professionals and contains a lot of really cool material. The best part of the magazine is that it is free for the community! <a href="https://www.sqlservergeeks.com/magazine/">Download</a> your copy now.</p>

<p>Oh, and speaking of speaking:</p>

<p><img src="/images/posts/bdc.jpg" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Webinar</em></p>

<p>As you see in <em>Figure 1</em> above, I am delivering a free session at DataPlatformGeeks Webinars about SQL Server and how it can be the central hub for all your data. Do join me! Oh, you ask where to register for the webinar? You register <a href="https://bit.ly/DataPlatformGeeks_Events">here</a>!</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 27, 2021]]></title>
    <link href="https://nielsberglund.com/2021/07/04/interesting-stuff---week-27-2021/" rel="alternate" type="text/html"/>
    <updated>2021-07-04T09:04:04+02:00</updated>
    <id>https://nielsberglund.com/2021/07/04/interesting-stuff---week-27-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="sql-server">SQL Server</h2>

<ul>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2021/06/30/looking-to-the-future-for-r-in-azure-sql-and-sql-server/">Looking to the future for R in Azure SQL and SQL Server</a>. If you follow my blog, you know that I have written a lot about <a href="https://nielsberglund.com/categories/sql-server-machine-learning-services/">SQL Server Machine Learning Services</a> (SQLML) and R in SQL Server throughout the last couple of years. The post linked to lays out the plans for R in SQL Server in upcoming SQL Server versions. The short version of the post is that Microsoft will go away from the proprietary R and Python packages in SQLML in favour of the open-source versions. If you are interested and want more than what is in the post, my good friend <a href="https://tecflix.com/instructors/rafal-lukawiecki">Rafal Lukawiecki</a> has written an <a href="https://tecflix.com/news/microsoft-open-sources-sql-server-machine-learning-and-discontinues-ml-server">excellent post</a> explaining in detail the changes.</li>
</ul>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://nemeiskiy-lef.medium.com/implementing-distributed-transaction-in-net-using-saga-pattern-1641172c122">Implementing distributed transaction in .NET using Saga pattern</a>. One of the biggest issues when moving from a monolithic system to a distributed microservices system is handling transactions. One solution to distributed transactions in a microservices system is using the <a href="https://microservices.io/patterns/data/saga.html">Saga</a> pattern. In this post, the author does an excellent job explaining the Saga pattern and how to implement it in .NET.</li>
</ul>

<h2 id="machine-learning">Machine Learning</h2>

<ul>
<li><a href="https://eng.uber.com/continuous-integration-deployment-ml/">Continuous Integration and Deployment for Machine Learning Online Serving and Models</a>. This post from Uber looks at how they implement CI/CD and model serving in their environment. This is a must-read if you are in the ML world!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/easily-manage-database-migrations-with-evolving-schemas-in-ksqldb/">Online, Managed Schema Evolution with ksqlDB Migrations</a>. In the database world, managing changes to the schema is (somewhat) easy. Well, at least you probably have some workflows for that. In the streaming world, it may not be that &ldquo;straightforward&rdquo;. In this post, the author looks at the tooling available for managing schema evolution in ksqlDB.</li>
<li><a href="https://itnext.io/eventual-consistency-with-spring-for-apache-kafka-cfbbed450b5e">Eventual Consistency with Spring for Apache Kafka: Part 1 of 2</a>. This post looks at how Spring for Kafka is used to manage a distributed data model across multiple microservices. You find <a href="https://itnext.io/eventual-consistency-with-spring-for-apache-kafka-part-2-of-2-23bedd512ccf">Part 2 here</a>.</li>
<li><a href="https://www.confluent.io/blog/data-enrichment-with-kafka-streams-foreign-key-joins/">Crossing the Streams: The New Streaming Foreign-Key Join Feature in Kafka Streams</a>. In relational databases, you more often than not have multiple one-to-many relationships (foreign keys). This is not well supported in KTables and streams in Kafka. At least it was not until Kafka 2.4, where non-key joining between KTables was introduced. The post I link to looks more in detail at how foreign-key joins are implemented in KStreams. This is only available in KStreams, but according to the post, we may expect to see it in the next release of ksqlDB; 0.19 - Yay!</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 26, 2021]]></title>
    <link href="https://nielsberglund.com/2021/06/27/interesting-stuff---week-26-2021/" rel="alternate" type="text/html"/>
    <updated>2021-06-27T09:21:01+02:00</updated>
    <id>https://nielsberglund.com/2021/06/27/interesting-stuff---week-26-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="machine-learning-ai">Machine Learning / AI</h2>

<ul>
<li><a href="https://databricks.com/blog/2021/06/23/need-for-data-centric-ml-platforms.html">Need for Data-centric ML Platforms</a>. When we do ML projects, it comes naturally to have a model-centric approach to the project. This Databricks blog post argues that having a data-centric view increases the chances for success. Very interesting!</li>
</ul>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://www.sqlshack.com/azure-data-explorer-and-the-kusto-query-language/">Azure Data Explorer and the Kusto Query Language</a>. This blog post is part one of a two-part series looking at Azure Data Explorer (ADX) and Kusto; its query language. This post introduces ADX and looks briefly into what you can do with Kusto. After having read this post, I look forward to part two!</li>
<li><a href="https://techcommunity.microsoft.com/t5/azure-data-explorer/update-on-geospatial-functions/ba-p/2446350">Update on geospatial functions</a>. In last weeks <a href="/2021/06/20/interesting-stuff---week-25-2021/">roundup</a>, I mentioned a post about geospatial functions in Apache Pinot. The post I link to is also about geospatial functions, but this time in Azure Data Explorer. Man, you can do some cool stuff with geospatial queries!</li>
<li><a href="https://databricks.com/blog/2021/06/22/get-your-free-copy-of-delta-lake-the-definitive-guide-early-release.html">Get Your Free Copy of Delta Lake: The Definitive Guide (Early Release)</a>. Databricks has for a long time been talking about the Lakehouse architecture and how their Delta Lake table format can help create Lakehouses. This post announces the first book about Delta Lake and Lakehouses. I have already downloaded the book, and I suggest you do the same.</li>
</ul>

<h2 id="streaming-data-architecture">Streaming / Data Architecture</h2>

<ul>
<li><a href="https://www.confluent.io/blog/distributed-domain-driven-architecture-data-mesh-best-practices/">Saxo Bank&rsquo;s Best Practices for a Distributed Domain-Driven Architecture Founded on the Data Mesh</a>. I have covered posts about the Data Mesh in quite a few previous roundups, and this post is another one around data meshes. It looks at how Saxo Bank in Denmark has moved to a domain-driven data architecture with Kafka as an integral part and the lessons learned/best practices. The post also contains very, very useful references! The post is a must-read if you are interested in data architecture!</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p><img src="/images/posts/Neils_Berglund_IAmSpeaking.jpg" alt="" /></p>

<p><strong>Figure 1:</strong> <em>I Am Speaking</em></p>

<p>As I mentioned in the <a href="/2021/06/13/interesting-stuff---week-24-2021/">roundup</a> for week 24, I am speaking about Apache Kafka and Azure Data Explorer at the <a href="https://dataplatformgeeks.com/dps2021/"><strong>2021 Data Platform Summit</strong></a>. So right now, I am researching and prepping for that.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 25, 2021]]></title>
    <link href="https://nielsberglund.com/2021/06/20/interesting-stuff---week-25-2021/" rel="alternate" type="text/html"/>
    <updated>2021-06-20T07:52:57+02:00</updated>
    <id>https://nielsberglund.com/2021/06/20/interesting-stuff---week-25-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://databricks.com/blog/2021/06/17/announcing-photon-public-preview-the-next-generation-query-engine-on-the-databricks-lakehouse-platform.html">Announcing Photon Public Preview: The Next Generation Query Engine on the Databricks Lakehouse Platform</a>. This post looks at the new vectorized query engine on Databricks; Photon. It is included as part of a new high-performance runtime designed to run SQL workloads faster and reduce the total cost per workload.</li>
<li><a href="https://medium.com/apache-pinot-developer-blog/introduction-to-geospatial-queries-in-apache-pinot-b63e2362e2a9">Introduction to Geospatial Queries in Apache Pinot</a>. Geospatial data has become increasingly important when analyzing Big Data. Deriving insights from timely and accurate geospatial data can enable mission-critical use cases in organizations and give them a competitive edge. Geospatial support has recently been added to Apache Pinot, and this post looks at the challenges of analyzing geospatial at scale and how it is implemented in Apache Pinot.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.gentlydownthe.stream/">Gently down the stream: A gentle introduction to Apache Kafka</a>. This is a fantastic introduction to Kafka! Nothing more needs to be said!</li>
<li><a href="https://www.confluent.io/blog/rethinking-distributed-stream-processing-in-kafka/">Consistency and Completeness: Rethinking Distributed Stream Processing in Apache Kafka</a>. As the title implies, this post looks at how Consistency and Completeness are implemented in Apache Kafka and Kafka Streams. Very interesting!</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<ul>
<li><a href="https://www.linkedin.com/posts/derivco_derivco-tech-thursday-webinar-activity-6811654769594253312-PGbL/">Revolutionise how you develop software with GitHub</a>. This is not so much what I am doing, but what a colleague and mate is doing. <a href="https://gordonbeeming.xyz/">Gordon Beeming</a>, Microsoft MVP and Devops guru, is doing a <a href="/derivco">Derivco</a> Tech-Thursday Webinar Thursday, June 24. Gordon looks at some new GitHub features in the webinar, allowing you to deliver more value to your customers. The webinar is public, and you can sign up <a href="https://derivco.co.za/tech-thursday-webinar/">here</a>. Don&rsquo;t miss it!</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 24, 2021]]></title>
    <link href="https://nielsberglund.com/2021/06/13/interesting-stuff---week-24-2021/" rel="alternate" type="text/html"/>
    <updated>2021-06-13T10:39:45+02:00</updated>
    <id>https://nielsberglund.com/2021/06/13/interesting-stuff---week-24-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://thechief.io/c/editorial/introduction-to-chaos-engineering/">Introduction to Chaos Engineering</a>. The post linked here looks at the origin, principles, and benefits of Chaos Engineering. Chaos Engineering is when you try to disrupt and break an application system to build resilience. Notice that the post is behind a paywall.</li>
</ul>

<h2 id="machine-learning-ai">Machine Learning / AI</h2>

<ul>
<li><a href="https://www.ververica.com/blog/flink-powered-model-serving-real-time-feature-generation-at-razorpay">Flink-powered model serving &amp; real-time feature generation at Razorpay</a>. This post, which is from back in December 2020, looks at how Apache Flink is being utilized as a way to overcome challenges around feature generation and machine learning model serving in real-time. Very interesting!</li>
<li><a href="https://databricks.com/blog/2021/06/09/how-to-build-a-scalable-wide-and-deep-product-recommender.html">How to Build a Scalable Wide and Deep Product Recommender</a>. A Wide and Deep Learning Model consists of two parts; a machine learning part (linear model) and a neural network part. This type of model is often used in recommender systems, and the blog post linked looks at how you can do it using Databricks.</li>
<li><a href="https://pub.towardsai.net/automate-machine-learning-using-databricks-automl-a-glass-box-approach-and-mlflow-2543a8143687">Automate Machine Learning using Databricks AutoML — A Glass Box Approach and MLFLow</a>. During the <a href="https://databricks.com/dataaisummit/north-america-2021">Data + AI Summit 2021</a>, Databricks announced their Databricks AutoML platform. This post looks at using Databricks AutoML Platform to automatically apply machine learning to a dataset and deploy the model to production using the REST API.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://tech.ebayinc.com/engineering/block-aggregator-real-time-data-ingestion-from-kafka-to-clickhouse-with-deterministic-retries/">Block Aggregator: Real-time Data Ingestion from Kafka to ClickHouse with Deterministic Retries</a>. This post discusses the message-processing engine eBay developed to avoid data loss or duplication during delivery from Kafka to ClickHouse. I found the post very interesting as we are looking at similar things at <a href="/derivco">Derivco</a> right now.</li>
<li><a href="https://thecodinginterface.com/blog/serverless-kafka-with-aws-lambda/">Serverless Event Driven Systems with Confluent Cloud and AWS Lambda</a>. This post presents an end-to-end example of a Serverless event-driven architecture using Confluent Cloud for stream processing paired with AWS Lambda for event responsive logic using the Serverless Application Model (SAM) framework.</li>
<li><a href="https://www.confluent.io/blog/create-kafka-messages-from-within-control-center-for-better-kafka-management/">How to Better Manage Apache Kafka by Creating Kafka Messages from within Control Center</a>. This post is the first in a series looking some new features to Control Center, introduced in Confluent Platform 6.2.0. Some very cool stuff here!</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>In <a href="/2021/06/06/interesting-stuff---week-23-2021/">last weeks roundup</a>, I mentioned I was looking into <a href="https://docs.microsoft.com/en-us/azure/data-explorer/data-explorer-overview"><strong>Azure Data Explorer</strong></a> as I was thinking about creating some presentations for upcoming conferences. I did submit a couple of topics to some conferences, and I had two conferences accepting talks! Woohoo! The conferences and talks are:</p>

<ul>
<li><a href="https://dataplatformgeeks.com/dps2021/"><strong>2021 Data Platform Summit</strong></a>: <strong>How to do Real-Time Analytics Using Apache Kafka and Azure Data Explorer</strong>.</li>
<li><a href="https://www.eventbrite.com/e/future-data-driven-summit-event-tickets-157885366381"><strong>Future Data Driven</strong></a>: <strong>Analyze Billions of Rows of Data in Real-Time Using Azure Data Explorer</strong>.</li>
</ul>

<p>So now having the above talks accepted, I really need to get going with prep. I am stoked!</p>

<p>Oh, and I am still doing the <a href="https://dataplatformgeeks.com/dps2021/product/big-data-analytics-with-sql-server-2019-big-data-cluster-by-niels-berglund/"><strong>Big Data &amp; Analytics with SQL Server 2019 Big Data Cluster</strong></a> training class for the <a href="https://dataplatformgeeks.com/dps2021/"><strong>2021 Data Platform Summit</strong></a>, and if you <a href="https://dataplatformgeeks.com/dps2021/product/big-data-analytics-with-sql-server-2019-big-data-cluster-by-niels-berglund/">sign up</a> for the class you get free access to the summit itself!</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 23, 2021]]></title>
    <link href="https://nielsberglund.com/2021/06/06/interesting-stuff---week-23-2021/" rel="alternate" type="text/html"/>
    <updated>2021-06-06T08:28:23+02:00</updated>
    <id>https://nielsberglund.com/2021/06/06/interesting-stuff---week-23-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="machine-learning-ai">Machine Learning / AI</h2>

<ul>
<li><a href="https://databricks.com/blog/2021/06/02/jump-start-your-data-projects-with-pre-built-solution-accelerators.html">Jump Start Your Data Projects with Pre-Built Solution Accelerators</a>. There is pressure on data science teams/data engineers/etc., to deliver value fast. However, it takes time to research, build data pipelines, models, etc., and this can suck up momentum. To help to overcome these obstacles, Databricks introduces in this blog post <a href="https://databricks.com/solutions/accelerators"><strong>Databricks Solution Accelerators</strong></a>. These accelerators are fully-functional pre-built code to tackle the most common and high-impact use cases for data scientists, data engineers, etc.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://blog.flowdirector.io/new-smart-app-kafka-to-kafka-bridge-788883a68e63">New Smart App: Kafka to Kafka Bridge</a>. This post introduces the <strong>Kafka to Kafka Bridge</strong>. The bridge allows you to create message bridges between topics of the same or different Kafka brokers. We have developed something similar here at <a href="/derivco">Derivco</a>, but I will definitely check this out!</li>
<li><a href="https://www.confluent.io/blog/streaming-data-with-confluent-and-ksqldb-for-new-use-cases-with-ais/">Detecting Patterns of Behaviour in Streaming Maritime AIS Data with Confluent</a>. This is part two in a blog series on streaming a data feed Apache Kafka (the first part is <a href="https://www.confluent.io/blog/streaming-etl-and-analytics-for-real-time-location-tracking/">here</a>). This post looks at how <strong>ksqlDB</strong> can be used to do aggregations and pattern matching against a data feed. Very cool!</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>Apart from working on my <a href="https://dataplatformgeeks.com/dps2021/product/big-data-analytics-with-sql-server-2019-big-data-cluster-by-niels-berglund/"><strong>Big Data &amp; Analytics with SQL Server 2019 Big Data Cluster</strong></a> training class for the <a href="https://dataplatformgeeks.com/dps2021/"><strong>2021 Data Platform Summit</strong></a>, I have started looking into <a href="https://docs.microsoft.com/en-us/azure/data-explorer/data-explorer-overview"><strong>Azure Data Explorer</strong></a>.</p>

<p>Azure Data Explorer is a fast, fully managed data analytics service for real-time analysis on large volumes of data streaming from applications, websites, IoT devices, and more. I am looking into it because I am thinking of creating some presentations for upcoming conferences and maybe a training class. Watch this space!</p>

<p>Oh, and don&rsquo;t forget - if you <a href="https://dataplatformgeeks.com/dps2021/product/big-data-analytics-with-sql-server-2019-big-data-cluster-by-niels-berglund/">sign up</a> for my training class, mentioned above, at the <strong>2021 Data Platform Summit</strong>, you get free access to the summit itself!</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 22, 2021]]></title>
    <link href="https://nielsberglund.com/2021/05/30/interesting-stuff---week-22-2021/" rel="alternate" type="text/html"/>
    <updated>2021-05-30T06:33:15+02:00</updated>
    <id>https://nielsberglund.com/2021/05/30/interesting-stuff---week-22-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h1 id="machine-learning-ai">Machine Learning / AI</h1>

<ul>
<li><a href="https://blogs.microsoft.com/ai/from-conversation-to-code-microsoft-introduces-its-first-product-features-powered-by-gpt-3/">From conversation to code: Microsoft introduces its first product features powered by GPT-3</a>. Back in September 2020, Microsoft announced it had <a href="https://blogs.microsoft.com/blog/2020/09/22/microsoft-teams-up-with-openai-to-exclusively-license-gpt-3-language-model/">exclusively licensed GPT-3</a> -  the autoregressive language model that uses deep learning to produce human-like text. The linked-to post discusses the first incarnation of GPT-3 in Microsoft&rsquo;s Power Apps. Very, very interesting!</li>
<li><a href="https://databricks.com/blog/2021/05/27/introducing-databricks-machine-learning-a-data-native-collaborative-full-ml-lifecycle-solution.html">Introducing Databricks Machine Learning: a Data-native, Collaborative, Full ML Lifecycle Solution</a>. The <a href="https://databricks.com/dataaisummit/north-america-2021"><strong>Data + AI Summit 2021</strong></a> took place the week just gone by. The post I linked to talks about one of the announcements made during the summit; the launch of <strong>Databricks Machine Learning</strong>, an enterprise ML solution that is data-native, collaborative, and supports the entire ML lifecycle. I am pretty &ldquo;chuffed&rdquo; to see how the solution includes, among other things, a feature store!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://youtu.be/Ov5XgYzyGkU">Engaging Database Partials with Apache Kafka for Distributed System Consistency ft. Pat Helland</a>. When using various data from different systems, for example, for reports, obtaining the correct data when you need it in real-time can be difficult. In this videoed podcast, Pat Helland - (Principal Architect, Salesforce) - explains how to make educated partial answers when you need to use the Apache Kafka® platform.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p><img src="/images/posts/data-summit-2021.jpg" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Big Data &amp; Analytics</em></p>

<p>As mentioned in previous roundups, I deliver an Analytics on SQL Server Big Data Cluster training class at the <a href="https://dataplatformgeeks.com/dps2021/"><strong>2021 Data Platform Summit</strong></a>! So right now, I am creating content for the course. I am so looking forward to it!</p>

<p>If you are interested, <a href="https://youtu.be/Ov5XgYzyGkU">sign up</a> for the class. When signing up, you also get free access to the Summit! Hope to see you there!</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 21, 2021]]></title>
    <link href="https://nielsberglund.com/2021/05/23/interesting-stuff---week-21-2021/" rel="alternate" type="text/html"/>
    <updated>2021-05-23T08:34:42+02:00</updated>
    <id>https://nielsberglund.com/2021/05/23/interesting-stuff---week-21-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="data-architecture">Data Architecture</h2>

<ul>
<li><a href="https://databricks.com/blog/2021/05/19/evolution-to-the-data-lakehouse.html">Evolution to the Data Lakehouse</a>. Data lakehouses have been a hot topic the last year or two, and Databricks, with its lakehouse implementation Delta Lake has been at the forefront. The post linked to looks - as the title implies - at the evolution from data warehouses to data lakes to data lakehouses. Very interesting!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://medium.com/wix-engineering/6-event-driven-architecture-patterns-part-1-93758b253f47">6 Event-Driven Architecture Patterns — Part 1</a>. This post is the first of two. The author looks at key patterns of event-driven messaging designs that have facilitated creating a robust distributed system that can easily handle increasing traffic and storage needs. The second part of the series is <a href="https://medium.com/wix-engineering/6-event-driven-architecture-patterns-part-2-455cc73b22e1">here</a>.</li>
<li><a href="https://mert.codes/harder-better-faster-stronger-apache-pinot-as-a-kafka-consumer-and-datastore-for-fast-7df25bcc7d02">&ldquo;Harder, Better, Faster, Stronger&rdquo;: Apache Pinot as a Kafka Consumer and Datastore for Fast On-the-Fly Aggregations</a>. You who read my blog have probably noticed that I am quite partial to Kafka and Apache Pinot. Well, in this blog post, we get the best of both worlds. It covers how to use Apache Pinot to do aggregations in &ldquo;near&rdquo; real-time. I found the post very interesting.</li>
<li><a href="https://www.confluent.io/blog/error-handling-patterns-in-kafka/">Error Handling Patterns for Apache Kafka Applications</a>. In one of the teams I worked at in <a href="/derivco">Derivco</a>, we had a saying when we were going to do something we were not 100% sure of: <em>How hard can it be, what can possibly go wrong?</em>. In some cases, quite a lot could go wrong, and if we look at distributed systems, what can go wrong often goes wrong. This blog post covers different ways to handle errors and retries in event streaming applications.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p><img src="/images/posts/data-summit-2021.jpg" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Big Data &amp; Analytics</em></p>

<p>Don&rsquo;t forget to <a href="http://bit.ly/dps_2021">sign up</a> for my <strong>Big Data &amp; Analytics with SQL Server 2019 Big Data Cluster</strong> class at the <a href="https://dataplatformgeeks.com/dps2021/"><strong>2021 Data Platform Summit</strong></a>!</p>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 20, 2021]]></title>
    <link href="https://nielsberglund.com/2021/05/16/interesting-stuff---week-20-2021/" rel="alternate" type="text/html"/>
    <updated>2021-05-16T08:36:10+02:00</updated>
    <id>https://nielsberglund.com/2021/05/16/interesting-stuff---week-20-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="databases">Databases</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/netflix-cdc-events-cassandra/">Change Data Capture for Distributed Databases @Netflix</a>. In this <a href="https://www.infoq.com/">Infoq</a> presentation, the presenter covers the challenges associated with capturing CDC events from Cassandra, discussing the Flink ecosystem and the use of RocksDB. Very interesting!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://scattered-thoughts.net/writing/why-query-planning-for-streaming-systems-is-hard/">Why query planning for streaming systems is hard</a>. This post looks at some of the issues of doing query plans for streaming systems. Really interesting, and I would like to hear more about this.</li>
<li><a href="https://medium.com/leboncoin-engineering-blog/cooling-down-hot-data-from-kafka-to-athena-5918a628bd98">Cooling down hot data: From Kafka to Athena</a>. The post linked to looks at how to move data from online Kafka Clusters to a data lake for long term data retention, analytics, legal requirements, and as input for machine learning.</li>
<li><a href="https://www.confluent.io/blog/confluent-for-kubernetes-offers-cloud-native-kafka-automation/">Introducing Confluent for Kubernetes</a>. This post looks at Confluent for Kubernetes (CFK), a cloud-native control plane for deploying and managing Confluent in private cloud environments. It provides a standard and simple interface to customize, deploy, and manage Confluent Platform through declarative APIs.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>A couple of days ago, I received some very good news:</p>

<p><img src="/images/posts/data-summit-2021.jpg" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Big Data &amp; Analytics</em></p>

<p>As you see in <em>Figure 1</em> I will be giving a training class at the 2021 Data Platform Summit. The class is <strong>Big Data &amp; Analytics with SQL Server 2019 Big Data Cluster</strong>. So right now, I am creating the content for that class - it will be awesome.</p>

<p>If you are interested, <a href="http://bit.ly/dps_2021">sign up</a> for the class. When signing up, you also get free access to the Summit! Hope to see you there!</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 19, 2021]]></title>
    <link href="https://nielsberglund.com/2021/05/09/interesting-stuff---week-19-2021/" rel="alternate" type="text/html"/>
    <updated>2021-05-09T07:07:52+02:00</updated>
    <id>https://nielsberglund.com/2021/05/09/interesting-stuff---week-19-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="machine-learning">Machine Learning</h2>

<ul>
<li><a href="https://eng.uber.com/optimal-feature-discovery-ml/">Optimal Feature Discovery: Better, Leaner Machine Learning Models Through Information Theory</a>. This post talks about an approach at Uber called Optimal Feature Discovery. It searches for the most compact set of features out of all available at Uber for a given model, both boosting accuracy and reducing feature count at the same time.<br /></li>
</ul>

<h2 id="databases">Databases</h2>

<ul>
<li><a href="https://www.infoq.com/articles/serverless-data-api/">Why a Serverless Data API Might be Your Next Database</a>. This <a href="https://www.infoq.com/">InfoQ</a> article discusses database as a service (DBaaS) and serverless data API for cloud-based data management.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://chathura-ekanayake.medium.com/eda-implementation-integration-scenarios-66895923439">EDA implementation — Integration scenarios</a>. The post linked to is a follow up to <a href="https://chathura-ekanayake.medium.com/applying-event-driven-architecture-in-digital-transformation-projects-acbcb27440af">Applying Event-Driven Architecture in Digital Transformation Projects</a>, which discussed a generic architecture for event-driven architecture (EDA) based systems. This post explores implementation approaches for such event-driven systems by focusing on specific products and their interactions.</li>
<li><a href="https://www.confluent.io/blog/streaming-etl-with-confluent-kafka-message-routing-and-fan-out/">Streaming ETL with Confluent: Routing and Fan-Out of Apache Kafka Messages with ksqlDB</a>. This post looks at how we can use ksqlDB for message routing. I quite liked the post, and I will definitely see if we can implement some of this at <a href="/derivco">Derivco</a>.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 18, 2021]]></title>
    <link href="https://nielsberglund.com/2021/05/02/interesting-stuff---week-18-2021/" rel="alternate" type="text/html"/>
    <updated>2021-05-02T07:55:53+02:00</updated>
    <id>https://nielsberglund.com/2021/05/02/interesting-stuff---week-18-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="sql-server">SQL Server</h2>

<ul>
<li><a href="https://techcommunity.microsoft.com/t5/sql-server/inference-of-ml-models-in-sql-server-via-external-languages/ba-p/2216226">Inference of ML Models in SQL Server via External Languages</a>. SQL Server 2019 introduced the notion of External Languages, whereby we can execute calls against an external language, (Java, Python, etc.), from inside SQL Server. This post looks at scoring of ONNX models where ONXX has been registered as an external language. Very cool!</li>
</ul>

<h2 id="data-analytics">Data Analytics</h2>

<ul>
<li><a href="https://medium.com/apache-pinot-developer-blog/deploying-apache-pinot-at-a-large-retail-chain-42aed2921a38">Deploying Apache Pinot at a Large Retail Chain</a>. The post linked to here looks at the use of Apache Pinot at a large retailer and how it is used to some of the big challenges around data analytics.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/announcing-ksqldb-0-17-0-new-features-and-updates/">Announcing ksqlDB 0.17.0</a>. The title of this post says it all; it looks at the latest release of ksqlDB! Reading the post, there are quite a few interesting new features in this release. Personally, I am quite excited about the ability to do table scans! At <a href="/derivco">Derivco</a>, we have waited for that for a while.</li>
<li><a href="https://www.confluent.io/blog/how-to-survive-a-kafka-outage/">How to Survive a Kafka Outage</a>. Another post where the title says it all. The post looks at various types of potential Kafka outages and options to handle the outages.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 17, 2021]]></title>
    <link href="https://nielsberglund.com/2021/04/25/interesting-stuff---week-17-2021/" rel="alternate" type="text/html"/>
    <updated>2021-04-25T08:21:37+02:00</updated>
    <id>https://nielsberglund.com/2021/04/25/interesting-stuff---week-17-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="data-architecture">Data Architecture</h2>

<ul>
<li><a href="https://lakefs.io/hudi-iceberg-and-delta-lake-data-lake-table-formats-compared/">Hudi, Iceberg and Delta Lake: Data Lake Table Formats Compared</a>. This blog post compares the lake formats Hudi, Iceberg, and Delta Lake on their platform compatibility, performance &amp; throughput, and concurrency. Interesting!</li>
<li><a href="https://medium.com/explorium-ai/benchmarking-sql-engines-for-data-serving-prestodb-trino-and-redshift-1c5f16d6e5da">Benchmarking SQL engines for Data Serving: PrestoDb, Trino, and Redshift</a>. The linked-to post benchmarks the SQL engines, Redshift, Trino &amp; Presto. Read the post for some interesting findings.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://scattered-thoughts.net/writing/2021-q1-roundup/">2021 Q1 roundup</a>. The author of this post is a freelance researcher, and he is doing quite a lot of work related to streaming. This post is a roundup of what he has done during the first quarter of this year. There are some very interesting pieces in there!</li>
<li><a href="https://www.confluent.io/blog/kafka-2-8-0-features-and-improvements-with-early-access-to-kip-500/">What&rsquo;s New in Apache Kafka 2.8</a>. This post, as the title implies, announces the latest version of Apache Kafka: 2.8. Ok, so what is the big deal with that? The big deal is that this version is the first version where you can run Kafka without ZooKeeper! This is not recommended for production, but you can definitely test it out!</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 16, 2021]]></title>
    <link href="https://nielsberglund.com/2021/04/18/interesting-stuff---week-16-2021/" rel="alternate" type="text/html"/>
    <updated>2021-04-18T11:52:42+02:00</updated>
    <id>https://nielsberglund.com/2021/04/18/interesting-stuff---week-16-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="databases">Databases</h2>

<ul>
<li><a href="https://rockset.com/blog/rocksdb-is-eating-the-database-world/">RocksDB Is Eating the Database World</a>. This blog post looks at RocksDB, what it is and how it works. When reading the post, I was surprised by how many distributed databases used RocksDB as the layer to abstract access to local storage.</li>
</ul>

<h2 id="machine-learning-ai">Machine Learning / AI</h2>

<ul>
<li><a href="https://www.infoq.com/news/2021/04/microsoft-zero3-offload">Microsoft Releases AI Training Library ZeRO-3 Offload</a>. This <a href="https://www.infoq.com/">InfoQ</a> article looks at Microsoft&rsquo;s ZeRO-3 Offload, which is an extension of the DeepSpeed AI training library. ZeRO-3 Offload improves memory efficiency while training very large deep-learning models. It allows users to train models with up to 40 billion parameters on a single GPU and over 2 trillion parameters on 512 GPUs. Impressive!</li>
</ul>

<h2 id="azure-sql">Azure SQL</h2>

<ul>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2021/04/12/azure-sql-digital-event-innovate-today-with-azure-sql/">Azure SQL digital event: Innovate today with Azure SQL</a>. The post linked to here is an invitation to an Azure SQL event. The event looks at how to build an effective cloud database management strategy. I will definitely attend!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://youtu.be/-50rsy0qK9M">Kafka Connect 101: Configuration, Connectors, Converters, and Transforms</a>. This is an excellent video doing what the title says; it covers how Kafka Connect works! I found it very informative! Oh, and BTW - Tim and I are not related!</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 15, 2021]]></title>
    <link href="https://nielsberglund.com/2021/04/11/interesting-stuff---week-15-2021/" rel="alternate" type="text/html"/>
    <updated>2021-04-11T10:05:00+02:00</updated>
    <id>https://nielsberglund.com/2021/04/11/interesting-stuff---week-15-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="sql-server-2019-big-data-cluster">SQL Server 2019 Big Data Cluster</h2>

<ul>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2021/04/07/whats-new-with-sql-server-big-data-clusters-cu10-release/">What&rsquo;s new with SQL Server Big Data Clusters—CU10 release</a>. This post announces the release of SQL Server 2019 Big Data Cluster CU10 and some of the new and improved functionality. As soon as I have time, I will install it and &ldquo;take it for a ride&rdquo;.</li>
</ul>

<h2 id="data-architecture">Data Architecture</h2>

<ul>
<li><a href="https://towardsdatascience.com/data-discovery-the-future-of-data-catalogs-for-data-lakes-7b50e2e8cb28">Data Discovery: The Future of Data Catalogs for Data Lakes</a>. The post linked to here discusses how we can prevent our data lakes from becoming data swamps. The key to this is data discovery and data catalogs. I like this post, and it has given me a lot to think about.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://medium.com/apache-pinot-developer-blog/introduction-to-upserts-in-apache-pinot-987c12149d93">Introduction to Upserts in Apache Pinot</a>. In version 0.6 of Apache Pinot, a new feature was made available for stream ingestion, allowing you to upsert events from an immutable log. You may be familiar with upserts from the database world, however in Apache Pinot, an upsert is somewhat different than what you have in a database, and this post looks at what it is and why it is exciting.</li>
<li><a href="https://whylabs.ai/blog/posts/integrating-whylogs-into-your-kafka-ml-pipeline">Integrating whylogs into your Kafka ML Pipeline</a>. WhyLogs is an open-source data quality library that uses advanced data science statistics to log and monitor data used in AI/ML applications. This blog post looks at how we can integrate WhyLogs in Kafka to evaluate, monitor and detect statistical anomalies in streaming data. This is very interesting!</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 14, 2021]]></title>
    <link href="https://nielsberglund.com/2021/04/04/interesting-stuff---week-14-2021/" rel="alternate" type="text/html"/>
    <updated>2021-04-04T12:11:33+02:00</updated>
    <id>https://nielsberglund.com/2021/04/04/interesting-stuff---week-14-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="machine-learning-data-science-ai">Machine Learning / Data Science / AI</h2>

<ul>
<li><a href="https://medium.com/data-for-ai/building-real-time-ml-pipelines-with-a-feature-store-9f90091eeb4">Building Real-Time ML Pipelines with a Feature Store</a>. The term Feature Store is gaining popularity in the Machine Learning world. It is - as the name implies - something that stores feature data. However, it also runs pipelines that transform raw data into feature values, and it serves feature data for training and inference purposes. Most feature stores are batch-oriented, but they must move beyond batch and also become able to handle real-time data. This blog post looks at transitioning from batch to real-time.</li>
<li><a href="https://databricks.com/blog/2021/04/02/data-ai-summit-is-back.html">Data + AI Summit Is Back</a>. This post leads to a link for registration for the North American leg of <a href="https://databricks.com/dataaisummit/north-america-2021">Data + AI Summit</a>. The schedule looks awesome, and I&rsquo;ll definitely register!</li>
</ul>

<h2 id="data-architecture">Data Architecture</h2>

<ul>
<li><a href="https://medium.com/pinterest-engineering/open-sourcing-querybook-pinterests-collaborative-big-data-hub-ba2605558883">Open sourcing Querybook, Pinterest&rsquo;s collaborative big data hub</a>. Pinterest is a data-driven company, and it is more important than ever for teams to be able to compose queries, create analyses, and collaborate with one another. To enable that, Pinterest built Querybook. This post looks at what Querybook is and how they got to the point of open-sourcing it.</li>
<li><a href="https://databricks.com/blog/2021/03/31/top-questions-from-customers-about-delta-lake.html">Top Questions from Customers about Delta Lake</a>. Databricks Delta Lake is a hot topic, and many people have questions about it. This post aims to answer some of those questions.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/kafka-without-zookeeper-a-sneak-peek/">Apache Kafka Made Simple: A First Glimpse of a Kafka Without ZooKeeper</a>. There has been lots of talk about removing ZooKeeper as a dependency for Kafka. Finally, we are almost there, and the upcoming Kafka release will have the ability to run without ZooKeeper - yay! The blog post linked to looks at the implications of the removal and its impact on - among other things - scalability and performance (spoiler alert: improvements!). Very cool &ldquo;stuff&rdquo;!</li>
<li><a href="https://www.confluent.io/blog/monitor-kafka-clusters-with-prometheus-grafana-and-confluent/">Monitoring Your Event Streams: Integrating Confluent with Prometheus and Grafana</a>. Managing and monitoring a system like Kafka, is not and easy feat. But there are help; this is part 1 of a three-part blog series that will explain how to effectively monitor your event streams. This post looks at integration with third party tools such as Prometheus and Grafana.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 13, 2021]]></title>
    <link href="https://nielsberglund.com/2021/03/28/interesting-stuff---week-13-2021/" rel="alternate" type="text/html"/>
    <updated>2021-03-28T09:41:52+02:00</updated>
    <id>https://nielsberglund.com/2021/03/28/interesting-stuff---week-13-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://medium.com/event-driven-utopia/5-reasons-why-you-should-use-microsoft-dapr-to-build-event-driven-microservices-cb2202c579a0">5 Reasons Why You Should Use Microsoft Dapr to Build Event-driven Microservices</a>. Dapr is a portable, event-driven runtime, and it helps developers build event-driven, resilient distributed applications. As the name implies, the post linked to looks at some of the reasons to use <a href="https://dapr.io/">Dapr</a>.</li>
<li><a href="https://www.infoq.com/news/2021/03/alibaba-dapr/">Alibaba Cloud Uses Dapr to Support Its Business Growth</a>. I doubt that the Alibaba crowd read the post above, but they use Dapr extensively. This <a href="https://www.infoq.com/">InfoQ</a> post looks at some of the reasons why Alibaba chose Dapr. Quite interesting read!</li>
<li><a href="http://muratbuffalo.blogspot.com/2021/03/sundial-fault-tolerant-clock.html">Sundial: Fault-tolerant Clock Synchronization for Datacenters</a>. The post linked to here gives an excellent explanation of time synchronization challenges and fundamental techniques to achieve precise time synchronization in data centers.</li>
</ul>

<h2 id="sql-server">SQL Server</h2>

<ul>
<li><a href="https://techcommunity.microsoft.com/t5/sql-server/configure-sql-server-ag-read-scale-for-sql-containers-deployed/ba-p/2224742">Configure SQL Server AG (Read-Scale) for SQL Containers deployed on Kubernetes using Helm</a>. This post looks at how to setup Always On Availability Group (AG) between SQL instances deployed as SQL containers on Kubernetes. As a SQL old-timer, I still can&rsquo;t believe we can run SQL on Linux and in Kubernetes!</li>
</ul>

<h2 id="data-architecture">Data Architecture</h2>

<ul>
<li><a href="https://medium.com/nerd-for-tech/catching-data-in-a-data-mesh-principles-part-i-2b2e11e9e33a">Catching Data In a Data Mesh: Principles (Part I)</a>. This post is part of a series intending to give an in-depth view of how the Data Mesh architectural paradigm helps to build a data platform. This is part one, explaining the thinking about the mesh and the guiding principles for the implementation. After reading this post, be sure to read <a href="https://medium.com/codex/catching-data-in-a-data-mesh-applied-part-ii-114cae4d139a">part two</a> as well.</li>
<li><a href="https://rpradeepmenon.medium.com/making-data-lakehouse-real-yet-effective-f09e84fae0fa">Making Data Lakehouse real yet effective</a>. In previous roundups, I have linked to posts about data warehouses, data lakes, data lakehouses, etc. The post I link to here tries to explore and explain how all this hangs together.</li>
<li><a href="https://towardsdatascience.com/feature-store-data-platform-for-machine-learning-455122c48229">Feature Store: Data Platform for Machine Learning</a>. Feature stores are a relatively recent &ldquo;innovation&rdquo; in the Machine Learning world. A feature store is a data system that runs data pipelines that transform raw data into feature values, stores and manages the feature data, and serves feature data consistently for training and inference purposes. The post I link to here surveys the leading feature stores. As a side note: if you want to read more about feature stores, browse over to <a href="https://www.featurestore.org/">Feature Store for ML</a>.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/real-time-search-and-analytics-with-confluent-cloud-azure-redis-spring-cloud/">Integrating Azure and Confluent: Real-Time Search Powered by Azure Cache for Redis, Spring Cloud</a>. This article looks at an architecture pattern that transforms an existing traditional transaction system into a real-time data processing system. Don&rsquo;t get hung up on the components used (Redis Cache, Spring Cloud), as what the article covers is applicable for different tech-stacks as well.</li>
<li><a href="https://levelup.gitconnected.com/event-driven-architecture-demo-29f5649144b7">Event-Driven Architecture Demo</a>. As the title says, this post looks at a completely event-driven system, incorporating Kafka and GraphQL. Very cool!</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
</feed>

