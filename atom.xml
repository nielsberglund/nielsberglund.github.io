<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Niels Berglund</title>
  <link href="http://nielsberglund.com/atom.xml" rel="self"/>
  <link href="http://nielsberglund.com"/>
  <updated>2019-06-23T16:19:22+02:00</updated>
  <id>http://nielsberglund.com/</id>
  <generator uri="http://gohugo.io/">Hugo</generator>

  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 25, 2019]]></title>
    <link href="http://nielsberglund.com/2019/06/23/interesting-stuff---week-25-2019/" rel="alternate" type="text/html"/>
    <updated>2019-06-23T16:19:22+02:00</updated>
    <id>http://nielsberglund.com/2019/06/23/interesting-stuff---week-25-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://azure.microsoft.com/mediahandler/files/resourcefiles/kubernetes-learning-path/Kubernetes%20Learning%20Path%20version1.0.pdf">50 days from zero to hero with Kubernetes</a>. If you want to get to know Kubernetes this guide is for you! It will help you to get to know the basics of Kubernetes, and you also get hands-on experience with its various components, capabilities, and solutions.</li>
</ul>

<h2 id="misc">Misc.</h2>

<ul>
<li><a href="https://www.tbray.org/ongoing/When/201x/2019/06/12/Go-Creeping-In">Go Creeping In</a>. In this post, <a href="https://twitter.com/timbray">Tim Bray</a>, discusses the <a href="https://golang.org/">Go</a> language, and some of its features. Very interesting!</li>
</ul>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://devblogs.microsoft.com/dotnet/the-evolving-infrastructure-of-net-core/">The Evolving Infrastructure of .NET Core</a>. This blog post gives an interesting insight into the infrastructure history behind .NET Core.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.lightbend.com/blog/managing-streaming-and-queryable-state-in-spark-akka-streams-kafka-streams-flink">Managing Streaming And Queryable State In Spark, Akka Streams, Kafka Streams, And Flink</a>. This post takes a look at the built-in support for state management and queryable state, when available, or how they can be implemented in Apache Spark, Apache Flink, Kafka Streams, and Akka Streams.</li>
<li><a href="https://medium.com/thousandeyes-engineering/kafka-topics-pitfalls-and-insights-38bafc791a83">Kafka Topics: Pitfalls and Insights</a>. In this post, there are quite a few useful insights about Kafka topics. If you use Kafka in production, you should read this post.</li>
<li><a href="https://databricks.com/blog/2019/06/18/simplifying-streaming-stock-analysis-using-delta-lake-and-apache-spark-on-demand-webinar-and-faq-now-available.html">Simplifying Streaming Stock Analysis using Delta Lake and Apache Spark</a>. A blog post further expanding on a webinar on how to build streaming systems to analyze stock data in real-time, by using Databricks Delta Lake and Apache Spark.</li>
<li><a href="/2019/06/18/confluent-platform--kafka-for-a-.net-developer-on-windows/">Confluent Platform &amp; Kafka for a .NET Developer on Windows</a>. This is a blog post by yours truly, where I look at how to run Confluent Kafka in Docker on a Windows machine and how we write .NET Code against it.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Confluent Platform &amp; Kafka for a .NET Developer on Windows]]></title>
    <link href="http://nielsberglund.com/2019/06/18/confluent-platform--kafka-for-a-.net-developer-on-windows/" rel="alternate" type="text/html"/>
    <updated>2019-06-18T04:49:36+02:00</updated>
    <id>http://nielsberglund.com/2019/06/18/confluent-platform--kafka-for-a-.net-developer-on-windows/</id>
    <content type="html"><![CDATA[<p>As many of you know, I am an un-ashamed .NET developer on the Windows platform. Seeing that I do a lot of SQL Server development my development and OS platforms of choice is sufficient for my work, as I on my development box (Windows of course) install SQL Server Developer edition and whatever .NET framework I use.</p>

<p>That has been sufficient up until now when I want to develop against Kafka. At <a href="/derivco">Derivco</a> we are getting more and more interested in Kafka, and obviously, I want to install it so I can <del>play around</del> investigate it. However, to install it on my development machine as I would typically do with technologies I am interested in, SQL Server, RabbitMQ, etc., is difficult, if not impossible.</p>

<blockquote>
<p><strong>NOTE:</strong> Yes I know, there are articles on the web discussing how to run Kafka on Windows, but it is a hit and miss whether it works.</p>
</blockquote>

<p>A while ago I wrote a <a href="/2018/07/10/install-confluent-platform-kafka-on-windows/">post</a> about how to run Kafka under <strong>Windows Subsystem for Linux</strong> (WSL), and yes it works, but I have had issues with it, and to me, it is still a hack. So the options then (if we rule out WSL) are:</p>

<ul>
<li>Running it on a Linux in a virtualized environment, (Virtual Box, VMWare, Hyper-V, etc.).</li>
<li>Docker.</li>
</ul>

<p>The rest of this post goes through how to set up the <em>Confluent Platform</em> in a Docker environment and use it from .NET.</p>

<p></p>

<h2 id="pre-reqs">Pre-reqs</h2>

<p>The assumption is that you use a Windows box as your development machine, and you want to run Kafka on your box. Based on that assumption, these are the pre-reqs:</p>

<ul>
<li><a href="https://docs.docker.com/docker-for-windows/">Docker Desktop for Windows</a>. The install instructions and download link are <a href="https://docs.docker.com/docker-for-windows/install/">here</a>.</li>
<li>.NET Framework or .NET Core.</li>
<li>Your IDE of choice. I kind of like <a href="https://code.visualstudio.com/"><strong>VS Code</strong></a>.</li>
</ul>

<p>As I in this post use <em>Confluent Platform</em>, (more about that below), I need to allocate at least 8Gb of memory to Docker.</p>

<h2 id="confluent-platform"><em>Confluent Platform</em></h2>

<p>I mentioned above how in this post we install and use <em>Confluent Platform</em>. Some of you may ask why I use the enterprise edition, (which <em>Confluent Platform</em> is), which you need a license for and not the Community Edition which is open source and license free.</p>

<p>There are two reasons really for this:</p>

<ul>
<li>With <em>Confluent Platform</em> I get ALL the goodies, including <strong>Control Center</strong>.</li>
<li>With the introduction of <em>Confluent Platform</em> 5.2. Confluent <a href="https://www.confluent.io/blog/introducing-confluent-platform-5-2">announced</a> that <em>Confluent Platform</em> is &ldquo;free forever&rdquo; on a single Kafka broker! In other words, it is like a &ldquo;Developer Edition&rdquo; of <em>Confluent Platform</em>. That to me, is excellent, as I can now build awesome streaming and event-driven applications on Apache Kafka using the powerful capabilities of <em>Confluent Platform</em>.</li>
</ul>

<blockquote>
<p><strong>NOTE:</strong> I confess; I &ldquo;lifted&rdquo; parts of the last bullet point from the previously mentioned <a href="https://www.confluent.io/blog/introducing-confluent-platform-5-2">announcement</a>.</p>
</blockquote>

<h2 id="install-confluent-platform-using-docker">Install <em>Confluent Platform</em> Using Docker</h2>

<p>It is time to install the Docker images we need for <em>Confluent Platform</em>. All the images for the individual components of <em>Confluent Platform</em> are on <a href="https://hub.docker.com/u/confluentinc/">Docker Hub</a>, and we could get them from there. However, if we did that, then we would need to &ldquo;compose&rdquo; them together, and I am way too lazy for that.</p>

<blockquote>
<p><strong>NOTE:</strong> Docker Hub is a cloud-based repository for container images, to which organizations upload their container images.</p>
</blockquote>

<p>Instead of us grabbing the individual containers, we use a Docker Compose file that Confluent have been kind enough to create. For those of you who don&rsquo;t know what Docker Compose is, you can read more about it <a href="https://docs.docker.com/compose/overview/">here</a>.</p>

<p>We get the file by:</p>

<ul>
<li>Cloning the <a href="https://github.com/confluentinc/cp-docker-images"><em>Confluent Platform</em> Docker Images GitHub Repository</a>. That gives us a directory <code>cp-docker-images</code>.</li>
<li><code>cd</code> into the directory and check out the branch <code>5.2.1-post</code>: <code>git checkout 5.2.1-post</code>.</li>
</ul>

<p>After the check out the <code>cp-docker-images</code> directory looks like so:</p>

<p><img src="/images/posts/confluent_kafka_checkout1.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Directory Structure After Checkout</em></p>

<p>Notice in <em>Figure 1</em> how we have a directory named <code>examples</code>. Underneath this directory are directories for different examples, (who&rsquo;d &ldquo;thunk&rdquo;), of Kafka setups. We are interested in an example, (and directory), named <code>cp-all-in-one</code>:</p>

<p><img src="/images/posts/confluent_kafka_cp-all-in-one.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Example Directory</em></p>

<p>When we navigate to the <code>cp-all-in-one</code> we see something like what we see in <em>Figure 2</em>. Among the files in the directory, there is a <code>docker-compose.yml</code> file, which includes all of the <em>Confluent Platform</em> components. After opening the <code>docker-compose.yml</code> file in a text editor, we see what components make up the <em>Confluent Platform</em>:</p>

<ul>
<li>Zookeeper.</li>
<li>Kafka broker.</li>
<li>Schema registry.</li>
<li>Kafka connect.</li>
<li>Control center.</li>
<li>KSQL server.</li>
<li>KSQL CLI.</li>
<li>KSQL datagen.</li>
<li>Rest proxy.</li>
</ul>

<p>When we look at the list above, there is one component that we should not use in production: the <code>ksql-datagen</code> component. It is included for development and test purposes, and we can use it to generate data loads, and you can read more about it <a href="https://docs.confluent.io/current/ksql/docs/tutorials/generate-custom-test-data.html">here</a>.</p>

<blockquote>
<p><strong>NOTE:</strong> This post does not cover in any greater detail the various components. Stay tuned for future posts for that.</p>
</blockquote>

<p>While we look at the <code>docker-compose.yml</code> file, let us look a bit closer at the <code>broker</code> section, which describes the Kafka broker:</p>

<p><img src="/images/posts/confluent_kafka_docker_yml.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Kafka Broker</em></p>

<p>In <em>Figure 3</em> we see two areas outlined in red, and those two areas have to do with what ports the Kafka broker listens on. In a non-Docker Kafka installation, the port Kafka exposes is typically 9092, and clients, as well as internal components, can connect without any issues. However, in a Docker environment, things are somewhat different, as you have both a Docker internal network, as well as an external network (host machine to Docker containers, for example). That&rsquo;s why we define two <code>ports</code>, (29092 and 9092), and set up two listeners. The <code>broker:29092</code> is for the internal Docker network, and the <code>localhost:9092</code> is for external connections.</p>

<blockquote>
<p><strong>NOTE:</strong> <a href="https://twitter.com/rmoff">Robin Moffat</a>, who is a Kafka guru, has written a blog post about port addresses and listeners: <a href="https://rmoff.net/2018/08/02/kafka-listeners-explained/">Kafka Listeners - Explained</a>. If you are interested in Kafka, you should read that post, and whatever else Robin publishes. He knows his stuff!</p>
</blockquote>

<p>A final word before we install the Docker containers; please do not forget to increase memory for Docker to at least 8Gb:</p>

<p><img src="/images/posts/confluent_kafka_docker_memory.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Docker Memory</em></p>

<p>In <em>Figure 4</em> we see how we have increased the memory for Docker to 8192MB. After you do this, you need to restart Docker.</p>

<p>We are now ready for installation so <code>cd</code> into the <code>cp-all-in-one</code> directory. In there, from the command line, execute:</p>

<pre><code class="language-bash">$ docker-compose up -d --build
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Execute Docker Command</em></p>

<p>The code in <em>Code Snippet 1</em>:</p>

<ul>
<li>Pulls the different images for the <em>Confluent Platform</em> (if they are not on the machine already).</li>
<li>* The <code>--build</code> flag builds the Kafka Connect image together with the <code>datagen</code> connector.</li>
<li>Creates the containers and starts up the <em>Confluent Platform</em>.</li>
</ul>

<p>When you execute the command in <em>Code Snippet 1</em>, you see something like so:</p>

<p><img src="/images/posts/confluent_kafka_pull_images.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Pull Images</em></p>

<p>Unless the images for <em>Confluent Platform</em> exists on the machine, the <code>docker-compose</code> command starts to pull them from the registry, and in <em>Figure 5</em> we see how we initially pull ZooKeeper. Eventually, all images have been pulled and the containers created.</p>

<p><img src="/images/posts/confluent_kafka_create_images.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>Create Images</em></p>

<p>As we see in <em>Figure 6</em> we have now created the containers for <em>Confluent Platform</em>, and everything should be up and running. Let us first ensure that that is the case (containers running). We use the <code>docker-compose ps</code> command, which lists containers related to images declared in the <code>docker-compose.yml</code> file:</p>

<p><img src="/images/posts/confluent_kafka_container_status.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>Container Status</em></p>

<p>From what we see in <em>Figure 7</em> all containers have a <code>State</code> of <code>Up</code>, so everything should be good. That we see that the <code>State</code> is <code>Up</code> do not necessarily indicate that the individual components are up and running, (they most likely are), so to be on the safe side we can check that the Kafka broker is up.</p>

<p>We can confirm this in various ways, and here we use the command line and have a look at logs. For this, we use the <code>docker logs</code> command, which shows information logged by a running container: <code>docker logs &lt;container_name&gt;</code>. So the question is then what the <code>container_name</code> is for the Kafka broker? Well, if we look in the <code>docker-compose.yml</code> file, we see that for the Kafka broker we have a <code>container-name</code> of <code>broker</code>, and as we see in <em>Figure 6</em> we have a corresponding <code>broker</code> container. With this in mind, the <code>logs</code> command looks like so:</p>

<pre><code class="language-bash">$ docker logs broker | Select-String -Pattern 'Started'
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>View Logs</em></p>

<p>I use PowerShell for this, and I also do a <code>grep</code> like selection by the <code>Select-String</code> command-let, which we see in <em>Code Snippet 2</em>. The reason for this is to filter out what the <code>logs</code> command returns. When I run it, I see:</p>

<p><img src="/images/posts/confluent_kafka_broker_started.png" alt="" /></p>

<p><strong>Figure 8:</strong> <em>Kafka Broker Started</em></p>

<p>Cool, from what we see in <em>Figure 8</em> it looks like the Kafka broker is up and running! Now if we want to, we can go ahead and create topics etc. Once again, there are various ways we can do this. One way is to do it from the command line; we spin up the <code>bash</code> shell in the Kafka broker container and use: <code>kafka-topics --create ...</code>. However, since I said one big reason for me to use <em>Confluent Platform</em> is <em>Control Center</em>, let us create topics via <em>Control Center</em>.</p>

<h2 id="control-center">Control Center</h2>

<p><em>Control Center</em> is a web UI for managing and monitoring Kafka. It does a lot more though than just managing/monitoring the Kafka broker. With <em>Control Center</em> you can manage and monitor:</p>

<ul>
<li>Data Streams</li>
<li>System Health</li>
<li>Configuration of Kafka Connect</li>
<li>and more &hellip;</li>
</ul>

<p>In a Docker installation, you find where to load the UI from by looking at the <code>Ports</code> column for <code>control-center</code> after you run <code>docker-compose ps</code>. In <em>Figure 6</em> we see <code>control-center</code> exposed by port 9021. So let us, in our favourite browser, browse to: <code>localhost:9021</code>:</p>

<p><img src="/images/posts/confluent_kafka_control_center_ui.png" alt="" /></p>

<p><strong>Figure 9:</strong> <em>Control Center</em></p>

<p>In <em>Figure 9</em> we see the user interface for <em>Control Center</em>. We see charts showing information about our data pipelines, and on the left a menu where we can choose between different functions of <em>Control Center</em>. For now, as we want to create a topic, what we are interested in, is the <strong>Topics</strong> menu, outlined in red. We click on it, and we see a screen like so:</p>

<p><img src="/images/posts/confluent_kafka_control_center_topics1.png" alt="" /></p>

<p><strong>Figure 10:</strong> <em>Control Center Topics</em></p>

<p>We see in <em>Figure 10</em> the <strong>Topics</strong> screen, and some pre-configured topics (this is a new Kafka installation). To create a topic we click on the <em>+ Create topic</em> button, (outlined in red), on the right-hand side of the screen:</p>

<p><img src="/images/posts/confluent_kafka_control_center_create_topic.png" alt="" /></p>

<p><strong>Figure 11:</strong> <em>Control Center New Topic</em></p>

<p>In <em>Figure 11</em> we see the <strong>New topic</strong> screen, and we see - outlined in blue - the area where we define the topic name and how many partitions we want for the topic. I am about to create a new topic: <code>testTopic</code> with default settings and a partition count of 1. I then click on the <em>Create with defaults</em> button, (outlined in red), and I go back to the <em>Topics</em> screen:</p>

<p><img src="/images/posts/confluent_kafka_control_center_testTopic.png" alt="" /></p>

<p><strong>Figure 12:</strong> <em>New Topic</em></p>

<p>We see in <em>Figure 12</em> our new <code>testTopic</code>, and we can now start to use it.</p>

<p>Before we look at how to publish and consume messages with .NET, let us make sure the topic actually works. We do that by using a couple of Kafka command line tools that ship with any Kafka installation. In fact Kafka ships with quite a few command line tools, (we spoke above of one of them: <code>kafka-topics</code>), and the two we use here are:</p>

<ul>
<li><code>kafka-console-consumer</code>: reads data from a Kafka topic and writes the data to standard output.</li>
<li><code>kafka-console-producer</code>: the opposite of <code>kafka-console-consumer</code>, it reads data from standard output and writes it to a Kafka topic.</li>
</ul>

<p>To use the two tools we:</p>

<ul>
<li>Open two command prompt windows.</li>
<li>In both windows, <code>docker exec</code> into the <code>bash</code> shell of the Kafka broker container:</li>
</ul>

<pre><code class="language-bash">$ docker exec -it broker bash
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Execute into Bash Shell</em></p>

<p>We see in <em>Code Snippet 3</em> the code to get into the Docker container&rsquo;s shell, and when we execute the code in both command windows we see:</p>

<p><img src="/images/posts/confluent_kafka_command_windows1.png" alt="" /></p>

<p><strong>Figure 13:</strong> <em>Docker Container Shell</em></p>

<p>As we see in <em>Figure 13</em> we are now in the shell as <code>root</code>. The reason we have &ldquo;spun&rdquo; up two command windows is so we can use one for publishing and the other for consuming messages.</p>

<p>Let us start with the command window for consumption:</p>

<pre><code class="language-bash">$ cd /usr/bin
$ ./kafka-console-consumer --bootstrap-server broker:29092 --topic testTopic
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Console Consumer</em></p>

<p>In <em>Code Snippet 4</em> we first <code>cd</code> into the <code>/usr/bin</code> directory where the Kafka command line tools are. We then execute the <code>kafka-console-consumer</code> command, where we say what broker and topic we want to connect to. Notice how we define the internal Docker connection here, (<code>broker:29092</code>), as this is on the internal network.</p>

<p>When we execute the code in <em>Code Snippet 4</em>, we see how the command window now waits for data to read from the broker. Now we set up the producer in the same way (in the second command window):</p>

<pre><code class="language-bash">$ cd /usr/bin
$ ./kafka-console-producer --broker-list broker:29092 --topic testTopic
</code></pre>

<p><strong>Code Snippet 5:</strong> <em>Console Producer</em></p>

<p>When we execute the code in <em>Code Snippet 5</em> we see how the command window waits for input. We enter some strings, and we see how the &ldquo;consume&rdquo; window reads the data from the broker topic and writes it to standard output.</p>

<p><img src="/images/posts/confluent_kafka_publish_consume.png" alt="" /></p>

<p><strong>Figure 14:</strong> <em>Publish Consume</em></p>

<p>The upper window we see in <em>Figure 14</em> is the publish window, and we see how I entered three strings: <code>Hello</code>, <code>World</code>, and <code>Again</code>, and how they appear in the second window. This simple test shows that our topic works, and we can now move on to .NET.</p>

<h2 id="net">.NET</h2>

<p>For the .NET part I use <em>VS Code</em> and .NET Core (2.2). As I am a &ldquo;newbie&rdquo; when it comes to <em>VS Code</em> and .NET Core, I list the steps I to get up and running so I have something to come back to.</p>

<blockquote>
<p><strong>NOTE:</strong> Actually, I am pretty much a newbie in all topics in this post.</p>
</blockquote>

<p>I start with the publisher, and I:</p>

<ul>
<li>Create a new folder for my publisher project.</li>
<li>In <em>VS Code</em> I open that folder.</li>
</ul>

<p>From the integrated terminal in <em>VS Code</em>, (<strong>View &gt; Integrated Terminal</strong>), I create a new console project: <code>dotnet new console</code>:</p>

<p><img src="/images/posts/confluent_kafka_vscode_new_console.png" alt="" /></p>

<p><strong>Figure 15:</strong> <em>New Console Project</em></p>

<p>When I execute the outlined statement in <em>Figure 15</em>, some files get created in the chosen directory:</p>

<p><img src="/images/posts/confluent_kafka_vscode_created_project.png" alt="" /></p>

<p><strong>Figure 16:</strong> <em>Newly Created Project</em></p>

<p>What we see in <em>Figure 16</em> are the various project related files, including the source file <code>Program.cs</code>. What is missing now is a Kafka client. For .NET there exists a couple of clients, and theoretically, you can use any one of them. However, in practice, there is only one, and that is the <a href="https://github.com/confluentinc/confluent-kafka-dotnet"><strong>Confluent Kafka DotNet</strong></a> client. The reason I say this is because it has the best parity with the original Java client. The client has NuGet packages, and you install it via <em>VS Code</em>&rsquo;s integrated terminal: <code>dotnet add package Confluent.Kafka --version 1.0.1.1</code>:</p>

<p><img src="/images/posts/confluent_kafka_vscode_install_nuget.png" alt="" /></p>

<p><strong>Figure 17:</strong> <em>Install NuGet Package</em></p>

<p>When you execute the <code>dotnet add package</code> the result is as we see in <em>Figure 17</em>; <em>VS Code</em> downloads necessary files and then installs the package, (outlined in blue). We can now &ldquo;code up&rdquo; our publisher.</p>

<blockquote>
<p><strong>NOTE:</strong> The code here is purely for demonstration purposes, no error handling, etc.</p>
</blockquote>

<h4 id="publish">Publish</h4>

<p>When publishing messages to Kafka with the Confluent .NET client, you need an instance of a <code>Publisher</code> class. When creating a <code>Publisher</code>, you need a <code>PublisherConfig</code> class which - as the name implies - configures the <code>Publisher</code>. In the configuration, you set up things like:</p>

<ul>
<li>Bootstrap servers - a list of brokers for the client to connect to.</li>
<li>Retries.</li>
<li>Max message sizes.</li>
<li>etc., etc.</li>
</ul>

<p>To create the <code>Publisher</code>, you use the <code>ProducerBuilder</code> class which expects a <code>PublisherConfig</code> in the constructor. The code to create a <code>Publisher</code> looks something like so:</p>

<pre><code class="language-csharp">using System;
using System.Threading;
using Confluent.Kafka;

namespace publisher
{
  class KafkaPublishTest
  {
    static readonly AutoResetEvent _closing = new AutoResetEvent(false);

    static IProducer&lt;string, string&gt; producer = null;
    static ProducerConfig producerConfig = null;

    static void Main(string[] args){}
          
    static void CreateConfig() {
      producerConfig = new ProducerConfig
      {
        BootstrapServers = &quot;localhost:9092&quot;
      };
    }

    static void CreateProducer() {
      var pb = new ProducerBuilder&lt;string, string&gt;(producerConfig);
      producer = pb.Build();
    }
  }
}

</code></pre>

<p><strong>Code Snippet 6:</strong> <em>PublisherConfig and ProducerBuilder</em></p>

<p>The code we see in <em>Code Snippet 6</em> is the beginning of the publish application. The <code>Main</code> method is not &ldquo;hooked&rdquo; up yet; we do that later. We see how we have a <code>using</code> statement for the <code>Confluent.Kafka</code> namespace, and how we declare two class variables of the types <code>IProducer</code>, and <code>ProducerConfig</code>. In the method <code>CreateConfig</code> we instantiate <code>ProducerConfig</code> and set the <code>BootstrapServer</code> property to our Kafka broker. Notice how we use the external listener port, as we now connect into the Docker container from outside the Docker internal network. Oh, the <code>AutoResetEvent</code> class variable is used to react on <code>CTRL-C</code> key press to exit the application.</p>

<p>The <code>producerConfig</code> is then used in the <code>CreateProducer</code> method where we see how we use the <code>Build</code> method on <code>ProducerBuilder</code> to get an <code>IProducer</code> instance.</p>

<p>Having a producer, we now code the method to publish messages:</p>

<pre><code class="language-csharp">using System;
using System.Threading;
using Confluent.Kafka;

namespace publisher
{
  class KafkaPublishTest
  {
    static readonly AutoResetEvent _closing = new AutoResetEvent(false);

    static IProducer&lt;string, string&gt; producer = null;
    static ProducerConfig producerConfig = null;

    static void Main(string[] args){}
          
    static void CreateConfig() {...}

    static void CreateProducer() {...}

    static async void SendMessage(string topic, string message) {
      var msg = new Message&lt;string, string&gt; {
          Key = null,
          Value = message
      };

      var delRep = await producer.ProduceAsync(topic, msg);
      var topicOffset = delRep.TopicPartitionOffset;

      Console.WriteLine($&quot;Delivered '{delRep.Value}' to: {topicOffset}&quot;);
    }

  }
}

</code></pre>

<p><strong>Code Snippet 7:</strong> <em>Publishing a Message</em></p>

<p>The method to publish messages is <code>SendMessage</code> as we see in <em>Code Snippet 7</em>. The method takes two parameters; the topic we want to send to, and the actual message we want to send.</p>

<p>In the method, we create an instance of the <code>Message</code> class. That class has two properties:</p>

<ul>
<li><code>Key</code> - this is for if/when our topic has multiple partitions. It refers to the value we want to use for Kafka to decide what partition to target. In our case, we have not defined partitions, so we set the value to <code>null</code>.</li>
<li><code>Value</code> - the message we want to send. In the code, we set it to the incoming <code>message</code> parameter.</li>
</ul>

<p>To publish the message, we call the <code>ProduceAsync</code> method, which expects a topic name, and an instance of the <code>Message</code> class. The method returns an instance of the <code>DeliveryReport</code> class. This class contains information about the delivery of the message, and we are interested to see the original message and partition and offset it was sent to. This, we then write out to the console.</p>

<p>The final thing to do is to &ldquo;hook up&rdquo; everything in the <code>Main</code> method:</p>

<pre><code class="language-csharp">using System;
using System.Threading;
using Confluent.Kafka;

namespace publisher
{
  class KafkaPublishTest
  {
    static readonly AutoResetEvent _closing = new AutoResetEvent(false);

    static IProducer&lt;string, string&gt; producer = null;
    static ProducerConfig producerConfig = null;

    static void Main(string[] args)
    {
      CreateConfig();
      CreateProducer();
      SendMessage(&quot;testTopic&quot;, &quot;This is a test42&quot;);
      Console.WriteLine(&quot;Press Ctrl+C to exit&quot;);
      
      Console.CancelKeyPress += new ConsoleCancelEventHandler(OnExit);
      _closing.WaitOne();
    }
    
    static void OnExit(object sender, ConsoleCancelEventArgs args)
    {
      Console.WriteLine(&quot;Exit&quot;);
      _closing.Set();
    }
          
    static void CreateConfig() {...}

    static void CreateProducer() {...}

    static async void SendMessage(string topic, string message) {...}

  }
}

</code></pre>

<p><strong>Code Snippet 8:</strong> <em>Main Method</em></p>

<p>In <em>Code Snippet 8</em> we see how we, in <code>Main</code>, call the different methods. We also have some code to capture <code>CTRL-C</code> to exit the application. We should now be able to publish messages to the broker. We can test that this works without having a consumer, and ensure that we get an offset back from the delivery report.</p>

<blockquote>
<p><strong>NOTE:</strong> If you want to you can also use <code>kafka-console-consumer</code> and the code in <em>Code Snippet 4</em>.</p>
</blockquote>

<p>To make sure everything works, we use the integrated terminal in <em>VS Code</em> and execute <code>dotnet build</code>, followed by <code>dotnet run</code>:</p>

<p><img src="/images/posts/confluent_kafka_vscode_build_run_publish.png" alt="" /></p>

<p><strong>Figure 18:</strong> <em>Build and Run</em></p>

<p>What we see in <em>Figure 18</em> is how the <code>build</code> command succeeded, (sections outlined in yellow), and <code>run</code> also succeeded, (sections outlined in red). The delivery report says that the message was delivered to the topic <code>testTopic</code> on partition 0, and the offset for the message is 12, (@12).</p>

<p>Cool, let us now create an application to consume messages.</p>

<h4 id="consume">Consume</h4>

<p>To consume messages, we create a new application much along the lines of what we did for the publishing app:</p>

<ul>
<li>Create a new folder for the consumer project.</li>
<li>Open that folder in <em>VS Code</em>.</li>
<li>In <em>VS Code</em>&rsquo;s integrated terminal create a new console application: <code>dotnet new console</code>.</li>
<li>After the project has been created, add the <a href="https://github.com/confluentinc/confluent-kafka-dotnet"><strong>Confluent Kafka DotNet</strong></a> client to the project as we did above.</li>
</ul>

<p>The way we create a Kafka consumer is more or less the same way we did with the publisher; we have a configuration, <code>ConsumerConfig</code>, and a builder: <code>ConsumerBuilder</code>:</p>

<pre><code class="language-csharp">using System;
using System.Threading;
using Confluent.Kafka;

namespace consumer
{
  class KafkaConsumer
  {
    static CancellationTokenSource cts = new CancellationTokenSource();
    static ConsumerConfig consumerConfig = null;
    static void Main(string[] args)
    {
       CreateConfig();
       CreateConsumerAndConsume();
    }

    static void CreateConfig() {
        consumerConfig = new ConsumerConfig {
            BootstrapServers = &quot;localhost:9092&quot;,
            GroupId = &quot;test-group&quot;,
            AutoOffsetReset = AutoOffsetReset.Earliest
        };
    }

    static void CreateConsumerAndConsume() {

        var cb = new ConsumerBuilder&lt;string, string&gt;(consumerConfig);
                  
        Console.WriteLine(&quot;Press Ctrl+C to exit&quot;);
                   
        Console.CancelKeyPress += new ConsoleCancelEventHandler(OnExit);
                
        using(var consumer = cb.Build() ) {
          consumer.Subscribe(&quot;testTopic&quot;);

          try {
            while(!cts.IsCancellationRequested) {
              var cr = consumer.Consume(cts.Token);
              var offset = cr.TopicPartitionOffset
              Console.WriteLine($&quot;Message '{cr.Value}' at: '{offset}'.&quot;);
            }
          }
          catch (Exception e) {
            Console.WriteLine(e.Message);
            consumer.Close();
          }
        }
    }

    static void OnExit(object sender, ConsoleCancelEventArgs args)
    {
      args.Cancel = true;
      Console.WriteLine(&quot;In OnExit&quot;);
      cts.Cancel();
        
    }
  }
}
</code></pre>

<p><strong>Code Snippet 9:</strong> <em>Consumer Code</em></p>

<p>We see in <em>Code Snippet 9</em> the two interesting methods <code>CreateConfig</code> and <code>CreateConsumerAndConsume</code>. In <code>CreateConfig</code> we set three properties:</p>

<ul>
<li><code>BootstrapServers</code> - as for the publisher, this is a list of brokers to connect to.</li>
<li><code>GroupId</code> - the <code>GroupId</code> is the name of the consumer group you connect as to the broker. The <a href="https://www.oreilly.com/library/view/kafka-the-definitive/9781491936153/ch04.html">article here</a> explains consumer groups fairly well.</li>
<li><code>AutoOffsetReset</code> - this tells Kafka where to start reading offsets from.</li>
</ul>

<p>In <code>CreateConsumerAndConsume</code> we:</p>

<ul>
<li>Create a <code>ConsumerBuilder</code> based on the <code>ConsumerConfig</code> instance we created above.</li>
<li>Subscribe to the topic(s) we are interested in.</li>
<li>Consume in a <code>while</code> loop.</li>
</ul>

<p>The <code>Consume</code> method returns a <code>ConsumeResult</code> instance which we use to print information from to the console. What is left now is to build and run from the integrated terminal as we did with the publisher:</p>

<p><img src="/images/posts/confluent_kafka_vscode_build_run_consumer.png" alt="" /></p>

<p><strong>Figure 19:</strong> <em>Build and Run Consumer</em></p>

<p>Dependent on the <code>AutoOffsetReset</code> value you now either see a list of messages (if there are any), or the consumer &ldquo;sits&rdquo; and wait for messages to arrive (as in <em>Figure 19</em>). Let us finish this post with sending some messages from the publish application above. In my code, I changed the <code>Main</code> method to instead of just sending one message, send messages in a <code>while</code> loop:</p>

<pre><code class="language-csharp">while(x &lt; 100) {
  SendMessage(&quot;testTopic&quot;, $&quot;This test: {x}&quot;);
  x++;
  Thread.Sleep(200);
}
</code></pre>

<p><strong>Code Snippet 10:</strong> <em>Loop and Send Messages</em></p>

<p>In <em>Code Snippet 10</em> we see the <code>while</code> loop, and how we send 100 messages, pausing 200ms between each message. The reason I pause is that it makes it clearer to see what happens when I run the code. I build the project, and when I run the code I see in the publisher terminal:</p>

<p><img src="/images/posts/confluent_kafka_publish_messages.png" alt="" /></p>

<p><strong>Figure 20:</strong> <em>Publishing Messages</em></p>

<p>At the same time, I see in the consumer terminal:</p>

<p><img src="/images/posts/confluent_kafka_consume_messages.png" alt="" /></p>

<p><strong>Figure 21:</strong> <em>Consuming Messages</em></p>

<p>So from what we see in *Figure 20, and <em>Figure 21</em> both the publishing application, as well as the consuming application, works! Awesome!</p>

<h2 id="summary">Summary</h2>

<p>In this blog post, we saw how we can install Docker containers for the <em>Confluent Platform</em> on a Windows development machine. Thanks to the announcement of one broker &ldquo;free forever&rdquo; I have the ability to write streaming applications fully utilizing the <em>Confluent Platform</em>.</p>

<p>In the second part of the post we saw how we can use <em>VS Code</em> together with the <em>Confluent Kafka DotNet</em> client.</p>

<p>In future posts, I will &ldquo;dig&rdquo; deeper into both Kafka as well as the .NET client.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 24, 2019]]></title>
    <link href="http://nielsberglund.com/2019/06/16/interesting-stuff---week-24-2019/" rel="alternate" type="text/html"/>
    <updated>2019-06-16T11:12:38+02:00</updated>
    <id>http://nielsberglund.com/2019/06/16/interesting-stuff---week-24-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="databases">Databases</h2>

<ul>
<li><a href="https://medium.com/yugabyte/why-distributed-sql-beats-polyglot-persistence-for-building-microservices-c19dc76b16d0">Why Distributed SQL Beats Polyglot Persistence for Building Microservices?</a>. It is a common belief, (misconception?), that in the microservices world, each microservice should have its own persistence store - polyglot persistence. The blog post I link to here highlights the loss of agility that microservices development and operations suffer when adopting polyglot persistence. The post reviews how distributed SQL serves as an alternative approach that doesn&rsquo;t compromise this agility.</li>
</ul>

<h2 id="misc">Misc.</h2>

<ul>
<li><a href="https://www.paraesthesia.com/archive/2019/06/12/windows-and-linux-docker-containers-side-by-side/">Windows and Linux Docker Containers: Side by Side!</a>. Docker is awesome! One thing though, it is near impossible to run Windows and Linux containers side by side. Wouldn&rsquo;t it be great if you could? That is what this blog post discusses! Good stuff!</li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/petastorm-ml-pipelines/">Petastorm: A Light-Weight Approach to Building ML Pipelines</a>. This is an <a href="https://www.infoq.com/">InfoQ</a> presentation describing how <a href="https://eng.uber.com/petastorm/">Petastorm</a> facilitates tighter integration between Big Data and Deep Learning worlds; simplifies data management and data pipelines; and speeds up model experimentation.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>Right now, I am busy finishing off a blog post looking at the Confluent Platform, (Kafka), from the perspective of a .NET developer on the Windows platform. I hope to publish the post sometime this coming week.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 23, 2019]]></title>
    <link href="http://nielsberglund.com/2019/06/09/interesting-stuff---week-23-2019/" rel="alternate" type="text/html"/>
    <updated>2019-06-09T06:38:03+02:00</updated>
    <id>http://nielsberglund.com/2019/06/09/interesting-stuff---week-23-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="sql-server">SQL Server</h2>

<ul>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2019/06/06/the-june-release-of-azure-data-studio-is-now-available/">The June release of Azure Data Studio is now available</a>. What the title says. You can now download the June release of Azure Data Studio, or, if you already have it installed, it should upgrade automatically. There are quite a few highlights in this release. The one that interests me the most is the improvements in SQL Notebooks</li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://blog.acolyer.org/2019/06/03/ease-ml-ci/">Continuous integration of machine learning models with ease.ml/ci</a>. The white paper <a href="https://twitter.com/adriancolyer">Adrian</a> dissects here is about what does a continuous integration testing environment look like for a machine learning model? The paper presents <strong>ease.ml/ci</strong> which is a continuous integration system for machine learning.</li>
<li><a href="https://azure.microsoft.com/en-us/blog/build-more-accurate-forecasts-with-new-capabilities-in-automated-machine-learning/">Build more accurate forecasts with new capabilities in automated machine learning</a>. This blog post discusses new capabilities in Azure Machine Learning service related to time-series forecasting. Very interesting!</li>
<li><a href="https://blog.acolyer.org/2019/06/05/data-validation-for-machine-learning/">Data validation for machine learning</a>. Machine learning is good and all, and you can achieve a lot with ML. However, unless the data passed into ML is correct, you only get garbage. The paper <a href="https://twitter.com/adriancolyer">Adrian</a> dissects in this post focuses on the problem of validation the input data fed to ML pipelines.</li>
</ul>

<h2 id="sql-server-2019">SQL Server 2019</h2>

<ul>
<li><a href="/2019/06/06/sql-server-2019-extensibility-framework--external-languages/">SQL Server 2019 Extensibility Framework &amp; External Languages</a>. A couple of weeks ago, I wrote a <a href="/2019/05/26/java--sql-server-2019-extensibility-framework-the-sequel/">blog post</a> about changes in SQL Server 2019 CTP 2.5 related to how you write Java code for use by SQL Server. Well, a bit later, Microsoft released SQL Server 2019 CTP 3.0 with more changes around this topic (Java code in SQL Server), and in this blog post, I discuss these changes.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SQL Server 2019 Extensibility Framework &amp; External Languages]]></title>
    <link href="http://nielsberglund.com/2019/06/06/sql-server-2019-extensibility-framework--external-languages/" rel="alternate" type="text/html"/>
    <updated>2019-06-06T05:37:52+02:00</updated>
    <id>http://nielsberglund.com/2019/06/06/sql-server-2019-extensibility-framework--external-languages/</id>
    <content type="html"><![CDATA[<p>A little while ago I wrote a blog post, <a href="/2019/05/26/java--sql-server-2019-extensibility-framework-the-sequel/"><strong>Java &amp; SQL Server 2019 Extensibility Framework: The Sequel</strong></a>, about changes in SQL Server 2019 CTP 2.5 impacting how we write Java code for use from SQL Server. While I wrote that post, Microsoft released SQL Server 2019 CTP 3.0, and, (surprise, surprise), that release contains more changes affecting Java code in SQL Server.</p>

<p>This post covers those changes as well as discusses what SQL Server Extensibility Framework and Language Extensions are.</p>

<p></p>

<p>Before we &ldquo;dive&rdquo; into the &ldquo;nitty-gritty&rdquo; let look at the data we use in this post.</p>

<h2 id="demo-data">Demo Data</h2>

<p>The data we see here is for you who want to &ldquo;code along&rdquo;. It is lifted from the <a href="/2019/05/26/java--sql-server-2019-extensibility-framework-the-sequel/">post</a> mentioned above:</p>

<pre><code class="language-sql">USE master;
GO
SET NOCOUNT ON;
GO
DROP DATABASE IF EXISTS JavaTestDB;
GO
CREATE DATABASE JavaTestDB;
GO
USE JavaTestDB;
GO

GO
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Create Database</em></p>

<p>We see from <em>Code Snippet 1</em> how we create a database where we want to run some Java code from.</p>

<h2 id="background">Background</h2>

<p>In SQL Server 2016, Microsoft introduced SQL Server R Services. That allowed you to, from inside SQL Server, call to the R engine via a special procedure, <code>sp_execute_external_script</code>, and execute R scripts. The R engine was, (and is), part of the SQL server installation but it runs as an external process, (not in SQL Server&rsquo;s process), and subsequently, R is seen as an external language.</p>

<p>In SQL Server 2017, Microsoft added Python as an external language and renamed SQL Server R Services to SQL Server Machine Learning Services. The way Python works in SQL Server is the same as R:</p>

<ul>
<li>The Python engine is included in the SQL Server installation.</li>
<li>You execute Python code using the <code>sp_execute_external_script</code>.</li>
<li>Python runs in an external process.</li>
</ul>

<p>The communication between SQL Server and the external engine goes over the <em>Launchpad</em> service:</p>

<p><img src="/images/posts/sql_r_services_ect_script1.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>External Script and Language</em></p>

<p>We see in <em>Figure 1</em> how:</p>

<ul>
<li>We execute the procedure <code>sp_execute_external_script</code>.</li>
<li>That calls into the <em>Launchpad</em> service.</li>
<li>The <em>Launchpad</em> service passes the script into the relevant launcher based on the <code>@language</code> parameter in <code>sp_execute_external_script</code>. The knowledge of what launcher to call lives inside of the <em>Launchpad</em> service.</li>
<li>The launcher dll loads the relevant external engine, passes the script to the engine and executes.</li>
</ul>

<p>The above is a very high-level overview of how it works, and you can read more about the inner workings of it in <a href="/sql_server_2k16_r_services"><strong>SQL Server R Services</strong></a>.</p>

<p>So, a launcher dll is a piece of code, typically written in C++, who knows how to interact with the external engine.</p>

<p>After the introduction of Python in SQL Server 2017, the documentation started to mention how R and Python code runs in an extensibility framework, which is isolated from the core engine processes. Around this time, Microsoft started to mention the possibility of other languages becoming part of the extensibility framework.</p>

<h2 id="sql-server-2019-java">SQL Server 2019 &amp; Java</h2>

<p>At the time of SQL Server 2017 and the inclusion of Python, the extensibility framework was more just a name or - at least - it was purely some internal Microsoft SQL Server code. It was nothing that you and I could use directly. Then came SQL Server 2019.</p>

<p>In CTP 2.0 of SQL Server 2019, Microsoft made Java publicly available as an external language. Having Java as an external language may not seem that much different from R/Python, but there are some differences:</p>

<ul>
<li>Java is a compiled language, where we call into a specific method. R/Python are scripting languages where we send a script to the engine.</li>
<li>R/Python are part of the SQL Server install, together with launcher dll&rsquo;s and so forth. For Java, there is an equivalent of a launcher dll, (<code>javaextension.dll</code>), which calls into the JVM. The difference here between R/Python and Java is that the JVM is not part of the SQL Server install but must be installed separately.</li>
</ul>

<p>What Microsoft could have done with the Java integration in SQL Server 2019 was to just treat it as R/Python, and &ldquo;hardcode&rdquo; Java as a language in the <em>Launchpad</em> service and let the <em>Launchpad</em> service call the <code>javaextension.dll</code>.</p>

<blockquote>
<p><strong>NOTE:</strong> There are most likely quite substantial differences between the <code>javaextension.dll</code> and the R/Python launcher dll&rsquo;s, but in his post, I treat them as being more or less equivalent.</p>
</blockquote>

<p>However, Microsoft did not &ldquo;hack&rdquo; the <em>Launchpad</em> service, but what they did was, with the view to &ldquo;properly&rdquo; expose an extensibility framework with multiple external languages, that they introduced some new components (hosts). The <em>Launchpad</em> service calls these hosts for all languages except R/Python. Yes, yes, I do know that for now (we are now at CTP 3.0), it is only Java, but&hellip;</p>

<blockquote>
<p><strong>NOTE:</strong> In future posts I will talk more about these new components.</p>
</blockquote>

<p>Having read this far in the post you may say: <em>Hey Niels, this is all interesting and all, but you have not said anything we don&rsquo;t already know</em>.</p>

<h2 id="external-language">External Language</h2>

<p>Ok, so let us see what this is all about. Remember how we, in the <a href="/2019/05/26/java--sql-server-2019-extensibility-framework-the-sequel/"><strong>Java &amp; SQL Server 2019 Extensibility Framework: The Sequel</strong></a> post, discussed how SQL Server CTP 2.5 introduced a Java SDK, <code>mssql-java-lang-extension.jar</code>, that we as developers need to develop against when we write Java code we want to execute from SQL Server. That is a requirement in CTP 3.0 as well, but the way you get the <code>.jar</code> file is different. In CTP 2.5 you downloaded the file, whereas in CTP 3.0 the file is part of the SQL Server distribution, and you find it at: <code>..\&lt;path_to_sql_instance&gt;\MSSQL\Binn\mssql-java-lang-extension.jar</code>.</p>

<blockquote>
<p><strong>NOTE:</strong> The file name of the SDK is somewhat misleading as it is not the Java language extension itself, it is the SDK for the Java language extension.</p>
</blockquote>

<p>We know from the <a href="/2019/05/26/java--sql-server-2019-extensibility-framework-the-sequel/">post mentioned above</a> that we need to create an external library based on the <code>.jar</code> file, so I copy the file to a more accessible location and then:</p>

<pre><code class="language-sql">USE JavaTestDB;
GO

CREATE EXTERNAL LIBRARY javaSDK 
FROM (CONTENT = 'W:\mssql-java-lang-extension.jar')
WITH (LANGUAGE = 'Java');
GO
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Create External SDK Library</em></p>

<p>In CTP 2.5 the code in <em>Code Snippet 2</em> runs just fine, but when we run it in CTP 3.0 we get an exception:</p>

<p><img src="/images/posts/sql_2k19_ext_lang_error1.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Exception when Creating External Library</em></p>

<p>The exception we see in <em>Figure 2</em> is due to one of the changes in CTP 3.0: an external language needs to be &ldquo;registered&rdquo; with SQL Server before we can reference it. Registering a language with SQL Server allows Microsoft and/or 3rd parties to expose arbitrary languages to be used from SQL Server.</p>

<p>What we register is the actual language extension file for that particular language, together with a name for the language.</p>

<h2 id="create-external-language">Create External Language</h2>

<p>The way we register/create an external language is similar to how we create an external library; we use a <code>CREATE EXTERNAL ...</code> DDL statement: <code>CREATE EXTERNAL LANGUAGE</code>:</p>

<pre><code class="language-sql">CREATE EXTERNAL LANGUAGE language_name  
[ AUTHORIZATION owner_name ]  
FROM (CONTENT = { &lt;file_spec&gt; }
     FILE_NAME = &lt;file_name&gt;
    [ , PLATFORM = &lt;platform&gt; ]
    [ , PARAMETERS = &lt;parameters&gt; ]
    [ , ENVIRONMENT_VARIABLES = &lt;env_variables&gt; )
[ ; ] 
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Signature CREATE EXTERNAL LIBARY</em></p>

<p>The arguments we see in <em>Code Snippet 3</em> are:</p>

<ul>
<li><code>language_name</code>: A unique name for the language.</li>
<li><code>owner_name</code>: This optional parameter specifies the name of the user or role that owns the language.</li>
<li><code>file_spec</code>: The <code>file_spec</code> specifies the content of the language extension file for a specific platform, and it can either be in the form of a file location (local path/network path) or a hex literal. If we install the package from a file location, the file needs to be in the form of an archive file (<code>zip on Windows,</code>tar.gz` on Linux).</li>
<li><code>file_name</code>: Name of the language extension <code>dll</code> or <code>so</code> file.</li>
<li><code>platform</code>: The <code>PLATFORM</code> parameter, which defines the platform for the content of the library. The <code>PLATFORM</code> can be Windows or Linux, and it defaults to Windows.</li>
<li><code>parameters</code>: Optional parameters for the external language runtime. Not supported in CTP 3.0.</li>
<li><code>env_variables</code>: Optional parameter to set environment variables for the external language runtime. Not supported in CTP 3.0.</li>
</ul>

<p>The above is a somewhat simplified explanation of the arguments, but it should be enough for us to get started. You find a more in-depth description <a href="https://docs.microsoft.com/en-us/sql/t-sql/statements/create-external-language-transact-sql?view=sqlallproducts-allversions">here</a>.</p>

<h2 id="using-create-external-language">Using CREATE EXTERNAL LANGUAGE</h2>

<p>Before we write code to create an external language, let us think back to what I wrote in <a href="/2019/03/10/sql-server-2019-java--external-libraries---i/">SQL Server 2019, Java &amp; External Libraries - I</a>, and <a href="/2019/04/10/installing-r-packages-in-sql-server-machine-learning-services---iii/">Installing R Packages in SQL Server Machine Learning Services - III</a> about <code>CREATE EXTERNAL LIBRARY</code> and how there were some new system catalog views introduced together with <code>CREATE EXTERNAL LIBRARY</code>: more specifically <code>sys.external_libraries</code> and friends. The same is true for <code>CREATE EXTERNAL LANGUAGE</code>:</p>

<ul>
<li><code>sys.external_languages</code> - contains a row for each external language in the database.</li>
<li><code>sys.external_language_files</code> - contains a row for each external language extension file in the database.</li>
</ul>

<p>Let us look at what we see if we run a <code>SELECT</code> against those two catalog views. I do this on a freshly installed CTP 3.0 where I have not created any external languages. I have only enabled <em>Machine Learning Services</em> together with R and Python. When I execute my <code>SELECT</code>&rsquo;s, I see:</p>

<p><img src="/images/posts/sql_2k19_ext_lang_cat_views1.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>External Languages</em></p>

<p>What I see in <em>Figure 3</em> surprises me somewhat; even though I have not created any external languages myself, the mere fact that I have enabled <em>Machine Learning Services</em> bootstraps two languages: R and Python, as we see in the upper result grid, (<code>SELECT * FROM sys.external_languages</code>). Notice also how in the lower result grid, (<code>SELECT * FROM sys.external_language_files</code>), I see files for both the Windows as well as the Linux platforms.</p>

<p>So let us create Java as an external language. We know from above that the Java language extension file is the <code>javaextension.dll</code>, which is part of the SQL Server distribution and you find it in the same directory as the SDK <code>.jar</code> mentioned above: <code>..\&lt;path_to_sql_instance&gt;\MSSQL\Binn\javaextension.dll</code>. However, you cannot use it directly in the <code>CREATE EXTERNAL LIBRARY</code> call; you need to archive it into a <code>.zip</code> file first - as mentioned above.</p>

<p>I zipped the dll and placed it in the same location as the <code>.jar</code> file in <em>Code Snippet 2</em> and I am now ready to create the external language:</p>

<pre><code class="language-sql">CREATE EXTERNAL LANGUAGE Java
FROM (CONTENT = 'W:\javaextension.zip'
      , FILE_NAME = 'javaextension.dll');
GO
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Creating External Language</em></p>

<p>The reason we, in <em>Code Snippet 4</em>, set the file name in the <code>FILE_NAME</code> parameter is that the zip file may contain multiple files and the file name defines the language extension. After we execute the code in <em>Code Snippet 4</em>, we run the <code>SELECT</code> statements we used above against the external language catalog views, and we get:</p>

<p><img src="/images/posts/sql_2k19_ext_lang_cat_views2.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Java External Language</em></p>

<p>WWe see in <em>Figure 4</em> how we have added Java as an external language to the <code>JavaTestDB</code> database. In the lower result grid, we see how the binary representation of the zip file is persisted as well, the same as it is for external libraries. Speaking of external libraries, in the posts I did about those, we discussed how the external libraries, when resolved, were copied to file directories: <code>..\&lt;path_to_sql_instance&gt;\MSSQL\ExternalLibraries</code>. I wonder if it is the same for external languages?</p>

<p>Sure enough, when looking at <code>..\&lt;path_to_sql_instance&gt;\MSSQL</code> I see an <code>ExternalLanguages</code> directory, and as with external libraries, it is empty. Remember from the posts mentioned above, how the <code>ExternalLibraries</code> directory got populated when we resolved an external library. Let us see if it is the same for external languages.</p>

<p>As we have created the external language, we can now do what we tried to do earlier; create the external SDK library. So, we run the code in <em>Code Snippet 2</em> again, and now it succeeds. We verify that by executing: <code>SELECT * FROM sys.external_libraries</code>. When we have deployed the SDK, we can deploy our Java code that we want to call from inside SQL Server. In this post, I use the same Java code as in the first example in <a href="/2019/05/26/java--sql-server-2019-extensibility-framework-the-sequel/"><strong>Java &amp; SQL Server 2019 Extensibility Framework: The Sequel</strong></a>:</p>

<pre><code class="language-java">package sql;

import com.microsoft.sqlserver.javalangextension.PrimitiveDataset;
import java.util.LinkedHashMap;
import com.microsoft.sqlserver.javalangextension.\
            AbstractSqlServerExtensionExecutor;
public class JavaTest1 extends AbstractSqlServerExtensionExecutor {

  public PrimitiveDataset execute(PrimitiveDataset input, 
                                  LinkedHashMap&lt;String, 
                                  Object&gt; params) {

      int x = (int)params.get(&quot;x&quot;);
      int y = (int)params.get(&quot;y&quot;);

      System.out.printf(&quot;The result of adding %d and %d = %d&quot;, 
                         x, y, x + y);  
      return null;

  }
}
</code></pre>

<p><strong>Code Snippet 5:</strong> <em>JavaTest1 Class and Execute Method</em></p>

<p>I compile the code in <em>Code Snippet 5</em> into a <code>.jar</code> file which I then deploy:</p>

<pre><code class="language-sql">CREATE EXTERNAL LIBRARY mySqlJar 
FROM (CONTENT = 'W:\sql-1.0.jar')
WITH (LANGUAGE = 'Java');
GO
</code></pre>

<p><strong>Code Snippet 6:</strong> <em>Deploy Java Code</em></p>

<p>We see after we ran the code in <em>Code Snippet 6</em> that nothing changed in the <code>ExternalLanguage</code> directory, and nothing changed for that matter in <code>ExternalLibraries</code> either. Hopefully, we see some changes when we execute the code calling into our class:</p>

<pre><code class="language-sql">DECLARE @p1 int = 21;
DECLARE @p2 int = 21;
EXEC sp_execute_external_script
  @language = N'Java'
, @script = N'sql.JavaTest1'
, @params = N'@x int, @y int'
, @x = @p1
, @y = @p2   
GO
</code></pre>

<p><strong>Code Snippet 7:</strong> <em>Execute Java Code</em></p>

<p>The result after running the code in <em>Code Snippet 7</em>:</p>

<p><img src="/images/posts/sql_2k19_ext_lang_exec1.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Success</em></p>

<p>As we see in <em>Figure 5</em>, the code we ran in <em>Code Snippet 7</em> executed successfully. So, what has happened in the file system:</p>

<p><img src="/images/posts/sql_2k19_ext_lang_filesystem1.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>File System External Languages</em></p>

<p>In <em>Figure 6</em> we see how, when executing the code in <em>Code Snippet 7</em>, the language extension file gets copied to a directory with the structure <code>ExternalLanguage | Database Id | External Language Id | File Name</code>. As with external libraries, SQL Server loads the extension from that directory.</p>

<p>What is interesting is that even though R/Python shows as external languages in the catalog views, when you execute some R/Python code, the launcher dll&rsquo;s do not get copied to the external languages directory.</p>

<h2 id="summary">Summary</h2>

<p>In this post, we discussed the requirement in SQL Server CTP 3.0 to register any external language other than R/Python which you want to use from inside SQL Server.</p>

<blockquote>
<p><strong>NOTE:</strong> At the time of writing this post the only external language besides R/Python is Java, but other languages will most definitely become available.</p>
</blockquote>

<p>So, what did we say:</p>

<ul>
<li>Before you can use Java as an external language, you need to register it with SQL Server in the database you want to call Java code from.</li>
<li>You register not only the language name but also the language extension: the bridge between SQL Server and the external runtime.</li>
<li>To register you call <code>CREATE EXTERNAL LANGUAGE</code>, where you can either use a file path or a binary representation of the archive file containing the language extension.</li>
<li>In future releases you can send in parameters as well as environment variables in the <code>CREATE EXTERNAL LANGUAGE</code> call.</li>
</ul>

<p>Something we didn&rsquo;t touch upon in this post was that the security model for executing against an external language has changed somewhat. We cover that in a future post.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 22, 2019]]></title>
    <link href="http://nielsberglund.com/2019/06/03/interesting-stuff---week-22-2019/" rel="alternate" type="text/html"/>
    <updated>2019-06-03T18:29:57+02:00</updated>
    <id>http://nielsberglund.com/2019/06/03/interesting-stuff---week-22-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://devblogs.microsoft.com/dotnet/porting-desktop-apps-to-net-core/">Porting desktop apps to .NET Core</a>. This is a very informative post discussing porting of .NET Framework applications to .NET Core. Seeing that the apps to port can be of different grades of complexity, this first post covers simple use cases, and a follow-up post covers the more complex scenarios.</li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://towardsdatascience.com/apache-spark-mllib-tutorial-ec6f1cb336a9">Apache Spark MLlib Tutorial: Linear Regression</a>. This post is the first part of a tutorial on how to work with MLLib in Apache Spark. For me, this is interesting as I can see us at <a href="/derivco">Derivco</a> start to use Spark.</li>
</ul>

<h2 id="vs-code">VS Code</h2>

<ul>
<li><a href="https://blog.usejournal.com/visual-studio-code-for-java-the-ultimate-guide-2019-8de7d2b59902">Visual Studio Code for Java: The Ultimate Guide 2019</a>. I have written a couple of posts where I have used <em>VS Code</em> and Java. However, since I am a .NET guy &ldquo;at heart&rdquo;, the Java ecosystem is a mystery to me, and I have &ldquo;fumbled&rdquo; my way through. I wish I had come across the post I link to here, as it gives you awesome information about how to work with <em>VS Code</em> and Java.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://azure.microsoft.com/en-us/blog/visual-data-ops-for-apache-kafka-on-azure-hdinsight-powered-by-lenses/">Visual data ops for Apache Kafka on Azure HDInsight, powered by Lenses</a>. This post looks at how you can manage your streaming data operations, from visibility to monitoring, by the use of <a href="https://lenses.io/">Lenses</a>. The post looks at how to do it in Azure, but it is as applicable on-prem as well. Once again, this is interesting to me as, at <a href="/derivco">Derivco</a>, we are great fans of Kafka.</li>
<li><a href="https://www.confluent.io/blog/deploying-kafka-streams-and-ksql-with-gradle-part-2-managing-ksql-implementations">Deploying Kafka Streams and KSQL with Gradle – Part 2: Managing KSQL Implementations</a>. The second part in a series about how to develop, and deploy Kafka Streams and KSQL parts of streaming applications using <a href="https://gradle.org/">Gradle</a>.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 21, 2019]]></title>
    <link href="http://nielsberglund.com/2019/05/26/interesting-stuff---week-21-2019/" rel="alternate" type="text/html"/>
    <updated>2019-05-26T10:02:45+02:00</updated>
    <id>http://nielsberglund.com/2019/05/26/interesting-stuff---week-21-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/schemas-contracts-compatibility">Schemas, Contracts, and Compatibility</a>. This blog post looks at how Kafka together with its schema registry can be used in a microservices environment, potentially as a replacement for REST. Very interesting!</li>
<li><a href="https://www.confluent.io/blog/kafka-summit-london-2019-session-videos">Kafka Summit London 2019 Session Videos</a>. In last weeks <a href="/2019/05/19/interesting-stuff---week-20-2019/">roundup</a> I mentioned that I had attended the Kafka Summit in London. The organizers have now made all session videos and slides available. So go to the post I link to and look at the sessions that interest you!</li>
</ul>

<h2 id="sql-server-2019">SQL Server 2019</h2>

<ul>
<li><a href="/2019/05/26/java--sql-server-2019-extensibility-framework-the-sequel/">Java &amp; SQL Server 2019 Extensibility Framework: The Sequel</a>. In SQL Server 2019 CTP 2.5, Microsoft made some changes to requirements for writing Java code to be used by SQL Server. In this post, I look at what those changes are, and what our Java code should look like going forward.</li>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2019/05/22/sql-server-2019-community-technology-preview-3-0-is-now-available/">SQL Server 2019 community technology preview 3.0 is now available</a>. The title says it all. Microsoft just released SQL Server 2019 CTP 3.0. Go and get it while it is hot! Oh, above I mentioned about changes in CTP 2.5 impacting how we write Java code for SQL Server. The 3.0 release has some additional changes, so expect a follow-up blog post about that.</li>
</ul>

<h2 id="next-weeks-roundup">Next Weeks Roundup</h2>

<p>I am away the whole of next week, and not back until Tuesday, June 4. Due to this, the roundup for next week may be delayed for a couple of days.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Java &amp; SQL Server 2019 Extensibility Framework: The Sequel]]></title>
    <link href="http://nielsberglund.com/2019/05/26/java--sql-server-2019-extensibility-framework-the-sequel/" rel="alternate" type="text/html"/>
    <updated>2019-05-26T07:20:09+02:00</updated>
    <id>http://nielsberglund.com/2019/05/26/java--sql-server-2019-extensibility-framework-the-sequel/</id>
    <content type="html"><![CDATA[<p>As you may know, a while back I wrote some posts about the support for Java in SQL Server 2019: <a href="/s2k19_ext_framework_java"><strong>SQL Server 2019 Extensibility Framework &amp; Java</strong></a>. The posts covered in some detail how Java in SQL Server worked, and how to write Java code for use in SQL Server. However, a week or two ago &ldquo;the sky came tumbling down&rdquo; when Microsoft released SQL Server 2019 CTP 2.5.</p>

<blockquote>
<p><strong>NOTE:</strong> CTP stands for Community Technology Preview and is like a beta release.</p>
</blockquote>

<p>What Microsoft did in CTP 2.5 was to introduce Java Language Extension SDK, and your Java code now needs to inherit an abstract base class from the SDK. This requirement makes a large part of my previous posts &ldquo;null and void&rdquo;, so in this post, we look at what to do going forward.</p>

<p></p>

<p>What happened here, (functionality introduced that negates previous functionality), is the danger when writing about beta releases. I should know, as it has happened before. Back in 2003 some colleagues and I wrote a book about the upcoming SQL Server 2005 release: <a href="https://www.amazon.com/First-Look-Server-2005-Developers/dp/0321180593/ref=sr_1_fkmrnull_1">A First Look at SQL Server 2005 for Developers</a>, and we wrote the book based on beta releases. When Microsoft eventually released SQL Server 2005, at least a couple of chapters in the book covered functionality that no longer existed. Well, what can you do?</p>

<p>Anyway, let us go back to SQL Server 2019 and Java.</p>

<h2 id="recap-pre-ctp-2-5">Recap (pre CTP 2.5)</h2>

<p>When I started this post, my idea was to do a brief recap of what the Java implementation looked like in the previous CTP&rsquo;s, to show what it used to be, and refer to that in this post. After I had written 90% of the <em>Recap</em> I realized it had become way too long, so I decided to skip it.</p>

<p>If you are interested in what it used to be, you can go back and read the posts in the <a href="/s2k19_ext_framework_java"><strong>SQL Server 2019 Extensibility Framework &amp; Java</strong></a> series. The most relevant posts are:</p>

<ul>
<li><a href="/2018/12/02/sql-server-2019-extensibility-framework--java---hello-world/">SQL Server 2019 Extensibility Framework &amp; Java - Hello World</a>: We looked at installing and enabling the Java extension, as well as some very basic Java code.</li>
<li><a href="/2018/12/08/sql-server-2019-extensibility-framework--java---passing-data/">SQL Server 2019 Extensibility Framework &amp; Java - Passing Data</a>: In this post, we discussed what is required to pass data back and forth between SQL Server and Java.</li>
<li><a href="/2018/12/19/sql-server-2019-extensibility-framework--java---null-values/">SQL Server 2019 Extensibility Framework &amp; Java - Null Values</a>: This, the <a href="/2018/12/19/sql-server-2019-extensibility-framework--java---null-values/">Null Values</a>, post is a follow up to the <a href="/2018/12/08/sql-server-2019-extensibility-framework--java---passing-data/">Passing Data</a> post, and we look at how to handle <code>null</code> values in data passed to Java.</li>
</ul>

<h2 id="demo-data">Demo Data</h2>

<p>In this post, we use some data from the database, so let us set up the necessary database, tables, and load data into the tables:</p>

<pre><code class="language-sql">USE master;
GO
SET NOCOUNT ON;
GO
DROP DATABASE IF EXISTS JavaTestDB;
GO
CREATE DATABASE JavaTestDB;
GO
USE JavaTestDB;
GO

DROP TABLE IF EXISTS dbo.tb_Rand10
CREATE TABLE dbo.tb_Rand10(RowID int identity primary key, x int, 
                          y int;

INSERT INTO dbo.tb_Rand10(x, y)
SELECT TOP(10) CAST(ABS(CHECKSUM(NEWID())) % 14 AS int) 
  , CAST(ABS(CHECKSUM(NEWID())) % 20 AS int)
FROM sys.objects o1
CROSS JOIN sys.objects o2
GO
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Create Database Objects</em></p>

<p>We see from <em>Code Snippet 1</em> how we:</p>

<ul>
<li>Create a database: <code>JavaTestDB</code>.</li>
<li>Create a table: <code>dbo.tb_Rand10</code>.</li>
<li>Insert some data into the table.</li>
</ul>

<p>The data we insert is entirely random, but it gives us something to &ldquo;play around&rdquo; with. Now, when we have a database and some data let us get started.</p>

<h2 id="microsoft-extensibility-sdk-for-java">Microsoft Extensibility SDK for Java</h2>

<p>As mentioned above, in CTP 2.5, Microsoft changes the way we implement Java code in SQL Server, and they do it to create a better developer experience when writing Java code for SQL Server.</p>

<blockquote>
<p><strong>NOTE:</strong> I am not totally sure this change gives us a better developer experience, I guess time will tell.</p>
</blockquote>

<p>In CTP 2.5 and onwards when you write Java code for SQL Server you implement your code using the <strong>Microsoft Extensibility SDK for Java</strong>, (SDK). The SDK acts sort of like an interface as it exposes abstract classes that your code need to extend/target, (more about that later).</p>

<p>The SDK comes in the form of a <code>.jar</code> file, and you download the SDK from <a href="http://aka.ms/mssql-java-lang-extension">here</a>. Since a <code>.jar</code> file is essentially an archive file you can open the SDK <code>.jar</code> with your favorite file archiver utility and when you do, you see:</p>

<p><img src="/images/posts/sql_2k19_java_sdk_jar1.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>SDK Jar - I</em></p>

<p>We see in <em>Figure 1</em> how I have extracted the SDK <code>.jar</code> to a folder, and how the <code>.jar</code> file contains at the top level two folders: <code>com</code> and <code>META-INF</code>. The <code>com</code> folder is the top level folder for the Java SDK package, and below we look a bit more into it. The <code>META-INF</code> folder contains metadata information about the <code>.jar</code> package, and in this post we do not care about it.</p>

<p>Coming back to the <code>com</code> folder I mentioned it was the top level folder for the package, and if we drill down into it, it looks something like so:</p>

<p><img src="/images/posts/sql_2k19_java_sdk_jar2.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>SDK Jar - II</em></p>

<p>In <em>Figure 2</em>, outlined in blue, we see how the package name follows the standard of a hierarchical naming pattern: <code>com.microsoft.sqlserver.javalengextension</code>. Below <code>javalangextension</code> we have three classes outlined in red - these are the classes mentioned above:</p>

<ul>
<li><strong><code>AbstractSqlServerExtensionExecutor</code></strong></li>
<li><strong><code>AbstractSqlServerExtensionDataset</code></strong></li>
<li><strong><code>PrimitiveDataset</code></strong></li>
</ul>

<p>Let us look at what these classes do.</p>

<h2 id="abstractsqlserverextensionexecutor">AbstractSqlServerExtensionExecutor</h2>

<p>The <code>AbstractSqlServerExtensionExecutor</code> abstract class is the class you need to inherit from/extend in the classes that SQL Server calls. The source code looks like so (I have copied the code from <a href="https://docs.microsoft.com/en-us/sql/advanced-analytics/java/java-sdk?view=sqlallproducts-allversions#abstractsqlserverextensionexecutor-source-code">here</a>):</p>

<pre><code class="language-java"> package com.microsoft.sqlserver.javalangextension;

import com.microsoft.sqlserver.javalangextension.AbstractSqlServerExtensionDataset;
import java.lang.UnsupportedOperationException;
import java.util.LinkedHashMap;

/**
 * Abstract class containing interface used by the Java extension
 */
public abstract class AbstractSqlServerExtensionExecutor {
  /* Supported versions of the Java extension */
  public final int SQLSERVER_JAVA_LANG_EXTENSION_V1 = 1;

  /* Members used by the extension to determine application specifics */
  protected int executorExtensionVersion;
  protected String executorInputDatasetClassName;
  protected String executorOutputDatasetClassName;

  public AbstractSqlServerExtensionExecutor() { }

  public void init(String sessionId, int taskId, int numTasks) {
    /* Default implementation of init() is no-op */
  }

  public AbstractSqlServerExtensionDataset execute(
               AbstractSqlServerExtensionDataset input, 
               LinkedHashMap&lt;String, Object&gt; params) {
    throw new UnsupportedOperationException(
       &quot;AbstractSqlServerExtensionExecutor execute() is not implemented&quot;);
  }

  public void cleanup() {
    /* Default implementation of cleanup() is no-op */
  }
}
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>AbstractSqlServerExtensionExecutor</em></p>

<p>When looking at the code in <em>Code Snippet 2</em> we see how the class:</p>

<ul>
<li>Has three class members that according to the comments have something to do with application specifics.</li>
<li>Has three methods: <code>init</code>, <code>execute</code>, and <code>cleanup</code>.</li>
</ul>

<p>Later in the post, I come back to the class members, but now I want to look at the three methods. More specifically, I want to look at the <code>execute</code> method since <code>init</code>, and <code>cleanup</code> are fairly self-explanatory: <code>init</code> if any initialization needs to be done, and <code>cleanup</code> for any, well, clean up after usage.</p>

<h4 id="execute">execute</h4>

<p>That leaves us <code>execute</code>. Notice in <em>Code Snippet 2</em> how both <code>init</code> and <code>cleanup</code> are no-ops, whereas <code>execute</code> is not. Furthermore, if someone calls <code>execute</code> in a class which extends <code>AbstractSqlServerExtensionExecutor</code>, and there is no implementation of <code>execute</code> the method throws an <code>UnsupportedOperationException</code> error. So who would call <code>execute</code>?</p>

<p>To answer the question about who is calling <code>execute</code>, let us remind ourselves what happens when we call <code>sp_execute_external_script</code>. We do that by looking at what happens when we execute R/Python code. In my <a href="/sql_server_2k16_r_services">SQL Server R Services</a> series we  talked about the components which make up <strong>SQL Server Machine Learning Services</strong>, and we saw how the flow when we execute an external script, looks something like so:</p>

<p><img src="/images/posts/sql_2k19_java_intro_flow1.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Components &amp; Flow</em></p>

<p>The flow is similar when we execute Java code; e.g. when we execute <code>sp_execute_external_script</code> SQL Server calls into the <em>Launchpad</em> service which then &ldquo;spins&rdquo; up the external engine and your code runs. In this case the call goes into the Java extension library (a <code>.dll</code>), and the extension library calls into the JVM. So it is the extension library that calls the <code>execute</code> method. This is different to pre CTP 2.5 where the extension called a method specified in the <code>@script</code> parameter: <code>@script = N'packagename.classname.method'</code>, and now it is: <code>@script = N'packagename.classname'</code>.</p>

<p>The implication of this is that in pre CTP 2.5 you could have multiple &ldquo;entry&rdquo; points, (methods), to call into, whereas now the entry point is the <code>execute</code> method.</p>

<p>Above I mentioned that one of the reasons for introducing the SDK was to create a better developer experience, and the signature of <code>execute</code> gives some hints about this:</p>

<pre><code class="language-java">public AbstractSqlServerExtensionDataset execute(
               AbstractSqlServerExtensionDataset input, 
               LinkedHashMap&lt;String, Object&gt; params) {...}
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Execute Method</em></p>

<p>From the signature in <em>Code Snippet 3</em> we see how the <code>execute</code> method takes two parameters and has a return type. This in itself is interesting as pre CTP 2.5 the methods you called into did not allow parameters, and had to be <code>void</code>.</p>

<p>When we look at the parameters, the <code>execute</code> method expects we see they are:</p>

<ul>
<li><code>AbstractSqlServerExtensionDataset input</code></li>
<li><code>LinkedHashMap&lt;String, Object&gt; params</code></li>
</ul>

<p>The <code>input</code> parameter references any dataset you pass in the class, (from the <code>@input_data_1</code> parameter in <code>sp_execute_external_script</code>). We talk more about <code>AbstractSqlServerExtensionDataset</code> below.</p>

<p>What about the <code>params</code> parameter? As the name implies, it has to do with passing in parameters to the <code>execute</code> method. Remember that in pre CTP 2.5 a method could not have parameters and if you wanted to send in parameters you first defined them in the <code>@params</code> parameter in <code>sp_execute_external_script</code> and declared them like so:</p>

<pre><code class="language-sql">DECLARE @p1 int = 21;
DECLARE @p2 int = 21;
EXEC sp_execute_external_script
  @language = N'Java'
, @script = N'JavaTest1.adder'
, @params = N'@x int, @y int'
, @x = @p1
, @y = @p2   
GO
</code></pre>

<p><strong>Code Snippet 4</strong> <em>Call from T-SQL to Java with Parameters</em></p>

<p>What we see in <em>Code Snippet 4</em> how we add two parameter definitions, (<code>@x</code> and <code>@y</code>), to the <code>@params</code> parameter, and how we then declare and assign values to them: <code>@x = @p1</code>, and <code>@y = @p2</code>. In the Java code, as the methods had to be parameterless, we added the parameters as class members and named them the same as in the SQL code, but without the <code>@</code> sign. In the methods, we then used those class members.</p>

<p>In CTP 2.5 and onwards, we still declare the parameters as before in <code>sp_execute_external_script</code>, but we no longer need to define the parameters as class members in the Java code. The Java extension dll takes the parameters and populates the <code>LinkedHasMap</code>, with the parameters defined in <code>sp_execute_external_script</code>. The extension adds them as key-value pairs, with the key being the parameter name, (without the <code>@</code>), and the value is the value of the parameter. In the <code>execute</code> method, you retrieve them from the <code>params</code> parameter and use them.</p>

<p>So far I have not mentioned anything about the return type of the <code>execute</code> method, other than it being an <code>AbstractSqlServerExtensionDataset</code>, (as is the first input parameter in <code>execute</code>). So, let us discuss <code>AbstractSqlServerExtensionDataset</code>.</p>

<h2 id="abstractsqlserverextensiondataset">AbstractSqlServerExtensionDataset</h2>

<p>As the name implies, the <code>AbstractSqlServerExtensionDataset</code> &ldquo;deals&rdquo; with datasets. In pre CTP 2.5 if you wanted to send in a dataset like: <code>SELECT col1, col2 FROM someTable</code>, you had to - in your class - define arrays as class members representing the columns in the dataset. For return datasets, you had to do the same. Both for input datasets as well as return datasets the class members had to have well-known names: <code>inputDataCol*N*</code>, and <code>outputDataCol*N*</code>, where <em>N</em> is the column number (1 based). For input datasets, the Java extension populated the <code>inputDataCol</code> class members, and in your code, you looped through them. When returning a dataset from your code, you populated the <code>outputDataCol</code> class members, and the Java extension converted it to a result set when returning.</p>

<p>Many developers found the above complex and convoluted, so the Java SDK introduces the <code>AbstractSqlServerExtensionDataset</code>. The class contains methods for handling input and output data, and you see the source code for it <a href="https://docs.microsoft.com/en-us/sql/advanced-analytics/java/java-sdk?view=sqlallproducts-allversions#abstractsqlserverextensiondataset">here</a>. As a developer, you - instead of defining all the various input and output column arrays - create an implementation of the <code>AbstractSqlServerExtensionDataset</code> and uses that in the code. Unless you have specific requirements, you do not even have to create the implementation of <code>AbstractSqlServerExtensionDataset</code>; an implementation already exists in the SDK, the <code>PrimitiveDataset</code>.</p>

<h2 id="primitivedataset">PrimitiveDataSet</h2>

<p>The <code>PrimitiveDataset</code> is a concrete implementation of the <code>AbstractSqlServerExtensionDataset</code>, and it is similar to how we handled datasets pre CTP 2.5 in that it stores simple types as primitives arrays. You find the source of the class <a href="https://docs.microsoft.com/en-us/sql/advanced-analytics/java/java-sdk?view=sqlallproducts-allversions#primitivedataset">here</a>, and below, we see how we use it.</p>

<h2 id="java-code">Java Code</h2>

<p>It is time for some code, but before we do that, ensure you have downloaded the SDK from <a href="http://aka.ms/mssql-java-lang-extension">here</a>. For the code I write here I use <em>VS Code</em> together with the <em>Maven</em> extension. I wrote a blog post about <em>VS Code</em>, Java and <em>Maven</em> <a href="/2019/01/17/sql-server-2019--java-with-visual-studio-code/">here</a> if you want to refresh your memory.</p>

<p>I start with creating a <em>Maven</em> project based on the <em>Maven</em> archetype <code>maven-archetype-quickstart</code>. This gives me a &ldquo;starter&rdquo; class <code>App</code> containing a <code>public static void main()</code> entry point. I add to the project a class <code>JavatTest1</code> in the source file <code>JavaTest1.java</code>, and this is the class that I want to inherit from <code>AbstractSqlServerExtensionExecutor</code>. So I write some code like so:</p>

<pre><code class="language-java">public class JavaTest1 extends AbstractSqlServerExtensionExecutor {

}
</code></pre>

<p><strong>Code Snippet 5:</strong> <em>Extending AbstractSqlServerExtensionExecutor</em></p>

<p>As we see in <em>Code Snippet 5</em> I extend the <code>AbstractSqlServerExtensionExecutor</code> class, but when I do it I immediately see red &ldquo;squiggles&rdquo; under <code>AbstractSqlServerExtensionExecutor</code>, and when I mouse over I get a dialog like so:</p>

<p><img src="/images/posts/sql_2k19_java_sdk_dep1.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Inheritance Error</em></p>

<h4 id="maven-dependencies">Maven Dependencies</h4>

<p>As we see in <em>Figure 2</em>, it looks like <em>Maven</em>/<em>VS Code</em> cannot resolve the name <code>AbstractSqlServerExtensionExecutor</code>. That is not that strange as we do not have any dependency on the <code>.jar</code> file. So how do we set a dependency on the downloaded SDK? Well, we add a dependency in the <code>pom.xml</code> file, and, (for <em>Maven</em>), it needs to be in the form of:</p>

<pre><code class="language-xml">&lt;dependencies&gt;
  &lt;dependency&gt;
    &lt;groupId&gt;some_groupId&lt;/groupId&gt;
    &lt;artifactId&gt;the_artifactId&lt;/artifactId&gt;
    &lt;version&gt;some_version&lt;/version&gt;
  &lt;/dependency&gt;
  ...
&lt;/dependencies&gt;
</code></pre>

<p><strong>Code Snippet 6:</strong> <em>Dependency</em></p>

<p>We see in <em>Code Snippet 6</em> how a dependency consists of a <code>groupId</code>, <code>artifactId</code>, and <code>version</code>. Usually, you follow the <em>Maven</em> <a href="https://maven.apache.org/guides/mini/guide-naming-conventions.html">naming standards</a>, but in our case, where we have downloaded the SDK <code>jar</code> directly, we do not have to do that. Regardless of that, the <code>artifactId</code> needs to match the filename, sans extension, and a version number is required.</p>

<p>The dependency points out where to find the dependent file in the local <em>Maven</em> repository, or to be downloaded to from a remote repository. On Windows, we find the local <em>Maven</em> repository at <code>%USERPROFILE%\.m2\repository</code>. Coming back to <em>Code Snippet 6</em>, the <code>groupId</code>\<code>artifactId</code>\<code>version</code> defines the folder hierarchy in the local <em>Maven</em> repository:</p>

<pre><code class="language-xml">&lt;dependencies&gt;
  &lt;dependency&gt;
    &lt;groupId&gt;nielsb&lt;/groupId&gt;
    &lt;artifactId&gt;mssql-java-lang-extension&lt;/artifactId&gt;
    &lt;version&gt;1.0&lt;/version&gt;
  &lt;/dependency&gt;
  ...
&lt;/dependencies&gt;
</code></pre>

<p><strong>Code Snippet 7:</strong> <em>Java SDK Dependency</em></p>

<p>The <code>dependency</code> in <em>Code Snippet 7</em> sets the expectation that the <code>.jar</code> file is located at: <code>%USERPROFILE%\.m2\repository\nielsb\mssql-java-lang-extension\1.0</code>. The <code>nielsb</code> directory is just a random directory, and it could be anything. The one thing to think about is that when you copy the actual file to the directory, the file-name needs to include the version. So as per <em>Code Snippet 7</em>, the file name is: <code>mssql-java-lang-extension-1.0.jar</code>:</p>

<p><img src="/images/posts/sql_2k19_java_sdk_dep_hierarch.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Folder Hierarchy Dependency</em></p>

<p>In <em>Figure 3</em> we see the &ldquo;layout&rdquo; of the local <em>Maven</em> repository after I have set it up for the SDK dependency. Outlined in blue we see the different folders below<code>..\m2\repository</code>, and the outline in red shows the renamed SDK file. Having done this <em>VS Code</em> now &ldquo;picks up&rdquo; the dependency and we can start using it in our code.</p>

<h4 id="use-the-sdk">Use the SDK</h4>

<p>Our project should now compile OK, so let us add some logic to the <code>JavaTest1</code> class. We start with writing similar code to what we saw in the [<strong>SQL Server 2019 Extensibility Framework &amp; Java - Hello World</strong>] post; the <code>adder</code> method where we took two variables and added them together.</p>

<p>However, now when we use the SDK, we do not have to declare the variables as global class members, they are instead  part of the <code>params</code> parameter in the <code>execute</code> method:</p>

<pre><code class="language-java">package sql;

import com.microsoft.sqlserver.javalangextension.PrimitiveDataset;
import java.util.LinkedHashMap;
import com.microsoft.sqlserver.javalangextension.\
            AbstractSqlServerExtensionExecutor;
public class JavaTest1 extends AbstractSqlServerExtensionExecutor {

  public PrimitiveDataset execute(PrimitiveDataset input, 
                                  LinkedHashMap&lt;String, 
                                  Object&gt; params) {

      int x = (int)params.get(&quot;x&quot;);
      int y = (int)params.get(&quot;y&quot;);

      System.out.printf(&quot;The result of adding %d and %d = %d&quot;, 
                         x, y, x + y);  
      return null;

  }
}
</code></pre>

<p><strong>Code Snippet 8:</strong> <em>JavaTest1 Class and Execute Method</em></p>

<p>In <em>Code Snippet 8</em> we see the complete <code>JavaTest1</code> source code. We see how we do some <code>import</code> of classes we use, and in the <code>execute</code> method, we <code>get</code> the two parameters we want from the <code>params</code> parameter. We return <code>null</code> since we do not have any resultset to pass back. Oh, the Java language extension does, still, not support output parameters.</p>

<p>In the <em>VS Code</em> project we have an <code>App.java</code> source file with a <code>main</code> method, by which we can test that our code works:</p>

<pre><code class="language-java">public static void main( String[] args )
{
    JavaTest1 j1 = new JavaTest1();
    LinkedHashMap&lt;String, Object&gt; lh = 
            new LinkedHashMap&lt;String, Object&gt;();
    lh.put(&quot;x&quot;, 21);
    lh.put(&quot;y&quot;, 21);

    j1.execute(null, lh);

}
</code></pre>

<p><strong>Code Snippet 9:</strong> <em>Main Method</em></p>

<p>In <em>Code Snippet 9</em> we see a big difference between pre CTP 2.5 and now, in that now the method (<code>execute</code>) is not required to be <code>static</code> any more. The Java language extension now &ldquo;news up&rdquo; an instance of the class that we call into.</p>

<p>Let us create a <code>.jar</code> file out of our project so we can deploy to SQL Server. Since I am using <em>Maven</em>, in the <em>VS Code</em>&rsquo;s&rsquo; <em>Maven</em> extension I click on <code>package</code>, (read more about it in the <a href="/2019/01/17/sql-server-2019--java-with-visual-studio-code/">Java with Visual Studio Code</a> post). What happens is that <em>Maven</em> recompiles, (if any changes have taken place), and then builds the <code>.jar</code> file, and places it in the <code>..\target</code> directory.</p>

<p>Theoretically when we have the <code>.jar</code> file we can deploy it to the database where we want to execute the code from, by using the <code>CREATE EXTERNAL LIBRARY</code> statement we discussed in the <strong>SQL Server 2019, Java &amp; External Libraries</strong> - <a href="/2019/03/10/sql-server-2019-java--external-libraries---i/">I</a>, and <a href="/2019/03/17/sql-server-2019-java--external-libraries---ii/">II</a> posts. The issue with that is if we try to do that in a database where we have not deployed any Java code to, exceptions happen when we execute against the code, as the SDK is not present in the database (the <code>.jar</code> does not contain the SDK). So we first need to deploy the SDK, and as we do it on the local machine, we can deploy it based on file location:</p>

<pre><code class="language-sql">CREATE EXTERNAL LIBRARY javaSDK 
FROM (CONTENT = 'W:\mssql-java-lang-extension.jar')
WITH (LANGUAGE = 'Java');
GO 
</code></pre>

<p><strong>Code Snippet 10:</strong> <em>Create SDK External Library</em></p>

<p>When you have run the code in <em>Code Snippet 10</em> you can check that everything worked by executing: <code>SELECT * FROM sys.external_libraries</code>, and you see an entry named <code>javaSDK</code>. Oh, the name we give the library is of no importance. Having done this, we deploy our <code>.jar</code> to the database, also using <code>CREATE EXTERNAL LIBRARY</code>:</p>

<pre><code class="language-sql">CREATE EXTERNAL LIBRARY mySqlJar 
FROM (CONTENT = 'W:\sql-1.0.jar')
WITH (LANGUAGE = 'Java');
GO 
</code></pre>

<p><strong>Code Snippet 11:</strong> <em>Create External Library from Java Project</em></p>

<p>After executing the code in <em>Code Snippet 11</em> we try and execute the Java code:</p>

<pre><code class="language-sql">DECLARE @p1 int = 21;
DECLARE @p2 int = 21;
EXEC sp_execute_external_script
  @language = N'Java'
, @script = N'sql.JavaTest1'
, @params = N'@x int, @y int'
, @x = @p1
, @y = @p2   
GO
</code></pre>

<p><strong>Code Snippet 12</strong> <em>Call from T-SQL to Java with Parameters</em></p>

<p>The code in <em>Code Snippet 12</em> is almost identical to what we see in <em>Code Snippet 4</em>, apart from that we no longer call into a method, (<code>adder</code>), but instead a class: <code>JavaTest1</code>. Unfortunately, when we run the code in <em>Code Snippet 12</em> we get an exception:</p>

<pre><code class="language-sql">Started executing query at Line 17
Msg 39004, Level 16, State 20, Line 0

A 'Java' script error occurred during execution of 
  'sp_execute_external_script' with HRESULT 0x80004004.

STDOUT message(s) from external script: 
2019-05-25 08:24:25.01  Error: 
         Unsupported executor version encountered

Total execution time: 00:00:01.230
</code></pre>

<p><strong>Code Snippet 13:</strong> <em>Exception</em></p>

<p>The exception is, as we see in <em>Code Snippet 13</em>: <code>Unsupported executor version encountered</code>, hmm what is that? Go back and look at <em>Code Snippet 2</em>, and the beginning of the <code>AbstractSqlServerExtensionExecutor</code> class:</p>

<pre><code class="language-java">public abstract class AbstractSqlServerExtensionExecutor {
  /* Supported versions of the Java extension */
  public final int SQLSERVER_JAVA_LANG_EXTENSION_V1 = 1;

  /* Members used by the extension to determine application specifics */
  protected int executorExtensionVersion;
  protected String executorInputDatasetClassName;
  protected String executorOutputDatasetClassName;

  ...

}
</code></pre>

<p><strong>Code Snippet 14:</strong> <em>AbstractSqlServerExtensionExecutor</em></p>

<p>Notice in <em>Code Snippet 14</em> the four members:</p>

<ul>
<li><code>public final int SQLSERVER_JAVA_LANG_EXTENSION_V1 = 1;</code></li>
<li><code>protected int executorExtensionVersion;</code></li>
<li><code>protected String executorInputDatasetClassName;</code></li>
<li><code>protected String executorOutputDatasetClassName;</code></li>
</ul>

<p>The four members above are there for the Java language extension to use. They indicate what version of the extension it is and what class to use for input and output dataset. These are required, and we set them like so:</p>

<pre><code class="language-java">...
public class JavaTest1 extends AbstractSqlServerExtensionExecutor {
    
    public JavaTest1() {
        executorExtensionVersion = SQLSERVER_JAVA_LANG_EXTENSION_V1;
        executorInputDatasetClassName = PrimitiveDataset.class.getName();
        executorOutputDatasetClassName = PrimitiveDataset.class.getName();
    }
  
  public PrimitiveDataset execute(PrimitiveDataset input, 
                                  LinkedHashMap&lt;String, 
                                  Object&gt; params) {...}
}
</code></pre>

<p><strong>Code Snippet 15:</strong> <em>Executor Version and Data Set Class Names</em></p>

<p>As we see in <em>Code Snippet 15</em> we set the members in the class <code>ctor</code>, and when we have done it we:</p>

<ul>
<li>Re-build the <code>.jar</code>.</li>
<li>Drop the external library.</li>
<li>Re-create the external library as in <em>Code Snippet 11</em>.</li>
</ul>

<p>When we now execute the code in <em>Code Snippet 12</em>:</p>

<p><img src="/images/posts/sql_2k19_java_sdk_success_1.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Success</em></p>

<p>So, after we have set the various member values, all works OK. It is worth noticing that even though we do not pass any datasets, we still need to set the values for <code>executorInputDatasetClassName</code> and <code>executorOutputDatasetClassName</code>. Having said that, let us look at how we use datasets.</p>

<p>To look at datasets we want to pass in data from the table <code>dbo.tb_Rand10</code>, in fact, we want to pass in the <code>RowID</code>, <code>x</code>, and <code>y</code> columns: <code>SELECT * FROM dbo.tb_Rand10</code>. In our Java code, we then add the value of the <code>x</code>, and <code>y</code> columns together and return a dataset containing the <code>RowID</code> and the result. So we create a new class, (and source file), <code>JavaTest2</code>. In the <code>execute</code> method, we do as follows:</p>

<pre><code class="language-java">public PrimitiveDataset execute(PrimitiveDataset input, LinkedHashMap&lt;String, Object&gt; params) {

    /*
      grab the RowID, x and y columns
      and convert them into arrays
    */
    int[] rowIds = input.getIntColumn(0);
    int[] xCol = input.getIntColumn(1);
    int[] yCol = input.getIntColumn(2);
    int rowCount = rowIds.length;

    //arrays for output data
    int[] outIds = new int[rowCount];
    int[] outRes = new int[rowCount];

    for(int i = 0; i &lt; rowCount; i++) {
        int x = xCol[i];
        int y = yCol[i];
        outIds[i] = rowIds[i];
        outRes[i] = x + y;
    }

    //Create the return dataset
    PrimitiveDataset outData = new PrimitiveDataset();
    //set up metadata
    outData.addColumnMetadata(0, &quot;RowID&quot;, java.sql.Types.INTEGER, 0, 0);
    outData.addColumnMetadata(1, &quot;Result&quot;, java.sql.Types.INTEGER, 0, 0);
    
    //add the arrays to the dataset
    outData.addIntColumn(0, outIds, null);
    outData.addIntColumn(1, outRes, null);
            
    return outData;

}

</code></pre>

<p><strong>Code Snippet 16:</strong> <em>Input and Output Datasets</em></p>

<p>In the <code>execute</code> method in <em>Code Snippet 16</em> we see how we expect the Java language extension to pass in an instance of a <code>PrimitiveDataset</code> as the <code>input</code> parameter. In our code, we then:</p>

<ul>
<li>Take the individual columns and convert them to arrays.</li>
<li>Create two output arrays, one for the <code>RowID</code>, and one for the result.</li>
</ul>

<p>When we have the output arrays, we loop the input arrays, and:</p>

<ul>
<li>Assign the <code>RowID</code> to the array for <code>RowID</code>.</li>
<li>Get the values for the <code>x</code> and <code>y</code> column arrays.</li>
<li>Add them together and assign the value to the output result array, (<code>outRes</code>).</li>
</ul>

<p>We then create an instance of the <code>PrimitiveDataset</code> class, and:</p>

<ul>
<li>Add column meta data for the columns we want to return.</li>
<li>Assign the output arrays to the output columns.</li>
<li>Finally we return the <code>PrimitiveDataset</code> instance.</li>
</ul>

<p>We can now compile the code and create a <code>.jar</code> file, and deploy to the database as we did after <em>Code Snippet 15</em>. The code to call into the class looks like so:</p>

<pre><code class="language-sql">EXEC sp_execute_external_script
  @language = N'Java'
, @script = N'sql.JavaTest2'
, @input_data_1 = N'SELECT * FROM dbo.tb_Rand10'
WITH RESULT SETS ((RowID int, Result int))
GO
</code></pre>

<p><strong>Code Snippet 17:</strong> <em>SQL Code to Pass in Data</em></p>

<p>In <em>Code Snippet 17</em> we pass in data via the <code>@input_data_1</code> parameter, and we use the <code>WITH RESULT SETS</code> to format the output. The result when we execute looks like so:</p>

<p><img src="/images/posts/sql_2k19_java_sdk_success_2.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Data Passing</em></p>

<p>We see in <em>Figure 5</em> that our code is working, and we get back the result of adding the <code>x</code>, and <code>y</code> columns. Happy Days!</p>

<h2 id="summary">Summary</h2>

<p>In this post, we set out to look at how the introduction of the Java Language Extension SDK changes the programming model when writing Java code that should be called from SQL Server. However, before we started to look into the programming model, we looked at how we can add dependencies to <em>VS Code</em> and the <em>Maven</em> extension. We saw that:</p>

<ul>
<li>We add a <code>&lt;dependency&gt;</code> to the <code>&lt;dependencies&gt;</code> section.</li>
<li>The <code>&lt;dependency&gt;</code> consists (at least) of <code>groupId</code>, <code>artifactId</code>, and <code>version</code>.</li>
<li>The <code>groupId</code>, <code>artifactId</code>, and <code>version</code> should match the directory structure of the local <em>Maven</em> repository.</li>
<li><code>artifactId</code> corresponds to the dependency file, sans extension.</li>
<li>The name of the dependency file we copy to the local repository must include the <code>&lt;version&gt;</code> number.</li>
</ul>

<p>So what about the SDK programming model? We saw that:</p>

<ul>
<li>Our classes which we want to call into need to inherit from <code>AbstractSqlServerExtensionExecutor</code>.</li>
<li>We have to implement an &ldquo;entry-point&rdquo; method: <code>execute</code>, which is what the Java language extension calls.</li>
<li>We no longer need to create class member variables for parameters, as the <code>execute</code> method accepts a <code>LinkedHashMap</code> corresponding to the parameters we want to pass in.</li>
<li>We no longer need to create class member variables for input dataset , as the <code>execute</code> method accepts a concrete implementation of <code>AbstractSqlServerExtensionDataset</code>.</li>
<li>The SDK contains a concrete implementation of <code>AbstractSqlServerExtensionDataset</code>: <code>PrimitiveDataset</code>.</li>
<li>For return datasets we use a concrete implementation of <code>AbstractSqlServerExtensionDataset</code>, for example <code>PrimitiveDataset</code>.</li>
<li>The class we call into needs to expose certain members indicating version of the language extension and class name of the <code>AbstractSqlServerExtensionDataset</code> implementation.</li>
</ul>

<p>This post was a high level overview of the new programming model using the SDK, and I have only &ldquo;scraped the surface&rdquo; on certain parts of it. Expect follow-up posts going deeper into the programming model, for example how to handle <code>null</code> values within the <code>AbstractSqlServerExtensionDataset</code>.</p>

<p><strong>STOP THE PRESSES</strong></p>

<p>While I wrote this blog post Microsoft released SQL Server CTP 3.0, which introduces further changes to the Extension Language programming model. Instead of delaying this post, I cover that in future posts.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 20, 2019]]></title>
    <link href="http://nielsberglund.com/2019/05/19/interesting-stuff---week-20-2019/" rel="alternate" type="text/html"/>
    <updated>2019-05-19T16:28:12+02:00</updated>
    <id>http://nielsberglund.com/2019/05/19/interesting-stuff---week-20-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://devblogs.microsoft.com/dotnet/default-implementations-in-interfaces/">Default implementations in interfaces</a>. A blog post introducing a new feature in C# 8.0: default method implementations in interfaces. This comes in real handy if you, for example, want to add new methods to an existing interface.</li>
<li><a href="https://devblogs.microsoft.com/dotnet/performance-improvements-in-net-core-3-0/">Performance Improvements in .NET Core 3.0</a>. A blog post which takes a tour through some of the many improvements, big and small, that have gone into the .NET Core 3.0 runtime and core libraries in order to make applications and services leaner and faster.</li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/h2o-driverless-ai">H2O&rsquo;s Driverless AI: An AI that Creates AI</a>.  An <a href="https://www.infoq.com/">InfoQ</a> presentation where the presenter shares an approach on automating ML using H2O’s Driverless AI. Driverless AI employs the techniques of expert data scientists in an easy-to-use application that helps scale data science efforts; empowers data scientists to work on projects faster using automation and state-of-the-art computing power from GPUs to accomplish tasks in minutes that used to take months. Very cool!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<p>I attended the <a href="https://kafka-summit.org/events/kafka-summit-london-2019/">Kafka Summit</a>, in London the week of May 13. The conference was very well organized, and I came away impressed by that. However, I am not that impressed by the speakers at the conference. Don&rsquo;t get me wrong, the Confluent speakers were all top-notch, engaging and presenting interesting concepts. What not impressed me was the 3rd party speakers. In my opinion, the topics were not centred around Kafka, and the speakers were in general, not that engaging.</p>

<p>Anyway, below is a couple of links related to announcements during the conference.</p>

<ul>
<li><a href="https://www.confluent.io/blog/announcing-confluent-community-catalyst-program">Announcing the Confluent Community Catalyst Program</a>. The conference started with <a href="https://twitter.com/tlberglund">Tim Berglund</a> announcing the Confluent Community Catalyst Program, an MVP like program for Kafka.</li>
<li><a href="https://www.confluent.io/blog/introducing-cloud-native-experience-for-apache-kafka-in-confluent-cloud">Introducing a Cloud-Native Experience for Apache Kafka in Confluent Cloud</a>. As part of the keynote, Neha Narkhede, (co-founder of Confluent), announced the availability of Apache Kafka as a service in the cloud. She demonstrated the ease of setting up Kafka in the cloud - 5 seconds to fully functional Kafka! Kafka as a service is initially available on AWS and Google Cloud, let us hope it comes to Azure soon!</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>The CTP 2.5 release of SQL Server 2019 changed a lot regarding the Java language extension and how to write Java code to be executed from <code>sp_execute_external_script</code>. I am at the moment writing a blog post, where I look at the changes to the programming model. I plan to try and publish it in about a week.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 19, 2019]]></title>
    <link href="http://nielsberglund.com/2019/05/11/interesting-stuff---week-19-2019/" rel="alternate" type="text/html"/>
    <updated>2019-05-11T06:39:50+02:00</updated>
    <id>http://nielsberglund.com/2019/05/11/interesting-stuff---week-19-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://devblogs.microsoft.com/dotnet/net-core-is-the-future-of-net/">.NET Core is the Future of .NET</a>. .NET is Dead! Long Live .NET! Somewhat melodramatic, but anyway. So, in this blog post Microsoft announces what almost everyone already knew - that .NET Framework 4.8 is the last major version of .NET Framework. Going forward Microsoft&rsquo;s efforts will be on .NET Core.<br /></li>
<li><a href="https://devblogs.microsoft.com/dotnet/introducing-net-5/">Introducing .NET 5</a>. Hot on the heels of the blog post above, announcing the death of .NET Framework, comes this post, laying out the future of .NET Core, .NET 5.</li>
<li><a href="https://devblogs.microsoft.com/dotnet/introducing-diagnostics-improvements-in-net-core-3-0/">Introducing diagnostics improvements in .NET Core 3.0</a>. Yet another .NET Core post. This post discusses a suite of tools that utilize new features in the .NET runtime that makes it easier to diagnose and solve performance problems.</li>
</ul>

<h2 id="data-science-machine-learning">Data Science / Machine Learning</h2>

<ul>
<li><a href="https://devblogs.microsoft.com/dotnet/announcing-ml-net-1-0/">Announcing ML.NET 1.0</a>. I have during the last months now and then posted in these &ldquo;roundups&rdquo; about new releases of ML.NET. Microsoft has now released version 1.0 with a lot of interesting new features, and that is what this post is about.</li>
<li><a href="https://www.infoq.com/presentations/h2o-model-spark">Productionizing H2O Models with Apache Spark</a>. This is an <a href="https://www.infoq.com/">InfoQ</a> presentation demonstrating the creation of pipelines integrating H2O machine learning models and their deployments using Scala or Python.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.infoq.com/news/2019/05/kafka-zeebe-streams-workflows">Event Streams and Workflow Engines – Kafka and Zeebe</a>. An <a href="https://www.infoq.com/">InfoQ</a> discussing how Kafka fits in an Event-Driven Architecture, and how workflow engines can handle complex business processes. The article also mentioned how Zeebe, a new highly scalable workflow engine, can be used with Kafka.</li>
<li><a href="https://www.confluent.io/blog/apache-kafka-data-access-semantics-consumers-and-membership">Apache Kafka Data Access Semantics: Consumers and Membership</a>. This is an article discussing in detail how the Kafka consumer works. It also talks about consumer groups, how their state is saved, and consistency is ensured. It discusses how consumer groups are managed in a distributed way, and finally, the article looks at the rebalance protocol.</li>
<li><a href="https://www.confluent.io/blog/journey-to-event-driven-part-4-four-pillars-of-event-streaming-microservices">Journey to Event Driven – Part 4: Four Pillars of Event Streaming Microservices</a>. This is the fourth &ldquo;episode&rdquo; in the &ldquo;Journey to Event Driven&rdquo;. This time the discussion is around the four individual parts that make up event streaming. I cannot wait to hear more about it next week at the Kafka Summit in London!</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 18, 2019]]></title>
    <link href="http://nielsberglund.com/2019/05/05/interesting-stuff---week-18-2019/" rel="alternate" type="text/html"/>
    <updated>2019-05-05T07:13:49+02:00</updated>
    <id>http://nielsberglund.com/2019/05/05/interesting-stuff---week-18-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li>[Designing Distributed Systems with TLA+][]. An <a href="https://www.infoq.com/">InfoQ</a> presentation where the presenter discusses the ideas behind TLA+, which is a specification language that describes a system, its properties, and how it works.</li>
</ul>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://devblogs.microsoft.com/dotnet/using-net-and-docker-together-dockercon-2019-update/">Using .NET and Docker Together – DockerCon 2019 Update</a>. This post is about the improvements and new features in .NET Core 3.0 related to Docker and running your code in Docker.</li>
</ul>

<h2 id="databases">Databases</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/yugabytedb">YugaByte DB - A Planet-scale Database for Low Latency Transactional Apps</a>. This is an <a href="https://www.infoq.com/">InfoQ</a> presentation introducing and demoing YugaByte DB, a large scale DB, highlighting distributed transactions with global consistency.</li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://databricks.com/blog/2019/05/02/detecting-financial-fraud-at-scale-with-decision-trees-and-mlflow-on-databricks.html">Detecting Financial Fraud at Scale with Decision Trees and MLflow on Databricks</a>. An excellent post about how to use Databricks to detect fraud. Why I like this article is because of the sample code, it makes it easy to follow along.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/optimizing-kafka-streams-applications">Optimizing Kafka Streams Applications</a>. This article discusses how one can optimize Kafka Stream applications based on the new processor topology optimization framework which Kafka Streams 2.1 introduced.</li>
<li><a href="https://www.confluent.io/blog/pipelinedb-team-joins-confluent">The PipelineDB Team Joins Confluent</a>. I had no idea that <a href="https://www.pipelinedb.com/">PipelineDB</a> existed before this blog post. In my mind, PipelineDB joining Confluent can be huge, and I cannot wait to see what they dream up.</li>
<li><a href="https://www.buzzsprout.com/186154/1073627-load-balanced-apache-kafka-derivco-s-globally-distributed-gaming-business">Load-Balanced Apache Kafka: Derivco&rsquo;s Globally Distributed Gaming Business</a>. My colleague <a href="https://twitter.com/charllamprecht">Charl Lamprecht</a> and I had the pleasure of being interviewed by Tim Berglund (no relations) for a Kafka podcast. We, or rather Charl as I had audio issues, spoke about Kafka, load balancing via F5&rsquo;s and the journey we have had to get Kafka implemented in <a href="/derivco">Derivco</a>.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 17, 2019]]></title>
    <link href="http://nielsberglund.com/2019/04/28/interesting-stuff---week-17-2019/" rel="alternate" type="text/html"/>
    <updated>2019-04-28T08:12:38+02:00</updated>
    <id>http://nielsberglund.com/2019/04/28/interesting-stuff---week-17-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://devblogs.microsoft.com/dotnet/introducing-net-for-apache-spark/">Introducing .NET for Apache® Spark™ Preview</a>. What the title says! Microsoft has released a preview of .NET that you can use together with Apache Spark. It is built on the Spark interop layer, designed to provide high-performance bindings to multiple languages. Being able to write C# code for Spark is so awesome; hopefully, we can soon use it in Notebooks as well.</li>
<li><a href="https://towardsdatascience.com/why-kubernetes-is-a-great-choice-for-data-scientists-e130603b9b2d">Why Kubernetes is a Great Choice for Data Scientists</a>. This is an interesting post discussing how Kubernetes can be used in a data science world.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://debezium.io/blog/2019/02/19/reliable-microservices-data-exchange-with-the-outbox-pattern/">Reliable Microservices Data Exchange With the Outbox Pattern</a>. At <a href="/derivco">work</a> we have started looking at <a href="https://twitter.com/debezium">Debezium</a> as a way to get data from the database into other systems, and while I was investigating this, I came across the linked blog-post. If you are interested in how to turn your databases into event stream sources, then this post is a must read!</li>
</ul>

<h2 id="sql-server-2019">SQL Server 2019</h2>

<ul>
<li><a href="https://docs.microsoft.com/en-us/sql/sql-server/sql-server-ver15-release-notes?view=sqlallproducts-allversions">SQL Server 2019 preview release notes</a>. Earlier this week, Microsoft released SQL Server 2019 preview CTP 2.5. Some very cool new features! Go and get it!</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 16, 2019]]></title>
    <link href="http://nielsberglund.com/2019/04/21/interesting-stuff---week-16-2019/" rel="alternate" type="text/html"/>
    <updated>2019-04-21T16:36:40+02:00</updated>
    <id>http://nielsberglund.com/2019/04/21/interesting-stuff---week-16-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://devblogs.microsoft.com/dotnet/announcing-net-core-3-preview-4/">Announcing .NET Core 3 Preview 4</a>. Some new interesting features in Preview 4 of .NET Core 3.0. Hope we&rsquo;ll see RTM soon.</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://towardsdatascience.com/learn-enough-docker-to-be-useful-b7ba70caeb4b">Learn Enough Docker to be Useful Part 1: The Conceptual Landscape</a>. This post is the first &ldquo;episode&rdquo; in a series about how to learn Docker. It is excellent, and for a Docker newbie as myself quite invaluable!</li>
<li><a href="https://towardsdatascience.com/key-kubernetes-concepts-62939f4bc08e">Key Kubernetes Concepts</a>. This article covers essential Kubernetes concepts, and it’ll help you make a mental model of the most important Kubernetes terms to speed your understanding of the technology.</li>
<li><a href="https://towardsdatascience.com/https-medium-com-bachwehbi-data-lake-an-asset-or-a-liability-c424c74cfde8">Data Lake: an asset or a liability?</a>. This post looks at several important points to take into account when starting a Data Lake project.</li>
</ul>

<h2 id="sql-server">SQL Server</h2>

<ul>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2019/04/18/the-april-release-of-azure-data-studio-is-now-available/">The April release of Azure Data Studio is now available</a>. As the title says; Microsoft has released a new version (April 2019) of Azure Data Studio. There are quite a few new interesting features, and I cannot wait to try out the <a href="https://microsoft.github.io/SandDance/">SandDance</a> extension.</li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://towardsdatascience.com/how-to-create-your-own-deep-learning-project-in-azure-509660d8297">How to create your own Deep Learning Project in Azure</a>. In this article, the author takes us through how to use Azure Databricks with Tensorflow and Keras to build a deep learning project.</li>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2019/04/16/automated-machine-learning-from-sql-server-with-azure-machine-learning/">Automated machine learning from SQL Server with Azure Machine Learning</a>. This post discusses how to leverage Azure Machine Learning Service from SQL Server Machine Learning Services.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/kafka-summit-new-york-2019-session-videos">Kafka Summit New York 2019 Session Videos</a>. The Kafka Summit took place in New York earlier this month. This post links to the session videos.</li>
<li><a href="https://www.confluent.io/blog/dawn-of-devops-managing-and-evolving-schemas-with-confluent-control-center">Dawn of DevOps: Managing and Evolving Schemas with Confluent Control Center</a>. This post is the first in a three-part series of DevOps related &ldquo;stuff&rdquo;. This first post looks at how to use Confluence Control Center to manage and evolve schemas.</li>
<li><a href="https://towardsdatascience.com/beat-cache-invalidation-in-asp-net-core-using-kafka-and-debezium-65cd1d80554d">Beat Cache Invalidation in ASP.NET Core Using Kafka and Debezium</a>. This article discusses how to create a better in-memory cache in ASP.NET Core by using Change Data Capture on a database to send events to Apache Kafka via Debezium.</li>
<li><a href="https://rmoff.net/2019/04/17/pivoting-aggregates-in-ksql/">Pivoting Aggregates in Ksql</a>. This is an excellent post by <a href="https://twitter.com/rmoff">Robin</a> where he show how we can do pivoting of aggregates in KSQL.</li>
</ul>

<h2 id="sql-server-machine-learning-services">SQL Server Machine Learning Services</h2>

<ul>
<li><a href="/2019/04/21/installing-r-packages-in-sql-server-machine-learning-services---iv-permissions/">Installing R Packages in SQL Server Machine Learning Services - IV: Permissions</a>. In this post, I look at permissions required when using <code>CREATE EXTERNAL LIBRARY</code> as well as ownership of the created libraries.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Installing R Packages in SQL Server Machine Learning Services - IV: Permissions]]></title>
    <link href="http://nielsberglund.com/2019/04/21/installing-r-packages-in-sql-server-machine-learning-services---iv-permissions/" rel="alternate" type="text/html"/>
    <updated>2019-04-21T11:15:19+02:00</updated>
    <id>http://nielsberglund.com/2019/04/21/installing-r-packages-in-sql-server-machine-learning-services---iv-permissions/</id>
    <content type="html"><![CDATA[<p>This post is the fourth in a series about installing R packages in <strong>SQL Server Machine Learning Services</strong> (SQL Server ML Services). To see all posts in the series go to <a href="/sql_server_ml_services_install_packages"><strong>Install R Packages in SQL Server ML Services Series</strong></a>.</p>

<p>Why this series came about is a colleague of mine <a href="https://www.linkedin.com/in/dane-bax/">Dane</a> pinged me and asked if I had any advice as he had issues installing an R package into one of their SQL Server instances. I tried to help him and then thought it would make a good topic for a blog post. Of course, at that time I didn&rsquo;t think it would be more posts than one, but here we are.</p>

<p>In this post, we look at:</p>

<ul>
<li>What permissions <code>CREATE EXTERNAL LIBRARY</code> requires.</li>
<li>The ability to create external libraries with different owners and what impact it has.</li>
</ul>

<p></p>

<p>Let us do a recap to see where we are.</p>

<h2 id="recap">Recap</h2>

<p>In the last post; <a href="/2019/04/10/installing-r-packages-in-sql-server-machine-learning-services---iii/">Installing R Packages in SQL Server Machine Learning Services - III</a> we looked at how to deploy R packages to SQL Server without having to have file system access to the machine SQL Server runs on.</p>

<p>We achieve this by creating an external library, using a DDL statement <code>CREATE EXTERNAL LIBRARY</code>, on the database we want to use the R package on. What <code>CREATE EXTERNAL LIBRARY</code> does, is it uploads package files to a database from a file path or byte stream. The signature looks like so:</p>

<pre><code class="language-sql">CREATE EXTERNAL LIBRARY library_name  
[ AUTHORIZATION owner_name ]  
FROM (CONTENT = { &lt;file_spec&gt; }  
    [, PLATFORM = &lt;platform&gt; ]) 
WITH ( LANGUAGE = '&lt;language&gt;' )  
[ ; ] 
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Signature CREATE EXTERNAL LIBRARY</em></p>

<p>The arguments we see in <em>Code Snippet 1</em> are:</p>

<ul>
<li><code>library_name</code>: A unique name for the package. The unique:ness is based on the name and the principal id under which it is created. We look closer at that in this post.</li>
<li><code>owner_name</code>: This optional parameter specifies the name of the user or role that owns the external library. More about that later in this post as well.</li>
<li><code>file_spec</code>: The <code>file_spec</code> specifies the content of the package for a specific platform, and it can either be in the form of a file location (local path/network path) or a hex literal.</li>
<li><code>platform</code>: An optional parameter and right now only Windows is supported.</li>
<li><code>language</code>: Specifies the language of the package. In SQL Server 2017 the only supported language is R.</li>
</ul>

<p>One of the examples we used throughout the post looked like this:</p>

<pre><code class="language-sql">CREATE EXTERNAL LIBRARY randomForest
FROM (CONTENT = 'W:\randomForest_4.6-14.zip') 
WITH (LANGUAGE = 'R'); 
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Create External Library</em></p>

<p>In <em>Code Snippet 2</em> we:</p>

<ul>
<li>Name the external library <code>randomForest</code>.</li>
<li>Indicate where the package file is (it has to be a zipped file).</li>
<li>Set R as the language.</li>
</ul>

<p>The code works fine, but the problem is that the package file has to be in a location where SQL Server can read the file, and this - most likely - requires access to the box where SQL Server is installed.</p>

<p>In the <a href="/2019/04/10/installing-r-packages-in-sql-server-machine-learning-services---iii/">previous post</a> we discussed how we could create an external library from the hex-literal of the package, and we mentioned two different ways to accomplish this:</p>

<ul>
<li>From a local database.</li>
<li>Generate binary from code.</li>
</ul>

<h4 id="local-datbase">Local Datbase</h4>

<ol>
<li>Create an external library from the R package based on the file path in a local SQL Server where we have access to the file system (like <code>localhost</code>).</li>
<li>Get the binary representation from the <code>content</code> column in <code>sys.external_library_files</code> via some XML &ldquo;magic&rdquo;.</li>
<li>Assign the retrieved value to the <code>CONTENT</code> parameter in <code>CREATE EXTERNAL LIBRARY</code>.</li>
<li>Execute <code>CREATE EXTERNAL LIBRARY</code>.</li>
</ol>

<h4 id="generate-from-code">Generate from Code</h4>

<ol>
<li>Write script code which generates the binary representation.</li>
<li>Follow from step 3 above (local database).</li>
</ol>

<p>Alternatively, you can connect to the database from inside the script and call <code>CREATE EXTERNAL LIBRARY</code> from the script.</p>

<h2 id="housekeeping">Housekeeping</h2>

<p>Before we &ldquo;dive&rdquo; into today&rsquo;s topics let us look at the code we use today. This section is here for those of who want to follow along in what we are doing in the post.</p>

<pre><code class="language-sql">USE master;
GO

DROP DATABASE IF EXISTS DataScienceDB;
GO

IF EXISTS(SELECT 1 FROM sys.server_principals WHERE name = 'dane')
BEGIN
  DROP LOGIN dane;
END

CREATE LOGIN dane
WITH PASSWORD = 'password1234$';

IF EXISTS(SELECT 1 FROM sys.server_principals WHERE name = 'nielsb')
BEGIN
  DROP LOGIN nielsb;
END

CREATE LOGIN nielsb
WITH PASSWORD = 'password1234$';

CREATE DATABASE DataScienceDB;
GO

USE DataScienceDB;
GO

CREATE USER dane
FROM LOGIN dane;

CREATE USER nielsb
FROM LOGIN nielsb;

ALTER ROLE db_owner
  ADD MEMBER nielsb;
GO

USE master;
GO

GRANT EXECUTE ON sp_execute_external_script TO public;
GO

USE DataScienceDB;
GO

GRANT EXECUTE ANY EXTERNAL SCRIPT TO dane;
GRANT EXECUTE ANY EXTERNAL SCRIPT TO nielsb;
GO

USE DataScienceDB;
GO
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Create Logins, Database and Users</em></p>

<p>In <em>Code Snippet 3</em> we create some logins as well as a database and in that database users for the logins. As you see, we do continue with the &ldquo;theme&rdquo; of Dane the data scientist wanting to do &ldquo;stuff&rdquo; in the database. As <code>nielsb</code> is seen to be &ldquo;trustworthy&rdquo; (take that Dane), we add him to the <code>db_owner</code> role.</p>

<p>In the last part of <em>Code Snippet 3</em> we assign some permissions to <code>sp_execute_external_script</code>, as we did in the post <a href="/2018/06/24/sp_execute_external_script-and-permissions/">sp_execute_external_script and Permissions</a>.</p>

<p>Oh, and if you want to follow along, ensure you download the <code>randomForest</code> package from <a href="https://cran.r-project.org/bin/windows/contrib/3.6/randomForest_4.6-14.zip">here</a>.</p>

<h2 id="permissions">Permissions</h2>

<p>Let us look at what permissions we need when creating an external library. Here is what we do:</p>

<ul>
<li>Log on to SQL Server and the <code>DataScienceDB</code> database as <code>sa</code>.</li>
<li>Drop the <code>randomForest</code> external library if you have created it: <code>DROP EXTERNAL LIBRARY randomForest</code>.</li>
<li>Restart the <em>Launchpad</em> service, this is to clean up properly.</li>
</ul>

<p>After we restart the <em>Launchpad</em> service we want to create an external library as the user <code>dane</code>:</p>

<pre><code class="language-sql">EXECUTE AS USER = 'dane';

CREATE EXTERNAL LIBRARY randomForest
FROM (CONTENT = 'W:\randomForest_4.6-14.zip') 
WITH (LANGUAGE = 'R');
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Creating External Library as Dane</em></p>

<p>In <em>Code Snippet 4</em> we see how we emulate being logged in as user <code>dane</code>: <code>EXECUTE AS USER = 'dane'</code> and how we then execute. Unfortunately, when we run the code we get an error:</p>

<p><img src="/images/posts/sql_ml_install_r_pckgs4_perm_error1.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Permission Error</em></p>

<p>In <em>Figure 1</em> we see that <code>dane</code> does not have permission to <code>CREATE EXTERNAL LIBRARY</code>. We can fix that quickly:</p>

<pre><code class="language-sql">REVERT

GRANT CREATE EXTERNAL LIBRARY TO Dane;

EXECUTE AS USER = 'dane';

CREATE EXTERNAL LIBRARY randomForest
FROM (CONTENT = 'W:\randomForest_4.6-14.zip') 
WITH (LANGUAGE = 'R');
</code></pre>

<p><strong>Code Snippet 5:</strong> <em>Grant Permissions</em></p>

<p>In <em>Code Snippet 5</em> we:</p>

<ul>
<li><code>REVERT</code> back from the <code>dane</code> user to <code>sa</code>.</li>
<li><code>GRANT</code> permissions to <code>dane</code> to create external libraries.</li>
<li>Switch back to <code>dane</code>.</li>
<li>Execute as <code>dane</code>.</li>
</ul>

<p>However, when we execute as <code>dane</code> we get another error:</p>

<p><img src="/images/posts/sql_ml_install_r_pckgs4_perm_error2.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Impersonation Error</em></p>

<p>We have moved past the permission error, as we in <em>Figure 2</em> see that we do not get the permission error, but we get another error, something about impersonation. What is this about, can it be related to what roles <code>dane</code> is in (remember he is only part of <code>PUBLIC</code>)? Let us test that theory, and let us use <code>nielsb</code> who is more trusted than <code>dane</code>, and is part of <code>db_owner</code>.</p>

<p>So what we do is we copy the code in <em>Code Snippet 5</em>, but replace <code>EXECUTE AS USER = 'dane'</code> with <code>EXECUTE AS USER = 'nielsb'</code>. The assumption is that being part of <code>db_owner</code> should fix this, but when <code>nielsb</code> executes he gets the same error as in <em>Figure 2</em>.</p>

<blockquote>
<p><strong>NOTE:</strong> Notice that we did not have to give <code>nielsb</code> explicit permissions to create external libraries. He has those permissions implicitly just by being part of the <code>db_owner</code> role.</p>
</blockquote>

<p>The problem we run into here is that even if you have the correct permissions to create an external library, you do not have the correct permissions to execute something that reads from the file system. So how do we solve this, we have two options:</p>

<ol>
<li>Add the user to the <code>sysadmin</code> server role.</li>
<li>Create the external library from the package hex-literal.</li>
</ol>

<p>Option 1 is quick and dirty, but I would not recommend it (<code>dane</code> as <code>sysadmin</code>???!!!). Option 2 is better and seeing that you most likely use hex-literal anyway when you deploy to a remote SQL Server it makes sense.</p>

<p>So if you want to follow along in this post, I recommend you go and read up on, in <a href="/2019/04/10/installing-r-packages-in-sql-server-machine-learning-services---iii/">Installing R Packages in SQL Server Machine Learning Services - III</a>, how to generate a hex literal from an R Package. We continue when you are back.</p>

<p>Welcome back!</p>

<p>After having read the <a href="/2019/04/10/installing-r-packages-in-sql-server-machine-learning-services---iii/">post</a> above we now have a hex-literal for the <code>randomForest</code> package. Let <code>dane</code> use that to create an external library from:</p>

<pre><code class="language-sql">REVERT

EXECUTE AS USER = 'dane';

DECLARE @hexLit varbinary(max) = 
0x504B03040A00000000009982964C0000000000000000000000000D00000072...
4154494f4e95514d6f83300c3d0f89ff60e504520b1dbd4c953854d5a61dda1e...
...

CREATE EXTERNAL LIBRARY randomForest
FROM (CONTENT = @hexLit)
WITH (LANGUAGE = 'R');
GO

SELECT * FROM sys.external_libraries
</code></pre>

<p><strong>Code Snippet 6:</strong>  <em>Create External Library from Hex Literal Variable</em></p>

<p>In <em>Code Snippet 6</em> we see how we:</p>

<ul>
<li>Emulate <code>dane</code>.</li>
<li>Assign the hex-literal value to the variable.</li>
<li>Call <code>CREATE EXTERNAL LIBRARY</code>.</li>
</ul>

<p>All works OK, but the last <code>SELECT</code> does not return anything. Did we silently fail? Let us try to find out:</p>

<pre><code class="language-sql">REVERT

SELECT * FROM sys.external_libraries
</code></pre>

<p><strong>Code Snippet 7:</strong>  <em>Retrieving External Libraries as sa</em></p>

<p>We see in <em>Code Snippet 7</em> how:</p>

<ul>
<li>We<code>REVERT</code> back to <code>sa.</code></li>
<li>We do a <code>SELECT</code> against <code>sys.external_libraries</code>.</li>
</ul>

<p>When we run the code, the result is like so:</p>

<p><img src="/images/posts/sql_ml_install_r_pckgs4_view_libs1.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Result of Selecting as sa</em></p>

<p>Aha, <em>Figure 3</em> shows us that <code>dane</code> managed to create the external library, cool! If we now want to drop the library, only <code>dane</code> can do that, and he needs to have <code>ALTER EXTERNAL LIBRARY</code> permissions. We discuss more why <code>dane</code> is the only one that can drop the library later in this post, together with why I have outlined three of the columns in <em>Figure 3</em>.</p>

<blockquote>
<p><strong>NOTE:</strong> The reason <code>dane</code> does not get any results when he tries to <code>SELECT</code> against <code>sys.external_libraries</code> is because of a bug in SQL Server 2017. That particular bug is fixed in CU2, so it should not be an issue.</p>
</blockquote>

<h4 id="permissions-summary">Permissions Summary</h4>

<p>Let us do a quick summary of what we have discussed so far:</p>

<ul>
<li>To create an external library from a hex-literal you need to be part of the <code>db_owner</code> role, or have explicit <code>CREATE EXTERNAL LIBRARY</code> permissions.</li>
<li>To create an external library from a file path you need to be part of the <code>sysadmin</code> server role.</li>
</ul>

<h2 id="authorization-ownership">Authorization &amp; Ownership</h2>

<p>Now, when <code>dane</code> has created an external library let us just check that <code>dane</code> actually can use it:</p>

<pre><code class="language-sql">REVERT

EXECUTE AS USER = 'dane';

EXEC sp_execute_external_script 
                    @language = N'R', 
                    @script = N'library(&quot;randomForest&quot;)'
</code></pre>

<p><strong>Code Snippet 8:</strong> <em>Execute as dane Using External Library</em></p>

<p>When we run the code in <em>Code Snippet 8</em> it all works! Let us now see what happens when <code>sa</code> tries to execute:</p>

<pre><code class="language-sql">REVERT

EXEC sp_execute_external_script 
                    @language = N'R', 
                    @script = N'library(&quot;randomForest&quot;)'
</code></pre>

<p><strong>Code Snippet 9:</strong> <em>Execute as sa Using External Library</em></p>

<p>We see in <em>Code Snippet 9</em> how we <code>REVERT</code> back to <code>sa</code>, (as that was what we logged in as), and we then call <code>sp_execute_external_script</code>. However, when we execute, the result is:</p>

<p><img src="/images/posts/sql_ml_install_r_pckgs4_exec_error1.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Error When Executing as sa</em></p>

<p>That is strange, the error we see in <em>Figure 4</em> says that the <code>randomForest</code> package does not exist, even though <em>Figure 3</em> shows it. The reason for this can be explained by looking at <em>Figure 3</em> more closely, and especially the three outlined columns: <code>principal_id</code>, <code>scope</code>, and <code>scope_desc</code>.</p>

<p>We see in <em>Figure 3</em> how the <code>principal_id</code> column, (outlined in red), has a value of 5, which happens to be the database principal id of <code>dane</code>. When you create an external library, and you do not specifically set a value for <code>owner_name</code> you become the owner. So what about the two columns outlined in yellow; <code>scope</code>, and <code>scope_desc</code>. They define who can use the library, and any library with an owner other than <code>dbo</code> is private, which means that only the owner can use it. So that explains, (from above), why only <code>dane</code> can drop the library.</p>

<p>So what about <code>nielsb</code>, he is part of the <code>db_owner</code> role, what happens when he creates an external library? Well, do what we did in <em>Code Snippet 6</em>, but replace <code>EXECUTE AS USER = 'dane'</code> with <code>EXECUTE AS USER = 'nielsb'</code>, and run the code. Since <code>nielsb</code> is part of <code>db_owner</code> the <code>SELECT</code> statement works and returns this:</p>

<p><img src="/images/posts/sql_ml_install_r_pckgs4_view_libs2.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Multiple Libraries - I</em></p>

<p>We now have two <code>randomForest</code> libraries, as we see in <em>Figure 5</em>, and these two libraries have different owners as we see from the <code>principal_id</code> (5 for <code>dane</code>, and 6 for <code>nielsb</code>). They are both <code>PRIVATE</code> in scope, so only <code>dane</code> can use the external library with an id of 1, and <code>nielsb</code> only the library with an id of 2. If <code>sa</code> tried to run the code in <em>Code Snippet 9</em> it would fail as in <em>Figure 4</em>.</p>

<p>The above makes sense, kind of. The question is why the library <code>nielsb</code> created is <code>PRIVATE</code> as <code>nielsb</code> belongs to the <code>db_owner</code> role? The answer is what I wrote above, about not setting a value for <code>owner_name</code>. As <code>nielsb</code> did not indicate an owner name, he became the owner, and any libraries not owned by the <code>dbo</code> principal is always <code>PRIVATE</code>. However, seeing that <code>nielsb</code> is in the <code>db_owner</code> role, he can run some code like this:</p>

<pre><code class="language-sql">REVERT

EXECUTE AS USER = 'nielsb';

DECLARE @hexLit varbinary(max) = 
0x504B03040A00000000009982964C0000000000000000000000000D00000072...
4154494f4e95514d6f83300c3d0f89ff60e504520b1dbd4c953854d5a61dda1e...
...

CREATE EXTERNAL LIBRARY randomForest
AUTHORIZATION dbo
FROM (CONTENT = @hexLit)
WITH (LANGUAGE = 'R');
GO

SELECT * FROM sys.external_libraries
</code></pre>

<p><strong>Code Snippet 10:</strong> <em>Create External Library with dbo as Owner</em></p>

<p>In <em>Code Snippet 10</em> we see how we set the <code>owner_name</code> to <code>dbo</code>, and when we run the code the result of the <code>SELECT</code> is like so:</p>

<p><img src="/images/posts/sql_ml_install_r_pckgs4_view_libs3.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>Multiple Libraries - II</em></p>

<p>We see <em>Figure 6</em> 3 libraries and the last one has a <code>principal_id</code> of 1 (<code>dbo</code>), and the scope is <code>PUBLIC</code>. If you want to you can <code>REVERT</code> back to <code>sa</code> and execute the code in <em>Code Snippet 9</em>. This time it works, as one of the <code>randomForest</code> libraries are <code>PUBLIC</code>.</p>

<blockquote>
<p><strong>NOTE:</strong> No, <code>dane</code> cannot set the <code>owner_name</code> to <code>dbo</code> as he does not have sufficient privileges, (he is not part of <code>db_owner</code>).</p>
</blockquote>

<p>We have now three different libraries with the same name, how does the engine know what library to load, and from where? That is a good question, let us run some code we used in the <a href="/2019/04/10/installing-r-packages-in-sql-server-machine-learning-services---iii/">previous post</a>:</p>

<pre><code class="language-sql">EXEC sp_execute_external_script 
                    @language = N'R', 
                    @script = N'
                    OutputDataSet &lt;- data.frame(
                    installed.packages()[,c(&quot;Package&quot;, &quot;LibPath&quot;)]);'
WITH RESULT SETS ((Package nvarchar(255), LibPath nvarchar(2000)));
</code></pre>

<p><strong>Code Snippet 11:</strong> <em>View R Packages</em></p>

<p>The code in <em>Code Snippet 11</em> retrieves installed R packages, and if we run the code as <code>sa</code> we get the following result:</p>

<p><img src="/images/posts/sql_ml_install_r_pckgs4_lib_path1.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>Randomforest Library Path for sa</em></p>

<p>What is interesting in <em>Figure 7</em> is that we only see one <code>randomForest</code> library, whereas if we execute the same code as <code>dane</code> we see:</p>

<p><img src="/images/posts/sql_ml_install_r_pckgs4_lib_path2.png" alt="" /></p>

<p><strong>Figure 8:</strong> <em>Randomforest Library Path for dane</em></p>

<p>When we look at <em>Figure 8</em> we see two different library paths (where the package is) for the two <code>randomForest</code> packages. We see how they differ based on database id, (5 in both cases), and principal id, where the first one has a principal id of 5, (<code>dane</code>), and the second 1, (<code>dbo</code>). What we have seen here explains the error we saw in <em>Figure 4</em> when we tried to execute as <code>sa</code> - the package was not available to <code>sa</code>. We can also assume that packages load based on principal id, and the resolution logic and order is like so:</p>

<ol>
<li>Load a package which matches on name and principal id.</li>
<li>Load a package which matches on name and is public.</li>
<li>Load a package which matches on name and is located in the default library path.</li>
</ol>

<h2 id="summary">Summary</h2>

<p>In this post, we looked at permissions required when creating external libraries, and also ownership of the libraries.</p>

<p>To create an external library, you need to have explicit <code>CREATE EXTERNAL LIBRARY</code> permissions, or be - at least - part of the <code>db_owner</code> role. If you want to create a library based on a package path, instead of a hex-literal, you need to also to be in the <code>sysadmin</code> server role.</p>

<p>When you create an external library the library is owned by you, and can only be used by you - it is <code>PRIVATE</code>. However, if you set the <code>owner_name</code> to <code>dbo</code>, the library is <code>PUBLIC</code> and can be used by any user.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 15, 2019]]></title>
    <link href="http://nielsberglund.com/2019/04/14/interesting-stuff---week-15-2019/" rel="alternate" type="text/html"/>
    <updated>2019-04-14T18:24:53+02:00</updated>
    <id>http://nielsberglund.com/2019/04/14/interesting-stuff---week-15-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/steeltoe-pcf">Enabling .NET Apps with Monitoring and Management Using Steeltoe</a>. An <a href="https://www.infoq.com/">InfoQ</a> presentation discussing using the Steeltoe Management framework to enable a .NET application with performance monitoring, management diagnostic endpoints, and distributed tracing on PCF.</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="http://highscalability.com/blog/2019/4/8/from-bare-metal-to-kubernetes.html">From Bare-Metal To Kubernetes</a>. A very interesting blog post which talks about going from a bare metal infrastructure to a highly scalable Kubernets infrastructure running in the cloud. If you are thinking about moving to Kubernetes, the post is well worth a read.</li>
</ul>

<h2 id="azure">Azure</h2>

<ul>
<li><a href="http://muratbuffalo.blogspot.com/2019/04/azure-cosmos-db-microsofts-cloud-born.html">Azure Cosmos DB: Microsoft&rsquo;s Cloud-Born Globally Distributed Database</a>. An excellent blog post by <a href="https://twitter.com/muratdemirbas">Murat</a> where he discusses the workings of Azure Cosmos DB. This post is a must read if you are interested in the inner workings of Cosmos DB.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/monitoring-data-replication-in-multi-datacenter-apache-kafka-deployments">Monitoring Data Replication in Multi-Datacenter Apache Kafka Deployments</a>. A blog post which describes how to use Confluent Replicator and Confluent Control Center to monitor Kafka deployment and replication between multiple data centers.</li>
</ul>

<h2 id="sql-server-machine-learning-services">SQL Server Machine Learning Services</h2>

<ul>
<li><a href="/2019/04/10/installing-r-packages-in-sql-server-machine-learning-services---iii/">Installing R Packages in SQL Server Machine Learning Services - III</a>. Six months after I posted the <a href="/2018/06/30/installing-r-packages-in-sql-server-machine-learning-services---ii/">second post</a> in the <a href="/sql_server_ml_services_install_packages">Install R Packages in SQL Server ML Services</a> series I posted the third &ldquo;episode&rdquo;. In this post, we look at how to install R packages in SQL Server Machine Learning Services using the T-SQL DDL command <code>CREATE EXTERNAL LIBRARY</code>.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Installing R Packages in SQL Server Machine Learning Services - III]]></title>
    <link href="http://nielsberglund.com/2019/04/10/installing-r-packages-in-sql-server-machine-learning-services---iii/" rel="alternate" type="text/html"/>
    <updated>2019-04-10T06:36:16+02:00</updated>
    <id>http://nielsberglund.com/2019/04/10/installing-r-packages-in-sql-server-machine-learning-services---iii/</id>
    <content type="html"><![CDATA[<p>This post is the third in a series about installing R packages in <strong>SQL Server Machine Learning Services</strong> (SQL Server ML Services). To see all posts in the series go to <a href="/sql_server_ml_services_install_packages"><strong>Install R Packages in SQL Server ML Services Series</strong></a>.</p>

<p>Why this series came about is a colleague of mine <a href="https://www.linkedin.com/in/dane-bax/">Dane</a> pinged me and asked if I had any advice as he had issues installing an R package into one of their SQL Server instances. I tried to help him and then thought it would make a good topic for a blog post. Of course, at that time I didn&rsquo;t think it would be more posts than one, but here we are.</p>

<p>In this post, we look at how we can use T-SQL and DDL commands to install packages in a remote SQL Server.</p>

<p></p>

<p>Let us do a recap to see where we are.</p>

<h2 id="recap">Recap</h2>

<p>The first <a href="/2018/06/23/installing-r-packages-in-sql-server-machine-learning-services---i/">post</a> in the series gave an overview of what ways we can install packages in the external R engine in SQL Server ML Services:</p>

<ul>
<li>R packet managers</li>
<li>T-SQL</li>
<li>RevoScaleR</li>
</ul>

<p>The <a href="/2018/06/23/installing-r-packages-in-sql-server-machine-learning-services---i/">post</a> then went into details about using R packet managers, where an R packet manager is an R command line tool or GUI installed on the SQL Server Machine Learning Services machine. The packet manager should be run with elevated permissions and target the R engine for the instance on which you want to install the package. The easiest is to use either of the R tools that come as part of SQL Server&rsquo;s R service:</p>

<ul>
<li>The command line tool: <code>Rterm.exe</code>.</li>
<li>The GUI: <code>Rgui.exe</code>.</li>
</ul>

<p>These two packet managers live in the <code>\\&lt;path_to_SQL_Server_instance&gt;\R_SERVICES\bin\x64</code> directory. When you install packages via an R packet manager, they can only be installed to the default packet library for that instance. You find the library at: <code>\\&lt;path_to_SQL_Server_instance&gt;\R_SERVICES\library</code>.  The file system folder for this library has restricted access and you need elevated permissions to write to this folder. Typical code for installing packages from a packet manager can look like so:</p>

<pre><code class="language-r"># set the library path
libPath &lt;- C:\\path_to_SQL_Server_instance&gt;\\R_SERVICES\\library
install.packages(&quot;pkg_name&quot;, lib = libPath, 
                  repos = &quot;url_for_the_repo&quot;, 
                  dependencies = TRUE)
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Install Packages Command</em></p>

<p>In <em>Code Snippet 1</em> we use <code>install.packages</code> to install &ldquo;pkg_name&rdquo; to a hardcoded library path.</p>

<p>Using an R Package manager is the most straight forward way to install R packages, but the downside with it is that you need admin rights on the SQL Server box. Having admin rights on a SQL Server box in production can be an issue, and in <a href="/2018/06/30/installing-r-packages-in-sql-server-machine-learning-services---ii/">Installing R Packages in SQL Server Machine Learning Services - II</a> we looked at how we can install packages without having admin rights, by using RevoScaleR:</p>

<ul>
<li>To use RevoScaleR for package installation both the SQL Server instance as well as the database need to be enabled for package management. You enable package management via <code>RegisterRExt.exe</code> tool and the <code>/installpkgmgmt</code> option. There are additional flags for database enabling, authentication and so forth.</li>
<li>When enabling the database the process creates a table, stored procedures and roles.</li>
<li>For a user to be able to install packages he needs to have necessary permissions on <code>sp_execute_external_script</code> as well as the <code>EXECUTE ANY EXTERNAL SCRIPT</code> permission. He also needs to be in a role which allows him to install packages.</li>
<li>The roles that the enabling process creates are: <code>rpkgs-users</code>, <code>rpkgs-private</code> and <code>rpkgs-shared</code>.</li>
<li>The roles which allow the user to install packages are <code>rpkgs-private</code> and <code>rpkgs-shared</code> (and <code>db_owner</code>).</li>
<li>The roles define the scope of the installed packages: <code>private</code> and <code>shared</code>.</li>
<li>When a user installs a package with <code>private</code> scope, only he can see and use the package.</li>
<li>If the user installs a package with <code>shared</code> scope, all users in any of the roles, including <code>rpkgs-users</code> can use that package. The user needs to be in the <code>rpkgs-shared</code> (or <code>db_owner</code>) to install a <code>shared</code> package.</li>
<li>You use the function <code>rxInstallPackages</code> to install a package, and the function needs to run in an <em>SQLCC</em>.</li>
<li>When the user calls <code>rxInstallPackages</code> he needs to define which scope the package has through the <code>scope</code> argument. If the <code>scope</code> is not defined, it defaults to <code>private</code>.</li>
<li>To use a package, either in <code>private</code> or <code>shared</code> scope, the code needs to run in <em>SQLCC</em>.</li>
<li>For a package that does not know <em>SQLCC</em>, the functions in the package can be run via <code>rxExec</code>.</li>
</ul>

<h2 id="housekeeping">Housekeeping</h2>

<p>Before we &ldquo;dive&rdquo; into today&rsquo;s topics let us look at the code we use today. This section is here for those of you who want to follow along in what we are doing in the post.</p>

<pre><code class="language-sql">USE master;
GO

DROP DATABASE IF EXISTS DataScienceDB;
GO

DROP DATABASE IF EXISTS DataScienceDBRemote;
GO

CREATE DATABASE DataScienceDB;
GO

CREATE DATABASE DataScienceDBRemote;
GO
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Create Databases</em></p>

<p>In <em>Code Snippet 2</em> we create two databases; <code>DataScienceDB</code> and <code>DataScienceDBRemote</code> where the latter is to emulate a database on a remote SQL Server instance. In previous posts, we have created logins and users, but in this post, we only log in as <code>sa</code>.</p>

<h2 id="installing-r-packages-using-t-sql">Installing R Packages Using T-SQL</h2>

<p>In the <em>Recap</em> above we said that in previous posts we have looked at installing R packages either by using R package managers on the SQL Server box, or doing it remotely via script using RevoScaleR. The third option we have is to do it via a T-SQL statement. More specifically through a statement introduced in SQL Server 2017: <code>CREATE EXTERNAL LIBRARY</code>.</p>

<blockquote>
<p><strong>NOTE:</strong> In SQL Server 2017 only R packages are supported whereas, in SQL Server 2019 R, Python and Java are supported. For both SQL Server 2017 and 2019 (up to and including CTP 2.3) only the Windows platform is supported. For SQL Server 2019, Linux may be added as a supported platform in later CTP releases.</p>
</blockquote>

<p>What <code>CREATE EXTERNAL LIBRARY</code> does is it uploads package files to a database from a file path or byte stream. The signature looks like so:</p>

<pre><code class="language-sql">CREATE EXTERNAL LIBRARY library_name  
[ AUTHORIZATION owner_name ]  
FROM (CONTENT = { &lt;file_spec&gt; }  
    [, PLATFORM = &lt;platform&gt; ]) 
WITH ( LANGUAGE = '&lt;language&gt;' )  
[ ; ] 
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Signature CREATE EXTERNAL LIBARY</em></p>

<p>The arguments we see in <em>Code Snippet 3</em> are:</p>

<ul>
<li><code>library_name</code>: A unique name for the package. When we create an external library for an R package, the name has to be the actual package name. While this may seem obvious, I mention it as when you create external libraries for Java code the name does not matter. We discussed this in the <a href="/2019/03/17/sql-server-2019-java--external-libraries---ii/">SQL Server 2019, Java &amp; External Libraries - II</a> post. When I say the package name has to be unique, the unique:ness is based on the name and the principal id under which it is created. We talk more about that in a future post.</li>
<li><code>owner_name</code>: This optional parameter specifies the name of the user or role that owns the external library. More about this in a future post as well.</li>
<li><code>file_spec</code>: The <code>file_spec</code> specifies the content of the package for a specific platform, and it can either be in the form of a file location (local path/network path) or a hex literal. If we want to install an R package from a file location, the package needs to be in the form of a zipped archive file. If we install based on a hex-literal, the hex-literal need to derive from the package zip file.</li>
<li><code>platform</code>: The <code>PLATFORM</code> parameter, which defines the platform for the content of the library. The <code>PLATFORM</code> parameter defaults to the platform on which SQL Server runs on, and since <code>CREATE EXTERNAL LIBRARY</code> is only supported on Windows, for now, we do not set it.</li>
<li><code>language</code>: Specifies the language of the package. For this post we only deal with <code>R</code>, and - as I mentioned above - in SQL Server 2017, R is the only language supported.</li>
</ul>

<h2 id="using-create-external-library">Using CREATE EXTERNAL LIBRARY</h2>

<p>To see how to use <code>CREATE EXTERNAL LIBRARY</code> we want to install the <code>randomForest</code> package into our <code>DataScienceDB</code> database. We start with downloading the <code>randomForest</code> zip archive to a directory which is readable by SQL Server. I have it at <code>W:\randomForest_4.6-14.zip</code>. We log in to the server and database as <code>dbo</code> (<code>sa</code> login), and we are ready to execute the <code>CREATE EXTERNAL LIBRARY</code> DDL:</p>

<pre><code class="language-sql">CREATE EXTERNAL LIBRARY randomForest
FROM (CONTENT = 'W:\randomForest_4.6-14.zip') 
WITH (LANGUAGE = 'R'); 
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Create External Library</em></p>

<p>As we see in <em>Code Snippet 4</em> I name the external library <code>randomForest</code>, as that is the name of the R package, and I set the location of where the package <code>zip</code> file is. Before we execute the code in <em>Code Snippet 4</em>, let us look at what R packages we have installed already:</p>

<pre><code class="language-sql">EXEC sp_execute_external_script 
                    @language = N'R', 
                    @script = N'
                    OutputDataSet &lt;- data.frame(
                    installed.packages()[,c(&quot;Package&quot;, &quot;LibPath&quot;)]);'
WITH RESULT SETS ((Package nvarchar(255), LibPath nvarchar(2000)));
</code></pre>

<p><strong>Code Snippet 5:</strong> <em>View R Packages</em></p>

<p>When we execute the code in <em>Code Snippet 5</em> we see something like so:</p>

<p><img src="/images/posts/sql_ml_install_r_pckgs3_inst_pkgs.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>View Installed R Packages - I</em></p>

<p>In <em>Figure 1</em> we see some of the installed R packages and notice that we do not see randomForest. Also, notice the <code>LibPath</code> column outlined in red. Remember how I mentioned in <a href="/2018/06/23/installing-r-packages-in-sql-server-machine-learning-services---i/">Installing R Packages in SQL Server Machine Learning Services - I</a> how, when using SQL Server ML Services, we install packages to a specific library which SQL Server then loads the packages from. That location is the <code>LibPath</code> in <em>Figure 1</em>.</p>

<blockquote>
<p><strong>NOTE:</strong> As we see later in this post, what I said above about only one location is not entirely true.</p>
</blockquote>

<p>The last thing to do before we execute the code in <em>Code Snippet 4</em> is to browse around in <em>File Explorer</em> and look at a directory under <code>C:\&lt;path_to_SQL_instance&gt;\MSSQL</code>. When we look around we see a directory named <code>ExternalLibraries</code>, and when we &ldquo;drill&rdquo; into it we see:</p>

<p><img src="/images/posts/sql_ml_install_r_pckgs3_ext_lib1.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>ExternalLibraries Directory</em></p>

<p>Hmm, what is so interesting with this directory we see in <em>Figure 2</em>, it has a subdirectory named <code>R</code>, but otherwise, it is empty? Well, the name is interesting: <code>ExternalLibraries</code>. I wonder if it has anything to do with creating external libraries? So to find out we execute the code in <em>Code Snippet 4</em>.</p>

<p>Strange, after we execute the code in <em>Code Snippet 4</em> nothing changes in the directories we look at. Are we wrong in our assumptions about the directories, or did the code fail? In either case, how can we find out?</p>

<p>Creating an external library is very similar to creating an SQLCLR assembly, and if you have ever created an SQLCLR assembly you are probably aware of a couple of catalog views that gives us information about the assemblies:</p>

<ul>
<li><code>sys.assemblies</code>: base catalog view for assemblies with one row per assembly created in the database.</li>
<li><code>sys.assembly_files</code>: contains the binary representation of the assembly files.</li>
</ul>

<p>For external libraries we have similar catalog views:</p>

<ul>
<li><code>sys.external_libraries</code>: base catalog view for external libraries with one row per external library created in the database.</li>
<li><code>sys.external_library_files</code>: contains the binary representation of the external library files.</li>
</ul>

<p>So, if we successfully created the external library we <em>should</em> see something in <code>sys.external_libraries</code>:</p>

<pre><code class="language-sql">SELECT * 
FROM sys.external_libraries;
</code></pre>

<p><strong>Code Snippet 6:</strong> <em>View External Libraries</em></p>

<p>When we execute the code in <em>Code Snippet 6</em> we see:</p>

<p><img src="/images/posts/sql_ml_install_r_pckgs3_view_ext_lib.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Installed External Library</em></p>

<p>Yes, when we look at <em>Figure 3</em> we see that we have created an external library. The columns we see represents:</p>

<ul>
<li><code>external_library_id</code>: the id of the external library as assigned by the database.</li>
<li><code>name</code>: name given to it during creation.</li>
<li><code>principal_id</code>: id of the owner, (principal), of the library.</li>
<li><code>language</code>: name of the language of the library. As mentioned before; in SQL Server 2017, only R, in SQL Server 2019; R, Python, and/or Java.</li>
<li><code>scope</code>: defines who can access the library, 0 for <code>PUBLIC</code>, 1 for <code>PRIVATE</code>. More about that in a.</li>
<li><code>scope_desc</code>: literal description of the scope.</li>
</ul>

<p>Let us see if we can use it the external library:</p>

<pre><code class="language-sql">EXEC sp_execute_external_script 
                    @language = N'R', 
                    @script = N'library(&quot;randomForest&quot;)'
</code></pre>

<p><strong>Code Snippet 6:</strong> <em>Load R Package</em></p>

<p>Admittedly the code in <em>Code Snippet 6</em> does not accomplish much, but when we execute it we can tell whether we have succeeded in creating the external library:</p>

<p><img src="/images/posts/sql_ml_install_r_pckgs3_ext_lib2.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Execute randomForest</em></p>

<p>From what is outlined in red in <em>Figure 4</em> we see that we have successfully executed against the <code>randomForest</code> package. We also see how external libraries only get loaded and &ldquo;properly&rdquo; installed at first use (blue outline). Cool, so that worked. What about the <code>ExternalLibraries</code> directory:</p>

<p><img src="/images/posts/sql_ml_install_r_pckgs3_ext_lib3.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>ExternalLibraries after First Execution</em></p>

<p>So, our assumption above regarding <code>ExternalLibraries</code> were correct; the directory contains the actual packages for the external libraries we create. We see in <em>Figure 5</em> how there are new directories, and how we have a <code>randomForest</code> directory which contains the <code>randomForest</code> package.</p>

<blockquote>
<p><strong>NOTE:</strong> The number 5 in <em>Figure 5</em> refers to the database id, and the number 1 beneath the 5 is the id of the external library (<code>external_library_id</code>). So the databases have their own top-level directory, named after the database id. Underneath the database id directory is the individual external library directories identified by the external library id.</p>
</blockquote>

<p>When we execute the code in <em>Code Snippet 5</em> we get:</p>

<p><img src="/images/posts/sql_ml_install_r_pckgs3_ext_lib4.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>View Installed R Packages - II</em></p>

<p>We now see in <em>Figure 6</em> how the <code>randomForest</code> package comes up as an installed packet (outlined in red), and we see the installation path (highlighted in yellow), and this is where it loads from. So what I said in <a href="/2018/06/23/installing-r-packages-in-sql-server-machine-learning-services---i/">Installing R Packages in SQL Server Machine Learning Services - I</a> about SQL only loads packages from one directory is not entirely true, SQL Server can load packages from different locations.</p>

<p>What we have seen so far looks quite good, but the problem is similar to what we discussed in <a href="/2018/06/23/installing-r-packages-in-sql-server-machine-learning-services---i/">Installing R Packages in SQL Server Machine Learning Services - I</a>:</p>

<ul>
<li>In the post we said we needed elevated access to the box where the SQL Server instance is.</li>
<li>Here we need access to a directory to which we can copy the package(s) we want to create the external library(s) from, and SQL Server needs read access to that directory. This directory is most likely on the SQL Server server, so we still have the same problem as before.</li>
</ul>

<p>Fortunately, there is a way to solve this. Remember how we said above that the <code>file_spec</code> parameter which, up until now, has been a file path, also can be a hex-literal.</p>

<h2 id="hex-literal-create-external-library">Hex Literal &amp; CREATE EXTERNAL LIBRARY</h2>

<p>The question is then how do I get the hex-literal for a package?</p>

<blockquote>
<p><strong>NOTE:</strong> What follows related to hex literal is more or less a copy from my post <a href="/2019/03/17/sql-server-2019-java--external-libraries---ii/">SQL Server 2019, Java &amp; External Libraries - II</a>.</p>
</blockquote>

<p>The hex-literal is the actual binary representation of the package, so let us look at a couple of ways we can get hold of the binary package representation:</p>

<ul>
<li>From a local database.</li>
<li>Generate binary from code.</li>
</ul>

<h4 id="local-database">Local Database</h4>

<p>We know (from above) that the catalog view <code>sys.external_library_files</code> contains the binary representation of the package, and we see that using a query like so:</p>

<pre><code class="language-sql">SELECT l.external_library_id, l.name, lf.content
FROM sys.external_libraries l
JOIN sys.external_library_files lf
  ON l.external_library_id = lf.external_library_id
WHERE l.name = 'randomForest'
</code></pre>

<p><strong>Code Snippet 7:</strong> <em>View External Library</em></p>

<p>In <em>Code Snippet 7</em> we <code>SELECT</code> out the library id, name from the <code>sys.external_libraries</code> view, and <code>content</code> from <code>sys.external_library_files</code>. When we run the code the result looks like so:</p>

<p><img src="/images/posts/sql_ml_install_r_pckgs3_ext_lib_content1.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>Binary Representation</em></p>

<p>What we see highlighted in <em>Figure 7</em> is the <code>content</code> column, and we see it contains the hex-literal for the <code>randomForest</code> package.</p>

<p>So if we want to create an external library on a remote SQL Server on which we do not have access to the file system, but we have access to a local SQL Server, we can do this:</p>

<ul>
<li>Create an external library in a database on the local machine, like in <em>Code Snippet 4</em>.</li>
<li>Get the hex-literal from the <code>content</code> column and save it.</li>
</ul>

<p>The naive way, (what I did initially), to get the hex-literal is to use code like this:</p>

<pre><code class="language-sql">DECLARE @hexLit varbinary(max);

SELECT @hexLit = lf.content
FROM sys.external_libraries l
JOIN sys.external_library_files lf
  ON l.external_library_id = lf.external_library_id
WHERE l.name = 'randomForest' 

PRINT @hexLit;
</code></pre>

<p><strong>Code Snippet 8:</strong> <em>Get the Hex Literal</em></p>

<p>To get the hex-literal, we see in <em>Code Snippet 8</em> how we:</p>

<ul>
<li>Declare a <code>varbinary(max)</code> variable into which we <code>SELECT</code> the <code>content</code> column.</li>
<li>Print that variable so we can use it.</li>
</ul>

<p>When we execute the code, it looks like so:</p>

<p><img src="/images/posts/sql_ml_install_r_pckgs3_ext_lib_hexlit.png" alt="" /></p>

<p><strong>Figure 8:</strong> <em>Selecting out Hex Literal</em></p>

<p>In <em>Figure 8</em> we see part of the hex literal. However, I mentioned above that what we see in <em>Code Snippet 8</em> is a naive way to do it, and - in most cases - it does not work. Sure you get something that looks like your hex-literal, but if you compare the size of the printed output of the variable, with the size of the value in the column, you see how the size in the column is much bigger. This is because when you do a <code>PRINT</code> either in SSMS or Azure Data Studio the output is limited to a max size of 8000.</p>

<blockquote>
<p><strong>NOTE:</strong> In the <a href="/2019/03/17/sql-server-2019-java--external-libraries---ii/">SQL Server 2019, Java &amp; External Libraries - II</a> post I used the method above, and it worked. The reason was that the <code>.jar</code> file I wanted to create an external library from, had a size of ~1.5k.</p>
</blockquote>

<p>So what do we do if we want to capture the value of the variable? Well, by using some xml &ldquo;magic&rdquo; we can achieve what we want:</p>

<pre><code class="language-sql">SELECT CONVERT(varchar(max), lf.content, 1)
FROM sys.external_libraries l
JOIN sys.external_library_files lf
  ON l.external_library_id = lf.external_library_id
WHERE l.name = 'randomForest'
FOR XML PATH('');
</code></pre>

<p><strong>Code Snippet 9:</strong> <em>Output as XML</em></p>

<p>We see that the code in <em>Code Snippet 9</em> is not that much different from <em>Code Snippet 8</em>. Instead of selecting the <code>content</code> column value into a variable which we <code>PRINT</code>, we <code>CONVERT</code> the binary value to <code>varchar(max)</code> and then indicate we want it exposed as xml (<code>FOR XML PATH('')</code>). When we execute the result is like so:</p>

<p><img src="/images/posts/sql_ml_install_r_pckgs3_hexlit_xml.png" alt="" /></p>

<p><strong>Figure 9:</strong> <em>Hex Literal XML</em></p>

<p>When you see <em>Figure 9</em> you may ask what the difference is from what we have seen before? When we copy out the content of the column, will we not get just a part of the full value? The answer to that is yes, however, as the column data type is xml, and if we click on it we see something different:</p>

<p><img src="/images/posts/sql_ml_install_r_pckgs3_hexlit_aml_output.png" alt="" /></p>

<p><strong>Figure 10:</strong> <em>XML Output</em></p>

<p>When we clicked on the column a new file opens, and in that file, we get the full hex-literal value, as we see in <em>Figure 10</em>. We can now use the full hex literal to create the <code>randomForest</code> external library on another SQL Server instance.</p>

<p>In this post I do not have access to a remote SQL Server, so what we do instead is that we emulate doing it; we do it against the second database we created in <em>Code Snippet 2</em>; <code>DataScienceDBRemote</code>. After we ensure we have the full hex literal saved off somewhere we:</p>

<ul>
<li>Switch over to <code>DataScienceDBRemote</code> (as <code>sa</code>).</li>
<li>Open a new query window.</li>
</ul>

<p>In the new query window we declare a new variable, let us call it <code>@hexLit</code>, as a <code>varbinary(max)</code>, and we assign the hex literal from <code>DataScienceDB</code> to the variable:</p>

<pre><code class="language-sql">USE DataScienceDBRemote;
GO

DECLARE @hexLit varbinary(max) = 0x504B03040A00000000009982964...
</code></pre>

<p><strong>Code Snippet 10:</strong> <em>Assign Hex Literal Value to Variable</em></p>

<p>When we have declared the variable and assigned the hex-literal value to it, we can use it in <code>CREATE EXTERNAL LIBRARY</code>:</p>

<pre><code class="language-sql">USE DataScienceDBRemote;
GO

DECLARE @hexLit varbinary(max) = 
0x504B03040A00000000009982964C0000000000000000000000000D00000072...
...

CREATE EXTERNAL LIBRARY randomForest
FROM (CONTENT = @hexLit)
WITH (LANGUAGE = 'R');
GO

SELECT * FROM sys.external_libraries;
</code></pre>

<p><strong>Code Snippet 11:</strong> <em>Create External Library from Hex Literal Variable</em></p>

<p>Finally, we execute the code in <em>Code Snippet 11</em> and the <code>SELECT</code> shows us that we now have an external library named <code>randomForest</code> in the &ldquo;remote&rdquo; database.</p>

<p>So this is one way we can get a binary for a package. It may, however, be somewhat convoluted, so let us look at the second way.</p>

<h4 id="generate-binary-from-code">Generate Binary from Code</h4>

<p>Compared to the above, to get the binary representation based on code is probably somewhat easier, and I decided to use Python to create a script which writes the package binary to a file:</p>

<pre><code class="language-python">import binascii

packageFile = input(&quot;Provide full path to the R package \ 
                    file you want to use - \
                    Example: 'W:\\randomForest_4.6-14.zip': &quot;)
fileName = input(&quot;Provide name of the file \
                  you want to create to write the binary to: &quot;)

with open(packageFile, &quot;rb&quot;) as binaryfile :
    myArr = binaryfile.read()
    hex_bytes = '0x' + binascii.hexlify(bytearray(myArr)).decode('utf-8')

f = open(fileName, &quot;w+&quot;)
f.write(hex_bytes)
f.close()
</code></pre>

<p><strong>Code Snippet 12:</strong> <em>Python Script to Generate File with Hex Literal</em></p>

<p>We see in <em>Code Snippet 12</em> how the script:</p>

<ul>
<li>Asks for what package zip file to use.</li>
<li>What name to give the output file.</li>
<li>Generates the binary.</li>
<li>Saves it into a file.</li>
</ul>

<p>We now take the code in <em>Code Snippet 12</em> and copy it into a Python script file, for example <code>createBinary.py</code>. When we have the Python file we execute from the command prompt like so:</p>

<pre><code class="language-bash">$ python .\createBinary.py
</code></pre>

<p><strong>Code Snippet 13:</strong> <em>Run Python Script</em></p>

<p>After we run the script as per <em>Code Snippet 13</em> we open the created file and grab the hex-literal. We now follow the same procedure as we did in <em>Code Snippet 10</em> and <em>Code Snippet 11</em>, without having the package installed as an external library on the local machine.</p>

<p>However, why do copy and paste when we can connect directly from Python to the remote database and execute <code>CREATE EXTERNAL LIBRARY</code>:</p>

<pre><code class="language-python">import pyodbc
import binascii

extLibName = input(&quot;Provide a unique name for \
                   the external library you want to create: &quot;)
packageFile = input(&quot;Provide full path to the zip \
                  file you want to use - \
                  Example: 'W:\\randomForest_4.6-14.zip': &quot;)
dbServer = input(&quot;Provide name/ip address of your \
                  database server. If instance also instance name \ 
                  - Example: 'mydbServer\myInstance: &quot;)
dataBase = input(&quot;Provide name of the database where you \
                  want to create the external library: &quot;)
userName = input(&quot;Provide the user name with which you \
                   want to connect to the server: &quot;)
password = input(&quot;Provide password with which to \
                  connect to the database: &quot;)

with open(packageFile, &quot;rb&quot;) as binaryfile :
    myArr = binaryfile.read()
    hex_bytes = '0x' + binascii.hexlify(bytearray(myArr)).decode('utf-8')

drvr = '{ODBC Driver 17 for SQL Server}'
connStr = f'DRIVER={drvr};SERVER={dbServer};DATABASE={dataBase};UID={userName};PWD={password}'
conn = pyodbc.connect(connStr)
cursor = conn.cursor()

execStmt = f'CREATE EXTERNAL LIBRARY {extLibName}\n'
execStmt = execStmt + f'FROM (CONTENT = {hex_bytes})\n'
execStmt = execStmt + f&quot;WITH (LANGUAGE = 'R');\n&quot;

cursor.execute(execStmt)
conn.commit()    
</code></pre>

<p><strong>Code Snippet 14:</strong> <em>Execute Directly Against the Remote Database</em></p>

<p>So, what do we do in <em>Code Snippet 14</em>? Well, we use the same code as in <em>Code Snippet 12</em> to generate the binary representation. However, instead of writing it to a file we connect to the database using the <code>pyodbc</code> module, and the latest SQL Server ODBC driver. The <code>hex_bytes</code> variable is now a parameter in the <code>CREATE EXTERNAL LIBRARY</code> statement. The name of the external library is passed in as a parameter together with database connection details. It is worth noting that the way the script captures the password variable is not particularly secure. Instead of <code>input</code>, we should use <code>getpass</code> or something similar.</p>

<blockquote>
<p><strong>NOTE:</strong> Unless the user with which you connect is part of the <code>db_owner</code> role, the user needs explicit permissions to execute <code>CREATE EXTERNAL LIBRARY</code>. A future post covers permissions for <code>CREATE EXTERNAL LIBRARY</code>.</p>
</blockquote>

<p>To run this, we do as we did in <em>Code Snippet 12</em>; we copy the code into a Python file and run it from the command line. The code should run OK, and we have created an external library in a database in a remote SQL Server (well, in my case an emulated remote SQL Server).</p>

<h2 id="summary">Summary</h2>

<p>In this post, we set out to solve the issue of how to deploy an R package without having access to the filesystem of the SQL Server where we want to deploy the package to.</p>

<p>We have seen two ways of doing it:</p>

<h4 id="local-datbase">Local Datbase</h4>

<ol>
<li>Create an external library from the R package based on the file path in a local SQL Server where we have access to the filesystem (like <code>localhost</code>).</li>
<li>Get the binary representation from the <code>content</code> column in <code>sys.external_library_files</code> via some XML &ldquo;magic&rdquo;.</li>
<li>Assign the retrieved value to the <code>CONTENT</code> parameter in <code>CREATE EXTERNAL LIBRARY</code>.</li>
<li>Execute <code>CREATE EXTERNAL LIBRARY</code>.</li>
</ol>

<h4 id="generate-from-code">Generate from Code</h4>

<ol>
<li>Write script code which generates the binary representation.</li>
<li>Follow from step 3 above (local database).</li>
</ol>

<p>Alternatively, you can connect to the database from inside the script and call <code>CREATE EXTERNAL LIBRARY</code> from the script.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 14, 2019]]></title>
    <link href="http://nielsberglund.com/2019/04/07/interesting-stuff---week-14-2019/" rel="alternate" type="text/html"/>
    <updated>2019-04-07T19:33:23+02:00</updated>
    <id>http://nielsberglund.com/2019/04/07/interesting-stuff---week-14-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/starling-bank">Building a Reliable Cloud-Based Bank in Java</a>. This is an <a href="https://www.infoq.com/">InfoQ</a> presentation discussing the experience of Starling Bank, a mobile-only, cloud-based bank that launched in the UK in 2017. The presenter looks at the system architecture of the bank, the design principles that give them the ability to release quickly and reliably, and why they decided to build the back end using Java.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><p><a href="https://www.confluent.io/blog/introducing-confluent-platform-5-2">Introducing Confluent Platform 5.2</a>. During the week Confluent announced the release of Confluent Platform 5., and with it some exciting new features:</p>

<ul>
<li>Confluent Platform is free for single node clusters, it is like a developer edition!</li>
<li>The <code>librdkafka</code> library is now in version 1.0. That is interesting as it brings this library closer to parity with the Java client for Kafka.</li>
<li>New and enhanced query expressions in KSQL.</li>
</ul></li>

<li><p><a href="https://www.confluent.io/blog/putting-events-in-their-place-with-dynamic-routing">Putting Events in Their Place with Dynamic Routing</a>. This is a blog post about how Kafka Streams are a powerful way to enrich data streaming through event-driven architectures. We can dynamically route events to topics, even pulling in the output topic information from another end data system.</p></li>

<li><p><a href="https://www.infoq.com/presentations/starling-bank">KSQL: What’s New in 5.2</a>. As I mentioned above, there are new features in KSQL 5.2, and in this blog post <a href="https://twitter.com/rmoff">Robin</a> discusses some of them!</p></li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 13, 2019]]></title>
    <link href="http://nielsberglund.com/2019/03/31/interesting-stuff---week-13-2019/" rel="alternate" type="text/html"/>
    <updated>2019-03-31T08:43:04+02:00</updated>
    <id>http://nielsberglund.com/2019/03/31/interesting-stuff---week-13-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://blog.acolyer.org/2019/03/29/calvin-fast-distributed-transactions-for-partitioned-database-systems/">Calvin: fast distributed transactions for partitioned database systems</a>. In this white-paper dissection by <a href="https://twitter.com/adriancolyer">Adrian</a>, he looks at <a href="https://github.com/yaledb/calvin">Calvin</a> which is a transaction scheduling and data replication layer that uses a deterministic ordering guarantee to reduce the high contention costs associated with distributed transactions significantly.</li>
</ul>

<h2 id="sql-server">SQL Server</h2>

<ul>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2019/03/27/sql-server-2019-community-technology-preview-2-4-is-now-available/">SQL Server 2019 community technology preview 2.4 is now available</a>. What the title says. I downloaded the CTP a couple of days ago, and when I am done with this post, I will install it. Oh, word of warning - if you want to install the <strong>SQL Server 2019 Big Data Cluster</strong>, please remember to uninstall and reinstall <code>mssqlctl</code>, so you get the latest version.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/importance-of-distributed-tracing-for-apache-kafka-based-applications">The Importance of Distributed Tracing for Apache-Kafka-Based Applications</a>. This blog post looks at how to instrument Kafka-based applications with distributed tracing capabilities to make dataflows between event-based components more visible. Very interesting!</li>
<li><a href="https://www.confluent.io/blog/consuming-messages-out-of-apache-kafka-in-a-browser/2">Consuming Messages Out of Apache Kafka in a Browser</a>. The post covers what the title says; how to consume Kafka messages in a browser.</li>
<li><a href="https://rmoff.net/2019/03/28/exploring-ksql-stream-stream-joins/">Exploring KSQL Stream-Stream Joins</a>. This is an excellent post by <a href="https://twitter.com/rmoff">Robin</a> where he covers KSQL and stream to stream joins! I need to go off and <del>play with</del> research this now!</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (<em>What Is Niels Doing</em>)</h2>

<p>I am still working on the post about <code>CREATE EXTERNAL LIBRARY</code> in the <a href="/sql_server_ml_services_install_packages">Install R Packages in SQL Server ML Services</a> series. Expect it towards the end of this coming week.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 12, 2019]]></title>
    <link href="http://nielsberglund.com/2019/03/24/interesting-stuff---week-12-2019/" rel="alternate" type="text/html"/>
    <updated>2019-03-24T08:37:01+02:00</updated>
    <id>http://nielsberglund.com/2019/03/24/interesting-stuff---week-12-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/csharp-testing-strategy-tools">Unit Testing Strategies &amp; Patterns in C#</a>. This <a href="https://www.infoq.com/">InfoQ</a> presentation discusses design principles and ways to make C# code testable, as well as using testing tools such as Moq, Autofixture, &amp; MsTest.</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://medium.com/google-cloud/istio-routing-basics-14feab3c040e">Istio Routing Basics</a>. So, <a href="https://cloud.google.com/istio/">Istio</a> is an open source service mesh, and this blog post covers the basics of Istio and shows what it takes to build an Istio enabled &ldquo;Hello World&rdquo; application.</li>
<li><a href="https://medium.com/@masroor.hasan/tracing-infrastructure-with-jaeger-on-kubernetes-6800132a677">Distributed Tracing Infrastructure with Jaeger on Kubernetes</a>. The blog post I link to here looks at distributed tracing on Kubernetes using Jaeger.</li>
</ul>

<h2 id="sql-server">SQL Server</h2>

<ul>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2019/03/18/the-march-release-of-azure-data-studio-is-now-available/">The March release of Azure Data Studio is now available</a>. What the title says! There are quite a few new features in the March release of Azure Data Studio, among them: support for SQL Notebooks, PowerShell extension, and PostgresSQL support. Go and get it!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/kafka-streams-take-on-watermarks-and-triggers">Kafka Streams’ Take on Watermarks and Triggers</a>. This blog post discusses a new Kafka Streams operator: <code>Suppress</code>. It gives you the ability to control when to forward KTable updates. The <code>Suppress</code> operator comes in very handy in various CEP scenarios: &ldquo;tell me when someone has done &ldquo;a&rdquo; more than &ldquo;x&rdquo; times within &ldquo;y&rdquo; time period&rdquo;. What normally happens is that if someone achieves the &ldquo;a&rdquo;, &ldquo;x&rdquo; times within the &ldquo;y&rdquo; time period every following &ldquo;a&rdquo; would trigger as well. With <code>Suppress</code> you - wait for it - suppress the extra &ldquo;a&rdquo;, until the end of the time period.<br /></li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (<em>What Is Niels Doing</em>)</h2>

<p>Since I did the two posts about <code>CREATE EXTERNAL LIBRARY</code> for Java code (<a href="/2019/03/10/sql-server-2019-java--external-libraries---i/">here</a> and <a href="/2019/03/17/sql-server-2019-java--external-libraries---ii/">here</a>), I thought it would be a good idea to finish off my <a href="/sql_server_ml_services_install_packages">Install R Packages in SQL Server ML Services</a> series. So, I am at the moment working on a post discussing <code>CREATE EXTERNAL LIBRARY</code> in the R world. The post is somewhat like the ones covering Java, but it also covers permissions etc.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 11, 2019]]></title>
    <link href="http://nielsberglund.com/2019/03/17/interesting-stuff---week-11-2019/" rel="alternate" type="text/html"/>
    <updated>2019-03-17T21:16:22+02:00</updated>
    <id>http://nielsberglund.com/2019/03/17/interesting-stuff---week-11-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://devblogs.microsoft.com/dotnet/net-core-container-images-now-published-to-microsoft-container-registry/">.NET Core Container Images now Published to Microsoft Container Registry</a>. A post discussing how Microsoft are now publishing .NET Core container images to Microsoft Container Registry (MCR).</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/istio-microservices">Reducing Microservices Architecture Complexity with Istio and Kubernetes</a>. An <a href="https://www.infoq.com/">InfoQ</a> presentation which introduces Istio, and explains how the service mesh works, the technology behind it, and how to use it with microservices.</li>
<li><a href="https://www.infoq.com/news/2019/03/microservices-recommendations">Recommendations When Starting with Microservices: Ben Sigelman at QCon London</a>. This is an <a href="https://www.infoq.com/">InfoQ</a> article about the mistakes Google made in he beginning when adopting a microservices architecture, and recommendations to avoid making these mistakes when starting with microservices.</li>
</ul>

<h2 id="data-science-machine-learning">Data Science / Machine Learning</h2>

<ul>
<li><a href="https://towardsdatascience.com/machine-learning-with-big-data-86bcb39f2f0b">Machine Learning with Big Data</a>. Data is on overdrive. It’s being generated at break-neck pace. How do we analyze all this data? This article discusses how to easily create a scalable and parallelized machine learning platform on the cloud to process large-scale data.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://eng.uber.com/dbevents-ingestion-framework/">DBEvents: A Standardized Framework for Efficiently Ingesting Data into Uber’s Apache Hadoop Data Lake</a>. This blog post looks at Uber&rsquo;s  DBEvents, a change data capture system designed for high data quality and freshness. It facilitates bootstrapping, ingesting a snapshot of an existing table, and incremental, streaming updates.</li>
<li><a href="https://www.confluent.io/blog/kafka-connect-deep-dive-error-handling-dead-letter-queues">Kafka Connect Deep Dive – Error Handling and Dead Letter Queues</a>. In this blog post <a href="https://twitter.com/rmoff">Robin Moffat</a> looks at several common patterns for handling Kafka Connect problems and examines how the patterns can be implemented.</li>
</ul>

<h2 id="sql-server-2019">SQL Server 2019</h2>

<ul>
<li><a href="/2019/03/17/sql-server-2019-java--external-libraries---ii/">SQL Server 2019, Java &amp; External Libraries - II</a>. This post by yours truly looks at how to use <code>CREATE EXTERNAL LIBRARY</code> to deploy Java code without having access to SQL Server&rsquo;s filesystem.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
</feed>

