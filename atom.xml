<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Niels Berglund</title>
  <link href="https://nielsberglund.com/atom.xml" rel="self"/>
  <link href="https://nielsberglund.com"/>
  <updated>2022-01-02T09:48:17+02:00</updated>
  <id>https://nielsberglund.com/</id>
  <generator uri="http://gohugo.io/">Hugo</generator>

  
  <entry>
    <title type="html"><![CDATA[How to Use Kafka Client with Azure Event Hubs]]></title>
    <link href="https://nielsberglund.com/2022/01/02/how-to-use-kafka-client-with-azure-event-hubs/" rel="alternate" type="text/html"/>
    <updated>2022-01-02T09:48:17+02:00</updated>
    <id>https://nielsberglund.com/2022/01/02/how-to-use-kafka-client-with-azure-event-hubs/</id>
    <content type="html"><![CDATA[<p>This blog post came by, by accident, lol. A couple of weeks ago, I started to prepare for a webinar: <a href="https://www.meetup.com/SQLCape-Meetup/events/282241220/"><strong>Analyze Billions of Rows of Data in Real-Time Using Azure Data Explorer</strong></a>. One of the demos in that webinar is about ingesting data from Apache Kafka into <strong>Azure Data Explorer</strong>. When prepping, I noticed that my Confluent Cloud Kafka cluster didn&rsquo;t exist anymore, so I had to come up with a workaround. That workaround was to use <strong>Azure Event Hubs</strong> instead of Kafka.</p>

<p>Since I already had the code to publish data to Kafka and knew that you could use the Kafka client to publish to Event Hubs, I thought I&rsquo;d test it out. I did run into some minor snags along the way, so I thought I&rsquo;d write a blog post about it. Then, at least, I have something to go back to. This post also looks at how to set up an Event Hubs cluster.</p>

<p></p>

<h2 id="background">Background</h2>

<p>Azure Event Hubs is similar to Apache Kafka in that it is a big data streaming platform and event ingestion service. It is a fully managed Platform-as-a-Service (PaaS) with little configuration or management overhead, very much like Apache Kafka in <strong>Confluent Cloud</strong>. The one difference between Azure Event Hubs and Apache Kafka is that Event Hubs does not have an on-prem solution.</p>

<h4 id="kafka-vs-event-hubs-concepts">Kafka vs Event Hubs Concepts</h4>

<p>Event Hubs and Kafka are pretty similar, as I mentioned above. Let us compare the concepts of the two:</p>

<table>
<thead>
<tr>
<th>Kafka</th>
<th>Event Hubs</th>
</tr>
</thead>

<tbody>
<tr>
<td>Cluster</td>
<td>Namespace</td>
</tr>

<tr>
<td>Topic</td>
<td>EventHub</td>
</tr>

<tr>
<td>Partition</td>
<td>Partition</td>
</tr>

<tr>
<td>Consumer Group</td>
<td>Consumer Group</td>
</tr>

<tr>
<td>Offset</td>
<td>Offset</td>
</tr>
</tbody>
</table>

<p><strong>Table 1:</strong> <em>Kafka vs Event Hubs Concepts</em></p>

<p>As we see in <em>Table 1</em>, there is not much difference between Kafka and Event Hubs. The one difference worth noting is the Event Hubs namespace instead of the Kafka cluster.</p>

<h4 id="namespace">Namespace</h4>

<p>An Event Hubs namespace is a dedicated scoping container for event hubs, where an event hub as mentioned above is the equivalent to a Kafka topic. We can see it as a management container for individual event hubs (topics), and it provides a range of access control and network integration management features.</p>

<p>For this post, the important part is that the namespace provides IP endpoints allowing us to publish to the namespace and its individual event hubs (topics).</p>

<h4 id="event-hubs-kafka-endpoint">Event Hubs Kafka Endpoint</h4>

<p>One of the endpoints the namespace provides is an endpoint compatible with the Apache Kafka producer and consumer APIs at version 1.0 and above.</p>

<p>So if your application uses a Kafka client version 1.0+, you can use the Event Hubs Kafka endpoint from your applications without code changes, apart from configuration, compared to your existing Kafka setup.</p>

<blockquote>
<p><strong>NOTE:</strong> When I above say you only need to change the configuration, I assume that the Kafka topics have corresponding EventHubs (remember Kafka topic = Event Hubs EventHub).</p>
</blockquote>

<p>An interesting point here is that it is not only your Kafka applications that can consume from Event Hubs but any application that uses Kafka Client 1.0+, like Kafka Connect connectors!</p>

<h2 id="pre-reqs">Pre-Reqs</h2>

<p>This section is here if you want to follow along. This is what you need:</p>

<ul>
<li>An Azure account. If you don&rsquo;t have an Azure subscription, sign up for a <a href="https://azure.microsoft.com/en-us/free/">free account</a>.</li>
<li>Your favourite development language. In this post, I use Python.</li>
<li>Kafka client for your favourite language as per above. <a href="https://docs.confluent.io/platform/current/clients/index.htm">Here</a> you find a list of clients.</li>
<li>Application publishing to Kafka. The idea is that you can switch from publishing to Kafka to publish to Event Hubs in this application.</li>
</ul>

<p>Ensure the client is installed. In Python, I do it using <code>pip</code>, like so:</p>

<pre><code class="language-python">pip install confluent-kafka
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Install the Client</em></p>

<p>After having run the code in <em>Code Snippet 1</em>, or similar code for your language, you are good to go.</p>

<h4 id="kafka-application">Kafka Application</h4>

<p>Above I mentioned about an application publishing to Kafka being optional. Let us here take a look at a very simple example.</p>

<p>In this example, I have an Azure Confluent Cloud cluster looking like so:</p>

<p><img src="/images/posts/evthub-adx-confluent-cloud.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Confluent Cloud Cluster</em></p>

<p>In <em>Figure 1</em>, we see my Confluent Cloud cluster named <code>kafkaeventhubs</code> and how that cluster has one topic - <code>testtopic</code> - with four partitions.</p>

<blockquote>
<p><strong>NOTE:</strong> To see how to run Confluent Cloud in Azure, see my post <a href="/2021/08/14/run-confluent-cloud--serverless-apache-kafka-on-azure/"><strong>Run Confluent Cloud &amp; Serverless Apache Kafka on Azure</strong></a>.</p>
</blockquote>

<p>I have a very basic Python application publishing to the <code>testtopic</code> looking like so:</p>

<p><img src="/images/posts/evthub-adx-kafka-app.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Python Publish App</em></p>

<p>As we see in <em>Figure 2</em> the app is not doing anything advanced. It publishes a comma-delimited message containing the random key value, &ldquo;Hello World&rdquo;, and a timestamp.</p>

<p>What is interesting in the code is the configuration string (assigned to the variable <code>conf</code>) in lines 14 - 19, and specifically the following properties:</p>

<ul>
<li><code>bootstrap.servers</code>: The cluster endpoint.</li>
<li><code>security.protocol</code>: The protocol used to communicate with brokers. In this case, we use <code>SASL_SSL</code>, which uses SASL for authentication and SSL for encryption.</li>
<li><code>sasl.mechanism</code>: This indicates how we authenticate. By setting it to <code>PLAIN</code>, we use a &ldquo;simple&rdquo; username/password based authentication mechanism.</li>
<li><code>sasl.username</code>: The username to use. In Confluent Cloud, we use an API key mechanism, where the <code>username</code> is the API key, and the password is the API key&rsquo;s secret.</li>
<li><code>sasl.password</code>: As per above, this is the API key&rsquo;s secret.</li>
</ul>

<p>Why I say these are interesting is these are the ones that are in play if we want to publish to Event Hubs.</p>

<p>Having seen the pre-reqs lets us create a namespace in Event Hubs.</p>

<h2 id="create-event-hubs-namespace">Create Event Hubs Namespace</h2>

<p>There are multiple ways we can create an Event Hubs resource, where the Azure Portal is one of them.</p>

<p>Instead of doing a step-by-step explanation of creating an Event Hubs namespace, I suggest you read the Microsoft article <a href="https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-create"><strong>Quickstart: Create an event hub using Azure portal</strong></a>. While you are doing that, I will create an Event Hubs test namespace for this post.</p>

<blockquote>
<p><strong>NOTE:</strong> When you create the namespace, you have to ensure you create it under the <em>Standard</em> pricing tier (or higher), as <em>Basic</em> does not support Kafka.</p>
</blockquote>

<p>We are all back? Cool, I ended up with this:</p>

<p><img src="/images/posts/evthub-adx-topics-1.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Event Hubs Namespace with EventHub</em></p>

<p>As you see in <em>Figure 3</em> I ended up with an Event Hubs namespace called <code>kafkaeventhubs</code> and an Event Hub named <code>testtopic</code>, the same as I have in Confluent Cloud.</p>

<h2 id="event-hubs-security-authentication">Event Hubs Security &amp; Authentication</h2>

<p>So now we have all we need to replace Kafka with Event Hubs - almost. We still need to see how to configure the Event Hubs security and authentication.</p>

<p>When using Event Hubs, all data in transit is TLS encrypted, and we can satisfy that by using <code>SASL_SSL</code>. This is exactly as in the Kafka code. Using <code>SASL_SSL</code> we have basically two options for authentication: OAuth 2.0 or Shared Access Signature (SAS). In this post, I use SAS, which matches what I do using Kafka.</p>

<h4 id="create-shared-access-signature">Create Shared Access Signature</h4>

<p>In Event Hubs, we can create a SAS for either a namespace or an individual EventHub in a namespace. Let us create a SAS for our <code>testtopic</code> Event Hub.</p>

<p>If you click into the <code>testtopic</code> Event Hub we see in <em>Figure 3</em> and look at the left-hand side; you see something like so:</p>

<p><img src="/images/posts/evthub-adx-topic-sas.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Event Hub Shared Access Policies</em></p>

<p>We see under <em>Settings</em> in <em>Figure 4</em> <em>Shared access policies</em> (outlined in red). When we click on it:</p>

<p><img src="/images/posts/evthub-adx-topic-sas-2.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Shared Access Policies</em></p>

<p>We are being told that we don&rsquo;t have any policies set up, as outlined in yellow in <em>Figure 5</em>. Cool, let us create a policy. We do that by clicking on the <em>+ Add</em> button, which is outlined in red in <em>Figure 5</em>:</p>

<p><img src="/images/posts/evthub-adx-topic-sas-create.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>Add Policy</em></p>

<p>By clicking the <em>+ Add</em> button, we get a dialog <em>Add SAS Policy</em> as shown in <em>Figure 6</em>. We give it a name and the claims (what it allows), and then we click the <em>Create</em> button at the bottom of the dialog (not shown in <em>Figure 6</em>):</p>

<p><img src="/images/posts/evthub-adx-topic-sas-created.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>Policy Created</em></p>

<p>In <em>Figure 7</em>, we see the generated policy and the claims. Clicking on the policy, we get yet another dialog:</p>

<p><img src="/images/posts/evthub-adx-topic-sas-policy-keys.png" alt="" /></p>

<p><strong>Figure 8:</strong> <em>Policy Keys &amp; Connection Strings</em></p>

<p>The policy we created consists of two keys and corresponding connection strings as in <em>Figure 8</em>. The reason for having two is that in a production environment, you may want to cycle and regenerate the keys, so while you regenerate one, you can use the other.</p>

<p>Copy one of the connection strings as that is what we use for the Kafka client configuration:</p>

<p><img src="/images/posts/evthub-adx-topic-sas-policy-connstring.png" alt="" /></p>

<p><strong>Figure 9:</strong> <em>Connection String</em></p>

<p>In <em>Figure 9</em>, we see one of my connection strings. I have outlined in red the endpoint URL, and this is the one we use for the <code>bootstrap.servers</code> property in the configuration.</p>

<p>Now we should have everything we need to use the Kafka client to publish to the EventHub.</p>

<h2 id="configure-kafka-client">Configure Kafka Client</h2>

<p>Above I listed the configuration properties used when I connect to Confluent Cloud. Let us see what they should be when publishing to Event Hubs:</p>

<h4 id="bootstrap-servers">Bootstrap Servers</h4>

<p>The <code>bootstrap.servers</code> property defines the endpoint(s) where the client connects to. In <em>Figure 9</em>, I outlined the endpoint URL and said it would be used for <code>bootstrap.servers</code>. At the beginning of this post, I mentioned how Event Hubs exposes a Kafka endpoint. It does that on port <code>9093</code>. So:</p>

<pre><code class="language-python">`&quot;bootstrap.servers&quot;: &quot;kafkaeventhubs.servicebus.windows.net:9093&quot;`
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Bootstrap Servers</em></p>

<p>Onto the security and authorization part.</p>

<h4 id="security-protocol-sasl-mechanisms">Security Protocol` &amp; SASL Mechanisms</h4>

<p>As we use Shared Access Signature, We do not need to change <code>security.protocol</code> or <code>sasl.mechanism</code>; we keep them as <code>SASL_SSL</code> and <code>PLAIN</code>, respectively..</p>

<h4 id="username-password">Username &amp; Password</h4>

<p>At the very beginning of this post, I mentioned how I ran into some snags when trying to use the Kafka client to publish to Event Hubs. This is the part that caused me issues.</p>

<p>Reading documentation and blog posts when trying to connect to Event Hubs, I concluded that <code>sasl.password</code> should be set to the whole connection string you get from the SAS policy. OK, that&rsquo;s cool - but what about the user name?</p>

<p>Posts and docs talk about using <code>$ConnectionString</code>, but <code>$ConnectionString</code> looked like a variable to me, and I could not see where it was set. It finally dawned upon me that the user name property should literally be set to <code>$ConnectionString</code> - duh. So:</p>

<pre><code class="language-python">'sasl.username': &quot;$ConnectionString&quot;,
'sasl.password': &quot;Endpoint= sb://kafkaeventhubs.servicebus.windows.net/; \
                      SharedAccessKeyName=publishconsumepolicy; \
                      SharedAccessKey=&lt;secret-key&gt;, 
                      EntityPath=testtopic&quot;

</code></pre>

<p><strong>Code Snippet 3:</strong> <em>User Name &amp; Password</em></p>

<p>In <em>Code Snippet 3</em>, we see how <code>sasl.password</code> is set to the SAS policy&rsquo;s connection string and <code>sasl.username</code> to <code>$ConnectionString</code>.</p>

<p>The complete configuration required to connect to and publish to Event Hubs looks like so:</p>

<pre><code class="language-python">conf = {'bootstrap.servers': 'kafkaeventhubs.servicebus.windows.net:9093',
         'security.protocol': 'SASL_SSL',
         'sasl.mechanisms': 'PLAIN',
         'sasl.username': &quot;$ConnectionString&quot;,
         'sasl.password': &quot;Endpoint= \
                      sb://kafkaeventhubs.servicebus.windows.net/; \
                      SharedAccessKeyName=publishconsumepolicy; \
                      SharedAccessKey=&lt;secret-key&gt;; \
                      EntityPath=testtopic&quot;,
         'client.id': socket.gethostname()}

</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Event Hubs Configuration</em></p>

<p>You can now replace lines 14 - 19 that we see in <em>Figure 2</em> with what we have in <em>Code Snippet 4</em>, and you are good to go!</p>

<h2 id="publish-to-event-hubs">Publish to Event Hubs</h2>

<p>After editing the <code>conf</code> variable, you can run the application. Let it publish some messages and then check what you see in the overview for the <code>testtopic</code> Event Hub:</p>

<p><img src="/images/posts/evthub-adx-publish.png" alt="" /></p>

<p><strong>Figure 10:</strong> <em>Publish Events</em></p>

<p>There is no straightforward way to see if messages come into the Event Hub, so we look at the request stats. In <em>Figure 10</em>, we see how events have come into the Event Hub. Yay - it seems like it works!</p>

<p>I said above that there is no straightforward way to see if messages/events arrive into the Event Hub. Well, when we now know how to connect and publish to Event Hubs using the Kafka client, we could easily create a consuming application. However, I leave that up to you.</p>

<h2 id="summary">Summary</h2>

<p>In this post, we have seen how to use the Kafka client to connect and publish messages to an Azure Event Hub.</p>

<p>We compared the terminology of Kafka with Event Hubs, and saw that it is more or less the same. The two major differences are:</p>

<ol>
<li>In Kafka, we talk about clusters, whereas in Event Hubs, we have namespaces.</li>
<li>A topic in Kafka is called an Event Hub in Event Hubs.</li>
</ol>

<p>We use port <code>9093</code> on the Event Hubs endpoint to connect the Kafka client. When using <code>SASL_SSL</code> and the <code>PLAIN</code> <code>sasl.mechanism</code>, the user name we use is<code>$ConnectionString</code>, and the password is the entire connection string from the Event Hub&rsquo;s Shared Access Signature policy.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or [ping][ma] me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 50, 2021]]></title>
    <link href="https://nielsberglund.com/2021/12/12/interesting-stuff---week-50-2021/" rel="alternate" type="text/html"/>
    <updated>2021-12-12T10:37:02+02:00</updated>
    <id>https://nielsberglund.com/2021/12/12/interesting-stuff---week-50-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<blockquote>
<p><strong>NOTE:</strong> <em>It is now coming up on Christmas and New Year, and I will take a break with these posts and come back at the beginning of next year.</em></p>
</blockquote>

<p></p>

<h2 id="sql-server">SQL Server</h2>

<ul>
<li><a href="http://sql-sasquatch.blogspot.com/2021/12/sqlserver-column-store-object-pool.html">#SQLServer Column Store Object Pool &ndash; the Houdini Memory Broker Clerk AND Perfmon [\SQLServer:Buffer Manager\Target pages]</a>. In this post by Mr SQL Server NUMA, <a href="https://twitter.com/sqL_handLe">Lonny</a>, he &ldquo;spelunks&rdquo; around in SQL Server Buffer Pool. If you are interested in the &ldquo;innards&rdquo; of SQL Server, you need to read this post. Actually, you need to read everything Lonny <a href="http://sql-sasquatch.blogspot.com/">posts</a>.</li>
</ul>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://engineering.linkedin.com/blog/2021/evolving-linkedin-s-analytics-tech-stack">Evolving LinkedIn&rsquo;s analytics tech stack</a>. This is a fascinating post looking at lessons learned from LinkedIn&rsquo;s data platform migration. This post is a goldmine of information for anyone migrating from &ldquo;legacy&rdquo; data architecture to a modern one.</li>
<li><a href="https://databricks.com/blog/2021/12/06/deploying-dbt-on-databricks-just-got-even-simpler.html">Deploying dbt on Databricks Just Got Even Simpler</a>. Those interested in Big Data have probably heard about <a href="https://www.getdbt.com/"><strong>dbt</strong></a>, the open-source tool that allows you to build data pipelines using simple SQL. The post I link to announces the <strong>dbt-databricks</strong> adapter, which integrates dbt with the Databricks Lakehouse Platform. Cool stuff!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.infoq.com/news/2021/12/huyen-realtime-ml/">Chip Huyen on Streaming-First Infrastructure for Real-Time ML</a>. Even though you may do real-time ML predictions, you probably update your models manually. This <a href="https://www.infoq.com/">InfoQ</a> article looks at a QCon presentation where the presenter looked at, among other things, how a streaming-first infrastructure can help you do ML in real-time, both online prediction and continual learning.</li>
<li><a href="https://www.kai-waehner.de/blog/2021/12/08/apache-kafka-for-conversational-ai-nlp-chatbot/">Apache Kafka for Conversational AI, NLP and Chatbot</a>. The post looks at how event streaming with Apache Kafka is used in conjunction with Machine Learning platforms for reliable real-time conversational AI, NLP, and chatbots. The post looks at examples from the carmaker BMW, the online travel and booking portal Expedia, and Tinder&rsquo;s dating app. Very cool!</li>
<li><a href="https://www.confluent.io/blog/serverless-event-stream-processing/">Serverless Stream Processing with Apache Kafka, AWS Lambda, and ksqlDB</a>. This blog post defines what &ldquo;serverless stream processing&rdquo; is. Apart from just discussing concepts and implementations, it describes arguably the most essential pattern for building event streaming applications using ksqlDB. Read It!</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>The year is coming to a close, and as for presentations, webinars, etc., I have two left:</p>

<p><img src="/images/posts/sql-cape-adx.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>SQL Cape - Azure Data Explorer</em></p>

<p>On Tuesday (Dec. 14), I deliver the last <strong>Azure Data Explorer</strong> presentation for this year:</p>

<ul>
<li><a href="https://www.meetup.com/SQLCape-Meetup/events/282241220/"><strong>Analyze Billions of Rows of Data in Real-Time Using Azure Data Explorer</strong></a>. The presentation is a virtual event hosted by my mate <a href="https://www.linkedin.com/in/jodyrobertssql/"><strong>Jody Roberts</strong></a>. If you are interested in ADX, please <a href="https://www.meetup.com/SQLCape-Meetup/events/282241220/">sign up</a> (it is <strong>FREE</strong>) and come and join the fun. Any time Jody and I get together, regardless if it is IRL or a virtual event like this, some fun stuff happens!</li>
</ul>

<p>The second event is also virtual:</p>

<p><img src="/images/posts/tech-fun.jpg" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Tech Fun Space</em></p>

<p>The event takes place Thursday, Dec. 23. It is not a webinar but an event for the Global Data Community to get together to welcome 2022. The organiser is my good friend <a href="https://www.linkedin.com/in/jeandjoseph/"><strong>Jean Joseph</strong></a>. Read more about it (this event is also <strong>FREE</strong>) and <a href="https://www.eventbrite.com/e/tech-fun-space-welcome-2022-tickets-215046428657">sign up here</a>.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>

<p>Oh, and if I don&rsquo;t see you virtually or IRL before the holidays: <strong>Happy Holidays</strong>!</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 49, 2021]]></title>
    <link href="https://nielsberglund.com/2021/12/05/interesting-stuff---week-49-2021/" rel="alternate" type="text/html"/>
    <updated>2021-12-05T08:51:58+02:00</updated>
    <id>https://nielsberglund.com/2021/12/05/interesting-stuff---week-49-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://databricks.com/blog/2021/12/01/the-foundation-of-your-lakehouse-starts-with-delta-lake.html">The Foundation of Your Lakehouse Starts With Delta Lake</a>. The Databricks Delta Lake has continuously evolved during the last few years, and in May 2021, Delta Lake 1.0 was announced. The evolution of Delta Lake doesn&rsquo;t stop with the 1.0 release, and this blog post reviews the major features released so far and provides an overview of the upcoming roadmap.</li>
<li><a href="https://www.theseattledataguy.com/what-is-trino-and-why-is-it-great-at-processing-big-data/">What Is Trino And Why Is It Great At Processing Big Data</a>. Trino is an open-source distributed SQL query engine for ad-hoc and batch ETL queries against multiple types of data sources. It previously went under the name of Presto, but due to various reasons, it had to change its name. The post linked to looks at Trino and covers its positives and negatives. At <a href="/derivco">Derivco</a> we have contemplated using Trino. Let us see what the future brings.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://developer.confluent.io/podcast/ksqldb-fundamentals-how-apache-kafka-sql-and-ksqldb-work-together-ft-simon-aubury">ksqlDB Fundamentals: How Apache Kafka, SQL, and ksqlDB Work Together ft. Simon Aubury</a>. This link is to a podcast where <a href="https://twitter.com/tlberglund">Tim Berglund</a> talks to <a href="https://twitter.com/simonaubury">Simon Aubury</a> about everything ksqlDB. They cover basic ksqlDB, plus they look at how to use ksqlDB to find out which aeroplane wakes Simon&rsquo;s cat each morning. Very interesting!</li>
<li><a href="https://www.infoq.com/presentations/raft-kafka-api/">Co-Designing Raft + Thread-per-Core Execution Model for the Kafka-API</a>. This <a href="https://www.infoq.com/">InfoQ</a> presentation looks at, as the title says, codesign in Raft on a thread per core model for the Kafka API. This presentation is a must-see if you are interested in building low-latency software.</li>
<li><a href="https://www.confluent.io/blog/guide-to-stream-processing-and-ksqldb-fundamentals/">A Guide to Stream Processing and ksqlDB Fundamentals</a>. ksqlDB allows you to build applications that react to events as they happen and to take advantage of real-time data. Even though you use familiar SQL syntax when building your ksqlDB application, you might want some help. This post talks about the <strong>ksqlDB 101</strong> course on Confluent Developer, which offers both lectures and hands-on exercises.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>SQLBITS 2022 - The Greatest Data Show - is just around the corner, and I am happy to announce that I am doing a full-day training session:</p>

<p><img src="/images/posts/sqlbits-precon-adx.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>SQLBITS 2022 - A Day of Azure Data Explorer</em></p>

<p>Yes, I am doing a whole day of <strong>Azure Data Explorer</strong>. Read more at: <a href="https://sqlbits.com/information/event22/A_Day_of_Azure_Data_Explorer/trainingdetails"><strong>A Day of Azure Data Explorer</strong></a>.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 48, 2021]]></title>
    <link href="https://nielsberglund.com/2021/11/28/interesting-stuff---week-48-2021/" rel="alternate" type="text/html"/>
    <updated>2021-11-28T08:38:06+02:00</updated>
    <id>https://nielsberglund.com/2021/11/28/interesting-stuff---week-48-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://blogvisionarios.com/e-learning/articulos-data/databricks-photon-vs-azure-synapse/">Databricks Photon vs Azure Synapse</a>. This post is in Spanish, but as I know that the readers of my blog (all two of you) are fluent in Spanish, so that shouldn&rsquo;t be a problem. Or - you could do what I did, translate it to English. The translation didn&rsquo;t come out half bad, and the post compares the performance of Databricks and Azure Synapse. Interesting read!</li>
</ul>

<h2 id="azure">Azure</h2>

<ul>
<li><a href="https://blog.ediri.io/podtato-head-pulumi-and-azure-container-apps">Podtato-head, Pulumi and Azure Container Apps</a>. This post caught my eye as it mentions <a href="https://www.pulumi.com/"><strong>Pulumi</strong></a>. I am interested in Pulumi not so much for it being a cloud engineering platform, but for <a href="https://twitter.com/funcOfJoe"><strong>Joe Duffy</strong></a> being one of the founders. Joe is a very smart guy, and I keep an eye out for things he is doing. Anyway, I link to this post because of the reference to <strong>Azure Container Apps</strong>. ACA looks extremely cool, and I must look at it further.</li>
</ul>

<h2 id="azure-data-explorer">Azure Data Explorer</h2>

<ul>
<li><a href="https://techcommunity.microsoft.com/t5/azure-data-explorer-blog/using-azure-data-explorer-timeseries-capabilities-in-power-bi/ba-p/2727977">Using Azure Data Explorer timeseries capabilities in Power BI</a>. The more I see of <strong>Azure Data Explorer</strong> (ADX), the more impressed I become. Take this post, for example, which looks at using PowerBI to hook into the time series capabilities of ADX. Soo cool!</li>
<li><a href="https://medium.com/@ravikanthmusti/azure-data-explorer-for-real-time-alerts-in-healthcare-480158bcf5d3">Azure Data Explorer for Real time alerts in Healthcare</a>. One more post which makes me realise how awesome <strong>Azure Data Explorer</strong> is. This post looks at how you can &ldquo;hook-up&rdquo; <a href="https://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-overview"><strong>Azure Logic Apps</strong></a> with ADX.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-overview">Consuming over 1 billion Kafka messages per day at Ifood</a>. Well, the title says it all! In the post, the author looks at the journey he had towards being able to consume a &ldquo;shed-load&rdquo; (&ldquo;shed-load&rdquo; is a technical term) of messages per day. The post has quite a few &ldquo;tips and tricks&rdquo;, so you should definitely read this post if you are a high volume Kafka consumer.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>As with last weeks roundup, this is not so much of what I am doing as of what I did:</p>

<p><img src="/images/posts/analyze-billions-adx.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Future Data Driven</em></p>

<p>And also, like last week, it is a recording for a presentation I did a while back. The presentation is <a href="https://youtu.be/AUXzlBZtebg"><strong>Analyze Billions of Rows of Data in Real-Time Using Azure Data Explorer</strong></a>, and the conference was <strong>Future Data Driven</strong>.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 47, 2021]]></title>
    <link href="https://nielsberglund.com/2021/11/21/interesting-stuff---week-47-2021/" rel="alternate" type="text/html"/>
    <updated>2021-11-21T09:27:19+02:00</updated>
    <id>https://nielsberglund.com/2021/11/21/interesting-stuff---week-47-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="machine-learning-ai">Machine Learning / AI</h2>

<ul>
<li><a href="https://www.microsoft.com/en-us/research/blog/synapseml-a-simple-multilingual-and-massively-parallel-machine-learning-library/">SynapseML: A simple, multilingual, and massively parallel machine learning library</a>. This post introduces <strong>SynapseML</strong>. SynapseML was previously known as MMLSpark and is an open-source library that simplifies the creation of massively scalable machine learning (ML) pipelines. SynapseML unifies several ML frameworks and new Microsoft algorithms in a single, scalable API usable across Python, R, Scala, and Java.</li>
</ul>

<h2 id="databases">Databases</h2>

<ul>
<li><a href="http://muratbuffalo.blogspot.com/2021/11/whats-really-new-with-newsql.html">What&rsquo;s Really New with NewSQL?</a>. In this post, <a href="https://twitter.com/muratdemirbas">Murat</a> looks at the evolution of NoSQL into NewSQL and what NewSQL is. Very informative; I liked the post a lot.</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://databricks.com/blog/2021/11/19/ray-on-databricks.html">Ray on Databricks</a>. No, this is not a post where someone named <strong>Ray</strong> talks about Databricks. Ray is an open-source project that makes it simple to scale any compute-intensive Python workload. Running Ray on top of an Apache Spark cluster creates the ability to distribute the internal code of PySpark UDFs and Python code that used to be only run on the driver node. But hang on a sec; Spark is a distributed framework. Why would I want to run another distributed framework on top of Spark? Well, read the post and find out.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.kai-waehner.de/blog/2021/11/14/streaming-data-exchange-data-mesh-apache-kafka-in-motion/">Streaming Data Exchange with Kafka and a Data Mesh in Motion</a>. In quite a few roundups, I have linked to posts about <strong>Data Mesh</strong>. In even more roundups, I have linked to Kafka material and posts about streaming data. The post linked to looks at the principle behind the Data Mesh and why we need multiple technologies to build a Data Mesh. The post dives into why Kafka is a good solution for the foundation of a Data Mesh.</li>
<li><a href="https://www.confluent.io/blog/ksqldb-0-22-new-features-major-upgrades/">Announcing ksqlDB 0.22.0</a>. I guess the post title says it all: it looks at some of the new features of <strong>ksqlDB</strong> 0.22. And some very cool new features they are as well! Please read the post to find out more!</li>
<li><a href="https://www.confluent.io/blog/push-queries-v2-with-ksqldb-scalable-sql-query-subscriptions/">How to Efficiently Subscribe to a SQL Query for Changes</a>. This post looks at one of the new features in <strong>ksqlDB</strong> 0.22; enhancements to push queries and increased scalability of said queries. Very, very cool!</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>It is not so much of what I am doing as of what did I do:</p>

<p><img src="/images/posts/improve-clv-2.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Cloud Data Driven</em></p>

<p>I did a presentation a little while back at <a href="https://www.linkedin.com/company/clouddatadriven/"><strong>Cloud Data Driven</strong></a>, where I looked at <strong>Customer Lifetime Value</strong> (CLV), and how you can use Azure Databricks to calculate the CLV. As I said, it was a week or two ago, why i mention it now is because the recording of the webinar is up on YouTube. So, if you are interested - go and have a look at <a href="https://youtu.be/e6MJ4DRCgj8"><strong>Improve Customer Lifetime Value using Azure Databricks Delta Lake</strong></a>.</p>

<p>While you are at it, register with <a href="https://www.meetup.com/cloud-data-driven"><strong>Cloud Data Driven</strong></a>&rsquo;s Meetup group. The group is awesome if you are interested in everything data!</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 46, 2021]]></title>
    <link href="https://nielsberglund.com/2021/11/14/interesting-stuff---week-46-2021/" rel="alternate" type="text/html"/>
    <updated>2021-11-14T08:50:09+02:00</updated>
    <id>https://nielsberglund.com/2021/11/14/interesting-stuff---week-46-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://azure.microsoft.com/en-us/services/chaos-studio/">Azure Chaos Studio</a>. At <a href="https://myignite.microsoft.com/home">Microsoft Ignite</a>, a week or two ago, Microsoft announced the public preview of <strong>Azure Chaos Studio</strong>. Azure Chaos Studio is a fully-managed experimentation service to help customers track, measure, and mitigate faults with controlled chaos engineering to improve the resilience of their cloud applications. This looks very interesting, and we will definitely have a look at it.</li>
</ul>

<h2 id="azure-data-explorer">Azure Data Explorer</h2>

<ul>
<li><a href="https://docs.microsoft.com/en-us/azure/architecture/example-scenario/security/security-log-retention-azure-data-explorer">Long-term security log retention with Azure Data Explorer</a>. Having access to long-term security logs is essential. Querying long-term logs is critical for identifying the impact of threats and investigating illicit access attempts. This post outlines a solution for long-term retention of security logs where Azure Data Explorer is at the core of the architecture.</li>
<li><a href="https://techcommunity.microsoft.com/t5/azure-data-explorer/query-past-data-with-hot-windows/ba-p/2848114">Query past data with hot windows</a>. Azure Data Explorer has the notion of hot and cold data. Hot data is stored on SSD&rsquo;s on cluster nodes, whereas cold data is stored in Azure Blob Storage. Hot data offers the best query performance: an order of magnitude more performant than cold data. Sometimes you may want to query the hot data together with some of the cold. This post looks recently added functionality to Azure Data Explorer, creating a time window in the past which we want to be part of the hot data: Hot Window.<br /></li>
<li><a href="https://techcommunity.microsoft.com/t5/azure-data-explorer/train-your-model-on-spark-databricks-score-it-on-adx/ba-p/2098522">Train your Model on Spark/Databricks, score it on ADX</a>. Recently, I have been doing conference talks around Azure Databricks and Apache Spark and Azure Data Explorer. How cool would it be if you could combine the two?! The post linked to does just that. It looks at training and creating Machine Learning models using Azure Databricks and Spark and then using those models from Azure Data Explorer. Very cool! Oh, BTW - with Azure Data Explorer Pool&rsquo;s being made available in Azure Synapse, you no longer need Azure Databricks. You can do the same thing with Azure Synapse Analytics. The <a href="https://techcommunity.microsoft.com/t5/azure-data-explorer/query-past-data-with-hot-windows/ba-p/2848114"><strong>Azure Synapse Analytics - Operationalize your Spark ML model into Data Explorer pool for scoring</strong></a> post looks at that.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/introducing-confluent-platform-7-0/">Building Real-Time Hybrid Architectures with Cluster Linking and Confluent Platform 7.0</a>. Confluent recently released <a href="https://www.confluent.io/product/confluent-platform/"><strong>Confluent Platform 7.0</strong></a>, and this post looks at one of the new features in detail, the ability to directly connect clusters and mirror topics from one cluster to another: <a href="https://docs.confluent.io/platform/current/multi-dc-deployments/cluster-linking/index.html"><strong>Cluster Linking</strong></a>. This is something that we at <a href="/derivco">Derivco</a> are really interested in.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 45, 2021]]></title>
    <link href="https://nielsberglund.com/2021/11/07/interesting-stuff---week-45-2021/" rel="alternate" type="text/html"/>
    <updated>2021-11-07T08:13:38+02:00</updated>
    <id>https://nielsberglund.com/2021/11/07/interesting-stuff---week-45-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="sql-server">SQL Server</h2>

<ul>
<li><a href="https://techcommunity.microsoft.com/t5/sql-server/announcing-the-sql-server-2022-early-adoption-program/ba-p/2910617">Announcing the SQL Server 2022 Early Adoption Program</a>. The <a href="https://myignite.microsoft.com/home">Microsoft Ignite</a> conference was held during the week just gone by. As expected, there were quite a few announcements around new and improved products. One such announcement was related to this post; the next version of SQL Server is in the works, and Microsoft has just opened the <strong>Early Adoption Program</strong> (EAP) for <strong>SQL Server 2022</strong>. If you are interested in shaping the next version of SQL Server, I suggest you sign up!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://medium.com/event-driven-utopia/architecting-a-kafka-centric-retail-analytics-platform-part-2-137bfac04001">Architecting a Kafka-centric Retail Analytics Platform â€” Part 2</a>. In <a href="/2021/10/31/interesting-stuff---week-44-2021/">last weeks roundup</a>, I linked to the first post in a series, of which this post is the second instalment. In this post, the author, <a href="https://twitter.com/dunithd">Dunith Dhanushka</a>, looks at data ingestion into Kafka in detail. As the series is about retail, the post looks at the retail data landscape, what data to capture, and how it can be ingested into Kafka using the Kafka ecosystem.</li>
<li><a href="https://www.confluent.io/blog/streaming-databases-vs-traditional-databases/">Readings in Streaming Database Systems</a>. This post is the introduction/prequel to a series looking at streaming databases: <strong>The Streaming Database Series</strong>. This post gives a high level overview of what&rsquo;s coming in the posts in the series.  It also provides an overview of streaming databases.</li>
<li><a href="https://www.confluent.io/blog/databases-meet-stream-processing-the-future-of-sql/">The Future of SQL: Databases Meet Stream Processing</a>. This post is the first in the <strong>The Streaming Database Series</strong> mentioned above. The post discusses why the database world needs enhancements to handle data both at rest and in transit. The enhancements looked at are the <code>STREAM</code> abstraction, new query types, and extended semantics for handling time.</li>
<li><a href="https://www.confluent.io/blog/streaming-database-design-principles-and-guarantees/">4 Key Design Principles and Guarantees of Streaming Databases</a>. The second in the series mentioned above, this post summarizes a few challenging design principles for modern streaming databases that act as a source of truth for stream data management and query processing systems. The post also presents ksqlDB&rsquo;s persistent log-based approach to following the design principles.</li>
<li><a href="https://www.confluent.io/blog/streaming-databases-cloud-data-in-motion-never-rests/">How Do You Change a Never-Ending Query?</a>. The post linked to is the third in the <strong>The Streaming Database Series</strong>. The post looks at how we can evolve queries in a streaming database and some of the pitfalls that may occur.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>Now is the conference season, and I am presenting at <a href="https://www.linkedin.com/company/clouddatadriven/"><strong>Cloud Data Driven</strong></a>:</p>

<p><img src="/images/posts/improve-clv.jpg" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Cloud Data Driven</em></p>

<p>I will be talking about how to calculate Customer Lifetime Value using Azure Databricks. If you are interested, the registration is <strong>FREE</strong>, so go ahead and <a href="https://www.eventbrite.com/e/improve-customer-lifetime-value-using-azure-databricks-delta-lake-tickets-189230642947"><strong>register</strong></a>.</p>

<p>As you see, the presentation is on Thursday, November 11. If you read the last week&rsquo;s <a href="/2021/10/31/interesting-stuff---week-44-2021/">roundup</a>, you may have noticed this:</p>

<p><img src="/images/posts/PASS-I-am-speaking-800.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>PASS Session</em></p>

<p>Yes, I am doing a live PASS Q&amp;A the same day. The PASS session id for my <a href="https://passdatacommunitysummit.com/sessions/265026"><strong>Analyze Billions of Rows of Data in Real-Time Using Azure Data Explorer</strong></a> presentation. Fortunately, the PASS session is 3:15 - 3:45 pm UTC, and my Cloud Data Driven presentation is at 4 pm UTC. Phew!</p>

<p>So here is an idea; get a double dose of Niels:</p>

<ul>
<li><a href="https://reg.passdatacommunitysummit.com/flow/redgate/summit2021/registrationopeningpage/page/introlanding">Register</a> (FREE) for PASS now, view the <a href="https://passdatacommunitysummit.com/sessions/265026"><strong>Analyze Billions of Rows of Data in Real-Time Using Azure Data Explorer</strong></a> recorded video and come and chat to me <a href="https://passdatacommunitysummit.com/sessions/265026">Thursday, Nov 11 15:15 - 15:45 UTC</a>.</li>
<li><a href="https://www.eventbrite.com/e/improve-customer-lifetime-value-using-azure-databricks-delta-lake-tickets-189230642947">Register</a> and attend my Cloud Data Driven presentation.</li>
</ul>

<p>Yay, Niels on Thursday from 3:15 UTC. What could be better than that? Actually, don&rsquo;t answer that question.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 44, 2021]]></title>
    <link href="https://nielsberglund.com/2021/10/31/interesting-stuff---week-44-2021/" rel="alternate" type="text/html"/>
    <updated>2021-10-31T08:53:44+02:00</updated>
    <id>https://nielsberglund.com/2021/10/31/interesting-stuff---week-44-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h1 id="machine-learning-data-science">Machine Learning / Data Science</h1>

<ul>
<li><a href="https://netflixtechblog.com/open-sourcing-a-monitoring-gui-for-metaflow-75ff465f0d60">Open-Sourcing a Monitoring GUI for Metaflow, Netflix&rsquo;s ML Platform</a>. This Netflix post looks at <strong>Metaflow GUI</strong>. This GUI for their open-sourced, <strong>Metaflow</strong> library allows data scientists to monitor their workflows in real-time, track experiments, and see detailed logs and results for every executed task. The GUI can be extended with plugins, allowing the community to build integrations to other systems, etc.</li>
<li><a href="https://databricks.com/blog/2021/10/28/moneyball-2-0-real-time-decision-making-with-mlbs-statcast-data.html">Moneyball 2.0: Real-time Decision Making With MLB&rsquo;s Statcast Data</a>. Back in 2003, <a href="https://en.wikipedia.org/wiki/Michael_Lewis">Michael Lewis</a> wrote the book <a href="https://en.wikipedia.org/wiki/Moneyball">Moneyball</a>. The book was about how a baseball manager used data analysis to identify undervalued players. The post here looks at how baseball teams today use streaming data and Databricks to do real-time analysis and decisions. Very interesting! Oh, BTW, Micheal Lewis is an excellent author, and the book mentioned is great!</li>
</ul>

<h2 id="azure-data-explorer">Azure Data Explorer</h2>

<ul>
<li><a href="https://dps10.com/how-to-do-real-time-analytics-using-apache-kafka-and-azure-data-explorer/">How to do Real-Time Analytics Using Apache Kafka and Azure Data Explorer</a>. In a couple of blog posts, I have mentioned how I did a session about Apache Kafka and Azure Data Explorer at the <strong>Data Platform Summit 2021</strong>. The recordings from the Summit has now been made available for FREE, and the link is to my session. Notice that you need to join Data Platform Geeks unless you are a member already, but it is free, and by joining, you get access to all recordings!</li>
<li><a href="/2021/10/27/how-to-ingest-into-azure-data-explorer-from-apache-kafka-using-kafka-connect/">How to Ingest Into Azure Data Explorer From Apache Kafka using Kafka Connect</a>. This post is also from &ldquo;yours truly&rdquo;. In the post we look at how to configure and set up Kafka Connect to allow ingestion into Azure Data Explorer.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://medium.com/event-driven-utopia/architecting-a-kafka-native-retail-analytics-platform-part-1-bf1eba42a371">Architecting a Kafka-centric Retail Analytics Platform â€” Part 1</a>. This post is the first of a series looking at building a Kafka-centric analytics platform that ingests and processes business data at scale. By the way, the author of the post, <a href="https://twitter.com/dunithd">Dunith Dhanushka</a>, is someone you should follow if you are interested in event driven architecture, streaming, Kafka, etc. He is excellent, and I am subscribing to his writings on Medium!</li>
<li><a href="https://www.confluent.io/blog/stream-governance-how-it-works/">Stream Governance â€“ How it Works</a>. I have written previously about the new Stream Governance functionality Confluent introduced recently. This post is the first in a series about Stream Governance and how it works: <strong>Stream Governance â€“ How it Works</strong>. This first post looks at some of the key features of Stream Governance. At <a href="/derivco"><strong>Derivco</strong></a>, we are highly interested in the topic of data governance. I will follow this closely!</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>Well, I have mentioned it before, but:</p>

<p><img src="/images/posts/PASS-I-am-speaking-800.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>PASS Session</em></p>

<p>Yeah, I am delivering <a href="https://passdatacommunitysummit.com/sessions/265026"><strong>Analyze Billions of Rows of Data in Real-Time Using Azure Data Explorer</strong></a>. The session is recorded and will be available for viewing from when the conference starts. Then <a href="https://event.passdatacommunitysummit.com/widget/redgate/summit2021/sessioncatalog/session/1629311226474001sKl1">Thursday, Nov 11 15:15 - 15:45 UTC</a>, (you have to <a href="https://reg.passdatacommunitysummit.com/flow/redgate/summit2021/registrationopeningpage/page/introlanding">register</a> to access this link), is a virtual live Q&amp;A with me where we discuss <strong>Azure Data Explorer</strong>.</p>

<p>So, <a href="https://reg.passdatacommunitysummit.com/flow/redgate/summit2021/registrationopeningpage/page/introlanding">register</a> now, view the recorded video and come and chat to me <a href="https://event.passdatacommunitysummit.com/widget/redgate/summit2021/sessioncatalog/session/1629311226474001sKl1">Thursday, Nov 11 15:15 - 15:45 UTC</a>. The registration is FREE, and besides me, you get to hear from the people that really know what they are talking about!</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to Ingest Into Azure Data Explorer From Apache Kafka using Kafka Connect]]></title>
    <link href="https://nielsberglund.com/2021/10/27/how-to-ingest-into-azure-data-explorer-from-apache-kafka-using-kafka-connect/" rel="alternate" type="text/html"/>
    <updated>2021-10-27T14:13:37+02:00</updated>
    <id>https://nielsberglund.com/2021/10/27/how-to-ingest-into-azure-data-explorer-from-apache-kafka-using-kafka-connect/</id>
    <content type="html"><![CDATA[<p>If you follow my blog, you probably know that I am a huge fan of <strong>Apache Kafka</strong> and event streaming/stream processing. Recently <strong>Azure Data Explorer</strong> (<strong>ADX</strong>) has caught my eye. In fact, in the last few weeks, I did two conference sessions about ADX. A month ago, I published a blog post related to Kafka and ADX: <a href="/2021/09/06/run-self-managed-kusto-kafka-connector-serverless-in-azure-container-instances/"><strong>Run Self-Managed Kusto Kafka Connector Serverless in Azure Container Instances</strong></a>.</p>

<p>As the title of that post implies, it looked at the ADX Kafka sink connector and how to run it in Azure. What the post did not look at was how to configure the connector and connect it to ADX. That is what we will do in this post (and maybe in a couple of more posts).</p>

<p></p>

<h2 id="pre-reqs">Pre-Reqs</h2>

<p>This section is here if you want to follow along. This is what you need:</p>

<ul>
<li>Docker desktop. If you are on Windows and don&rsquo;t have it, you download it from <a href="https://docs.docker.com/desktop/windows/install/">here</a>.</li>
<li>An Azure account. If you don&rsquo;t have an Azure subscription, sign up for a <a href="https://azure.microsoft.com/en-us/free/">free account</a>.</li>
<li>Azure CLI. The Azure CLI is a cross-platform command-line tool connecting to Azure and executing administrative commands on Azure resources. You find downloads and install instructions <a href="https://docs.microsoft.com/en-us/cli/azure/install-azure-cli">here</a>.</li>
<li>An Azure Data Explorer cluster and database. To see how to set up a cluster and a database, look here: <a href="https://docs.microsoft.com/en-us/azure/data-explorer/create-cluster-database-portal">Quickstart: Create an Azure Data Explorer cluster and database</a>.</li>
<li>Kafka cluster. You can either run the cluster &ldquo;on-prem&rdquo; or in the cloud. I wrote a blog post about how to run Confluent Platform using Docker <a href="/2019/06/18/confluent-platform--kafka-for-a-.net-developer-on-windows/">here</a> and running Confluent Cloud on Azure <a href="/2021/08/14/run-confluent-cloud--serverless-apache-kafka-on-azure/">here</a>. In this post, I use Confluent Cloud in Azure.</li>
<li>You need to download the Kusto connector from <a href="https://github.com/Azure/kafka-sink-azure-kusto/releases">here</a>. In the <a href="/2021/09/06/run-self-managed-kusto-kafka-connector-serverless-in-azure-container-instances/"><strong>Run Self-Managed Kusto Kafka Connector Serverless in Azure Container Instances</strong></a> post, I downloaded and used the 2.0 version. In this post, we will use the <a href="https://github.com/Azure/kafka-sink-azure-kusto/releases/download/v2.2.0/kafka-sink-azure-kusto-2.2.0-jar-with-dependencies.jar">2.2 version</a>.</li>
</ul>

<h4 id="confluent-cloud-cli">Confluent Cloud CLI</h4>

<p>We need a way to post messages/events to Kafka, and usually, I do it using a .NET Core application. Since in this post I only will send a few messages, I am going to use the <a href="https://docs.confluent.io/ccloud-cli/current/overview.html"><strong>Confluent Cloud CLI</strong></a> (ccloud) tool.</p>

<p>This tool is a must for anyone using Confluent Cloud as it enables developers to create, manage, and deploy their Confluent components. You find download and install instructions <a href="https://docs.confluent.io/ccloud-cli/current/install.html#tarball-or-zip-installation">here</a> and some examples of how to use it <a href="https://docs.confluent.io/platform/current/tutorials/examples/clients/docs/ccloud.html">here</a>.</p>

<p>If your Kafka is &ldquo;bare metal&rdquo;, or Docker based, there are commands based on script files for various operations. In this post, the following commands are what you need if you are not using Confluent Cloud:</p>

<ul>
<li><code>/kafka-topics</code>: handle topics; list, create, etc.</li>
<li><code>/kafka-console-producer</code>: publish messages to a topic.</li>
<li><code>/kafka-console-consumer</code>: consume messages from a topic.</li>
</ul>

<p>Having sorted out the pre-reqs, and some Kafka tools, let us move on.</p>

<h2 id="scene-setting">Scene Setting</h2>

<p>Before we go further, let&rsquo;s see what data we are working with. Since I am working at <a href="/derivco">Derivco</a>, an online gaming company, I guess it is only natural that my sample data is gameplay-related (fictitious gameplay that is). The idea is that, in this post, we have online players playing various types of games, and the games are submitting game events to a Kafka topic. The event looks something like so:</p>

<pre><code class="language-json">{
  &quot;playerId&quot;: Int32,
  &quot;gameId&quot;: Int32,
  &quot;win&quot;: Int64,
  &quot;score&quot;: Int32,
  &quot;eventTime&quot;: DateTime
}
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Event Schema</em></p>

<p>The fields we see in <em>Code Snippet 1</em> are:</p>

<ul>
<li><code>playerId</code>: the unique id of a player.</li>
<li><code>gameId</code>: an identifier for the specific game a player is playing.</li>
<li><code>win</code>: playing the game may result in a &ldquo;win&rdquo;, which is a unit of &ldquo;something&rdquo;.</li>
<li><code>score</code>: each interaction in a game by a player earns the player a score.</li>
<li><code>eventTime</code>: the time in UTC when the event happened.</li>
</ul>

<p>The event we see the schema for above is what is submitted to Kafka.</p>

<h4 id="create-topic">Create Topic</h4>

<p>Let us create the topic to which the events are submitted:</p>

<pre><code class="language-bash"># list existing topics
$ ccloud kafka topic list

# the above returns no topics
# create a topic for gameplay
$ ccloud kafka topic create gameplay --partitions 4

# check that we have a topic
$ ccloud kafka topic list
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Create Kafka Topic</em></p>

<p>In <em>Code Snippet 2</em> we see:</p>

<ul>
<li>before I create the topic, I check what topics exist in the cluster. As this is a brand new Confluent Cloud cluster, there were no topics.</li>
<li>I create the <code>gameplay</code> topic, explicitly setting the number of partitions to four. If I hadn&rsquo;t set the number of partitions, it would default to six.</li>
<li>to ensure that the topic has been created, I check for topics again, and yes - it is there.</li>
</ul>

<p>Let us publish an event to finish the <em>Setting the Scene</em> topic (did you see what I did there?).</p>

<h4 id="test-publish">Test Publish</h4>

<p>We start with setting up a listener in a terminal window, so we can see when data arrives in the topic:</p>

<pre><code class="language-bash"># setup a listener, when clicking enter it will start listening
ccloud kafka topic consume gameplay -b
Starting Kafka Consumer. Use Ctrl-C to exit.
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Consume from Topic</em></p>

<p>The code in <em>Code Snippet 3</em> sets up a listener. After clicking enter, you see the &ldquo;Starting Kafka Consumer &hellip;&rdquo; as in the code snippet, and the listener is now ready to receive messages.</p>

<p>Time to publish an event. Open another terminal window and, if you are on Windows, make sure the terminal is Windows command prompt - not PowerShell. For some reason, when I try to publish with <code>ccloud</code> using PowerShell, I get errors. Anyway, the code:</p>

<pre><code class="language-bash">$ ccloud kafka topic produce gameplay --parse-key --delimiter ,
33, {&quot;playerId&quot;:33, &quot;gameId&quot;:23, &quot;win&quot;:55, &quot;score&quot;:123, \ 
      &quot;eventTime&quot;:&quot;2021-10-24 04:14:39.572&quot;}
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Publish to Topic</em></p>

<p>In <em>Code Snippet 4</em>, we see how we use the <code>ccloud kafka topic produce</code> command to publish to our <code>gameplay</code> topic. The two flags we see are:</p>

<ul>
<li>The <code>--parse-key</code> flag indicates we supply a message key with the message.</li>
<li>The <code>--delimiter</code> flag defines the delimiter between the message key and message value.</li>
</ul>

<p>After hitting enter after the first line, we enter the message. In <em>Code Snippet 4</em>, we start with the message key <code>33</code>, followed by the message value. The message key, <code>33</code>, represents the <code>playerId</code> as we want to partition the Kafka topic on <code>playerId</code>.</p>

<blockquote>
<p><strong>NOTE:</strong> If you copy the code from <em>Code Snippet 4</em>, be aware of the line continuation character (<code>\</code>) in the message.</p>
</blockquote>

<p>To publish the message, you hit enter, and when you go to the consumer terminal window, you should now see the message.</p>

<p>We now have a topic, and we can produce to that topic. So now, let us go back to connecting up our Kafka topic with Azure Data Explorer.</p>

<h2 id="create-a-kusto-connect-image">Create a Kusto Connect Image</h2>

<p>As we know from my previous post and what I mentioned at the beginning of this post, we use a Kafka connector to connect Kafka to ADX, and in the <em>Pre-Reqs</em> section, I downloaded the connector.</p>

<p>Suppose I had had an on-prem Kafka Connect installation. In that case, I could have taken my downloaded Kusto connector, copied it into my Kafka connect box, restarted the Kafka Connect process, and all would be good.</p>

<blockquote>
<p><strong>NOTE:</strong> What I wrote above is a slight simplification; I would have needed to set some connect properties etc., as well. But from a high level, that&rsquo;s what I would have done.</p>
</blockquote>

<p>The point above is moot as I don&rsquo;t have a Kafka Connect installation, so I will run the Kusto connector from Docker instead. To do that, I need to create a Docker image of the connector.</p>

<h4 id="create-a-dockerfile">Create a Dockerfile</h4>

<p>We build the image from a <code>Dockerfile</code>, so we start with creating the file:</p>

<p><img src="/images/posts/kusto-conn-docker-file.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Dockerfile</em></p>

<p>We see in <em>Figure 1</em> above:</p>

<ul>
<li>As the connector needs to run in a Kafka Connect process, we use the <code>FROM</code> statement to bring in Kafka Connect&rsquo;s latest Confluent base image (<code>cp-server-connect-base:latest</code>).</li>
<li>We copy the <code>.jar</code> file of the downloaded connector to a location in Kafka Connect where the <code>.jar</code> can be loaded from.</li>
<li>As I want to connect to a Kafka cluster requiring authentication, I set some security settings.</li>
</ul>

<p>The last bullet point above is really &ldquo;glossing over&rdquo; what we are doing, so let me explain in a bit more detail. When we use a Kafka connector, the connector consumes from one or more Kafka topics or publishes to one or more Kafka topics. The necessary connection details to the Kafka instance is set up in the individual connector&rsquo;s configuration, which we&rsquo;ll see an example of later.</p>

<p>However, in the <a href="/2021/09/06/run-self-managed-kusto-kafka-connector-serverless-in-azure-container-instances/">previous post</a>, we said how Kafka Connect is a JVM process (a worker) that loads and runs individual connectors such as our Kusto connector. The worker process needs to store configurations of the respective connectors and their state, and it stores this in Kafka. It doesn&rsquo;t have to store it in the Kafka instance the connector(s) consumes from or publishes to - it can use a completely separate Kafka instance and potentially a separate instance per connector. So when we want to use a connector, we need to set that information before configuring the connector. We see that in <em>Figure 1</em> lines 5 - 10, we set the security information to connect to the Kafka cluster where we want to store state and configuration.</p>

<p>That is it. We now have created a Docker file.</p>

<h4 id="build-the-image">Build the Image</h4>

<p>Having the Docker file, we can now go ahead and build the image:</p>

<p><img src="/images/posts/kusto-conn-docker-build.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Docker Build</em></p>

<p>In <em>Figure 2</em>, we see how we use the <code>docker build</code> command to build an image with a given name. We also use the <code>-t</code> flag to <em>tag</em> the image. Looking at <em>Figure 2</em> it seems like the build has succeeded. Let us see if we have an image by running <code>docker images</code>:</p>

<p><img src="/images/posts/kusto-conn-docker-image.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Kusto Docker Image</em></p>

<p>It definitely looks like we are in business as we in <em>Figure 3</em> see the newly built image. The image is now available locally, but we can also push it to the likes of Dockerhub, or - as I did in the <a href="/2021/09/06/run-self-managed-kusto-kafka-connector-serverless-in-azure-container-instances/">previous post</a>  - to Azure Container Instances.</p>

<h2 id="run-the-connector">Run the Connector</h2>

<p>For now, we will not push the image anywhere but run it locally, using Docker Compose.</p>

<h4 id="docker-compose-configuration">Docker Compose Configuration</h4>

<p>In the <a href="/2021/09/06/run-self-managed-kusto-kafka-connector-serverless-in-azure-container-instances/">previous</a> post, we did a quick run of the connector using Docker Compose. Here we will use almost the same Docker Compose file, but look a little bit more in detail what it consists of:</p>

<p><img src="/images/posts/kusto-conn-docker-compose.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Docker Compose File</em></p>

<p>In <em>Figure 4</em>, we see the compose file - named <code>docker-compose.yml</code> - I created to &ldquo;spin up&rdquo; the connector. We see how I have in <em>Figure 4</em> added some numbered &ldquo;bullet&rdquo; points at the left. They indicate relevant &ldquo;stuff&rdquo; for the container so let us look at those and see what they refer to:</p>

<ol>
<li><code>image</code>: the image name and tag which we are using.</li>
<li><code>container_name</code>: arbitrary name of the container.</li>
<li><code>ports</code>: this tells the container to listen on this particular port, and how to map the internal IP to external.</li>
<li><code>CONNECT_BOOTSTRAP_SERVERS</code>: host:port pair for the initial connection to the Kafka cluster. You can define multiple servers with a comma-separated host:port pairs.</li>
<li><code>CONNECT_REST_ADVERTISED_HOST_NAME</code>: hostname for other workers to connect to when we run a distributed Kafka Connect cluster. The <a href="https://rmoff.net/2019/11/22/common-mistakes-made-when-configuring-multiple-kafka-connect-workers/"><strong>Common mistakes made when configuring multiple Kafka Connect workers</strong></a> post by Kafka guru Robin Moffat talks more about this.</li>
<li><code>CONNECT_REST_PORT</code>: port for the REST API endpoint. It is <code>8083</code> by default, so I would not have had to define it in my compose file. Make sure that whatever port number you use is reflected in the <code>ports</code> entry in point 3. You use this port to manage and administer your connector.</li>
<li><code>CONNECT_GROUP_ID</code>: a string that identifies the Kafka Connect cluster group this worker belongs to.</li>
<li><code>CONNECT_CONFIG_STORAGE_TOPIC</code>: name of the topic in which to store connector and task configuration data.</li>
<li><code>CONNECT_OFFSET_STORAGE_TOPIC</code>: name of the topic in which to store offset data for connectors.</li>
<li><code>CONNECT_STATUS_STORAGE_TOPIC</code>: name of the topic in which to store state for connectors.</li>
<li><code>CONNECT_KEY_CONVERTER</code>: what class to use for conversion of the message key. This can be overridden by the configuration of an individual connector.</li>
<li><code>CONNECT_VALUE_CONVERTER</code>: what class to use for conversion of the message value. This can be overridden by the configuration of an individual connector.</li>
</ol>

<p>Oh, one thing about the <code>...STORAGE_TOPIC</code>&rsquo;s; if you have multiple Kafka Connect workers in a Connect group (<code>CONNECT_GROUP_ID</code>), then those workers need to use the same topics in the same cluster.</p>

<h4 id="run-the-container">Run the Container</h4>

<p>When we have created the Docker Compose file as per above, we can run the container. However, before we do that, I would like you to check what topics are in the Kafka Cluster: <code>ccloud kafka topic list</code>. When I run that code, I only see the topic we created above.</p>

<p>Cool, let us now &ldquo;spin up&rdquo; the container:</p>

<p><img src="/images/posts/kusto-conn-docker-compose-up.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Run Docker Compose</em></p>

<p>We see in <em>Figure 5</em> how we start the container using the <code>docker-compose up -d</code> command. I execute the command in the same directory I have the <code>docker-compose.yml</code> file.</p>

<blockquote>
<p><strong>NOTE:</strong> The <code>docker-compose up</code> command expects a file named <code>docker-compose.yml</code>. If you want to use another name, you tell the command the file name using the <code>-f</code> flag.</p>
</blockquote>

<p>In <em>Figure 5</em>, we also see how after the command has been executed, Docker creates an internal network and starts the container.</p>

<p>Seeing what we see in <em>Figure 5</em>, the assumption is that all has gone OK. Let us confirm by looking for new topics in the Kafka cluster. I do the same as I did above executing <code>ccloud kafka topic list</code>:</p>

<p><img src="/images/posts/kusto-conn-kafka-topics-2-new.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>Kafka Topics</em></p>

<p>Yay, when looking at <em>Figure 6</em> we see three new topics. I.e. the topics we defined in the <code>docker-compose.yml</code> file to store status, config and offset. So, so far, so good. Now let us take it one step further.</p>

<p>Remember from above how I said that the <code>CONNECT_REST_PORT</code> in the <code>docker-compose.yml</code> file defines the port we use to administer and configure the connector. We do this by calling endpoints exposed by the Kafka Connect REST API.</p>

<p>Let us now use one of the endpoints to see that the Kafka worker is up and running and that our connector is available. The endpoint we use is the same we used in the [previous][post]: <code>GET /connector-plugins</code>:</p>

<p><img src="/images/posts/kusto-conn-get-connector-plugins.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>Connector Plugins</em></p>

<p>As I run the Docker container on my machine, we see in <em>Figure 7</em> (outlined in red) how I make a <code>GET</code> request against <code>localhost:8083</code> and the <code>connector-plugins</code> endpoint. We also see in <em>Figure 7</em> that the request is successful. It returns an array of connector plugins, of which our plugin (outlined in yellow) is one.</p>

<p>This is awesome; we have now built a Docker image of the Kusto sink connector and &ldquo;spun&rdquo; it up on our local machine. Basically, we are at the same point as at the end of the <a href="/2021/09/06/run-self-managed-kusto-kafka-connector-serverless-in-azure-container-instances/">previous post</a>, apart from the container is not in Azure Container Registry.</p>

<p>Now it is almost time to &ldquo;hook&rdquo; the connector up to Kafka and Azure Data Explorer. However, we need to do some configuration of the ADX cluster and the database before hooking up the connector.</p>

<h2 id="setup-adx-properties">Setup ADX &amp; Properties</h2>

<p>The configuration we need to do of ADX is:</p>

<ul>
<li>Creating an Azure Active Directory (AAD) security principal, which the connector uses to write to the ADX table(s).</li>
<li>Getting some properties of the ADX cluster, so the connector knows where to connect and write data to.</li>
</ul>

<p>We start with the Service Principal (SPN).</p>

<h4 id="adx-service-principal">ADX Service Principal</h4>

<p>To set up the SPN, we use the Azure CLI mentioned above in the <em>Pre-Reqs</em>. This is what you do:</p>

<ul>
<li>log in to Azure: <code>az login</code>. This returns a list of your subscriptions after successful login.</li>
<li>if you have more than one subscription, you set the subscription you want to use: <code>az account set --subscription your-sub-id</code>.</li>
</ul>

<p>Having done the above, you now create the service principal:</p>

<pre><code class="language-bash">az ad sp create-for-rbac -n &quot;kusto-kafka-nielsblog&quot;
</code></pre>

<p><strong>Code Snippet 5:</strong> <em>Create Service Principal</em></p>

<p>The code in <em>Code Snippet 5</em> creates - as mentioned before - a service principal configured to access Azure resources. The output, when executing the code, looks something like so:</p>

<p><img src="/images/posts/kusto-conn-spn.png" alt="" /></p>

<p><strong>Figure 8:</strong> <em>Service Principal Properties</em></p>

<p>In <em>Figure 8</em>, we see how the result from the code returns JSON with 5 fields/properties. Take note of <code>appId</code>, <code>password</code>, and <code>tenant</code> as we will use them later when we configure the connector.</p>

<p>Actually, we&rsquo;ll use <code>appId</code> and <code>tenant</code> right now, as we will add the created service principal to your ADX database.</p>

<p>The easiest way to add the created service principal is to do it from the <em>Query Explorer</em> window for ADX in the Azure portal:</p>

<p><img src="/images/posts/kusto-conn-adx-db-1.png" alt="" /></p>

<p><strong>Figure 9:</strong> <em>ADX Database</em></p>

<p>I have in <em>Figure 9</em> logged in to my Azure subscription and navigated to my ADX cluster (highlighted in yellow). In the cluster, I have one database (outlined in blue). The database is selected, and to get to the query explorer, I click on the <em>Query</em> button outlined in red. When the query explorer opens, I can create the service principal as an admin user in the database:</p>

<pre><code class="language-sql">.add database nielsblogpostsdb1 admins ('aadapp=The-AppId;The-Tenant') 'AAD App'
</code></pre>

<p><strong>Code Snippet 6</strong> <em>Create Service Principal Admin User in Database</em></p>

<p>The code in <em>Code Snippet 6</em> creates an admin user in the <code>nielsblogpostsdb1</code> database. As the user is a service principal, we identify the user with <code>aadapp=appId;tenant</code> from the result in <em>Figure 8</em>. We also give the &ldquo;user&rdquo; a name: in this case: <code>AAD App</code>.</p>

<h4 id="adx-properties">ADX Properties</h4>

<p>The final thing we need to do with ADX is to find the equivalent of a connection string, i.e. where the connector can find ADX and connect to. That is exposed as endpoints in the overview page for the ADX cluster:</p>

<p><img src="/images/posts/kusto-conn-cluster-props.png" alt="" /></p>

<p><strong>Figure 10:</strong> <em>Connection Strings</em></p>

<p>We are looking for two endpoints outlined in red in <em>Figure 10</em>. Those two endpoints represent:</p>

<ul>
<li><code>URI</code>: endpoint for querying the cluster.</li>
<li><code>Data Ingestion URI</code>: this is the endpoint for ingesting into the cluster.</li>
</ul>

<p>Of the two endpoints above, only the ingestion endpoint is required when configuring the connector. When you have taken note of those endpoints, we can go on.</p>

<h2 id="configure-the-connector">Configure the Connector</h2>

<p>To do the &ldquo;hook-up,&rdquo; the connector needs to be configured. From a high level, we need to configure:</p>

<ul>
<li>what topic(s) to read data from.</li>
<li>the target Azure Data Explorer cluster.</li>
<li>the target database and table(s) to ingest the data into.</li>
</ul>

<p>We already have an ADX cluster and a database, so let us create a table.</p>

<h4 id="create-table">Create Table</h4>

<p>We need to create a table to ingest the data from the Kafka <code>gameplay</code> topic. We do it from the <em>Query Explorer</em> window for ADX, as we saw in <em>Figure 9</em>. Open the editor and write the create table code:</p>

<p><img src="/images/posts/kusto-conn-query-editor-1.png" alt="" /></p>

<p><strong>Figure 11:</strong> <em>Query Explorer</em></p>

<p>When we look at the code in <em>Figure 11</em> we see how the statement to create a table looks slightly different from what you were to write in SQL. That is because this is not SQL, but KQL - Kusto Query Language. Expect a future post looking more in detail into KQL.</p>

<p>In <em>Figure 11</em>, we see how the table matches the event schema in <em>Code Snippet 1</em>, and we can now execute the code by hitting enter or clicking the <em>Run</em> button outlined in yellow. After executing the code in <em>Figure 11</em>, you can check that the table has been created by executing: <code>.show tables</code>.</p>

<p><img src="/images/posts/kusto-conn-show-tables.png" alt="" /></p>

<p><strong>Figure 12:</strong> <em>Show Tables</em></p>

<p>As we see in <em>Figure 12</em>, the table has been created. There is one more thing we need to do in the database related to the table. We need to create a mapping between the event data and the columns in the table.</p>

<h4 id="ingestion-mapping">Ingestion Mapping</h4>

<p>As the event lands in Kafka as JSON (or some other format), Azure Data Explorer needs to understand how the fields in the event map to the columns in the table. We do this by creating a table ingestion mapping in the database:</p>

<pre><code class="language-sql">.create table GamePlay ingestion json mapping 'gameplay_json_mapping' 
'[{&quot;column&quot;:&quot;PlayerID&quot;, &quot;Properties&quot;:{&quot;path&quot;:&quot;$.playerId&quot;}}, 
{&quot;column&quot;:&quot;GameID&quot;, &quot;Properties&quot;:{&quot;path&quot;:&quot;$.gameId&quot;}}, 
{&quot;column&quot;:&quot;Win&quot;, &quot;Properties&quot;:{&quot;path&quot;:&quot;$.win&quot;}}, 
{&quot;column&quot;:&quot;Score&quot;, &quot;Properties&quot;:{&quot;path&quot;:&quot;$.score&quot;}}, 
{&quot;column&quot;:&quot;EventTime&quot;, &quot;Properties&quot;:{&quot;path&quot;:&quot;$.eventTime&quot;}} ]'
</code></pre>

<p><strong>Code Snippet 7:</strong> <em>Ingestion Mapping</em></p>

<p>The code in <em>Code Snippet 7</em> shows how we create a JSON ingestion mapping, naming it <code>gameplay_json_mapping</code>. We further see how the columns are mapped against the event fields, where <code>$</code> represents the event&rsquo;s root. After running the code in <em>Code Snippet 7</em>, we can check that the mapping exists executing: <code>.show table GamePlay ingestion mappings</code>, which returns all mappings for that table. The page <a href="https://docs.microsoft.com/en-us/azure/data-explorer/kusto/management/show-ingestion-mapping-command">.show ingestion mapping</a> from ADX documentation covers ingestion mappings in more detail.</p>

<p>Above I said that we had to do one more thing in the database, which was the ingestion mapping we just saw. Actually, there is another thing to do. It is not 100% necessary, but it impacts the ingestion latency. We should create an ingestion policy.</p>

<h4 id="ingestion-policy">Ingestion Policy</h4>

<p>A future blog post, will talk in detail about ADX various ingestion options, and what an ingestion policy is. For now, let us just &ldquo;roll&rdquo; with this, and take it for what it is worth. So to create an ingestion batching policy you:</p>

<pre><code class="language-sql">.alter table GamePlay policy ingestionbatching 
@'{&quot;MaximumBatchingTimeSpan&quot;:&quot;00:00:10&quot;, &quot;MaximumNumberOfItems&quot;: 5, &quot;MaximumRawDataSizeMB&quot;: 100}'
</code></pre>

<p><strong>Code Snippet 8:</strong> <em>Ingestion Batching Policy</em></p>

<p>An ingestion batching policy determines when data aggregation should stop during data ingestion according to the specified settings. In <em>Code Snippet 8</em> we see the code to set the policy with our preferred settings:</p>

<ul>
<li><code>MaximumBatchingTimeSpan</code>: maximum time to close the aggregation and ingest the data. In our case, we set it to 10 seconds, which is the minimum value allowed by ADX. The default value is 5 minutes.</li>
<li><code>MaximumNumberOfItems</code>: when the maximum number is reached, the aggregation is closed and the data ingested. Default is 1000 items.</li>
<li><code>MaximumRawDataSizeMB</code>: when aggregation reaches the maximum size, it closes and ingests the data. Default is 1000 MB.</li>
</ul>

<p>After running the code in <em>Code Snippet 8</em>, we can check and see that the policy has been created: <code>.show table GamePlay policy ingestionbatching</code>.</p>

<blockquote>
<p><strong>NOTE:</strong> If we didn&rsquo;t set the policy as in <em>Code Snippet 8</em>, it would run with the default values, and we may have to wait a while to see data in the table.</p>
</blockquote>

<p>We should now be all set and ready to configure the connector.</p>

<h4 id="connector-configuration">Connector Configuration</h4>

<p>You configure the connector using a JSON file and <code>POST</code>:ing it either via <code>curl</code> or Postman. Personally, I prefer Postman, so that is what I will use later.</p>

<p>The configuration file I use looks like so:</p>

<p><img src="/images/posts/kusto-conn-connector-config-1-new.png" alt="" /></p>

<p><strong>Figure 13:</strong> <em>Connector Config</em></p>

<p>Some of the properties we see in <em>Figure 13</em> is heavily cut, but don&rsquo;t worry; below is an explanation of the most important ones. For an explanation of all of them, see <a href="https://github.com/Azure/kafka-sink-azure-kusto#5-sink-properties">5. Sink properties</a>.</p>

<p>The numbers in <em>Figure 13</em> and their related properties:</p>

<ol>
<li>The <code>topics</code> property points to the topic(s) the connector consumes. It is defined as a comma-delimited string.</li>
<li>In <code>kusto.ingestion.url</code>, you set the ingestion endpoint you &ldquo;grabbed&rdquo; in <em>Figure 10</em>.</li>
<li>The same goes for the <code>kusto.query.url</code>. Remember, this property is optional.</li>
<li>The <code>aad.auth.authority</code> refers to the <code>tenant</code> you received when creating the service principa</li>
<li>The <code>aad.auth.appid</code> is the <code>appId</code> property from the service principal.</li>
<li>The last property referring to authentication is the <code>aad.auth.appkey</code>; the <code>password</code> property for the service principal.</li>
<li>Let us come back to <code>kusto.tables.topics.mapping</code> below.</li>
<li>I have found that I need to set both <code>key.converter.schemas.enable</code> and <code>value.converter.schemas.enable</code> to <code>false</code>. I get some errors otherwise. I need to drill into that a bit deeper at a later stage.</li>
<li>These are the connection and authentication details for the Kafka broker you consume from.</li>
</ol>

<p>Above, I said we&rsquo;d come back to <code>kusto.tables.topics.mapping</code>, so let us do that.</p>

<p><strong>kusto.tables.topics.mapping</strong></p>

<p>As the name implies, this property defines the mapping between the topic and the table(s) in the database(s). My mapping looks like so:</p>

<pre><code class="language-json">[{'topic': 'gameplay','db': 'nielsblogpostsdb1', 'table': 'GamePlay',' \
   format': 'json', 'mapping':'gameplay_json_mapping', 'streaming': 'false'}]
</code></pre>

<p><strong>Code Snippet 9:</strong> <em>Tables Topic Mapping</em></p>

<p>The mapping is an array of topics and their respective mapping to databases and tables in the cluster. In my example, I have only one mapping, and in the mapping, I define:</p>

<ul>
<li>the topic to consume from.</li>
<li>the database and table to ingest into.</li>
<li>the format of the data.</li>
<li>the ingestion mapping that has been set up for the table. We see this in <em>Code Snippet 7</em>.</li>
<li>whether the ingestion is streaming or batch. Default is batch. In a future post, I will talk more about stream and batch ingestion.</li>
</ul>

<p>When we have created the config JSON, we are ready to create the connector on the Kafka Connect cluster:</p>

<p><img src="/images/posts/kusto-conn-connector-config-post.png" alt="" /></p>

<p><strong>Figure 14:</strong> <em>Create the Connector</em></p>

<p>In <em>Figure 14</em>, we see part of the configuration file in <em>Figure 13</em>, and how we <code>POST</code> it to Kafka Connect&rsquo;s <code>/connectors</code> endpoint. When running the code (doing the <code>POST</code>), we should get a <code>201</code> response back, indicating all is well.</p>

<p>I didn&rsquo;t mention the &lsquo;name&rsquo; property we see at line 2 in <em>Figure 13</em>. The name is an arbitrary string, and it comes in useful if you want to manage this particular connector. Things you can do are:</p>

<ul>
<li>pause the connector.</li>
<li>restart the connector.</li>
<li>delete the connector.</li>
<li>check the status of the connector.</li>
<li>etc.</li>
</ul>

<p>Each individual operation is against the <code>/connectors</code> endpoint and takes the connector name as an input parameter, potentially followed by a resource request. An example would be that when we have created the connector, we want to check that it actually is up and running:*</p>

<p><img src="/images/posts/kusto-conn-connector-status.png" alt="" /></p>

<p><strong>Figure 15:</strong> <em>Check Connector Status</em></p>

<p>What <em>Figure 15</em> shows is how we make a <code>GET</code> request (outlined in red) against the <code>/connectors</code> endpoint (highlighted in yellow), for the connectors name (highlighted in red), and we are asking for the <code>status</code> (highlighted in blue).</p>

<p>Executing that request gives us:</p>

<p><img src="/images/posts/kusto-conn-connector-status-result.png" alt="" /></p>

<p><strong>Figure 16:</strong> <em>Status Result</em></p>

<p>We see in <em>Figure 16</em> how everything looks OK! The connector is now configured and up and running, and we should now be able to test it out.</p>

<h2 id="testing-the-connector">Testing the Connector</h2>

<p>Suppose you haven&rsquo;t torn down the topic and recreated it. In that case, the easiest way to do a quick test is that after 10 seconds after the connector is up and running, execute the following query in the <em>Query Editor</em>: <code>GamePlay | count</code>. That on my box results in a value of 1, which is the event we published in <em>Code Snippet 4</em>. The event sits in the topic; the connector connects and starts consuming from the topic ingesting into the table.</p>

<p>Let&rsquo;s now publish some more events:</p>

<pre><code class="language-bash">$ ccloud kafka topic produce gameplay --parse-key --delimiter ,
34, {&quot;playerId&quot;:34, &quot;gameId&quot;:27, &quot;win&quot;:0, &quot;score&quot;:99, \ 
      &quot;eventTime&quot;:&quot;2021-10-27 07:56:39.572&quot;}
38, {&quot;playerId&quot;:38, &quot;gameId&quot;:27, &quot;win&quot;:0, &quot;score&quot;:99, \ 
      &quot;eventTime&quot;:&quot;2021-10-27 07:56:39.572&quot;}
45, {&quot;playerId&quot;:45, &quot;gameId&quot;:17, &quot;win&quot;:10, &quot;score&quot;:103, \ 
      &quot;eventTime&quot;:&quot;2021-10-27 07:56:39.572&quot;}       
</code></pre>

<p><strong>Code Snippet 10:</strong> <em>Publish More Events to Topic</em></p>

<p>In <em>Code Snippet 10</em>, we see how we publish more events. While you publish, execute the <code>GamePlay | count</code> call. You should see how the count increases, not as fast as you publish due to the ingestion batching, but with a slight latency. After a while, you should see the correct count, and if you execute: <code>GamePlay</code>, you should see something similar to this:</p>

<p><img src="/images/posts/kusto-conn-query-result.png" alt="" /></p>

<p><strong>Figure 17:</strong> <em>Query Result</em></p>

<p>From what we published in *Code Snippet 4, together with what we published here in <em>Code Snippet 10</em>, we see how 4 rows in total have been ingested into the <code>GamePlay</code> table. Success!</p>

<h2 id="summary">Summary</h2>

<p>Phew, this was quite a journey, but we made it. So, in this post we have looked at:</p>

<ul>
<li>Confluent Cloud CLI, and how we use it to create topics, publish, and consume messages.</li>
<li>How to create and configure a Docker image from the Kusto Sink Connector.</li>
<li>Set up a Docker Compose file to use the image above.</li>
<li>Create an Azure Active Directory Service Principal using Azure CLI (<code>az</code>).</li>
<li>Create an admin user in our ADX database from that Service Principal.</li>
<li>Define ingestion mapping and ingestion batch policy in the ADX database.</li>
<li>Created the connector configuration file and got an insight into some of its properties.</li>
<li>Created the connector from the connector configuration file.</li>
</ul>

<p>After the above, we published some messages to Kafka. We saw when publishing how the messages were ingested into the table. Notice that depending on the batching policy, you may see different results in the latency when ingesting.</p>

<p>In a future post, we will look more in detail into the different ingestion methods and how to configure them. Until then!</p>

<h2 id="finally">Finally</h2>

<p>If you have comments, questions etc., please comment on this post or [ping][ma] me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 43, 2021]]></title>
    <link href="https://nielsberglund.com/2021/10/24/interesting-stuff---week-43-2021/" rel="alternate" type="text/html"/>
    <updated>2021-10-24T12:07:06+02:00</updated>
    <id>https://nielsberglund.com/2021/10/24/interesting-stuff---week-43-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://aka.ms/adls/hitchhikersguide">The Hitchhiker&rsquo;s Guide to the Data Lake</a>. This post discusses considerations and best practices around how to effectively utilize <strong>Azure Data Lake Storage Gen2</strong> in large scale Big Data platform architectures. I found this post to be extremely useful!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://databricks.com/blog/2021/10/19/introducing-apache-spark-3-2.html">Introducing Apache Spark 3.2</a>. In last week&rsquo;s <a href="/2021/10/17/interesting-stuff---week-42-2021/">roundup</a>, I linked to a post about a new Window type coming in Apache Spark 3.2: the session window. The post linked to here looks at other new interesting features in the 3.2 release!</li>
<li><a href="https://stlplaces.com/blog/best-apache-kafka-books-in-year">Best Apache Kafka Books in 2021</a>. Well, not much to say really here. As the title says, the post lists the Kafka books the author likes best.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>Today and tomorrow, I am putting the finishing touches on my video recording for the <a href="https://passdatacommunitysummit.com/"><strong>PASS Data Community Summit 2021</strong></a>:</p>

<ul>
<li><a href="https://passdatacommunitysummit.com/sessions/265026"><strong>Analyze Billions of Rows of Data in Real-Time Using Azure Data Explorer</strong></a>. In this session, I look at how <strong>Azure Data Explorer</strong> enables us to do near real-time analysis of Big Data.</li>
</ul>

<p>If you are interested you can register <a href="https://reg.passdatacommunitysummit.com/flow/redgate/summit2021/registrationopeningpage/page/introlanding">here</a>. The conference sessions are free!</p>

<p>In addition to the above, I am also working on a blog post looking at ingesting data from Kafka into Azure Data Explorer. I&rsquo;ve been working on it for quite a while now. Hopefully, I&rsquo;ll have it done within a week or two.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 42, 2021]]></title>
    <link href="https://nielsberglund.com/2021/10/17/interesting-stuff---week-42-2021/" rel="alternate" type="text/html"/>
    <updated>2021-10-17T08:52:05+02:00</updated>
    <id>https://nielsberglund.com/2021/10/17/interesting-stuff---week-42-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="azure-data-explorer">Azure Data Explorer</h2>

<ul>
<li><a href="https://youtu.be/52dPYC1V5a0">Azure Data Explorer Shorts: Managed Ingestion</a>. An excellent short (~9 minutes) video explaining the ins and outs of data ingestion into <strong>Azure Data Explorer</strong>.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/how-baader-built-a-predictive-analytics-machine-learning-system-with-kafka-and-rstudio/">Apache Kafka and R: Real-Time Prediction and Model (Re)training</a>. This blog post looks at how KStreams, ksqlDB, and R can be used to create a data pipeline in which a machine learning model is applied to streaming data. The post also looks at how the model can be automatically retrained once the prediction results exceed a certain threshold. Very Cool!</li>
<li><a href="https://databricks.com/blog/2021/10/12/native-support-of-session-window-in-spark-structured-streaming.html">Native Support of Session Window in Spark Structured Streaming</a>. The post linked to, looks at a new window type in the upcoming Apache Spark 3.2 version. Before Spark 3.2, Spark supported tumbling and sliding windows. In the 3.2 version, the session window is introduced. The interesting thing with a session window is that it has a dynamic size of window length depending on the input.</li>
<li><a href="https://www.confluent.io/blog/new-confluent-cloud-connector-features-and-single-message-transforms/">Introducing Single Message Transforms and New Connector Features on Confluent Cloud</a>. Part of Confluent cloud is managed Kafka Connect connectors, and this post announces new features for most of the managed connectors. I am quite &ldquo;chuffed&rdquo; about seeing Single Message Transforms as one such new feature.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 41, 2021]]></title>
    <link href="https://nielsberglund.com/2021/10/10/interesting-stuff---week-41-2021/" rel="alternate" type="text/html"/>
    <updated>2021-10-10T10:15:26+02:00</updated>
    <id>https://nielsberglund.com/2021/10/10/interesting-stuff---week-41-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="misc">Misc.</h2>

<ul>
<li><a href="https://devblogs.microsoft.com/python/python-in-visual-studio-code-october-2021-release/">Python in Visual Studio Code â€“ October 2021 Release</a>. In early October, Python 3.10 stable was released. Hot on the heels of that release comes what is mentioned in this blog post: a new release of the <strong>VS Code</strong> Python extension. The post looks at some of the significant new features and fixes. One of the new features is an improved &ldquo;getting started experience&rdquo; for Python in VS Code. Since I have had issues in the past getting Python up and running, I think I will uninstall my existing Python extension and try this improved extension from &ldquo;scratch&rdquo;.</li>
</ul>

<h2 id="sql-server-big-data-cluster">SQL Server Big Data Cluster</h2>

<ul>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2021/10/06/whats-new-with-sql-server-big-data-clusters-cu13-release/">What&rsquo;s new with SQL Server Big Data Clustersâ€”CU13 Release</a>. I guess the title of the blog post says it all. The post looks at new &ldquo;stuff&rdquo; in the CU13 release of <strong>SQL Server 2019 Big Data Cluster</strong>. The big one for me in this release is the switch from Apache Spark 2.4 to Apache Spark 3.1.2.</li>
</ul>

<h2 id="azure-data-explorer">Azure Data Explorer</h2>

<ul>
<li><a href="https://techcommunity.microsoft.com/t5/azure-data-explorer/aligning-timeseries-application-requirements-into-azure-data/ba-p/2758959">Aligning Timeseries Application Requirements into Azure Data Explorer (ADX)</a>. Time series analysis has become critical in almost any analytical application. This blog post looks at the support for time series analysis in Azure, more specifically in <strong>Azure Data Explorer</strong>. After reading the post, I think it is safe to say that the support in ADX is &ldquo;pretty darn good&rdquo;.</li>
<li><a href="https://davemccollough.com/2021/02/01/kusto-query-language-101/">Kusto Query Language 101</a>. Above, we spoke about time series analysis in <strong>Azure Data Explorer</strong>. It is all good and well that we have that functionality, but how do we query ADX? Well, if you read the post linked to, you will get a &ldquo;crash course&rdquo; in the query language for ADX: <strong>Kusto Query Language</strong> (KQL).</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/resources/demo/stream-governance-discover-understand-and-trust-your-data-in-motion/">Stream Governance: Discover, understand, and trust your data in motion</a>. A month or so ago, Confluent <a href="https://www.confluent.io/blog/governing-data-with-confluent-stream-governance/">announced the release</a> of their platform for stream governance: the <a href="https://www.confluent.io/product/stream-governance/"><strong>Stream Governance</strong> suite</a>. Since the release, Confluent has been busy creating learning resources etc., and the post linked to is the registration page for a Stream Governance webinar. If you are working with streaming data, I do suggest you sign up!</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 40, 2021]]></title>
    <link href="https://nielsberglund.com/2021/10/03/interesting-stuff---week-40-2021/" rel="alternate" type="text/html"/>
    <updated>2021-10-03T09:34:52+02:00</updated>
    <id>https://nielsberglund.com/2021/10/03/interesting-stuff---week-40-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://azure.microsoft.com/en-us/blog/govern-your-data-wherever-it-resides-with-azure-purview/">Govern your data wherever it resides with Azure Purview</a>. This post looks at <strong>Azure Purview</strong>. Azure Purview is a unified data governance solution that helps you achieve a complete understanding of your data. This is regardless of whether it&rsquo;s housed on-premises in services like SQL Server and Oracle, in different clouds like Amazon Web Services (AWS) S3, or SaaS applications like Salesforce. This is something we dearly need at <a href="/derivco">Derivco</a>!</li>
</ul>

<h2 id="azure-data-explorer">Azure Data Explorer</h2>

<ul>
<li><a href="https://techcommunity.microsoft.com/t5/azure-data-explorer/monitor-batching-ingestion-with-adx-insights/ba-p/2673509">Monitor batching ingestion with ADX Insights</a>. <strong>ADX Insights</strong> is a system providing comprehensive monitoring of your ADX clusters. This post talks about the new Ingestion monitoring feature that allows you to monitor the status of batching ingestion operations to ADX. Very cool!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/serverless-kafka-streaming-with-confluent-cloud-and-aws-lambda/">Trigger AWS Lambda Functions Directly from an Apache Kafka Topic</a>. This post looks at how you can stream data from Confluent Cloud Kafka topics into Amazon DynamoDB tables by triggering an AWS Lambda function - providing a completely serverless architecture. I need to test this out on Azure using <strong>Azure Functions</strong>!</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 39, 2021]]></title>
    <link href="https://nielsberglund.com/2021/09/26/interesting-stuff---week-39-2021/" rel="alternate" type="text/html"/>
    <updated>2021-09-26T10:49:18+02:00</updated>
    <id>https://nielsberglund.com/2021/09/26/interesting-stuff---week-39-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://medium.com/event-driven-utopia/understanding-materialized-views-3-stream-table-joins-with-cdc-77591d2d6fa0">Understanding Materialized Views â€” 3 : Stream-Table Joins with CDC</a>. In a roundup a couple of weeks ago, I linked to a post about materialized views, and I wrote how I couldn&rsquo;t wait for a follow-up post. Well, here it is. In this post, the author looks at joining streams with lookup tables to create materialized views. Very cool!</li>
<li><a href="https://www.confluent.io/blog/apache-kafka-3-0-major-improvements-and-new-features/">What&rsquo;s New in Apache Kafka 3.0.0</a>. I guess the title says it all. Apache Kafka version 3.0 has just been released, and this blog post looks at some of the new features, fixes, and improvements.</li>
<li><a href="https://k6.io/blog/load-test-your-kafka-producers-and-consumers-using-k6/">How to Load Test Your Kafka Producers and Consumers using k6</a>. A couple of weeks ago, I came across <a href="https://k6.io/">k6</a>, a modern load testing framework for both developers as testers. I thought it would be cool if I somehow could load-test Kafka producers and consumers in the framework. Well, I can now do it, and the post I have linked to discusses the newly developed Kafka k6 extension: xk6-kafka. I cannot wait to put it through its paces.</li>
<li><a href="https://www.confluent.io/blog/announcing-ksqldb-0-21-new-features-major-upgrades/">Announcing ksqlDB 0.21.0</a>. Above I linked to the announcement of Kafka 3.0. This post discusses the new ksqlDB 0.21.0 release and looks at some of the new features.</li>
<li><a href="https://www.kai-waehner.de/blog/2021/09/23/real-time-kappa-architecture-mainstream-replacing-batch-lambda/">Kappa Architecture is Mainstream Replacing Lambda</a>. In this post, the author looks at the benefits the Kappa architecture provides over the Lambda architecture. One of the major, major benefits is a much simpler infrastructure.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>Right now, I am &ldquo;prepping&rdquo; for two conference talks this coming week:</p>

<ul>
<li><a href="https://datadrivencommunity.com/speaker-NielsBerglund-session.html"><strong>Analyze Billions of Rows of Data in Real-Time Using Azure Data Explorer</strong></a>: On Wednesday (Sept 29), I deliver this presentation which is an overview of <strong>Azure Data Explorer</strong>, and how it is ideal for near-real-time analytics of huge volumes of data.</li>
<li><a href="https://azurebootcamp.co.za/"><strong>Improve Customer Lifetime Value using Azure Databricks &amp; Delta Lake</strong></a>. Then on Thursday (Sept 30), I present how you can calculate and improve Customer Lifetime Value (CLV) using Azure Databricks.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 38, 2021]]></title>
    <link href="https://nielsberglund.com/2021/09/19/interesting-stuff---week-38-2021/" rel="alternate" type="text/html"/>
    <updated>2021-09-19T10:37:19+02:00</updated>
    <id>https://nielsberglund.com/2021/09/19/interesting-stuff---week-38-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://www.startree.ai/blogs/launching-at-linkedin-the-story-of-apache-pinot/">Launching at LinkedIn: The Story of Apache Pinot</a>. This is the story of how Apache Pinot started from a &ldquo;simple&rdquo; beginning at LinkedIn, how it grew over time to being adopted at Uber. Very interesting!</li>
<li><a href="https://databricks.com/blog/2021/09/17/timeliness-and-reliability-in-the-transmission-of-regulatory-reports.html">Timeliness and Reliability in the Transmission of Regulatory Reports</a>. Regulations impact more and more companies. Part of most regulations is the requirement to create reports for the regulators. God knows that we at <a href="/derivco">Derivco</a> feel the pain around this subject. The post linked to here demonstrates the benefits of the Databricks Lakehouse architecture in the ingestion, processing, validation and transmission of regulatory data.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/governing-data-with-confluent-stream-governance/">Confluent Unlocks the Full Power of Event Streams with Stream Governance</a>. Data governance has become a requirement for most organizations, and the organizations have adopted the governance tools needed to manage their data. However, most of the tools are built for data at rest. What about streaming data? In this blog post, Confluent announces the release of their Stream Governance Suite, a set of tools allowing you to govern your streaming data. This is something we have wished for here at <a href="/derivco">Derivco</a>!</li>
<li><a href="https://www.confluent.io/blog/kafka-summit-americas-2021-recap/">Kafka Summit Americas 2021 Recap</a>. This year&rsquo;s final Kafka Summit (Kafka Summit Americas) was a wrap earlier this week. The blog post lists some of the highlights of the summit. Have a look and see what catches your interest!</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 37, 2021]]></title>
    <link href="https://nielsberglund.com/2021/09/12/interesting-stuff---week-37-2021/" rel="alternate" type="text/html"/>
    <updated>2021-09-12T09:43:08+02:00</updated>
    <id>https://nielsberglund.com/2021/09/12/interesting-stuff---week-37-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="http://babkin-cep.blogspot.com/2021/09/the-practice-of-parallel-programming.htm">The Practice of Parallel Programming</a>. A link to a free downloadable pdf version of the book <strong>The Practice of Parallel Programming</strong>. The pdf provides an advanced guide to the issues of parallel and multithreaded programming. It goes beyond the high-level design of the applications into the details that are often overlooked but vital to make the programs work. It is an excellent read!</li>
</ul>

<h2 id="sql-server-2019-language-extensions">SQL Server 2019 Language Extensions</h2>

<ul>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2021/09/08/open-sourcing-the-net-5-c-language-extension-for-sql-server/">Open sourcing the .NET 5 C# Language Extension for SQL Server</a>. If you follow my blog, you may know that I have been writing quite a lot about the <strong>SQL Server Language Extensions</strong> and how extensions for Python, R, and Java exist and are open source. Well, the time has now come for C#. The post linked to announces the open-source release of SQL Server Language Extensions for C#! That.Is.So.Awesome! Expect some blog posts from yours truly looking at this.</li>
</ul>

<h2 id="azure-data-explorer">Azure Data Explorer</h2>

<ul>
<li><a href="https://youtu.be/lN3HkAZ3oGA">Azure Data Explorer ADX Overview</a>. In this YouTube video, the presenter dives into Azure Data Explorer â€“ from data ingestion to dashboards â€“ and looks at how Azure Data Explorer allows us to focus on discovering insights in the data while simplifying infrastructure and managing cost.</li>
<li><a href="/2021/09/06/run-self-managed-kusto-kafka-connector-serverless-in-azure-container-instances/">Run Self-Managed Kusto Kafka Connector Serverless in Azure Container Instances</a>. This blog post by me looks at how to run the Kafka Connector for Azure Data Explorer server-less in Azure. We look at creating a Docker image for the connector and deploying it to Azure Container Instances.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://medium.com/event-driven-utopia/understanding-materialized-views-part-2-ae957d40a403">Understanding Materialized Views â€” Part 2</a>. This post is the second in a series about <strong>Materialized Views</strong>. In this post, the author looks at stream processing and explores two essential concepts in stateful stream processing; streams and tables. Based on streams and tables, he looks at how streams turn into tables that make materialized views. He concludes the post by learning how these materialized views can be scaled and recovered from failures. The first part of the series is [here], and I - for one - cannot wait for the next instalment!</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Run Self-Managed Kusto Kafka Connector Serverless in Azure Container Instances]]></title>
    <link href="https://nielsberglund.com/2021/09/06/run-self-managed-kusto-kafka-connector-serverless-in-azure-container-instances/" rel="alternate" type="text/html"/>
    <updated>2021-09-06T06:11:51+02:00</updated>
    <id>https://nielsberglund.com/2021/09/06/run-self-managed-kusto-kafka-connector-serverless-in-azure-container-instances/</id>
    <content type="html"><![CDATA[<p>A couple of weeks ago, I <a href="/2021/08/14/run-confluent-cloud--serverless-apache-kafka-on-azure/">posted</a> how I set up Kafka to run serverless in Azure by deploying Confluent Cloud.</p>

<p>If you have followed my blog lately, you have probably seen that I am interested in <strong>Azure Data Explorer</strong> and that I have a couple of conference talks coming. One being:</p>

<ul>
<li><strong>How to do Real-Time Analytics Using Apache Kafka and Azure Data Explorer</strong>: We are looking at how to stream events from Apache Kafka to Azure Data Explorer (ADX) and perform user-facing analytics in near real-time.</li>
</ul>

<p>The question is how to connect Kafka with ADX? You normally connect Kafka with another system using a Kafka Connect connector, and fortunately a connector exists for connecting Kafka with ADX: the <a href="https://github.com/Azure/kafka-sink-azure-kusto"><strong>Kafka Connect Kusto Sink Connector</strong></a>.</p>

<p>However, since I am running managed Kafka (Confluent Cloud, remember), I need a managed connector to run it in Confluent Cloud&rsquo;s Kafka Connect. In the previous paragraph, I mentioned I was fortunate as we had a Kafka connector for ADX. Unfortunately, it is not a managed connector, so I cannot run it in Confluent Cloud - bummer!</p>

<p>So, this post looks at the various options we have if we want to use the Kafka Connect Kusto Sink Connector connecting Confluent Cloud in Azure with Azure Data Explorer. However, if you are not interested in neither Kafka nor ADX, the post may still be of use for you. The reason being it also covers running Docker images in Azure Container Instances (ACI).</p>

<p></p>

<blockquote>
<p><strong>NOTE:</strong> Even though this post comes out of me wanting to use the Kafka Connect Kusto Sink Connector, the post is <strong>NOT</strong> about the usage or the configuration of the connector. That is covered in a future post.</p>
</blockquote>

<h2 id="credits">Credits</h2>

<p>Usually, the credits &ldquo;roll&rdquo; at the end of the <del>movie</del> post, but I feel I should start with the credits as this post would not have happened if it wasn&rsquo;t for this blog post:</p>

<ul>
<li><a href="https://rmoff.net/2021/01/11/running-a-self-managed-kafka-connect-worker-for-confluent-cloud/">Running a self-managed Kafka Connect worker for Confluent Cloud</a>. I came across this post by Kafka guru extraordinaire <a href="https://twitter.com/rmoff">Robin Moffat</a> when I looked into running a self-managed connector when you use Confluent Cloud. His post made me look into what it would take to run the connector in Azure Container Instances (ACI).</li>
</ul>

<p>As I said, <a href="https://twitter.com/rmoff">Robin Moffat</a> is a Kafka Guru, and if you are into Kafka, then you <strong>MUST</strong> read his <a href="https://rmoff.net/">blog</a>.</p>

<h2 id="pre-reqs">Pre-Reqs</h2>

<p>This section is here if you want to follow along. I am listing what you need if you&rsquo;re going to deploy and run a container in ACI; not all required components for Kafka and ADX:</p>

<ul>
<li>An Azure account. If you don&rsquo;t have an Azure subscription, sign up for a <a href="https://azure.microsoft.com/en-us/free/">free account</a>.</li>
<li>Azure CLI. The Azure CLI is a cross-platform command-line tool to connect to Azure and execute administrative commands on Azure resources. You find downloads and install instructions <a href="https://docs.microsoft.com/en-us/cli/azure/install-azure-cli">here</a>.</li>
<li>Docker Desktop: We will build a Docker image, so we need Docker Desktop.</li>
<li>Something to build the image from. The image I build for this post includes the Kusto Sink Connector, which I download from <a href="https://github.com/Azure/kafka-sink-azure-kusto/releases/download/v2.0.0/kafka-sink-azure-kusto-2.0.0-jar-with-dependencies.jar">here</a>.</li>
</ul>

<p>The version (2.0) of the Kusto Sink Connector I downloaded is not the latest you find on the <a href="https://github.com/Azure/kafka-sink-azure-kusto/releases">release page</a>, but I could not get the 2.1 version to work.</p>

<p>In addition to the above, I have Confluent Cloud deployed as per my <a href="/2021/08/14/run-confluent-cloud--serverless-apache-kafka-on-azure/">post</a> mentioned in the beginning. As I want to connect Kafka to Azure Data Explorer, I also have Azure Data Explorer installed.</p>

<h2 id="background">Background</h2>

<p>Before we get down into it, let us get an understanding of some of the components in this post:</p>

<ul>
<li>Azure Data Explorer</li>
<li>Confluent Cloud</li>
<li>Kafka Connect</li>
<li>Kafka Connect Kusto Sink Connector</li>
</ul>

<h4 id="azure-data-explorer">Azure Data Explorer</h4>

<p>Azure Data Explorer is a fully-managed big data analytics cloud platform and data-exploration service that ingests structured, semi-structured (like JSON) and unstructured data. The service then stores this data and answers analytic ad hoc queries on it with seconds of latency. It is a full-text indexing and retrieval database, including time series analysis capabilities, machine learning, regular expression evaluation, and text parsing.</p>

<p>It is ideal for analyzing large volumes of diverse data from any data source, such as websites, applications, IoT devices, and more. Essentially it is a distributed database running on a cluster of compute nodes in Azure.</p>

<h4 id="confluent-cloud">Confluent Cloud</h4>

<p>In my <a href="/2021/08/14/run-confluent-cloud--serverless-apache-kafka-on-azure/">post</a> setting up Confluent Cloud I said it is a resilient, scalable streaming data service based on Apache Kafka, delivered as a fully managed service. It is Confluent Platform, running as a managed service in the cloud, and you can run it on Azure, AWS, and Google Cloud.</p>

<p>As it is Confluent Platform, you get so much more than <em>just</em> Kafka. You get built-in stream processing through ksqlDB, schema registry for data integrity, managed Kafka Connect connectors for data sources/sinks, and more.</p>

<h4 id="kafka-connect">Kafka Connect</h4>

<p>Kafka Connect is a tool for streaming data between Apache Kafka and other systems scalable and reliable. The way you move data between systems and Kafka is using connectors, and there are two flavors of connectors:</p>

<ul>
<li>Source connectors that understand how to interact with the source system send records into Kafka</li>
<li>Sink connectors that propagate records from Kafka topics to other systems.</li>
</ul>

<p>Kafka Connect is a JVM process, and it operates separately from the Kafka Broker. The process loads connectors, and the connectors know how to interact with Kafka and the source/sink systems. The connectors are written in Java and packaged into <code>.jar</code> files.</p>

<h4 id="kafka-connect-kusto-sink-connector">Kafka Connect Kusto Sink Connector</h4>

<p>The Kusto Sink Connector is a Kafka Connect connector. It is - as the name implies - a sink connector, dequeuing events from Kafka topics and ingesting them into Azure Data Explorer. The ingestion is, for now, queued ingestion leveraging the Azure Data Explorer Java SDK, i.e. batch mode.</p>

<p>Since the Kusto connector is not a managed connector, we need to decide where and how to run it.</p>

<h2 id="options">Options</h2>

<p>Robin covered the various options in his <a href="https://rmoff.net/2021/01/11/running-a-self-managed-kafka-connect-worker-for-confluent-cloud/">post</a> a lot better than I ever could, so I won&rsquo;t repeat that. Furthermore, seeing that I say in the title of this post <strong>Azure Container Instances</strong> (ACI), it is probably safe to assume that&rsquo;s the option we&rsquo;ll go with. As a picture says more than a thousand words, our solution looks something like so:</p>

<p><img src="/images/posts/kusto-aci-conn.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Kafka - ADX Architecture</em></p>

<p>We see in <em>Figure 1</em> how we have:</p>

<ul>
<li>Kafka in Azure (Confluent Cloud).</li>
<li>an Azure Container Instances running the Kusto Sink Connector.</li>
<li>data ingested from the connector into Azure Data Explorer.</li>
</ul>

<p>Oh, and yes - the data being ingested is published to Kafka from the publisher we see in the upper left-hand corner.</p>

<p>In this post, the term Azure Container Instances has been mentioned a couple of times. What is that, then?</p>

<h4 id="azure-container-instances">Azure Container Instances</h4>

<p>ACI gives you an easy way to run containers in the Azure cloud, eliminating the need to manage virtual machines (VMs) or using more complex container orchestration services, like Kubernetes. For me, just testing this out, ACI is &ldquo;good enough&rdquo;.</p>

<p>There are a couple of ways you can deploy and run a container in ACI:</p>

<ul>
<li>Utilise the integration between Docker and Azure and execute <code>docker run</code>. You do this mostly in test scenarios.</li>
<li>Deploy the container image to ACI and run it in ACI.</li>
</ul>

<p>Even though I said I am using ACI for testing at the beginning of this section, I will not use <code>docker run</code> but instead do a &ldquo;proper&rdquo; deployment to ACI.</p>

<h2 id="create-kusto-sink-connector-image">Create Kusto Sink Connector Image</h2>

<p>We start with creating the <code>Dockerfile</code> for the image we want to deploy to ACI:</p>

<p><img src="/images/posts/kusto-aci-dockerfile.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Dockerfile</em></p>

<p>As we need to run the connector in Kafka Connect, we base the image on the <code>cp-server-connect-base</code> image (the <code>FROM</code> statement). This image contains the bare minimum for Kafka Connect.</p>

<p>In the Pre-Reqs section, I mentioned how I downloaded the Kusto connector. I downloaded it as a zip file and unzipped it to the same directory the <code>Dockerfile</code> is in. In line 3, we see how I copy the connector&rsquo;s <code>.jar</code> file to <code>/usr/share/java</code> in the base image. That path is a &ldquo;well known&rdquo; path to load connectors from.</p>

<p>The <code>...OVERRIDE_POLICY=All</code> statement on line 5 allows this connector to override consumer and producer properties to not impact all connectors running in that worker.</p>

<p>Lines 7 - 10 in the docker file are core config security settings that need to be set at the Kafka connect worker level and need to be &ldquo;baked&rdquo; into the Docker image. These settings are related to authentication and authorization against the Confluent Cloud Kafka.</p>

<h4 id="build-the-image">Build the Image</h4>

<p>Having created the <code>Dockerfile</code>, we can build the image:</p>

<p><img src="/images/posts/kusto-aci-docker-build.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Docker Build</em></p>

<p>In <em>Figure 3</em>, we see us executing the <code>docker build</code> command and the outcome. When the build has finished, we can check that all looks OK by running <code>docker images</code>:</p>

<p><img src="/images/posts/kusto-aci-kusto-image.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Kusto Docker Image</em></p>

<p>Success, we have an image, as we see in <em>Figure 4</em>. Well, at least partial success; we don&rsquo;t know if it works yet. Let us find out.</p>

<h4 id="run-locally">Run Locally</h4>

<p>To find out if it works, we can run the container image locally using <code>docker-compose</code>. We create a <code>docker-compose.yml</code> file containing the bare minimum for running the connector:</p>

<p><img src="/images/posts/kusto-aci-docker-compose.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Docker Compose File</em></p>

<p>We see in <em>Figure 5</em> the compose file I use to test that my container works. As mentioned before, the file contains the required properties to get this connector up and running. What you see outlined in red are properties naming topics needed to store Kafka offsets, configs and statuses. After we have &ldquo;spun up&rdquo; the container, we can check for these topics in our Kafka installation. Let us execute `docker-compose up -d&rsquo;:</p>

<p><img src="/images/posts/kusto-aci-docker-compose-up.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>Run Docker Compose</em></p>

<p>In <em>Figure 6</em>, we see that it looks like everything has worked OK and that we have created a connector instance <code>kusto-conn-1</code>. We can confirm that it has worked by checking the topics mentioned above or execute a REST call against the Kafka Connect API to <code>GET</code> the available connectors:</p>

<p><img src="/images/posts/kusto-aci-get-connectors.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>GET Connectors</em></p>

<p>From what we see in <em>Figure 7</em> it looks like we are OK. We see, outlined in blue, the <code>GET</code> call on port <code>8083</code>, and in the result below, we see the connector outlined in red.</p>

<h2 id="azure-container-instances-1">Azure Container Instances</h2>

<p>When we have confirmed that our image works, it is time to deploy it to ACI. When we run a container in ACI, the container is stored in <a href="https://azure.microsoft.com/en-us/services/container-registry/#overview">Azure Container Registry</a> (ACR).</p>

<p>We&rsquo;ll create a new container registry in a second, but before we do that, let us log in to Azure and set the subscription to use. To log in, we run <code>az login</code>. The command may take a second or two, and a dialog in your browser may ask you for login credentials. When login is done, you will see a JSON blob with information about the subscriptions you have access to. Choose the one you want to use:</p>

<pre><code class="language-bash"># az login
# az login above returns a JSON blob with subscriptions.
# set the subscription you want to use
az account set --subscription 00000000-0000-0000-0000-000000000000 
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Set Subscription</em></p>

<p>After we have logged in, we execute the code in <em>Code Snippet 1</em> to set the subscription we want to use. And no, the subscription Id in <em>Code Snippet 1</em> is not mine.</p>

<p>We are almost at the stage where we can create the ACR, but we need one more thing before creating the ACR. That one more thing is a resource group. I will use an existing resource group for this post, so I will not create a new one. If you need to create a resource group, you do:</p>

<pre><code class="language-bash">az group create --name name-of-rg --location azure-location  
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Create Resource Group</em></p>

<p>To see what locations are available for the <code>--location</code> parameter in <em>Code Snippet 2</em>, you can execute: <code>az account list-locations</code>.</p>

<p>Right, we now have a resource group. Let us press on.</p>

<h4 id="create-azure-container-registry">Create Azure Container Registry</h4>

<p>ACR is a private Docker registry service, similar to Dockerhub. As with Dockerhub, you push container images to your container registry.</p>

<p>To create a container registry, we use <code>az acr create ...</code>:</p>

<pre><code class="language-bash">az acr create --resource-group rg-kafka \
              --name nielsblog1 \
              --sku Basic \
              --admin-enabled true \
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Create Container Registry</em></p>

<p>In <em>Code Snippet 3</em>, we see how we create a container registry. The first two parameters define the resource group to create it in: <code>rg-kafka</code>, and the name of the registry: <code>nielsblog1</code>. You may ask what the last two parameters are:</p>

<ul>
<li><code>--sku</code>: this parameter defines the service tier: <code>Basic</code>, <code>Standard</code>, or <code>Premium</code>. Read more about service tiers <a href="https://docs.microsoft.com/en-us/azure/container-registry/container-registry-skus">here</a>.</li>
<li><code>--admin-enabled</code>: an admin user account is included when creating a container registry. The account is disabled by default. For testing purposes, you may want to have it enabled, so that is why I have included it in the creation. Read more about the admin account <a href="https://docs.microsoft.com/en-us/azure/container-registry/container-registry-authentication?tabs=azure-cli#admin-account">here</a>.<br /></li>
</ul>

<p>When you execute the code in <em>Code Snippet 3</em>, it will take a little while. When it finishes, you will see a JSON blob with some information about the created registry.</p>

<p>Having created the registry, we can now log in to it: <code>az acr login --name regname</code>. We are almost ready to push our image to the registry, but there are two things we need to do before that.</p>

<p><strong>Login Server</strong></p>

<p>When we push an image to the registry, we need an address to push to; the login server. To retrieve the login server, we do:</p>

<pre><code class="language-bash">az acr show --name registryname --query loginServer
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Get Login Server</em></p>

<p>Most likely, when you execute the code in <em>Code Snippet 4</em>, you get back <code>your-registry-name.azurecr.io</code>, but it is good practice to explicitly retrieve the login server.</p>

<p><strong>Credentials</strong></p>

<p>The second thing we need to do is get the credentials for the admin user we enabled in <em>Code Snippet 3</em>. We use the credentials later when we deploy our container:</p>

<p><img src="/images/posts/kusto-aci-acr-credentials.png" alt="" /></p>

<p><strong>Figure 8:</strong> <em>ACR Credentials</em></p>

<p>In <em>Figure 8</em>, we see outlined in yellow:ish the code to retrieve the credentials. Outlined in red, we see the two passwords. These passwords are created when the admin account is enabled, and they can also be re-generated. Finally, outlined in blue is the user name to use for the admin account.</p>

<h4 id="push-image-to-acr">Push Image to ACR</h4>

<p>We are getting there. Now, the time has come to push the image we built in <em>Figure 3</em> to the ACR. We do it in a two-step process:</p>

<p><strong>Tag the Image</strong></p>

<p>Tag the image with the login server string:</p>

<p><img src="/images/posts/kusto-aci-docker-tag.png" alt="" /></p>

<p><strong>Figure 9:</strong> <em>Tag the Image</em></p>

<p>We see in <em>Figure 9</em> outlined in:</p>

<ul>
<li>Blue: the <code>docker tag</code> statement we use.</li>
<li>Yellow: the name of the image we want to tag.</li>
<li>Red: the &ldquo;tagged&rdquo; new name of the image.</li>
</ul>

<p><strong>Push the Image</strong></p>

<p>Having tagged the image with the login server, we can push it to ACR:</p>

<p><img src="/images/posts/kusto-aci-docker-push-acr.png" alt="" /></p>

<p><strong>Figure 10:</strong> <em>Docker Push to ACR</em></p>

<p>When you run the code outlined in blue in <em>Figure 10</em>, you see how the image layers are pushed to the registry.</p>

<p>Looking at <em>Figure 10</em> everything looks OK, but - look at the statement outlined in red. What is this &ldquo;repository&rdquo; thing?</p>

<p>It turns out that when we push an image to the ACR, we push it not directly into the ACR. Instead, a repository is created, and we push it into that repository. A repository is a collection of container images or other artefacts in a registry with the same name but different tags.</p>

<p>That explains why we, when looking for images in the ACR we do something like so:</p>

<p><img src="/images/posts/kusto-aci-acr-repo-images.png" alt="" /></p>

<p><strong>Figure 11:</strong> <em>ACR Repository with Image(s)</em></p>

<p>In <em>Figure 11</em>, we see how we execute <code>az acr repository list ...</code> when looking for images (outlined in yellow) and how the result comes back as an array (outlined in red).</p>

<h2 id="deploy-the-container">Deploy the Container</h2>

<p>It is now time to deploy and run the image in Azure Container Instances. To create the container, we use the <code>az container create ...</code> command:</p>

<pre><code class="language-bash">az container create --resource-group rg-kafka `
&gt;&gt; --name nielsblog1 `
&gt;&gt; --image nielsblog1.azurecr.io/kusto-conn-1:latest `
&gt;&gt; --restart-policy OnFailure `
&gt;&gt; --ip-address Public `
&gt;&gt; --ports 8083 `
&gt;&gt; --registry-login-server nielsblog1.azurecr.io `
&gt;&gt; --registry-username nielsblog1 `
&gt;&gt; --registry-password some-super-secret-password
</code></pre>

<p><strong>Code Snippet 5:</strong> <em>Create Container - I</em></p>

<p>The code in <em>Code Snippet 5</em> is part of the code we need to run to create the container. The parameters we see are straightforward. The only thing worth mentioning is the <code>--ip-address Public</code> and <code>--ports 8083</code>. We need to indicate that we need a public IP address and that port 8083 should be open.</p>

<p>In the previous paragraph, I mentioned that the code in <em>Code Snippet 5</em> is only part of what we need to run. So what else do we need? Remember what we did when we tested the container locally, how we had a <code>.yml</code> file (<em>Figure 5</em>), with properties required to run Kafka Connect? We need the same here!</p>

<p>The question is, how do we supply those properties? The answer is that <code>az container create</code> has an <code>--environment-variables</code> parameter. This parameter is a list of environment variables for the container, where the list contains space-separated values in &lsquo;key=value&rsquo; format, something like so:</p>

<p><img src="/images/posts/kusto-aci-create-container-I.png" alt="" /></p>

<p><strong>Figure 12:</strong> <em>Create Container - II</em></p>

<p>In <em>Figure 12</em>, we see the entire command, including the required properties for Kafka.</p>

<blockquote>
<p><strong>NOTE:</strong> Notice how the <code>--environment-variables</code> do not have an quotes around them. That is because I am running from PowerShell. If you run the command from command prompt you need the environment variables be enclosed in single quotes.</p>
</blockquote>

<p>The text outlined in red at the bottom of <em>Figure 12</em> shows that the screenshot is taken while the command executes. While the command is running, you can execute the following to see the state it is in:</p>

<pre><code class="language-bash"> az container show --resource-group rg-kafka `
 &gt;&gt;--name nielsblog1 --query instanceView.state
</code></pre>

<p><strong>Code Snippet 6:</strong> <em>View Container State</em></p>

<p>Eventually, the creation finishes and results in a JSON blog with some information about the container. Most of the information in the blob is available from <code>az container ...</code> commands:</p>

<pre><code class="language-bash"># get log information from the container
az container logs --resource-group rg-kafka --name nielsblog1

# get ip address information
az container show --resource-group  rg-kafka `
&gt;&gt;--name nielsblog1 --query ipAddress
</code></pre>

<p><strong>Code Snippet 7:</strong> <em>Get Logs &amp; IP Address</em></p>

<p>The code in <em>Code Snippet 7</em> retrieves log information from the running container and the public IP address. Looking at the logs, everything looks OK, so let us use the IP address we retrieved in <em>Code Snippet 7</em> and do what we did in <em>Figure 7</em> (but now against the container in Azure):</p>

<p><img src="/images/posts/kusto-aci-get-connectors-az.png" alt="" /></p>

<p><strong>Figure 13:</strong> <em>Get Connectors - II</em></p>

<p>Yay, from what we see in <em>Figure 13</em> all is OK. We see the Kusto Sink Connector, outlined in red, being part of the returned result from the <code>GET</code> call. And in the <code>GET</code> call outlined in blue, we see we use the Azure IP address (highlighted in yellow). Well, you don&rsquo;t necessarily know it is the Azure IP, but trust me - it is. Yay, again!</p>

<h2 id="summary">Summary</h2>

<p>Wow, that was a lot! In this post, we saw how to:</p>

<ul>
<li>Build a docker image for the Kusto Kafka Sink Connector.</li>
<li>Test it locally.</li>
<li>Create an Azure Container Registry.</li>
<li>Push the image to the registry.</li>
<li>Deploy the image to, and run it in Azure Container Instances.</li>
</ul>

<p>Look out for a post covering how to configure and use the Kusto Sink Connector.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 36, 2021]]></title>
    <link href="https://nielsberglund.com/2021/09/05/interesting-stuff---week-36-2021/" rel="alternate" type="text/html"/>
    <updated>2021-09-05T10:38:44+02:00</updated>
    <id>https://nielsberglund.com/2021/09/05/interesting-stuff---week-36-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="misc">Misc.</h2>

<ul>
<li><a href="https://www.hanselman.com/blog/my-ultimate-powershell-prompt-with-oh-my-posh-and-the-windows-terminal">My Ultimate PowerShell prompt with Oh My Posh and the Windows Terminal</a>. I certainly hope that <a href="https://www.hanselman.com/">Scott Hanselman</a> doesn&rsquo;t need an introduction, but if you haven&rsquo;t heard of him, <a href="https://www.hanselman.com/blog">here</a> is the link to his blog. Anyway, he has blogged a bit about setting up the Windows terminal, so it looks cool. The post I link to here is the latest and greatest in setting up your terminal.</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://ably.com/blog/8-fallacies-of-distributed-computing">Navigating the 8 fallacies of distributed computing</a>. This post reviews the eight fallacies of distributed computing and provides several hints at how to handle them. I think I&rsquo;ll, in my next job interview, ask some questions about the eight fallacies!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://eng.uber.com/kafka-async-queuing-with-consumer-proxy/">Enabling Seamless Kafka Async Queuing with Consumer Proxy</a>. This post from Uber discusses how Kafka being a stream-oriented system, where message order is assumed in the system&rsquo;s design, can hinder certain message delivery patterns. The post talks about how more than 300 microservices at Uber are leveraging Kafka for pub-sub message queueing between microservices and how Uber developed the Consumer Proxy to work around some of the drawbacks with Kafka&rsquo;s message order oriented design.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 35, 2021]]></title>
    <link href="https://nielsberglund.com/2021/08/29/interesting-stuff---week-35-2021/" rel="alternate" type="text/html"/>
    <updated>2021-08-29T13:37:54+02:00</updated>
    <id>https://nielsberglund.com/2021/08/29/interesting-stuff---week-35-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="big-data-machine-learning">Big Data / Machine Learning</h2>

<ul>
<li><a href="https://rudderstack.com/blog/churn-prediction-with-bigqueryml">Churn Prediction With BigQueryML to Increase Mobile Game Revenue</a>. Seeing what we do at <a href="/derivco">Derivco</a>, this post is exciting. The post looks at how machine learning can identify high-value mobile game players dangerously close to churning. Very interesting!</li>
</ul>

<h2 id="data-architecture">Data Architecture</h2>

<ul>
<li><a href="https://medium.com/@jordan_volz/five-predictions-for-the-future-of-the-modern-data-stack-435b4e911413">Five Predictions for the Future of the Modern Data Stack</a>. This post looks at the developments of the modern data stack and the bright side of &ldquo;Modern Data Stack V2&rdquo;, focusing on AI, Data Sharing, Data Governance, Streaming &amp; Application Serving.</li>
</ul>

<h2 id="azure-data-explorer">Azure Data Explorer</h2>

<ul>
<li><a href="https://mrfoxsql.wordpress.com/2021/08/25/timeseries-analytics-capabilities-and-azure-data-explorer-adx/">Timeseries Analytics Capabilities, and Azure Data Explorer (ADX)</a>. I guess that for you who read my blog, it doesn&rsquo;t come as a surprise that I have a thing for Azure Data Explorer. The post here looks at time-series analytics and explores the types of core functionality typical for time-series data processing applications. It further looks at how functionality built into ADX aligns exceptionally well to meet these challenges head-on.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://towardsdatascience.com/real-time-anomaly-detection-with-apache-kafka-and-python-3a40281c01c9">Real-time anomaly detection with Apache Kafka and Python</a>. In this post, the author looks at making real-time anomaly predictions over streaming data coming from Kafka using Python.</li>
<li><a href="https://www.confluent.io/blog/ksqldb-architecture-and-advanced-features/">How ksqlDB Works: Internal Architecture and Advanced Features</a>. To effectively use ksqlDB, you should, apart from being familiar with its features and syntax, also have an understanding of what&rsquo;s happening &ldquo;under the cover&rdquo; of ksqlDB. This post covers some of the &ldquo;under the cover&rdquo; topics as well as points to resources at <a href="https://developer.confluent.io/learn-kafka/inside-ksqldb/streaming-architecture/">Confluent Developer</a>.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>By now, you probably know that I:</p>

<p><img src="/images/posts/Neils_Berglund_Breakout_Session.jpg" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Breakout Session</em></p>

<p>Yes, as we see in <em>Figure 1</em> I am presenting at the <a href="https://dataplatformgeeks.com/dps2021/"><strong>2021 Data Platform Summit</strong></a>:</p>

<ul>
<li><a href="https://dataplatformgeeks.com/dps2021/sessions-agenda-schedule/"><strong>How to do Real-Time Analytics Using Apache Kafka and Azure Data Explorer</strong></a>. We are looking at how to stream events from Apache Kafka to Azure Data Explorer and perform user-facing analytics in near real-time.</li>
</ul>

<p>I mentioned in a previous roundup how the organizers have managed to increase the capacity of the virtual platform to 10,000! So, they have opened up <strong>FREE</strong> booking for <strong>LIVE</strong> attendance for a limited time. They have an internal quota, and once that is full, the free booking will close.  So, what are you waiting for? Hurry up to <a href="https://dataplatformgeeks.com/dps2021/complimentary-registration"><strong>register for FREE</strong></a>!</p>

<p>Oh, I am not only doing the conference session above, but also a post-conference training class; 4 hours per day over 2 days:</p>

<ul>
<li><a href="https://dataplatformgeeks.com/dps2021/product/big-data-analytics-with-sql-server-2019-big-data-cluster-by-niels-berglund/"><strong>Big Data &amp; Analytics with SQL Server 2019 Big Data Cluster</strong></a>.</li>
</ul>

<p>There are still a couple of seats (virtual) available for my class, so - if you are interested - register <a href="https://dataplatformgeeks.com/dps2021/product/big-data-analytics-with-sql-server-2019-big-data-cluster-by-niels-berglund/">here</a>.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 34, 2021]]></title>
    <link href="https://nielsberglund.com/2021/08/22/interesting-stuff---week-34-2021/" rel="alternate" type="text/html"/>
    <updated>2021-08-22T08:35:37+02:00</updated>
    <id>https://nielsberglund.com/2021/08/22/interesting-stuff---week-34-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="misc">Misc.</h2>

<ul>
<li><a href="https://headleysj.medium.com/having-testers-made-my-team-worse-99d4cb9866aa">Having testers made my team worse</a>. This post, from a colleague and mate of mine, has caused a bit of stir here at <a href="/derivco">Derivco</a>. In the post, Simon talks about how his team lost all of their functional testers and how due to this, the developers had to sort out their CI/CD pipelines and write meaningful tests. This lead them to be in a much better position at the end of the day than before. I mentioned how the post had caused a stir; the stir was from the testers in the company. After reading the post, I thought it would be the developers wanting to &ldquo;lynch&rdquo; Simon. Primarily due to this: <em>but because having testers meant that as developers, we could get away with being lazy and not truly putting in the effort to write meaningful tests that run both in our CI and CD pipelines</em>. Anyway, it is an excellent post - I suggest you read it and think about how you can approve your pipelines and testing while reading it.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/ksqldb-2-0-introduces-date-and-time-data-types/">Announcing ksqlDB 0.20.0</a>. As the title says; ksqlDB version 0.20 is out &ldquo;in the wild&rdquo;. One big new feature of this version is support for <code>DATE</code> and <code>TIME</code> datatypes! Very cool!!</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>In last week&rsquo;s roundup, I mentioned how one of the webinars I presented a week or two back had still not come up on YouTube. Well, one or two days after I published the post, there it was:</p>

<ul>
<li><a href="https://youtu.be/DdyZgFErLFI"><strong>Let SQL Server Be the Central Hub For All Types of Data</strong></a>. In this webinar, I look at <strong>SQL Server 2019 Big Data Cluster</strong> and how Microsoft positions it to be a central hub for all types of data - not only relational data.</li>
</ul>

<p>Oh, don&rsquo;t forget the register for <a href="https://azurebootcamp.co.za/"><strong>Azure Bootcamp 2021 South Africa</strong></a>. It will be a fantastic event, and I have just submitted some talks to it. Hopefully, one or two will be accepted.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
</feed>

