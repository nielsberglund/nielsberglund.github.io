<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Niels Berglund</title>
  <link href="https://nielsberglund.com/atom.xml" rel="self"/>
  <link href="https://nielsberglund.com"/>
  <updated>2021-11-14T08:50:09+02:00</updated>
  <id>https://nielsberglund.com/</id>
  <generator uri="http://gohugo.io/">Hugo</generator>

  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 46, 2021]]></title>
    <link href="https://nielsberglund.com/2021/11/14/interesting-stuff---week-46-2021/" rel="alternate" type="text/html"/>
    <updated>2021-11-14T08:50:09+02:00</updated>
    <id>https://nielsberglund.com/2021/11/14/interesting-stuff---week-46-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://azure.microsoft.com/en-us/services/chaos-studio/">Azure Chaos Studio</a>. At <a href="https://myignite.microsoft.com/home">Microsoft Ignite</a>, a week or two ago, Microsoft announced the public preview of <strong>Azure Chaos Studio</strong>. Azure Chaos Studio is a fully-managed experimentation service to help customers track, measure, and mitigate faults with controlled chaos engineering to improve the resilience of their cloud applications. This looks very interesting, and we will definitely have a look at it.</li>
</ul>

<h2 id="azure-data-explorer">Azure Data Explorer</h2>

<ul>
<li><a href="https://docs.microsoft.com/en-us/azure/architecture/example-scenario/security/security-log-retention-azure-data-explorer">Long-term security log retention with Azure Data Explorer</a>. Having access to long-term security logs is essential. Querying long-term logs is critical for identifying the impact of threats and investigating illicit access attempts. This post outlines a solution for long-term retention of security logs where Azure Data Explorer is at the core of the architecture.</li>
<li><a href="https://techcommunity.microsoft.com/t5/azure-data-explorer/query-past-data-with-hot-windows/ba-p/2848114">Query past data with hot windows</a>. Azure Data Explorer has the notion of hot and cold data. Hot data is stored on SSD&rsquo;s on cluster nodes, whereas cold data is stored in Azure Blob Storage. Hot data offers the best query performance: an order of magnitude more performant than cold data. Sometimes you may want to query the hot data together with some of the cold. This post looks recently added functionality to Azure Data Explorer, creating a time window in the past which we want to be part of the hot data: Hot Window.<br /></li>
<li><a href="https://techcommunity.microsoft.com/t5/azure-data-explorer/train-your-model-on-spark-databricks-score-it-on-adx/ba-p/2098522">Train your Model on Spark/Databricks, score it on ADX</a>. Recently, I have been doing conference talks around Azure Databricks and Apache Spark and Azure Data Explorer. How cool would it be if you could combine the two?! The post linked to does just that. It looks at training and creating Machine Learning models using Azure Databricks and Spark and then using those models from Azure Data Explorer. Very cool! Oh, BTW - with Azure Data Explorer Pool&rsquo;s being made available in Azure Synapse, you no longer need Azure Databricks. You can do the same thing with Azure Synapse Analytics. The <a href="https://techcommunity.microsoft.com/t5/azure-data-explorer/query-past-data-with-hot-windows/ba-p/2848114"><strong>Azure Synapse Analytics - Operationalize your Spark ML model into Data Explorer pool for scoring</strong></a> post looks at that.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/introducing-confluent-platform-7-0/">Building Real-Time Hybrid Architectures with Cluster Linking and Confluent Platform 7.0</a>. Confluent recently released <a href="https://www.confluent.io/product/confluent-platform/"><strong>Confluent Platform 7.0</strong></a>, and this post looks at one of the new features in detail, the ability to directly connect clusters and mirror topics from one cluster to another: <a href="https://docs.confluent.io/platform/current/multi-dc-deployments/cluster-linking/index.html"><strong>Cluster Linking</strong></a>. This is something that we at <a href="/derivco">Derivco</a> are really interested in.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 45, 2021]]></title>
    <link href="https://nielsberglund.com/2021/11/07/interesting-stuff---week-45-2021/" rel="alternate" type="text/html"/>
    <updated>2021-11-07T08:13:38+02:00</updated>
    <id>https://nielsberglund.com/2021/11/07/interesting-stuff---week-45-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="sql-server">SQL Server</h2>

<ul>
<li><a href="https://techcommunity.microsoft.com/t5/sql-server/announcing-the-sql-server-2022-early-adoption-program/ba-p/2910617">Announcing the SQL Server 2022 Early Adoption Program</a>. The <a href="https://myignite.microsoft.com/home">Microsoft Ignite</a> conference was held during the week just gone by. As expected, there were quite a few announcements around new and improved products. One such announcement was related to this post; the next version of SQL Server is in the works, and Microsoft has just opened the <strong>Early Adoption Program</strong> (EAP) for <strong>SQL Server 2022</strong>. If you are interested in shaping the next version of SQL Server, I suggest you sign up!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://medium.com/event-driven-utopia/architecting-a-kafka-centric-retail-analytics-platform-part-2-137bfac04001">Architecting a Kafka-centric Retail Analytics Platform â€” Part 2</a>. In <a href="/2021/10/31/interesting-stuff---week-44-2021/">last weeks roundup</a>, I linked to the first post in a series, of which this post is the second instalment. In this post, the author, <a href="https://twitter.com/dunithd">Dunith Dhanushka</a>, looks at data ingestion into Kafka in detail. As the series is about retail, the post looks at the retail data landscape, what data to capture, and how it can be ingested into Kafka using the Kafka ecosystem.</li>
<li><a href="https://www.confluent.io/blog/streaming-databases-vs-traditional-databases/">Readings in Streaming Database Systems</a>. This post is the introduction/prequel to a series looking at streaming databases: <strong>The Streaming Database Series</strong>. This post gives a high level overview of what&rsquo;s coming in the posts in the series.  It also provides an overview of streaming databases.</li>
<li><a href="https://www.confluent.io/blog/databases-meet-stream-processing-the-future-of-sql/">The Future of SQL: Databases Meet Stream Processing</a>. This post is the first in the <strong>The Streaming Database Series</strong> mentioned above. The post discusses why the database world needs enhancements to handle data both at rest and in transit. The enhancements looked at are the <code>STREAM</code> abstraction, new query types, and extended semantics for handling time.</li>
<li><a href="https://www.confluent.io/blog/streaming-database-design-principles-and-guarantees/">4 Key Design Principles and Guarantees of Streaming Databases</a>. The second in the series mentioned above, this post summarizes a few challenging design principles for modern streaming databases that act as a source of truth for stream data management and query processing systems. The post also presents ksqlDB&rsquo;s persistent log-based approach to following the design principles.</li>
<li><a href="https://www.confluent.io/blog/streaming-databases-cloud-data-in-motion-never-rests/">How Do You Change a Never-Ending Query?</a>. The post linked to is the third in the <strong>The Streaming Database Series</strong>. The post looks at how we can evolve queries in a streaming database and some of the pitfalls that may occur.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>Now is the conference season, and I am presenting at <a href="https://www.linkedin.com/company/clouddatadriven/"><strong>Cloud Data Driven</strong></a>:</p>

<p><img src="/images/posts/improve-clv.jpg" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Cloud Data Driven</em></p>

<p>I will be talking about how to calculate Customer Lifetime Value using Azure Databricks. If you are interested, the registration is <strong>FREE</strong>, so go ahead and <a href="https://www.eventbrite.com/e/improve-customer-lifetime-value-using-azure-databricks-delta-lake-tickets-189230642947"><strong>register</strong></a>.</p>

<p>As you see, the presentation is on Thursday, November 11. If you read the last week&rsquo;s <a href="/2021/10/31/interesting-stuff---week-44-2021/">roundup</a>, you may have noticed this:</p>

<p><img src="/images/posts/PASS-I-am-speaking-800.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>PASS Session</em></p>

<p>Yes, I am doing a live PASS Q&amp;A the same day. The PASS session id for my <a href="https://passdatacommunitysummit.com/sessions/265026"><strong>Analyze Billions of Rows of Data in Real-Time Using Azure Data Explorer</strong></a> presentation. Fortunately, the PASS session is 3:15 - 3:45 pm UTC, and my Cloud Data Driven presentation is at 4 pm UTC. Phew!</p>

<p>So here is an idea; get a double dose of Niels:</p>

<ul>
<li><a href="https://reg.passdatacommunitysummit.com/flow/redgate/summit2021/registrationopeningpage/page/introlanding">Register</a> (FREE) for PASS now, view the <a href="https://passdatacommunitysummit.com/sessions/265026"><strong>Analyze Billions of Rows of Data in Real-Time Using Azure Data Explorer</strong></a> recorded video and come and chat to me <a href="https://passdatacommunitysummit.com/sessions/265026">Thursday, Nov 11 15:15 - 15:45 UTC</a>.</li>
<li><a href="https://www.eventbrite.com/e/improve-customer-lifetime-value-using-azure-databricks-delta-lake-tickets-189230642947">Register</a> and attend my Cloud Data Driven presentation.</li>
</ul>

<p>Yay, Niels on Thursday from 3:15 UTC. What could be better than that? Actually, don&rsquo;t answer that question.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 44, 2021]]></title>
    <link href="https://nielsberglund.com/2021/10/31/interesting-stuff---week-44-2021/" rel="alternate" type="text/html"/>
    <updated>2021-10-31T08:53:44+02:00</updated>
    <id>https://nielsberglund.com/2021/10/31/interesting-stuff---week-44-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h1 id="machine-learning-data-science">Machine Learning / Data Science</h1>

<ul>
<li><a href="https://netflixtechblog.com/open-sourcing-a-monitoring-gui-for-metaflow-75ff465f0d60">Open-Sourcing a Monitoring GUI for Metaflow, Netflix&rsquo;s ML Platform</a>. This Netflix post looks at <strong>Metaflow GUI</strong>. This GUI for their open-sourced, <strong>Metaflow</strong> library allows data scientists to monitor their workflows in real-time, track experiments, and see detailed logs and results for every executed task. The GUI can be extended with plugins, allowing the community to build integrations to other systems, etc.</li>
<li><a href="https://databricks.com/blog/2021/10/28/moneyball-2-0-real-time-decision-making-with-mlbs-statcast-data.html">Moneyball 2.0: Real-time Decision Making With MLB&rsquo;s Statcast Data</a>. Back in 2003, <a href="https://en.wikipedia.org/wiki/Michael_Lewis">Michael Lewis</a> wrote the book <a href="https://en.wikipedia.org/wiki/Moneyball">Moneyball</a>. The book was about how a baseball manager used data analysis to identify undervalued players. The post here looks at how baseball teams today use streaming data and Databricks to do real-time analysis and decisions. Very interesting! Oh, BTW, Micheal Lewis is an excellent author, and the book mentioned is great!</li>
</ul>

<h2 id="azure-data-explorer">Azure Data Explorer</h2>

<ul>
<li><a href="https://dps10.com/how-to-do-real-time-analytics-using-apache-kafka-and-azure-data-explorer/">How to do Real-Time Analytics Using Apache Kafka and Azure Data Explorer</a>. In a couple of blog posts, I have mentioned how I did a session about Apache Kafka and Azure Data Explorer at the <strong>Data Platform Summit 2021</strong>. The recordings from the Summit has now been made available for FREE, and the link is to my session. Notice that you need to join Data Platform Geeks unless you are a member already, but it is free, and by joining, you get access to all recordings!</li>
<li><a href="/2021/10/27/how-to-ingest-into-azure-data-explorer-from-apache-kafka-using-kafka-connect/">How to Ingest Into Azure Data Explorer From Apache Kafka using Kafka Connect</a>. This post is also from &ldquo;yours truly&rdquo;. In the post we look at how to configure and set up Kafka Connect to allow ingestion into Azure Data Explorer.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://medium.com/event-driven-utopia/architecting-a-kafka-native-retail-analytics-platform-part-1-bf1eba42a371">Architecting a Kafka-centric Retail Analytics Platform â€” Part 1</a>. This post is the first of a series looking at building a Kafka-centric analytics platform that ingests and processes business data at scale. By the way, the author of the post, <a href="https://twitter.com/dunithd">Dunith Dhanushka</a>, is someone you should follow if you are interested in event driven architecture, streaming, Kafka, etc. He is excellent, and I am subscribing to his writings on Medium!</li>
<li><a href="https://www.confluent.io/blog/stream-governance-how-it-works/">Stream Governance â€“ How it Works</a>. I have written previously about the new Stream Governance functionality Confluent introduced recently. This post is the first in a series about Stream Governance and how it works: <strong>Stream Governance â€“ How it Works</strong>. This first post looks at some of the key features of Stream Governance. At <a href="/derivco"><strong>Derivco</strong></a>, we are highly interested in the topic of data governance. I will follow this closely!</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>Well, I have mentioned it before, but:</p>

<p><img src="/images/posts/PASS-I-am-speaking-800.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>PASS Session</em></p>

<p>Yeah, I am delivering <a href="https://passdatacommunitysummit.com/sessions/265026"><strong>Analyze Billions of Rows of Data in Real-Time Using Azure Data Explorer</strong></a>. The session is recorded and will be available for viewing from when the conference starts. Then <a href="https://event.passdatacommunitysummit.com/widget/redgate/summit2021/sessioncatalog/session/1629311226474001sKl1">Thursday, Nov 11 15:15 - 15:45 UTC</a>, (you have to <a href="https://reg.passdatacommunitysummit.com/flow/redgate/summit2021/registrationopeningpage/page/introlanding">register</a> to access this link), is a virtual live Q&amp;A with me where we discuss <strong>Azure Data Explorer</strong>.</p>

<p>So, <a href="https://reg.passdatacommunitysummit.com/flow/redgate/summit2021/registrationopeningpage/page/introlanding">register</a> now, view the recorded video and come and chat to me <a href="https://event.passdatacommunitysummit.com/widget/redgate/summit2021/sessioncatalog/session/1629311226474001sKl1">Thursday, Nov 11 15:15 - 15:45 UTC</a>. The registration is FREE, and besides me, you get to hear from the people that really know what they are talking about!</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to Ingest Into Azure Data Explorer From Apache Kafka using Kafka Connect]]></title>
    <link href="https://nielsberglund.com/2021/10/27/how-to-ingest-into-azure-data-explorer-from-apache-kafka-using-kafka-connect/" rel="alternate" type="text/html"/>
    <updated>2021-10-27T14:13:37+02:00</updated>
    <id>https://nielsberglund.com/2021/10/27/how-to-ingest-into-azure-data-explorer-from-apache-kafka-using-kafka-connect/</id>
    <content type="html"><![CDATA[<p>If you follow my blog, you probably know that I am a huge fan of <strong>Apache Kafka</strong> and event streaming/stream processing. Recently <strong>Azure Data Explorer</strong> (<strong>ADX</strong>) has caught my eye. In fact, in the last few weeks, I did two conference sessions about ADX. A month ago, I published a blog post related to Kafka and ADX: <a href="/2021/09/06/run-self-managed-kusto-kafka-connector-serverless-in-azure-container-instances/"><strong>Run Self-Managed Kusto Kafka Connector Serverless in Azure Container Instances</strong></a>.</p>

<p>As the title of that post implies, it looked at the ADX Kafka sink connector and how to run it in Azure. What the post did not look at was how to configure the connector and connect it to ADX. That is what we will do in this post (and maybe in a couple of more posts).</p>

<p></p>

<h2 id="pre-reqs">Pre-Reqs</h2>

<p>This section is here if you want to follow along. This is what you need:</p>

<ul>
<li>Docker desktop. If you are on Windows and don&rsquo;t have it, you download it from <a href="https://docs.docker.com/desktop/windows/install/">here</a>.</li>
<li>An Azure account. If you don&rsquo;t have an Azure subscription, sign up for a <a href="https://azure.microsoft.com/en-us/free/">free account</a>.</li>
<li>Azure CLI. The Azure CLI is a cross-platform command-line tool connecting to Azure and executing administrative commands on Azure resources. You find downloads and install instructions <a href="https://docs.microsoft.com/en-us/cli/azure/install-azure-cli">here</a>.</li>
<li>An Azure Data Explorer cluster and database. To see how to set up a cluster and a database, look here: <a href="https://docs.microsoft.com/en-us/azure/data-explorer/create-cluster-database-portal">Quickstart: Create an Azure Data Explorer cluster and database</a>.</li>
<li>Kafka cluster. You can either run the cluster &ldquo;on-prem&rdquo; or in the cloud. I wrote a blog post about how to run Confluent Platform using Docker <a href="/2019/06/18/confluent-platform--kafka-for-a-.net-developer-on-windows/">here</a> and running Confluent Cloud on Azure <a href="/2021/08/14/run-confluent-cloud--serverless-apache-kafka-on-azure/">here</a>. In this post, I use Confluent Cloud in Azure.</li>
<li>You need to download the Kusto connector from <a href="https://github.com/Azure/kafka-sink-azure-kusto/releases">here</a>. In the <a href="/2021/09/06/run-self-managed-kusto-kafka-connector-serverless-in-azure-container-instances/"><strong>Run Self-Managed Kusto Kafka Connector Serverless in Azure Container Instances</strong></a> post, I downloaded and used the 2.0 version. In this post, we will use the <a href="https://github.com/Azure/kafka-sink-azure-kusto/releases/download/v2.2.0/kafka-sink-azure-kusto-2.2.0-jar-with-dependencies.jar">2.2 version</a>.</li>
</ul>

<h4 id="confluent-cloud-cli">Confluent Cloud CLI</h4>

<p>We need a way to post messages/events to Kafka, and usually, I do it using a .NET Core application. Since in this post I only will send a few messages, I am going to use the <a href="https://docs.confluent.io/ccloud-cli/current/overview.html"><strong>Confluent Cloud CLI</strong></a> (ccloud) tool.</p>

<p>This tool is a must for anyone using Confluent Cloud as it enables developers to create, manage, and deploy their Confluent components. You find download and install instructions <a href="https://docs.confluent.io/ccloud-cli/current/install.html#tarball-or-zip-installation">here</a> and some examples of how to use it <a href="https://docs.confluent.io/platform/current/tutorials/examples/clients/docs/ccloud.html">here</a>.</p>

<p>If your Kafka is &ldquo;bare metal&rdquo;, or Docker based, there are commands based on script files for various operations. In this post, the following commands are what you need if you are not using Confluent Cloud:</p>

<ul>
<li><code>/kafka-topics</code>: handle topics; list, create, etc.</li>
<li><code>/kafka-console-producer</code>: publish messages to a topic.</li>
<li><code>/kafka-console-consumer</code>: consume messages from a topic.</li>
</ul>

<p>Having sorted out the pre-reqs, and some Kafka tools, let us move on.</p>

<h2 id="scene-setting">Scene Setting</h2>

<p>Before we go further, let&rsquo;s see what data we are working with. Since I am working at <a href="/derivco">Derivco</a>, an online gaming company, I guess it is only natural that my sample data is gameplay-related (fictitious gameplay that is). The idea is that, in this post, we have online players playing various types of games, and the games are submitting game events to a Kafka topic. The event looks something like so:</p>

<pre><code class="language-json">{
  &quot;playerId&quot;: Int32,
  &quot;gameId&quot;: Int32,
  &quot;win&quot;: Int64,
  &quot;score&quot;: Int32,
  &quot;eventTime&quot;: DateTime
}
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Event Schema</em></p>

<p>The fields we see in <em>Code Snippet 1</em> are:</p>

<ul>
<li><code>playerId</code>: the unique id of a player.</li>
<li><code>gameId</code>: an identifier for the specific game a player is playing.</li>
<li><code>win</code>: playing the game may result in a &ldquo;win&rdquo;, which is a unit of &ldquo;something&rdquo;.</li>
<li><code>score</code>: each interaction in a game by a player earns the player a score.</li>
<li><code>eventTime</code>: the time in UTC when the event happened.</li>
</ul>

<p>The event we see the schema for above is what is submitted to Kafka.</p>

<h4 id="create-topic">Create Topic</h4>

<p>Let us create the topic to which the events are submitted:</p>

<pre><code class="language-bash"># list existing topics
$ ccloud kafka topic list

# the above returns no topics
# create a topic for gameplay
$ ccloud kafka topic create gameplay --partitions 4

# check that we have a topic
$ ccloud kafka topic list
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Create Kafka Topic</em></p>

<p>In <em>Code Snippet 2</em> we see:</p>

<ul>
<li>before I create the topic, I check what topics exist in the cluster. As this is a brand new Confluent Cloud cluster, there were no topics.</li>
<li>I create the <code>gameplay</code> topic, explicitly setting the number of partitions to four. If I hadn&rsquo;t set the number of partitions, it would default to six.</li>
<li>to ensure that the topic has been created, I check for topics again, and yes - it is there.</li>
</ul>

<p>Let us publish an event to finish the <em>Setting the Scene</em> topic (did you see what I did there?).</p>

<h4 id="test-publish">Test Publish</h4>

<p>We start with setting up a listener in a terminal window, so we can see when data arrives in the topic:</p>

<pre><code class="language-bash"># setup a listener, when clicking enter it will start listening
ccloud kafka topic consume gameplay -b
Starting Kafka Consumer. Use Ctrl-C to exit.
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Consume from Topic</em></p>

<p>The code in <em>Code Snippet 3</em> sets up a listener. After clicking enter, you see the &ldquo;Starting Kafka Consumer &hellip;&rdquo; as in the code snippet, and the listener is now ready to receive messages.</p>

<p>Time to publish an event. Open another terminal window and, if you are on Windows, make sure the terminal is Windows command prompt - not PowerShell. For some reason, when I try to publish with <code>ccloud</code> using PowerShell, I get errors. Anyway, the code:</p>

<pre><code class="language-bash">$ ccloud kafka topic produce gameplay --parse-key --delimiter ,
33, {&quot;playerId&quot;:33, &quot;gameId&quot;:23, &quot;win&quot;:55, &quot;score&quot;:123, \ 
      &quot;eventTime&quot;:&quot;2021-10-24 04:14:39.572&quot;}
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Publish to Topic</em></p>

<p>In <em>Code Snippet 4</em>, we see how we use the <code>ccloud kafka topic produce</code> command to publish to our <code>gameplay</code> topic. The two flags we see are:</p>

<ul>
<li>The <code>--parse-key</code> flag indicates we supply a message key with the message.</li>
<li>The <code>--delimiter</code> flag defines the delimiter between the message key and message value.</li>
</ul>

<p>After hitting enter after the first line, we enter the message. In <em>Code Snippet 4</em>, we start with the message key <code>33</code>, followed by the message value. The message key, <code>33</code>, represents the <code>playerId</code> as we want to partition the Kafka topic on <code>playerId</code>.</p>

<blockquote>
<p><strong>NOTE:</strong> If you copy the code from <em>Code Snippet 4</em>, be aware of the line continuation character (<code>\</code>) in the message.</p>
</blockquote>

<p>To publish the message, you hit enter, and when you go to the consumer terminal window, you should now see the message.</p>

<p>We now have a topic, and we can produce to that topic. So now, let us go back to connecting up our Kafka topic with Azure Data Explorer.</p>

<h2 id="create-a-kusto-connect-image">Create a Kusto Connect Image</h2>

<p>As we know from my previous post and what I mentioned at the beginning of this post, we use a Kafka connector to connect Kafka to ADX, and in the <em>Pre-Reqs</em> section, I downloaded the connector.</p>

<p>Suppose I had had an on-prem Kafka Connect installation. In that case, I could have taken my downloaded Kusto connector, copied it into my Kafka connect box, restarted the Kafka Connect process, and all would be good.</p>

<blockquote>
<p><strong>NOTE:</strong> What I wrote above is a slight simplification; I would have needed to set some connect properties etc., as well. But from a high level, that&rsquo;s what I would have done.</p>
</blockquote>

<p>The point above is moot as I don&rsquo;t have a Kafka Connect installation, so I will run the Kusto connector from Docker instead. To do that, I need to create a Docker image of the connector.</p>

<h4 id="create-a-dockerfile">Create a Dockerfile</h4>

<p>We build the image from a <code>Dockerfile</code>, so we start with creating the file:</p>

<p><img src="/images/posts/kusto-conn-docker-file.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Dockerfile</em></p>

<p>We see in <em>Figure 1</em> above:</p>

<ul>
<li>As the connector needs to run in a Kafka Connect process, we use the <code>FROM</code> statement to bring in Kafka Connect&rsquo;s latest Confluent base image (<code>cp-server-connect-base:latest</code>).</li>
<li>We copy the <code>.jar</code> file of the downloaded connector to a location in Kafka Connect where the <code>.jar</code> can be loaded from.</li>
<li>As I want to connect to a Kafka cluster requiring authentication, I set some security settings.</li>
</ul>

<p>The last bullet point above is really &ldquo;glossing over&rdquo; what we are doing, so let me explain in a bit more detail. When we use a Kafka connector, the connector consumes from one or more Kafka topics or publishes to one or more Kafka topics. The necessary connection details to the Kafka instance is set up in the individual connector&rsquo;s configuration, which we&rsquo;ll see an example of later.</p>

<p>However, in the <a href="/2021/09/06/run-self-managed-kusto-kafka-connector-serverless-in-azure-container-instances/">previous post</a>, we said how Kafka Connect is a JVM process (a worker) that loads and runs individual connectors such as our Kusto connector. The worker process needs to store configurations of the respective connectors and their state, and it stores this in Kafka. It doesn&rsquo;t have to store it in the Kafka instance the connector(s) consumes from or publishes to - it can use a completely separate Kafka instance and potentially a separate instance per connector. So when we want to use a connector, we need to set that information before configuring the connector. We see that in <em>Figure 1</em> lines 5 - 10, we set the security information to connect to the Kafka cluster where we want to store state and configuration.</p>

<p>That is it. We now have created a Docker file.</p>

<h4 id="build-the-image">Build the Image</h4>

<p>Having the Docker file, we can now go ahead and build the image:</p>

<p><img src="/images/posts/kusto-conn-docker-build.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Docker Build</em></p>

<p>In <em>Figure 2</em>, we see how we use the <code>docker build</code> command to build an image with a given name. We also use the <code>-t</code> flag to <em>tag</em> the image. Looking at <em>Figure 2</em> it seems like the build has succeeded. Let us see if we have an image by running <code>docker images</code>:</p>

<p><img src="/images/posts/kusto-conn-docker-image.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Kusto Docker Image</em></p>

<p>It definitely looks like we are in business as we in <em>Figure 3</em> see the newly built image. The image is now available locally, but we can also push it to the likes of Dockerhub, or - as I did in the <a href="/2021/09/06/run-self-managed-kusto-kafka-connector-serverless-in-azure-container-instances/">previous post</a>  - to Azure Container Instances.</p>

<h2 id="run-the-connector">Run the Connector</h2>

<p>For now, we will not push the image anywhere but run it locally, using Docker Compose.</p>

<h4 id="docker-compose-configuration">Docker Compose Configuration</h4>

<p>In the <a href="/2021/09/06/run-self-managed-kusto-kafka-connector-serverless-in-azure-container-instances/">previous</a> post, we did a quick run of the connector using Docker Compose. Here we will use almost the same Docker Compose file, but look a little bit more in detail what it consists of:</p>

<p><img src="/images/posts/kusto-conn-docker-compose.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Docker Compose File</em></p>

<p>In <em>Figure 4</em>, we see the compose file - named <code>docker-compose.yml</code> - I created to &ldquo;spin up&rdquo; the connector. We see how I have in <em>Figure 4</em> added some numbered &ldquo;bullet&rdquo; points at the left. They indicate relevant &ldquo;stuff&rdquo; for the container so let us look at those and see what they refer to:</p>

<ol>
<li><code>image</code>: the image name and tag which we are using.</li>
<li><code>container_name</code>: arbitrary name of the container.</li>
<li><code>ports</code>: this tells the container to listen on this particular port, and how to map the internal IP to external.</li>
<li><code>CONNECT_BOOTSTRAP_SERVERS</code>: host:port pair for the initial connection to the Kafka cluster. You can define multiple servers with a comma-separated host:port pairs.</li>
<li><code>CONNECT_REST_ADVERTISED_HOST_NAME</code>: hostname for other workers to connect to when we run a distributed Kafka Connect cluster. The <a href="https://rmoff.net/2019/11/22/common-mistakes-made-when-configuring-multiple-kafka-connect-workers/"><strong>Common mistakes made when configuring multiple Kafka Connect workers</strong></a> post by Kafka guru Robin Moffat talks more about this.</li>
<li><code>CONNECT_REST_PORT</code>: port for the REST API endpoint. It is <code>8083</code> by default, so I would not have had to define it in my compose file. Make sure that whatever port number you use is reflected in the <code>ports</code> entry in point 3. You use this port to manage and administer your connector.</li>
<li><code>CONNECT_GROUP_ID</code>: a string that identifies the Kafka Connect cluster group this worker belongs to.</li>
<li><code>CONNECT_CONFIG_STORAGE_TOPIC</code>: name of the topic in which to store connector and task configuration data.</li>
<li><code>CONNECT_OFFSET_STORAGE_TOPIC</code>: name of the topic in which to store offset data for connectors.</li>
<li><code>CONNECT_STATUS_STORAGE_TOPIC</code>: name of the topic in which to store state for connectors.</li>
<li><code>CONNECT_KEY_CONVERTER</code>: what class to use for conversion of the message key. This can be overridden by the configuration of an individual connector.</li>
<li><code>CONNECT_VALUE_CONVERTER</code>: what class to use for conversion of the message value. This can be overridden by the configuration of an individual connector.</li>
</ol>

<p>Oh, one thing about the <code>...STORAGE_TOPIC</code>&rsquo;s; if you have multiple Kafka Connect workers in a Connect group (<code>CONNECT_GROUP_ID</code>), then those workers need to use the same topics in the same cluster.</p>

<h4 id="run-the-container">Run the Container</h4>

<p>When we have created the Docker Compose file as per above, we can run the container. However, before we do that, I would like you to check what topics are in the Kafka Cluster: <code>ccloud kafka topic list</code>. When I run that code, I only see the topic we created above.</p>

<p>Cool, let us now &ldquo;spin up&rdquo; the container:</p>

<p><img src="/images/posts/kusto-conn-docker-compose-up.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Run Docker Compose</em></p>

<p>We see in <em>Figure 5</em> how we start the container using the <code>docker-compose up -d</code> command. I execute the command in the same directory I have the <code>docker-compose.yml</code> file.</p>

<blockquote>
<p><strong>NOTE:</strong> The <code>docker-compose up</code> command expects a file named <code>docker-compose.yml</code>. If you want to use another name, you tell the command the file name using the <code>-f</code> flag.</p>
</blockquote>

<p>In <em>Figure 5</em>, we also see how after the command has been executed, Docker creates an internal network and starts the container.</p>

<p>Seeing what we see in <em>Figure 5</em>, the assumption is that all has gone OK. Let us confirm by looking for new topics in the Kafka cluster. I do the same as I did above executing <code>ccloud kafka topic list</code>:</p>

<p><img src="/images/posts/kusto-conn-kafka-topics-2-new.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>Kafka Topics</em></p>

<p>Yay, when looking at <em>Figure 6</em> we see three new topics. I.e. the topics we defined in the <code>docker-compose.yml</code> file to store status, config and offset. So, so far, so good. Now let us take it one step further.</p>

<p>Remember from above how I said that the <code>CONNECT_REST_PORT</code> in the <code>docker-compose.yml</code> file defines the port we use to administer and configure the connector. We do this by calling endpoints exposed by the Kafka Connect REST API.</p>

<p>Let us now use one of the endpoints to see that the Kafka worker is up and running and that our connector is available. The endpoint we use is the same we used in the [previous][post]: <code>GET /connector-plugins</code>:</p>

<p><img src="/images/posts/kusto-conn-get-connector-plugins.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>Connector Plugins</em></p>

<p>As I run the Docker container on my machine, we see in <em>Figure 7</em> (outlined in red) how I make a <code>GET</code> request against <code>localhost:8083</code> and the <code>connector-plugins</code> endpoint. We also see in <em>Figure 7</em> that the request is successful. It returns an array of connector plugins, of which our plugin (outlined in yellow) is one.</p>

<p>This is awesome; we have now built a Docker image of the Kusto sink connector and &ldquo;spun&rdquo; it up on our local machine. Basically, we are at the same point as at the end of the <a href="/2021/09/06/run-self-managed-kusto-kafka-connector-serverless-in-azure-container-instances/">previous post</a>, apart from the container is not in Azure Container Registry.</p>

<p>Now it is almost time to &ldquo;hook&rdquo; the connector up to Kafka and Azure Data Explorer. However, we need to do some configuration of the ADX cluster and the database before hooking up the connector.</p>

<h2 id="setup-adx-properties">Setup ADX &amp; Properties</h2>

<p>The configuration we need to do of ADX is:</p>

<ul>
<li>Creating an Azure Active Directory (AAD) security principal, which the connector uses to write to the ADX table(s).</li>
<li>Getting some properties of the ADX cluster, so the connector knows where to connect and write data to.</li>
</ul>

<p>We start with the Service Principal (SPN).</p>

<h4 id="adx-service-principal">ADX Service Principal</h4>

<p>To set up the SPN, we use the Azure CLI mentioned above in the <em>Pre-Reqs</em>. This is what you do:</p>

<ul>
<li>log in to Azure: <code>az login</code>. This returns a list of your subscriptions after successful login.</li>
<li>if you have more than one subscription, you set the subscription you want to use: <code>az account set --subscription your-sub-id</code>.</li>
</ul>

<p>Having done the above, you now create the service principal:</p>

<pre><code class="language-bash">az ad sp create-for-rbac -n &quot;kusto-kafka-nielsblog&quot;
</code></pre>

<p><strong>Code Snippet 5:</strong> <em>Create Service Principal</em></p>

<p>The code in <em>Code Snippet 5</em> creates - as mentioned before - a service principal configured to access Azure resources. The output, when executing the code, looks something like so:</p>

<p><img src="/images/posts/kusto-conn-spn.png" alt="" /></p>

<p><strong>Figure 8:</strong> <em>Service Principal Properties</em></p>

<p>In <em>Figure 8</em>, we see how the result from the code returns JSON with 5 fields/properties. Take note of <code>appId</code>, <code>password</code>, and <code>tenant</code> as we will use them later when we configure the connector.</p>

<p>Actually, we&rsquo;ll use <code>appId</code> and <code>tenant</code> right now, as we will add the created service principal to your ADX database.</p>

<p>The easiest way to add the created service principal is to do it from the <em>Query Explorer</em> window for ADX in the Azure portal:</p>

<p><img src="/images/posts/kusto-conn-adx-db-1.png" alt="" /></p>

<p><strong>Figure 9:</strong> <em>ADX Database</em></p>

<p>I have in <em>Figure 9</em> logged in to my Azure subscription and navigated to my ADX cluster (highlighted in yellow). In the cluster, I have one database (outlined in blue). The database is selected, and to get to the query explorer, I click on the <em>Query</em> button outlined in red. When the query explorer opens, I can create the service principal as an admin user in the database:</p>

<pre><code class="language-sql">.add database nielsblogpostsdb1 admins ('aadapp=The-AppId;The-Tenant') 'AAD App'
</code></pre>

<p><strong>Code Snippet 6</strong> <em>Create Service Principal Admin User in Database</em></p>

<p>The code in <em>Code Snippet 6</em> creates an admin user in the <code>nielsblogpostsdb1</code> database. As the user is a service principal, we identify the user with <code>aadapp=appId;tenant</code> from the result in <em>Figure 8</em>. We also give the &ldquo;user&rdquo; a name: in this case: <code>AAD App</code>.</p>

<h4 id="adx-properties">ADX Properties</h4>

<p>The final thing we need to do with ADX is to find the equivalent of a connection string, i.e. where the connector can find ADX and connect to. That is exposed as endpoints in the overview page for the ADX cluster:</p>

<p><img src="/images/posts/kusto-conn-cluster-props.png" alt="" /></p>

<p><strong>Figure 10:</strong> <em>Connection Strings</em></p>

<p>We are looking for two endpoints outlined in red in <em>Figure 10</em>. Those two endpoints represent:</p>

<ul>
<li><code>URI</code>: endpoint for querying the cluster.</li>
<li><code>Data Ingestion URI</code>: this is the endpoint for ingesting into the cluster.</li>
</ul>

<p>Of the two endpoints above, only the ingestion endpoint is required when configuring the connector. When you have taken note of those endpoints, we can go on.</p>

<h2 id="configure-the-connector">Configure the Connector</h2>

<p>To do the &ldquo;hook-up,&rdquo; the connector needs to be configured. From a high level, we need to configure:</p>

<ul>
<li>what topic(s) to read data from.</li>
<li>the target Azure Data Explorer cluster.</li>
<li>the target database and table(s) to ingest the data into.</li>
</ul>

<p>We already have an ADX cluster and a database, so let us create a table.</p>

<h4 id="create-table">Create Table</h4>

<p>We need to create a table to ingest the data from the Kafka <code>gameplay</code> topic. We do it from the <em>Query Explorer</em> window for ADX, as we saw in <em>Figure 9</em>. Open the editor and write the create table code:</p>

<p><img src="/images/posts/kusto-conn-query-editor-1.png" alt="" /></p>

<p><strong>Figure 11:</strong> <em>Query Explorer</em></p>

<p>When we look at the code in <em>Figure 11</em> we see how the statement to create a table looks slightly different from what you were to write in SQL. That is because this is not SQL, but KQL - Kusto Query Language. Expect a future post looking more in detail into KQL.</p>

<p>In <em>Figure 11</em>, we see how the table matches the event schema in <em>Code Snippet 1</em>, and we can now execute the code by hitting enter or clicking the <em>Run</em> button outlined in yellow. After executing the code in <em>Figure 11</em>, you can check that the table has been created by executing: <code>.show tables</code>.</p>

<p><img src="/images/posts/kusto-conn-show-tables.png" alt="" /></p>

<p><strong>Figure 12:</strong> <em>Show Tables</em></p>

<p>As we see in <em>Figure 12</em>, the table has been created. There is one more thing we need to do in the database related to the table. We need to create a mapping between the event data and the columns in the table.</p>

<h4 id="ingestion-mapping">Ingestion Mapping</h4>

<p>As the event lands in Kafka as JSON (or some other format), Azure Data Explorer needs to understand how the fields in the event map to the columns in the table. We do this by creating a table ingestion mapping in the database:</p>

<pre><code class="language-sql">.create table GamePlay ingestion json mapping 'gameplay_json_mapping' 
'[{&quot;column&quot;:&quot;PlayerID&quot;, &quot;Properties&quot;:{&quot;path&quot;:&quot;$.playerId&quot;}}, 
{&quot;column&quot;:&quot;GameID&quot;, &quot;Properties&quot;:{&quot;path&quot;:&quot;$.gameId&quot;}}, 
{&quot;column&quot;:&quot;Win&quot;, &quot;Properties&quot;:{&quot;path&quot;:&quot;$.win&quot;}}, 
{&quot;column&quot;:&quot;Score&quot;, &quot;Properties&quot;:{&quot;path&quot;:&quot;$.score&quot;}}, 
{&quot;column&quot;:&quot;EventTime&quot;, &quot;Properties&quot;:{&quot;path&quot;:&quot;$.eventTime&quot;}} ]'
</code></pre>

<p><strong>Code Snippet 7:</strong> <em>Ingestion Mapping</em></p>

<p>The code in <em>Code Snippet 7</em> shows how we create a JSON ingestion mapping, naming it <code>gameplay_json_mapping</code>. We further see how the columns are mapped against the event fields, where <code>$</code> represents the event&rsquo;s root. After running the code in <em>Code Snippet 7</em>, we can check that the mapping exists executing: <code>.show table GamePlay ingestion mappings</code>, which returns all mappings for that table. The page <a href="https://docs.microsoft.com/en-us/azure/data-explorer/kusto/management/show-ingestion-mapping-command">.show ingestion mapping</a> from ADX documentation covers ingestion mappings in more detail.</p>

<p>Above I said that we had to do one more thing in the database, which was the ingestion mapping we just saw. Actually, there is another thing to do. It is not 100% necessary, but it impacts the ingestion latency. We should create an ingestion policy.</p>

<h4 id="ingestion-policy">Ingestion Policy</h4>

<p>A future blog post, will talk in detail about ADX various ingestion options, and what an ingestion policy is. For now, let us just &ldquo;roll&rdquo; with this, and take it for what it is worth. So to create an ingestion batching policy you:</p>

<pre><code class="language-sql">.alter table GamePlay policy ingestionbatching 
@'{&quot;MaximumBatchingTimeSpan&quot;:&quot;00:00:10&quot;, &quot;MaximumNumberOfItems&quot;: 5, &quot;MaximumRawDataSizeMB&quot;: 100}'
</code></pre>

<p><strong>Code Snippet 8:</strong> <em>Ingestion Batching Policy</em></p>

<p>An ingestion batching policy determines when data aggregation should stop during data ingestion according to the specified settings. In <em>Code Snippet 8</em> we see the code to set the policy with our preferred settings:</p>

<ul>
<li><code>MaximumBatchingTimeSpan</code>: maximum time to close the aggregation and ingest the data. In our case, we set it to 10 seconds, which is the minimum value allowed by ADX. The default value is 5 minutes.</li>
<li><code>MaximumNumberOfItems</code>: when the maximum number is reached, the aggregation is closed and the data ingested. Default is 1000 items.</li>
<li><code>MaximumRawDataSizeMB</code>: when aggregation reaches the maximum size, it closes and ingests the data. Default is 1000 MB.</li>
</ul>

<p>After running the code in <em>Code Snippet 8</em>, we can check and see that the policy has been created: <code>.show table GamePlay policy ingestionbatching</code>.</p>

<blockquote>
<p><strong>NOTE:</strong> If we didn&rsquo;t set the policy as in <em>Code Snippet 8</em>, it would run with the default values, and we may have to wait a while to see data in the table.</p>
</blockquote>

<p>We should now be all set and ready to configure the connector.</p>

<h4 id="connector-configuration">Connector Configuration</h4>

<p>You configure the connector using a JSON file and <code>POST</code>:ing it either via <code>curl</code> or Postman. Personally, I prefer Postman, so that is what I will use later.</p>

<p>The configuration file I use looks like so:</p>

<p><img src="/images/posts/kusto-conn-connector-config-1-new.png" alt="" /></p>

<p><strong>Figure 13:</strong> <em>Connector Config</em></p>

<p>Some of the properties we see in <em>Figure 13</em> is heavily cut, but don&rsquo;t worry; below is an explanation of the most important ones. For an explanation of all of them, see <a href="https://github.com/Azure/kafka-sink-azure-kusto#5-sink-properties">5. Sink properties</a>.</p>

<p>The numbers in <em>Figure 13</em> and their related properties:</p>

<ol>
<li>The <code>topics</code> property points to the topic(s) the connector consumes. It is defined as a comma-delimited string.</li>
<li>In <code>kusto.ingestion.url</code>, you set the ingestion endpoint you &ldquo;grabbed&rdquo; in <em>Figure 10</em>.</li>
<li>The same goes for the <code>kusto.query.url</code>. Remember, this property is optional.</li>
<li>The <code>aad.auth.authority</code> refers to the <code>tenant</code> you received when creating the service principa</li>
<li>The <code>aad.auth.appid</code> is the <code>appId</code> property from the service principal.</li>
<li>The last property referring to authentication is the <code>aad.auth.appkey</code>; the <code>password</code> property for the service principal.</li>
<li>Let us come back to <code>kusto.tables.topics.mapping</code> below.</li>
<li>I have found that I need to set both <code>key.converter.schemas.enable</code> and <code>value.converter.schemas.enable</code> to <code>false</code>. I get some errors otherwise. I need to drill into that a bit deeper at a later stage.</li>
<li>These are the connection and authentication details for the Kafka broker you consume from.</li>
</ol>

<p>Above, I said we&rsquo;d come back to <code>kusto.tables.topics.mapping</code>, so let us do that.</p>

<p><strong>kusto.tables.topics.mapping</strong></p>

<p>As the name implies, this property defines the mapping between the topic and the table(s) in the database(s). My mapping looks like so:</p>

<pre><code class="language-json">[{'topic': 'gameplay','db': 'nielsblogpostsdb1', 'table': 'GamePlay',' \
   format': 'json', 'mapping':'gameplay_json_mapping', 'streaming': 'false'}]
</code></pre>

<p><strong>Code Snippet 9:</strong> <em>Tables Topic Mapping</em></p>

<p>The mapping is an array of topics and their respective mapping to databases and tables in the cluster. In my example, I have only one mapping, and in the mapping, I define:</p>

<ul>
<li>the topic to consume from.</li>
<li>the database and table to ingest into.</li>
<li>the format of the data.</li>
<li>the ingestion mapping that has been set up for the table. We see this in <em>Code Snippet 7</em>.</li>
<li>whether the ingestion is streaming or batch. Default is batch. In a future post, I will talk more about stream and batch ingestion.</li>
</ul>

<p>When we have created the config JSON, we are ready to create the connector on the Kafka Connect cluster:</p>

<p><img src="/images/posts/kusto-conn-connector-config-post.png" alt="" /></p>

<p><strong>Figure 14:</strong> <em>Create the Connector</em></p>

<p>In <em>Figure 14</em>, we see part of the configuration file in <em>Figure 13</em>, and how we <code>POST</code> it to Kafka Connect&rsquo;s <code>/connectors</code> endpoint. When running the code (doing the <code>POST</code>), we should get a <code>201</code> response back, indicating all is well.</p>

<p>I didn&rsquo;t mention the &lsquo;name&rsquo; property we see at line 2 in <em>Figure 13</em>. The name is an arbitrary string, and it comes in useful if you want to manage this particular connector. Things you can do are:</p>

<ul>
<li>pause the connector.</li>
<li>restart the connector.</li>
<li>delete the connector.</li>
<li>check the status of the connector.</li>
<li>etc.</li>
</ul>

<p>Each individual operation is against the <code>/connectors</code> endpoint and takes the connector name as an input parameter, potentially followed by a resource request. An example would be that when we have created the connector, we want to check that it actually is up and running:*</p>

<p><img src="/images/posts/kusto-conn-connector-status.png" alt="" /></p>

<p><strong>Figure 15:</strong> <em>Check Connector Status</em></p>

<p>What <em>Figure 15</em> shows is how we make a <code>GET</code> request (outlined in red) against the <code>/connectors</code> endpoint (highlighted in yellow), for the connectors name (highlighted in red), and we are asking for the <code>status</code> (highlighted in blue).</p>

<p>Executing that request gives us:</p>

<p><img src="/images/posts/kusto-conn-connector-status-result.png" alt="" /></p>

<p><strong>Figure 16:</strong> <em>Status Result</em></p>

<p>We see in <em>Figure 16</em> how everything looks OK! The connector is now configured and up and running, and we should now be able to test it out.</p>

<h2 id="testing-the-connector">Testing the Connector</h2>

<p>Suppose you haven&rsquo;t torn down the topic and recreated it. In that case, the easiest way to do a quick test is that after 10 seconds after the connector is up and running, execute the following query in the <em>Query Editor</em>: <code>GamePlay | count</code>. That on my box results in a value of 1, which is the event we published in <em>Code Snippet 4</em>. The event sits in the topic; the connector connects and starts consuming from the topic ingesting into the table.</p>

<p>Let&rsquo;s now publish some more events:</p>

<pre><code class="language-bash">$ ccloud kafka topic produce gameplay --parse-key --delimiter ,
34, {&quot;playerId&quot;:34, &quot;gameId&quot;:27, &quot;win&quot;:0, &quot;score&quot;:99, \ 
      &quot;eventTime&quot;:&quot;2021-10-27 07:56:39.572&quot;}
38, {&quot;playerId&quot;:38, &quot;gameId&quot;:27, &quot;win&quot;:0, &quot;score&quot;:99, \ 
      &quot;eventTime&quot;:&quot;2021-10-27 07:56:39.572&quot;}
45, {&quot;playerId&quot;:45, &quot;gameId&quot;:17, &quot;win&quot;:10, &quot;score&quot;:103, \ 
      &quot;eventTime&quot;:&quot;2021-10-27 07:56:39.572&quot;}       
</code></pre>

<p><strong>Code Snippet 10:</strong> <em>Publish More Events to Topic</em></p>

<p>In <em>Code Snippet 10</em>, we see how we publish more events. While you publish, execute the <code>GamePlay | count</code> call. You should see how the count increases, not as fast as you publish due to the ingestion batching, but with a slight latency. After a while, you should see the correct count, and if you execute: <code>GamePlay</code>, you should see something similar to this:</p>

<p><img src="/images/posts/kusto-conn-query-result.png" alt="" /></p>

<p><strong>Figure 17:</strong> <em>Query Result</em></p>

<p>From what we published in *Code Snippet 4, together with what we published here in <em>Code Snippet 10</em>, we see how 4 rows in total have been ingested into the <code>GamePlay</code> table. Success!</p>

<h2 id="summary">Summary</h2>

<p>Phew, this was quite a journey, but we made it. So, in this post we have looked at:</p>

<ul>
<li>Confluent Cloud CLI, and how we use it to create topics, publish, and consume messages.</li>
<li>How to create and configure a Docker image from the Kusto Sink Connector.</li>
<li>Set up a Docker Compose file to use the image above.</li>
<li>Create an Azure Active Directory Service Principal using Azure CLI (<code>az</code>).</li>
<li>Create an admin user in our ADX database from that Service Principal.</li>
<li>Define ingestion mapping and ingestion batch policy in the ADX database.</li>
<li>Created the connector configuration file and got an insight into some of its properties.</li>
<li>Created the connector from the connector configuration file.</li>
</ul>

<p>After the above, we published some messages to Kafka. We saw when publishing how the messages were ingested into the table. Notice that depending on the batching policy, you may see different results in the latency when ingesting.</p>

<p>In a future post, we will look more in detail into the different ingestion methods and how to configure them. Until then!</p>

<h2 id="finally">Finally</h2>

<p>If you have comments, questions etc., please comment on this post or [ping][ma] me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 43, 2021]]></title>
    <link href="https://nielsberglund.com/2021/10/24/interesting-stuff---week-43-2021/" rel="alternate" type="text/html"/>
    <updated>2021-10-24T12:07:06+02:00</updated>
    <id>https://nielsberglund.com/2021/10/24/interesting-stuff---week-43-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://aka.ms/adls/hitchhikersguide">The Hitchhiker&rsquo;s Guide to the Data Lake</a>. This post discusses considerations and best practices around how to effectively utilize <strong>Azure Data Lake Storage Gen2</strong> in large scale Big Data platform architectures. I found this post to be extremely useful!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://databricks.com/blog/2021/10/19/introducing-apache-spark-3-2.html">Introducing Apache Spark 3.2</a>. In last week&rsquo;s <a href="/2021/10/17/interesting-stuff---week-42-2021/">roundup</a>, I linked to a post about a new Window type coming in Apache Spark 3.2: the session window. The post linked to here looks at other new interesting features in the 3.2 release!</li>
<li><a href="https://stlplaces.com/blog/best-apache-kafka-books-in-year">Best Apache Kafka Books in 2021</a>. Well, not much to say really here. As the title says, the post lists the Kafka books the author likes best.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>Today and tomorrow, I am putting the finishing touches on my video recording for the <a href="https://passdatacommunitysummit.com/"><strong>PASS Data Community Summit 2021</strong></a>:</p>

<ul>
<li><a href="https://passdatacommunitysummit.com/sessions/265026"><strong>Analyze Billions of Rows of Data in Real-Time Using Azure Data Explorer</strong></a>. In this session, I look at how <strong>Azure Data Explorer</strong> enables us to do near real-time analysis of Big Data.</li>
</ul>

<p>If you are interested you can register <a href="https://reg.passdatacommunitysummit.com/flow/redgate/summit2021/registrationopeningpage/page/introlanding">here</a>. The conference sessions are free!</p>

<p>In addition to the above, I am also working on a blog post looking at ingesting data from Kafka into Azure Data Explorer. I&rsquo;ve been working on it for quite a while now. Hopefully, I&rsquo;ll have it done within a week or two.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 42, 2021]]></title>
    <link href="https://nielsberglund.com/2021/10/17/interesting-stuff---week-42-2021/" rel="alternate" type="text/html"/>
    <updated>2021-10-17T08:52:05+02:00</updated>
    <id>https://nielsberglund.com/2021/10/17/interesting-stuff---week-42-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="azure-data-explorer">Azure Data Explorer</h2>

<ul>
<li><a href="https://youtu.be/52dPYC1V5a0">Azure Data Explorer Shorts: Managed Ingestion</a>. An excellent short (~9 minutes) video explaining the ins and outs of data ingestion into <strong>Azure Data Explorer</strong>.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/how-baader-built-a-predictive-analytics-machine-learning-system-with-kafka-and-rstudio/">Apache Kafka and R: Real-Time Prediction and Model (Re)training</a>. This blog post looks at how KStreams, ksqlDB, and R can be used to create a data pipeline in which a machine learning model is applied to streaming data. The post also looks at how the model can be automatically retrained once the prediction results exceed a certain threshold. Very Cool!</li>
<li><a href="https://databricks.com/blog/2021/10/12/native-support-of-session-window-in-spark-structured-streaming.html">Native Support of Session Window in Spark Structured Streaming</a>. The post linked to, looks at a new window type in the upcoming Apache Spark 3.2 version. Before Spark 3.2, Spark supported tumbling and sliding windows. In the 3.2 version, the session window is introduced. The interesting thing with a session window is that it has a dynamic size of window length depending on the input.</li>
<li><a href="https://www.confluent.io/blog/new-confluent-cloud-connector-features-and-single-message-transforms/">Introducing Single Message Transforms and New Connector Features on Confluent Cloud</a>. Part of Confluent cloud is managed Kafka Connect connectors, and this post announces new features for most of the managed connectors. I am quite &ldquo;chuffed&rdquo; about seeing Single Message Transforms as one such new feature.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 41, 2021]]></title>
    <link href="https://nielsberglund.com/2021/10/10/interesting-stuff---week-41-2021/" rel="alternate" type="text/html"/>
    <updated>2021-10-10T10:15:26+02:00</updated>
    <id>https://nielsberglund.com/2021/10/10/interesting-stuff---week-41-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="misc">Misc.</h2>

<ul>
<li><a href="https://devblogs.microsoft.com/python/python-in-visual-studio-code-october-2021-release/">Python in Visual Studio Code â€“ October 2021 Release</a>. In early October, Python 3.10 stable was released. Hot on the heels of that release comes what is mentioned in this blog post: a new release of the <strong>VS Code</strong> Python extension. The post looks at some of the significant new features and fixes. One of the new features is an improved &ldquo;getting started experience&rdquo; for Python in VS Code. Since I have had issues in the past getting Python up and running, I think I will uninstall my existing Python extension and try this improved extension from &ldquo;scratch&rdquo;.</li>
</ul>

<h2 id="sql-server-big-data-cluster">SQL Server Big Data Cluster</h2>

<ul>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2021/10/06/whats-new-with-sql-server-big-data-clusters-cu13-release/">What&rsquo;s new with SQL Server Big Data Clustersâ€”CU13 Release</a>. I guess the title of the blog post says it all. The post looks at new &ldquo;stuff&rdquo; in the CU13 release of <strong>SQL Server 2019 Big Data Cluster</strong>. The big one for me in this release is the switch from Apache Spark 2.4 to Apache Spark 3.1.2.</li>
</ul>

<h2 id="azure-data-explorer">Azure Data Explorer</h2>

<ul>
<li><a href="https://techcommunity.microsoft.com/t5/azure-data-explorer/aligning-timeseries-application-requirements-into-azure-data/ba-p/2758959">Aligning Timeseries Application Requirements into Azure Data Explorer (ADX)</a>. Time series analysis has become critical in almost any analytical application. This blog post looks at the support for time series analysis in Azure, more specifically in <strong>Azure Data Explorer</strong>. After reading the post, I think it is safe to say that the support in ADX is &ldquo;pretty darn good&rdquo;.</li>
<li><a href="https://davemccollough.com/2021/02/01/kusto-query-language-101/">Kusto Query Language 101</a>. Above, we spoke about time series analysis in <strong>Azure Data Explorer</strong>. It is all good and well that we have that functionality, but how do we query ADX? Well, if you read the post linked to, you will get a &ldquo;crash course&rdquo; in the query language for ADX: <strong>Kusto Query Language</strong> (KQL).</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/resources/demo/stream-governance-discover-understand-and-trust-your-data-in-motion/">Stream Governance: Discover, understand, and trust your data in motion</a>. A month or so ago, Confluent <a href="https://www.confluent.io/blog/governing-data-with-confluent-stream-governance/">announced the release</a> of their platform for stream governance: the <a href="https://www.confluent.io/product/stream-governance/"><strong>Stream Governance</strong> suite</a>. Since the release, Confluent has been busy creating learning resources etc., and the post linked to is the registration page for a Stream Governance webinar. If you are working with streaming data, I do suggest you sign up!</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 40, 2021]]></title>
    <link href="https://nielsberglund.com/2021/10/03/interesting-stuff---week-40-2021/" rel="alternate" type="text/html"/>
    <updated>2021-10-03T09:34:52+02:00</updated>
    <id>https://nielsberglund.com/2021/10/03/interesting-stuff---week-40-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://azure.microsoft.com/en-us/blog/govern-your-data-wherever-it-resides-with-azure-purview/">Govern your data wherever it resides with Azure Purview</a>. This post looks at <strong>Azure Purview</strong>. Azure Purview is a unified data governance solution that helps you achieve a complete understanding of your data. This is regardless of whether it&rsquo;s housed on-premises in services like SQL Server and Oracle, in different clouds like Amazon Web Services (AWS) S3, or SaaS applications like Salesforce. This is something we dearly need at <a href="/derivco">Derivco</a>!</li>
</ul>

<h2 id="azure-data-explorer">Azure Data Explorer</h2>

<ul>
<li><a href="https://techcommunity.microsoft.com/t5/azure-data-explorer/monitor-batching-ingestion-with-adx-insights/ba-p/2673509">Monitor batching ingestion with ADX Insights</a>. <strong>ADX Insights</strong> is a system providing comprehensive monitoring of your ADX clusters. This post talks about the new Ingestion monitoring feature that allows you to monitor the status of batching ingestion operations to ADX. Very cool!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/serverless-kafka-streaming-with-confluent-cloud-and-aws-lambda/">Trigger AWS Lambda Functions Directly from an Apache Kafka Topic</a>. This post looks at how you can stream data from Confluent Cloud Kafka topics into Amazon DynamoDB tables by triggering an AWS Lambda function - providing a completely serverless architecture. I need to test this out on Azure using <strong>Azure Functions</strong>!</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 39, 2021]]></title>
    <link href="https://nielsberglund.com/2021/09/26/interesting-stuff---week-39-2021/" rel="alternate" type="text/html"/>
    <updated>2021-09-26T10:49:18+02:00</updated>
    <id>https://nielsberglund.com/2021/09/26/interesting-stuff---week-39-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://medium.com/event-driven-utopia/understanding-materialized-views-3-stream-table-joins-with-cdc-77591d2d6fa0">Understanding Materialized Views â€” 3 : Stream-Table Joins with CDC</a>. In a roundup a couple of weeks ago, I linked to a post about materialized views, and I wrote how I couldn&rsquo;t wait for a follow-up post. Well, here it is. In this post, the author looks at joining streams with lookup tables to create materialized views. Very cool!</li>
<li><a href="https://www.confluent.io/blog/apache-kafka-3-0-major-improvements-and-new-features/">What&rsquo;s New in Apache Kafka 3.0.0</a>. I guess the title says it all. Apache Kafka version 3.0 has just been released, and this blog post looks at some of the new features, fixes, and improvements.</li>
<li><a href="https://k6.io/blog/load-test-your-kafka-producers-and-consumers-using-k6/">How to Load Test Your Kafka Producers and Consumers using k6</a>. A couple of weeks ago, I came across <a href="https://k6.io/">k6</a>, a modern load testing framework for both developers as testers. I thought it would be cool if I somehow could load-test Kafka producers and consumers in the framework. Well, I can now do it, and the post I have linked to discusses the newly developed Kafka k6 extension: xk6-kafka. I cannot wait to put it through its paces.</li>
<li><a href="https://www.confluent.io/blog/announcing-ksqldb-0-21-new-features-major-upgrades/">Announcing ksqlDB 0.21.0</a>. Above I linked to the announcement of Kafka 3.0. This post discusses the new ksqlDB 0.21.0 release and looks at some of the new features.</li>
<li><a href="https://www.kai-waehner.de/blog/2021/09/23/real-time-kappa-architecture-mainstream-replacing-batch-lambda/">Kappa Architecture is Mainstream Replacing Lambda</a>. In this post, the author looks at the benefits the Kappa architecture provides over the Lambda architecture. One of the major, major benefits is a much simpler infrastructure.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>Right now, I am &ldquo;prepping&rdquo; for two conference talks this coming week:</p>

<ul>
<li><a href="https://datadrivencommunity.com/speaker-NielsBerglund-session.html"><strong>Analyze Billions of Rows of Data in Real-Time Using Azure Data Explorer</strong></a>: On Wednesday (Sept 29), I deliver this presentation which is an overview of <strong>Azure Data Explorer</strong>, and how it is ideal for near-real-time analytics of huge volumes of data.</li>
<li><a href="https://azurebootcamp.co.za/"><strong>Improve Customer Lifetime Value using Azure Databricks &amp; Delta Lake</strong></a>. Then on Thursday (Sept 30), I present how you can calculate and improve Customer Lifetime Value (CLV) using Azure Databricks.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 38, 2021]]></title>
    <link href="https://nielsberglund.com/2021/09/19/interesting-stuff---week-38-2021/" rel="alternate" type="text/html"/>
    <updated>2021-09-19T10:37:19+02:00</updated>
    <id>https://nielsberglund.com/2021/09/19/interesting-stuff---week-38-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://www.startree.ai/blogs/launching-at-linkedin-the-story-of-apache-pinot/">Launching at LinkedIn: The Story of Apache Pinot</a>. This is the story of how Apache Pinot started from a &ldquo;simple&rdquo; beginning at LinkedIn, how it grew over time to being adopted at Uber. Very interesting!</li>
<li><a href="https://databricks.com/blog/2021/09/17/timeliness-and-reliability-in-the-transmission-of-regulatory-reports.html">Timeliness and Reliability in the Transmission of Regulatory Reports</a>. Regulations impact more and more companies. Part of most regulations is the requirement to create reports for the regulators. God knows that we at <a href="/derivco">Derivco</a> feel the pain around this subject. The post linked to here demonstrates the benefits of the Databricks Lakehouse architecture in the ingestion, processing, validation and transmission of regulatory data.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/governing-data-with-confluent-stream-governance/">Confluent Unlocks the Full Power of Event Streams with Stream Governance</a>. Data governance has become a requirement for most organizations, and the organizations have adopted the governance tools needed to manage their data. However, most of the tools are built for data at rest. What about streaming data? In this blog post, Confluent announces the release of their Stream Governance Suite, a set of tools allowing you to govern your streaming data. This is something we have wished for here at <a href="/derivco">Derivco</a>!</li>
<li><a href="https://www.confluent.io/blog/kafka-summit-americas-2021-recap/">Kafka Summit Americas 2021 Recap</a>. This year&rsquo;s final Kafka Summit (Kafka Summit Americas) was a wrap earlier this week. The blog post lists some of the highlights of the summit. Have a look and see what catches your interest!</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 37, 2021]]></title>
    <link href="https://nielsberglund.com/2021/09/12/interesting-stuff---week-37-2021/" rel="alternate" type="text/html"/>
    <updated>2021-09-12T09:43:08+02:00</updated>
    <id>https://nielsberglund.com/2021/09/12/interesting-stuff---week-37-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="http://babkin-cep.blogspot.com/2021/09/the-practice-of-parallel-programming.htm">The Practice of Parallel Programming</a>. A link to a free downloadable pdf version of the book <strong>The Practice of Parallel Programming</strong>. The pdf provides an advanced guide to the issues of parallel and multithreaded programming. It goes beyond the high-level design of the applications into the details that are often overlooked but vital to make the programs work. It is an excellent read!</li>
</ul>

<h2 id="sql-server-2019-language-extensions">SQL Server 2019 Language Extensions</h2>

<ul>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2021/09/08/open-sourcing-the-net-5-c-language-extension-for-sql-server/">Open sourcing the .NET 5 C# Language Extension for SQL Server</a>. If you follow my blog, you may know that I have been writing quite a lot about the <strong>SQL Server Language Extensions</strong> and how extensions for Python, R, and Java exist and are open source. Well, the time has now come for C#. The post linked to announces the open-source release of SQL Server Language Extensions for C#! That.Is.So.Awesome! Expect some blog posts from yours truly looking at this.</li>
</ul>

<h2 id="azure-data-explorer">Azure Data Explorer</h2>

<ul>
<li><a href="https://youtu.be/lN3HkAZ3oGA">Azure Data Explorer ADX Overview</a>. In this YouTube video, the presenter dives into Azure Data Explorer â€“ from data ingestion to dashboards â€“ and looks at how Azure Data Explorer allows us to focus on discovering insights in the data while simplifying infrastructure and managing cost.</li>
<li><a href="/2021/09/06/run-self-managed-kusto-kafka-connector-serverless-in-azure-container-instances/">Run Self-Managed Kusto Kafka Connector Serverless in Azure Container Instances</a>. This blog post by me looks at how to run the Kafka Connector for Azure Data Explorer server-less in Azure. We look at creating a Docker image for the connector and deploying it to Azure Container Instances.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://medium.com/event-driven-utopia/understanding-materialized-views-part-2-ae957d40a403">Understanding Materialized Views â€” Part 2</a>. This post is the second in a series about <strong>Materialized Views</strong>. In this post, the author looks at stream processing and explores two essential concepts in stateful stream processing; streams and tables. Based on streams and tables, he looks at how streams turn into tables that make materialized views. He concludes the post by learning how these materialized views can be scaled and recovered from failures. The first part of the series is [here], and I - for one - cannot wait for the next instalment!</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Run Self-Managed Kusto Kafka Connector Serverless in Azure Container Instances]]></title>
    <link href="https://nielsberglund.com/2021/09/06/run-self-managed-kusto-kafka-connector-serverless-in-azure-container-instances/" rel="alternate" type="text/html"/>
    <updated>2021-09-06T06:11:51+02:00</updated>
    <id>https://nielsberglund.com/2021/09/06/run-self-managed-kusto-kafka-connector-serverless-in-azure-container-instances/</id>
    <content type="html"><![CDATA[<p>A couple of weeks ago, I <a href="/2021/08/14/run-confluent-cloud--serverless-apache-kafka-on-azure/">posted</a> how I set up Kafka to run serverless in Azure by deploying Confluent Cloud.</p>

<p>If you have followed my blog lately, you have probably seen that I am interested in <strong>Azure Data Explorer</strong> and that I have a couple of conference talks coming. One being:</p>

<ul>
<li><strong>How to do Real-Time Analytics Using Apache Kafka and Azure Data Explorer</strong>: We are looking at how to stream events from Apache Kafka to Azure Data Explorer (ADX) and perform user-facing analytics in near real-time.</li>
</ul>

<p>The question is how to connect Kafka with ADX? You normally connect Kafka with another system using a Kafka Connect connector, and fortunately a connector exists for connecting Kafka with ADX: the <a href="https://github.com/Azure/kafka-sink-azure-kusto"><strong>Kafka Connect Kusto Sink Connector</strong></a>.</p>

<p>However, since I am running managed Kafka (Confluent Cloud, remember), I need a managed connector to run it in Confluent Cloud&rsquo;s Kafka Connect. In the previous paragraph, I mentioned I was fortunate as we had a Kafka connector for ADX. Unfortunately, it is not a managed connector, so I cannot run it in Confluent Cloud - bummer!</p>

<p>So, this post looks at the various options we have if we want to use the Kafka Connect Kusto Sink Connector connecting Confluent Cloud in Azure with Azure Data Explorer. However, if you are not interested in neither Kafka nor ADX, the post may still be of use for you. The reason being it also covers running Docker images in Azure Container Instances (ACI).</p>

<p></p>

<blockquote>
<p><strong>NOTE:</strong> Even though this post comes out of me wanting to use the Kafka Connect Kusto Sink Connector, the post is <strong>NOT</strong> about the usage or the configuration of the connector. That is covered in a future post.</p>
</blockquote>

<h2 id="credits">Credits</h2>

<p>Usually, the credits &ldquo;roll&rdquo; at the end of the <del>movie</del> post, but I feel I should start with the credits as this post would not have happened if it wasn&rsquo;t for this blog post:</p>

<ul>
<li><a href="https://rmoff.net/2021/01/11/running-a-self-managed-kafka-connect-worker-for-confluent-cloud/">Running a self-managed Kafka Connect worker for Confluent Cloud</a>. I came across this post by Kafka guru extraordinaire <a href="https://twitter.com/rmoff">Robin Moffat</a> when I looked into running a self-managed connector when you use Confluent Cloud. His post made me look into what it would take to run the connector in Azure Container Instances (ACI).</li>
</ul>

<p>As I said, <a href="https://twitter.com/rmoff">Robin Moffat</a> is a Kafka Guru, and if you are into Kafka, then you <strong>MUST</strong> read his <a href="https://rmoff.net/">blog</a>.</p>

<h2 id="pre-reqs">Pre-Reqs</h2>

<p>This section is here if you want to follow along. I am listing what you need if you&rsquo;re going to deploy and run a container in ACI; not all required components for Kafka and ADX:</p>

<ul>
<li>An Azure account. If you don&rsquo;t have an Azure subscription, sign up for a <a href="https://azure.microsoft.com/en-us/free/">free account</a>.</li>
<li>Azure CLI. The Azure CLI is a cross-platform command-line tool to connect to Azure and execute administrative commands on Azure resources. You find downloads and install instructions <a href="https://docs.microsoft.com/en-us/cli/azure/install-azure-cli">here</a>.</li>
<li>Docker Desktop: We will build a Docker image, so we need Docker Desktop.</li>
<li>Something to build the image from. The image I build for this post includes the Kusto Sink Connector, which I download from <a href="https://github.com/Azure/kafka-sink-azure-kusto/releases/download/v2.0.0/kafka-sink-azure-kusto-2.0.0-jar-with-dependencies.jar">here</a>.</li>
</ul>

<p>The version (2.0) of the Kusto Sink Connector I downloaded is not the latest you find on the <a href="https://github.com/Azure/kafka-sink-azure-kusto/releases">release page</a>, but I could not get the 2.1 version to work.</p>

<p>In addition to the above, I have Confluent Cloud deployed as per my <a href="/2021/08/14/run-confluent-cloud--serverless-apache-kafka-on-azure/">post</a> mentioned in the beginning. As I want to connect Kafka to Azure Data Explorer, I also have Azure Data Explorer installed.</p>

<h2 id="background">Background</h2>

<p>Before we get down into it, let us get an understanding of some of the components in this post:</p>

<ul>
<li>Azure Data Explorer</li>
<li>Confluent Cloud</li>
<li>Kafka Connect</li>
<li>Kafka Connect Kusto Sink Connector</li>
</ul>

<h4 id="azure-data-explorer">Azure Data Explorer</h4>

<p>Azure Data Explorer is a fully-managed big data analytics cloud platform and data-exploration service that ingests structured, semi-structured (like JSON) and unstructured data. The service then stores this data and answers analytic ad hoc queries on it with seconds of latency. It is a full-text indexing and retrieval database, including time series analysis capabilities, machine learning, regular expression evaluation, and text parsing.</p>

<p>It is ideal for analyzing large volumes of diverse data from any data source, such as websites, applications, IoT devices, and more. Essentially it is a distributed database running on a cluster of compute nodes in Azure.</p>

<h4 id="confluent-cloud">Confluent Cloud</h4>

<p>In my <a href="/2021/08/14/run-confluent-cloud--serverless-apache-kafka-on-azure/">post</a> setting up Confluent Cloud I said it is a resilient, scalable streaming data service based on Apache Kafka, delivered as a fully managed service. It is Confluent Platform, running as a managed service in the cloud, and you can run it on Azure, AWS, and Google Cloud.</p>

<p>As it is Confluent Platform, you get so much more than <em>just</em> Kafka. You get built-in stream processing through ksqlDB, schema registry for data integrity, managed Kafka Connect connectors for data sources/sinks, and more.</p>

<h4 id="kafka-connect">Kafka Connect</h4>

<p>Kafka Connect is a tool for streaming data between Apache Kafka and other systems scalable and reliable. The way you move data between systems and Kafka is using connectors, and there are two flavors of connectors:</p>

<ul>
<li>Source connectors that understand how to interact with the source system send records into Kafka</li>
<li>Sink connectors that propagate records from Kafka topics to other systems.</li>
</ul>

<p>Kafka Connect is a JVM process, and it operates separately from the Kafka Broker. The process loads connectors, and the connectors know how to interact with Kafka and the source/sink systems. The connectors are written in Java and packaged into <code>.jar</code> files.</p>

<h4 id="kafka-connect-kusto-sink-connector">Kafka Connect Kusto Sink Connector</h4>

<p>The Kusto Sink Connector is a Kafka Connect connector. It is - as the name implies - a sink connector, dequeuing events from Kafka topics and ingesting them into Azure Data Explorer. The ingestion is, for now, queued ingestion leveraging the Azure Data Explorer Java SDK, i.e. batch mode.</p>

<p>Since the Kusto connector is not a managed connector, we need to decide where and how to run it.</p>

<h2 id="options">Options</h2>

<p>Robin covered the various options in his <a href="https://rmoff.net/2021/01/11/running-a-self-managed-kafka-connect-worker-for-confluent-cloud/">post</a> a lot better than I ever could, so I won&rsquo;t repeat that. Furthermore, seeing that I say in the title of this post <strong>Azure Container Instances</strong> (ACI), it is probably safe to assume that&rsquo;s the option we&rsquo;ll go with. As a picture says more than a thousand words, our solution looks something like so:</p>

<p><img src="/images/posts/kusto-aci-conn.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Kafka - ADX Architecture</em></p>

<p>We see in <em>Figure 1</em> how we have:</p>

<ul>
<li>Kafka in Azure (Confluent Cloud).</li>
<li>an Azure Container Instances running the Kusto Sink Connector.</li>
<li>data ingested from the connector into Azure Data Explorer.</li>
</ul>

<p>Oh, and yes - the data being ingested is published to Kafka from the publisher we see in the upper left-hand corner.</p>

<p>In this post, the term Azure Container Instances has been mentioned a couple of times. What is that, then?</p>

<h4 id="azure-container-instances">Azure Container Instances</h4>

<p>ACI gives you an easy way to run containers in the Azure cloud, eliminating the need to manage virtual machines (VMs) or using more complex container orchestration services, like Kubernetes. For me, just testing this out, ACI is &ldquo;good enough&rdquo;.</p>

<p>There are a couple of ways you can deploy and run a container in ACI:</p>

<ul>
<li>Utilise the integration between Docker and Azure and execute <code>docker run</code>. You do this mostly in test scenarios.</li>
<li>Deploy the container image to ACI and run it in ACI.</li>
</ul>

<p>Even though I said I am using ACI for testing at the beginning of this section, I will not use <code>docker run</code> but instead do a &ldquo;proper&rdquo; deployment to ACI.</p>

<h2 id="create-kusto-sink-connector-image">Create Kusto Sink Connector Image</h2>

<p>We start with creating the <code>Dockerfile</code> for the image we want to deploy to ACI:</p>

<p><img src="/images/posts/kusto-aci-dockerfile.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Dockerfile</em></p>

<p>As we need to run the connector in Kafka Connect, we base the image on the <code>cp-server-connect-base</code> image (the <code>FROM</code> statement). This image contains the bare minimum for Kafka Connect.</p>

<p>In the Pre-Reqs section, I mentioned how I downloaded the Kusto connector. I downloaded it as a zip file and unzipped it to the same directory the <code>Dockerfile</code> is in. In line 3, we see how I copy the connector&rsquo;s <code>.jar</code> file to <code>/usr/share/java</code> in the base image. That path is a &ldquo;well known&rdquo; path to load connectors from.</p>

<p>The <code>...OVERRIDE_POLICY=All</code> statement on line 5 allows this connector to override consumer and producer properties to not impact all connectors running in that worker.</p>

<p>Lines 7 - 10 in the docker file are core config security settings that need to be set at the Kafka connect worker level and need to be &ldquo;baked&rdquo; into the Docker image. These settings are related to authentication and authorization against the Confluent Cloud Kafka.</p>

<h4 id="build-the-image">Build the Image</h4>

<p>Having created the <code>Dockerfile</code>, we can build the image:</p>

<p><img src="/images/posts/kusto-aci-docker-build.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Docker Build</em></p>

<p>In <em>Figure 3</em>, we see us executing the <code>docker build</code> command and the outcome. When the build has finished, we can check that all looks OK by running <code>docker images</code>:</p>

<p><img src="/images/posts/kusto-aci-kusto-image.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Kusto Docker Image</em></p>

<p>Success, we have an image, as we see in <em>Figure 4</em>. Well, at least partial success; we don&rsquo;t know if it works yet. Let us find out.</p>

<h4 id="run-locally">Run Locally</h4>

<p>To find out if it works, we can run the container image locally using <code>docker-compose</code>. We create a <code>docker-compose.yml</code> file containing the bare minimum for running the connector:</p>

<p><img src="/images/posts/kusto-aci-docker-compose.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Docker Compose File</em></p>

<p>We see in <em>Figure 5</em> the compose file I use to test that my container works. As mentioned before, the file contains the required properties to get this connector up and running. What you see outlined in red are properties naming topics needed to store Kafka offsets, configs and statuses. After we have &ldquo;spun up&rdquo; the container, we can check for these topics in our Kafka installation. Let us execute `docker-compose up -d&rsquo;:</p>

<p><img src="/images/posts/kusto-aci-docker-compose-up.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>Run Docker Compose</em></p>

<p>In <em>Figure 6</em>, we see that it looks like everything has worked OK and that we have created a connector instance <code>kusto-conn-1</code>. We can confirm that it has worked by checking the topics mentioned above or execute a REST call against the Kafka Connect API to <code>GET</code> the available connectors:</p>

<p><img src="/images/posts/kusto-aci-get-connectors.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>GET Connectors</em></p>

<p>From what we see in <em>Figure 7</em> it looks like we are OK. We see, outlined in blue, the <code>GET</code> call on port <code>8083</code>, and in the result below, we see the connector outlined in red.</p>

<h2 id="azure-container-instances-1">Azure Container Instances</h2>

<p>When we have confirmed that our image works, it is time to deploy it to ACI. When we run a container in ACI, the container is stored in <a href="https://azure.microsoft.com/en-us/services/container-registry/#overview">Azure Container Registry</a> (ACR).</p>

<p>We&rsquo;ll create a new container registry in a second, but before we do that, let us log in to Azure and set the subscription to use. To log in, we run <code>az login</code>. The command may take a second or two, and a dialog in your browser may ask you for login credentials. When login is done, you will see a JSON blob with information about the subscriptions you have access to. Choose the one you want to use:</p>

<pre><code class="language-bash"># az login
# az login above returns a JSON blob with subscriptions.
# set the subscription you want to use
az account set --subscription 00000000-0000-0000-0000-000000000000 
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Set Subscription</em></p>

<p>After we have logged in, we execute the code in <em>Code Snippet 1</em> to set the subscription we want to use. And no, the subscription Id in <em>Code Snippet 1</em> is not mine.</p>

<p>We are almost at the stage where we can create the ACR, but we need one more thing before creating the ACR. That one more thing is a resource group. I will use an existing resource group for this post, so I will not create a new one. If you need to create a resource group, you do:</p>

<pre><code class="language-bash">az group create --name name-of-rg --location azure-location  
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Create Resource Group</em></p>

<p>To see what locations are available for the <code>--location</code> parameter in <em>Code Snippet 2</em>, you can execute: <code>az account list-locations</code>.</p>

<p>Right, we now have a resource group. Let us press on.</p>

<h4 id="create-azure-container-registry">Create Azure Container Registry</h4>

<p>ACR is a private Docker registry service, similar to Dockerhub. As with Dockerhub, you push container images to your container registry.</p>

<p>To create a container registry, we use <code>az acr create ...</code>:</p>

<pre><code class="language-bash">az acr create --resource-group rg-kafka \
              --name nielsblog1 \
              --sku Basic \
              --admin-enabled true \
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Create Container Registry</em></p>

<p>In <em>Code Snippet 3</em>, we see how we create a container registry. The first two parameters define the resource group to create it in: <code>rg-kafka</code>, and the name of the registry: <code>nielsblog1</code>. You may ask what the last two parameters are:</p>

<ul>
<li><code>--sku</code>: this parameter defines the service tier: <code>Basic</code>, <code>Standard</code>, or <code>Premium</code>. Read more about service tiers <a href="https://docs.microsoft.com/en-us/azure/container-registry/container-registry-skus">here</a>.</li>
<li><code>--admin-enabled</code>: an admin user account is included when creating a container registry. The account is disabled by default. For testing purposes, you may want to have it enabled, so that is why I have included it in the creation. Read more about the admin account <a href="https://docs.microsoft.com/en-us/azure/container-registry/container-registry-authentication?tabs=azure-cli#admin-account">here</a>.<br /></li>
</ul>

<p>When you execute the code in <em>Code Snippet 3</em>, it will take a little while. When it finishes, you will see a JSON blob with some information about the created registry.</p>

<p>Having created the registry, we can now log in to it: <code>az acr login --name regname</code>. We are almost ready to push our image to the registry, but there are two things we need to do before that.</p>

<p><strong>Login Server</strong></p>

<p>When we push an image to the registry, we need an address to push to; the login server. To retrieve the login server, we do:</p>

<pre><code class="language-bash">az acr show --name registryname --query loginServer
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Get Login Server</em></p>

<p>Most likely, when you execute the code in <em>Code Snippet 4</em>, you get back <code>your-registry-name.azurecr.io</code>, but it is good practice to explicitly retrieve the login server.</p>

<p><strong>Credentials</strong></p>

<p>The second thing we need to do is get the credentials for the admin user we enabled in <em>Code Snippet 3</em>. We use the credentials later when we deploy our container:</p>

<p><img src="/images/posts/kusto-aci-acr-credentials.png" alt="" /></p>

<p><strong>Figure 8:</strong> <em>ACR Credentials</em></p>

<p>In <em>Figure 8</em>, we see outlined in yellow:ish the code to retrieve the credentials. Outlined in red, we see the two passwords. These passwords are created when the admin account is enabled, and they can also be re-generated. Finally, outlined in blue is the user name to use for the admin account.</p>

<h4 id="push-image-to-acr">Push Image to ACR</h4>

<p>We are getting there. Now, the time has come to push the image we built in <em>Figure 3</em> to the ACR. We do it in a two-step process:</p>

<p><strong>Tag the Image</strong></p>

<p>Tag the image with the login server string:</p>

<p><img src="/images/posts/kusto-aci-docker-tag.png" alt="" /></p>

<p><strong>Figure 9:</strong> <em>Tag the Image</em></p>

<p>We see in <em>Figure 9</em> outlined in:</p>

<ul>
<li>Blue: the <code>docker tag</code> statement we use.</li>
<li>Yellow: the name of the image we want to tag.</li>
<li>Red: the &ldquo;tagged&rdquo; new name of the image.</li>
</ul>

<p><strong>Push the Image</strong></p>

<p>Having tagged the image with the login server, we can push it to ACR:</p>

<p><img src="/images/posts/kusto-aci-docker-push-acr.png" alt="" /></p>

<p><strong>Figure 10:</strong> <em>Docker Push to ACR</em></p>

<p>When you run the code outlined in blue in <em>Figure 10</em>, you see how the image layers are pushed to the registry.</p>

<p>Looking at <em>Figure 10</em> everything looks OK, but - look at the statement outlined in red. What is this &ldquo;repository&rdquo; thing?</p>

<p>It turns out that when we push an image to the ACR, we push it not directly into the ACR. Instead, a repository is created, and we push it into that repository. A repository is a collection of container images or other artefacts in a registry with the same name but different tags.</p>

<p>That explains why we, when looking for images in the ACR we do something like so:</p>

<p><img src="/images/posts/kusto-aci-acr-repo-images.png" alt="" /></p>

<p><strong>Figure 11:</strong> <em>ACR Repository with Image(s)</em></p>

<p>In <em>Figure 11</em>, we see how we execute <code>az acr repository list ...</code> when looking for images (outlined in yellow) and how the result comes back as an array (outlined in red).</p>

<h2 id="deploy-the-container">Deploy the Container</h2>

<p>It is now time to deploy and run the image in Azure Container Instances. To create the container, we use the <code>az container create ...</code> command:</p>

<pre><code class="language-bash">az container create --resource-group rg-kafka `
&gt;&gt; --name nielsblog1 `
&gt;&gt; --image nielsblog1.azurecr.io/kusto-conn-1:latest `
&gt;&gt; --restart-policy OnFailure `
&gt;&gt; --ip-address Public `
&gt;&gt; --ports 8083 `
&gt;&gt; --registry-login-server nielsblog1.azurecr.io `
&gt;&gt; --registry-username nielsblog1 `
&gt;&gt; --registry-password some-super-secret-password
</code></pre>

<p><strong>Code Snippet 5:</strong> <em>Create Container - I</em></p>

<p>The code in <em>Code Snippet 5</em> is part of the code we need to run to create the container. The parameters we see are straightforward. The only thing worth mentioning is the <code>--ip-address Public</code> and <code>--ports 8083</code>. We need to indicate that we need a public IP address and that port 8083 should be open.</p>

<p>In the previous paragraph, I mentioned that the code in <em>Code Snippet 5</em> is only part of what we need to run. So what else do we need? Remember what we did when we tested the container locally, how we had a <code>.yml</code> file (<em>Figure 5</em>), with properties required to run Kafka Connect? We need the same here!</p>

<p>The question is, how do we supply those properties? The answer is that <code>az container create</code> has an <code>--environment-variables</code> parameter. This parameter is a list of environment variables for the container, where the list contains space-separated values in &lsquo;key=value&rsquo; format, something like so:</p>

<p><img src="/images/posts/kusto-aci-create-container-I.png" alt="" /></p>

<p><strong>Figure 12:</strong> <em>Create Container - II</em></p>

<p>In <em>Figure 12</em>, we see the entire command, including the required properties for Kafka.</p>

<blockquote>
<p><strong>NOTE:</strong> Notice how the <code>--environment-variables</code> do not have an quotes around them. That is because I am running from PowerShell. If you run the command from command prompt you need the environment variables be enclosed in single quotes.</p>
</blockquote>

<p>The text outlined in red at the bottom of <em>Figure 12</em> shows that the screenshot is taken while the command executes. While the command is running, you can execute the following to see the state it is in:</p>

<pre><code class="language-bash"> az container show --resource-group rg-kafka `
 &gt;&gt;--name nielsblog1 --query instanceView.state
</code></pre>

<p><strong>Code Snippet 6:</strong> <em>View Container State</em></p>

<p>Eventually, the creation finishes and results in a JSON blog with some information about the container. Most of the information in the blob is available from <code>az container ...</code> commands:</p>

<pre><code class="language-bash"># get log information from the container
az container logs --resource-group rg-kafka --name nielsblog1

# get ip address information
az container show --resource-group  rg-kafka `
&gt;&gt;--name nielsblog1 --query ipAddress
</code></pre>

<p><strong>Code Snippet 7:</strong> <em>Get Logs &amp; IP Address</em></p>

<p>The code in <em>Code Snippet 7</em> retrieves log information from the running container and the public IP address. Looking at the logs, everything looks OK, so let us use the IP address we retrieved in <em>Code Snippet 7</em> and do what we did in <em>Figure 7</em> (but now against the container in Azure):</p>

<p><img src="/images/posts/kusto-aci-get-connectors-az.png" alt="" /></p>

<p><strong>Figure 13:</strong> <em>Get Connectors - II</em></p>

<p>Yay, from what we see in <em>Figure 13</em> all is OK. We see the Kusto Sink Connector, outlined in red, being part of the returned result from the <code>GET</code> call. And in the <code>GET</code> call outlined in blue, we see we use the Azure IP address (highlighted in yellow). Well, you don&rsquo;t necessarily know it is the Azure IP, but trust me - it is. Yay, again!</p>

<h2 id="summary">Summary</h2>

<p>Wow, that was a lot! In this post, we saw how to:</p>

<ul>
<li>Build a docker image for the Kusto Kafka Sink Connector.</li>
<li>Test it locally.</li>
<li>Create an Azure Container Registry.</li>
<li>Push the image to the registry.</li>
<li>Deploy the image to, and run it in Azure Container Instances.</li>
</ul>

<p>Look out for a post covering how to configure and use the Kusto Sink Connector.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 36, 2021]]></title>
    <link href="https://nielsberglund.com/2021/09/05/interesting-stuff---week-36-2021/" rel="alternate" type="text/html"/>
    <updated>2021-09-05T10:38:44+02:00</updated>
    <id>https://nielsberglund.com/2021/09/05/interesting-stuff---week-36-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="misc">Misc.</h2>

<ul>
<li><a href="https://www.hanselman.com/blog/my-ultimate-powershell-prompt-with-oh-my-posh-and-the-windows-terminal">My Ultimate PowerShell prompt with Oh My Posh and the Windows Terminal</a>. I certainly hope that <a href="https://www.hanselman.com/">Scott Hanselman</a> doesn&rsquo;t need an introduction, but if you haven&rsquo;t heard of him, <a href="https://www.hanselman.com/blog">here</a> is the link to his blog. Anyway, he has blogged a bit about setting up the Windows terminal, so it looks cool. The post I link to here is the latest and greatest in setting up your terminal.</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://ably.com/blog/8-fallacies-of-distributed-computing">Navigating the 8 fallacies of distributed computing</a>. This post reviews the eight fallacies of distributed computing and provides several hints at how to handle them. I think I&rsquo;ll, in my next job interview, ask some questions about the eight fallacies!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://eng.uber.com/kafka-async-queuing-with-consumer-proxy/">Enabling Seamless Kafka Async Queuing with Consumer Proxy</a>. This post from Uber discusses how Kafka being a stream-oriented system, where message order is assumed in the system&rsquo;s design, can hinder certain message delivery patterns. The post talks about how more than 300 microservices at Uber are leveraging Kafka for pub-sub message queueing between microservices and how Uber developed the Consumer Proxy to work around some of the drawbacks with Kafka&rsquo;s message order oriented design.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 35, 2021]]></title>
    <link href="https://nielsberglund.com/2021/08/29/interesting-stuff---week-35-2021/" rel="alternate" type="text/html"/>
    <updated>2021-08-29T13:37:54+02:00</updated>
    <id>https://nielsberglund.com/2021/08/29/interesting-stuff---week-35-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="big-data-machine-learning">Big Data / Machine Learning</h2>

<ul>
<li><a href="https://rudderstack.com/blog/churn-prediction-with-bigqueryml">Churn Prediction With BigQueryML to Increase Mobile Game Revenue</a>. Seeing what we do at <a href="/derivco">Derivco</a>, this post is exciting. The post looks at how machine learning can identify high-value mobile game players dangerously close to churning. Very interesting!</li>
</ul>

<h2 id="data-architecture">Data Architecture</h2>

<ul>
<li><a href="https://medium.com/@jordan_volz/five-predictions-for-the-future-of-the-modern-data-stack-435b4e911413">Five Predictions for the Future of the Modern Data Stack</a>. This post looks at the developments of the modern data stack and the bright side of &ldquo;Modern Data Stack V2&rdquo;, focusing on AI, Data Sharing, Data Governance, Streaming &amp; Application Serving.</li>
</ul>

<h2 id="azure-data-explorer">Azure Data Explorer</h2>

<ul>
<li><a href="https://mrfoxsql.wordpress.com/2021/08/25/timeseries-analytics-capabilities-and-azure-data-explorer-adx/">Timeseries Analytics Capabilities, and Azure Data Explorer (ADX)</a>. I guess that for you who read my blog, it doesn&rsquo;t come as a surprise that I have a thing for Azure Data Explorer. The post here looks at time-series analytics and explores the types of core functionality typical for time-series data processing applications. It further looks at how functionality built into ADX aligns exceptionally well to meet these challenges head-on.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://towardsdatascience.com/real-time-anomaly-detection-with-apache-kafka-and-python-3a40281c01c9">Real-time anomaly detection with Apache Kafka and Python</a>. In this post, the author looks at making real-time anomaly predictions over streaming data coming from Kafka using Python.</li>
<li><a href="https://www.confluent.io/blog/ksqldb-architecture-and-advanced-features/">How ksqlDB Works: Internal Architecture and Advanced Features</a>. To effectively use ksqlDB, you should, apart from being familiar with its features and syntax, also have an understanding of what&rsquo;s happening &ldquo;under the cover&rdquo; of ksqlDB. This post covers some of the &ldquo;under the cover&rdquo; topics as well as points to resources at <a href="https://developer.confluent.io/learn-kafka/inside-ksqldb/streaming-architecture/">Confluent Developer</a>.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>By now, you probably know that I:</p>

<p><img src="/images/posts/Neils_Berglund_Breakout_Session.jpg" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Breakout Session</em></p>

<p>Yes, as we see in <em>Figure 1</em> I am presenting at the <a href="https://dataplatformgeeks.com/dps2021/"><strong>2021 Data Platform Summit</strong></a>:</p>

<ul>
<li><a href="https://dataplatformgeeks.com/dps2021/sessions-agenda-schedule/"><strong>How to do Real-Time Analytics Using Apache Kafka and Azure Data Explorer</strong></a>. We are looking at how to stream events from Apache Kafka to Azure Data Explorer and perform user-facing analytics in near real-time.</li>
</ul>

<p>I mentioned in a previous roundup how the organizers have managed to increase the capacity of the virtual platform to 10,000! So, they have opened up <strong>FREE</strong> booking for <strong>LIVE</strong> attendance for a limited time. They have an internal quota, and once that is full, the free booking will close.  So, what are you waiting for? Hurry up to <a href="https://dataplatformgeeks.com/dps2021/complimentary-registration"><strong>register for FREE</strong></a>!</p>

<p>Oh, I am not only doing the conference session above, but also a post-conference training class; 4 hours per day over 2 days:</p>

<ul>
<li><a href="https://dataplatformgeeks.com/dps2021/product/big-data-analytics-with-sql-server-2019-big-data-cluster-by-niels-berglund/"><strong>Big Data &amp; Analytics with SQL Server 2019 Big Data Cluster</strong></a>.</li>
</ul>

<p>There are still a couple of seats (virtual) available for my class, so - if you are interested - register <a href="https://dataplatformgeeks.com/dps2021/product/big-data-analytics-with-sql-server-2019-big-data-cluster-by-niels-berglund/">here</a>.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 34, 2021]]></title>
    <link href="https://nielsberglund.com/2021/08/22/interesting-stuff---week-34-2021/" rel="alternate" type="text/html"/>
    <updated>2021-08-22T08:35:37+02:00</updated>
    <id>https://nielsberglund.com/2021/08/22/interesting-stuff---week-34-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="misc">Misc.</h2>

<ul>
<li><a href="https://headleysj.medium.com/having-testers-made-my-team-worse-99d4cb9866aa">Having testers made my team worse</a>. This post, from a colleague and mate of mine, has caused a bit of stir here at <a href="/derivco">Derivco</a>. In the post, Simon talks about how his team lost all of their functional testers and how due to this, the developers had to sort out their CI/CD pipelines and write meaningful tests. This lead them to be in a much better position at the end of the day than before. I mentioned how the post had caused a stir; the stir was from the testers in the company. After reading the post, I thought it would be the developers wanting to &ldquo;lynch&rdquo; Simon. Primarily due to this: <em>but because having testers meant that as developers, we could get away with being lazy and not truly putting in the effort to write meaningful tests that run both in our CI and CD pipelines</em>. Anyway, it is an excellent post - I suggest you read it and think about how you can approve your pipelines and testing while reading it.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/ksqldb-2-0-introduces-date-and-time-data-types/">Announcing ksqlDB 0.20.0</a>. As the title says; ksqlDB version 0.20 is out &ldquo;in the wild&rdquo;. One big new feature of this version is support for <code>DATE</code> and <code>TIME</code> datatypes! Very cool!!</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>In last week&rsquo;s roundup, I mentioned how one of the webinars I presented a week or two back had still not come up on YouTube. Well, one or two days after I published the post, there it was:</p>

<ul>
<li><a href="https://youtu.be/DdyZgFErLFI"><strong>Let SQL Server Be the Central Hub For All Types of Data</strong></a>. In this webinar, I look at <strong>SQL Server 2019 Big Data Cluster</strong> and how Microsoft positions it to be a central hub for all types of data - not only relational data.</li>
</ul>

<p>Oh, don&rsquo;t forget the register for <a href="https://azurebootcamp.co.za/"><strong>Azure Bootcamp 2021 South Africa</strong></a>. It will be a fantastic event, and I have just submitted some talks to it. Hopefully, one or two will be accepted.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 33, 2021]]></title>
    <link href="https://nielsberglund.com/2021/08/15/interesting-stuff---week-33-2021/" rel="alternate" type="text/html"/>
    <updated>2021-08-15T09:27:43+02:00</updated>
    <id>https://nielsberglund.com/2021/08/15/interesting-stuff---week-33-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://eng.uber.com/challenges-opportunities-to-reduce-cost-ubers-big-data/">Challenges and Opportunities to Dramatically Reduce the Cost of Uber&rsquo;s Big Data</a>. I think we all agree that Big Data is good. However, there is no doubt that Big Data incurs costs, especially in large organisations. This post from Uber looks at the top challenges they had when assessing their Big Data Platform&rsquo;s costs and the overall strategy they devised to address them. Very interesting!</li>
<li><a href="https://eng.uber.com/cost-efficient-big-data-platform/">Cost-Efficient Open Source Big Data Platform at Uber</a>. Another post by Uber. In the previous post, Uber discussed their initiative to reduce costs on their data platform. They looked at three broad pillars: platform efficiency, supply, and demand. In this post, they discuss the efforts to improve the efficiency of the data platform and bring down costs.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://engineering.linkedin.com/blog/2021/lambda-learner--nearline-learning-on-data-streams">Lambda Learner: Nearline learning on data streams</a>. In this post, LinkedIn discusses an in-house system called Lambda Learner. Lambda Learner is a library for iterative, incremental training of a class of supervised machine learning models. The discussion is about how the Lambda Learner system allows for near real-time re-training of machine learning models. This is a very interesting post!</li>
<li><a href="/2021/08/14/run-confluent-cloud--serverless-apache-kafka-on-azure/">Run Confluent Cloud &amp; Serverless Apache Kafka on Azure</a>. This is a post by yours truly. As you may know, I have some conferences coming up, and Azure features in quite a few of the talks, together with Apache Kafka. I thought it would be cool if I could run Apache Kafka on Azure and bonus points if I could run it as SaaS, i.e. Confluent Cloud. So in this post, I look at what it takes to deploy Confluent Cloud on Azure.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>Apart from publishing the blog post mentioned above, I am prepping for the upcoming <a href="https://dataplatformgeeks.com/dps2021/"><strong>2021 Data Platform Summit</strong></a>. Speaking about the <strong>Data Platform Summit</strong>, the organizers have managed to increase the capacity of the virtual platform to 10,000! So, they have opened up <strong>FREE</strong> booking for <strong>LIVE</strong> attendance for a limited time. They have an internal quota, and once that is full, the free booking will close. Hurry up to <a href="https://dataplatformgeeks.com/dps2021/complimentary-registration"><strong>https://dataplatformgeeks.com/dps2021/complimentary-registration</strong></a> to register for <strong>FREE</strong>!</p>

<p>Related to conferences; during the last couple of weeks, I did two webinars, of which one is up on YouTube (I expect the other one to be up soon as well):</p>

<ul>
<li><a href="https://youtu.be/dmsM_NKjFGs"><strong>Stream Processing with Apache Kafka and .NET</strong></a>. A presentation about Apache Kafka for the .NET developer and some stuff about stream-processing and ksqlDB. Due to a power failure, there is a break and some distortion in this video; sorry about that.</li>
</ul>

<p>Obviously I&rsquo;ll let you know when the second webinar is up.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Run Confluent Cloud &amp; Serverless Apache Kafka on Azure]]></title>
    <link href="https://nielsberglund.com/2021/08/14/run-confluent-cloud--serverless-apache-kafka-on-azure/" rel="alternate" type="text/html"/>
    <updated>2021-08-14T12:58:24+02:00</updated>
    <id>https://nielsberglund.com/2021/08/14/run-confluent-cloud--serverless-apache-kafka-on-azure/</id>
    <content type="html"><![CDATA[<p>For readers of my blog, it should not come as a surprise that I think Apache Kafka is <a href="https://www.merriam-webster.com/dictionary/the%20best%2Fgreatest%20thing%20since%20sliced%20bread">&ldquo;the greatest thing since sliced bread&rdquo;</a>, and I have written <a href="https://nielsberglund.com/categories/kafka/">some posts</a> about it. The posts I have written has been geared towards the setup/deployment of Kafka, with a Windows/.NET slant to it. This post is in the same vein; how to set up Kafka but in the cloud.</p>

<p>The &ldquo;conference season&rdquo; is upon us, and I have some conference talks coming up. This year quite a lot of what I am talking about is in the cloud, specifically Azure. As some of the talks involve Apache Kafka, I thought it would be good if Kafka also ran in the cloud. Sure, I could always run Kafka on Docker on a VM in the cloud, but &ldquo;they&rdquo; say serverless is the new &ldquo;in&rdquo; thing, as well as managed services, so why not try that out.</p>

<p>So, this post is about how to deploy Confluent Cloud on Azure!</p>

<p></p>

<h2 id="background">Background</h2>

<p>Let us start with what Confluent Cloud is. I shamelessly stole the following paragraph from <a href="https://docs.confluent.io/cloud/current/get-started/index.html">here</a>:</p>

<p><em>Confluent Cloud is a resilient, scalable streaming data service based on Apache Kafka, delivered as a fully managed service. Confluent Cloud has a web interface and local command line interface. You can manage cluster resources, settings, and billing with the web interface.</em></p>

<p>OK, enough of marketing talk; Confluent Cloud is the Confluent Platform running as a managed service in the cloud, and you can run it on Azure, AWS, and Google Cloud. It was <a href="https://www.confluent.io/blog/announcing-confluent-cloud-apache-kafka-as-a-service/">introduced in 2017</a>, and - if I remember correctly - it initially ran on the Google Cloud Platform (GCP). Shortly after the introduction on GCP, it also became available on AWS.</p>

<p>Towards the end of 2019, Confluent and Microsoft released Confluent Cloud on Azure, but it was like a separate service; you did not provision it from the Azure Portal. You provisioned it from Confluent and chose Azure as your platform. One of the drawbacks with this was that you had to sign up specifically for Confluent Cloud and provide a credit card, as the billing was separate. This changed at the beginning of 2020 when Confluent Cloud became wholly integrated with Azure.</p>

<p>The cost for Confluent Cloud is now billed to your Azure subscription, and you provision a Confluent Cloud cluster from the Azure Marketplace! In the rest of this post, we see how that is done!</p>

<h2 id="pre-reqs">Pre-Reqs</h2>

<p>This post would not be a &ldquo;Niels post&rdquo; if there were no pre-reqs section. So, the pre-reqs - if you want to follow along - are:</p>

<ul>
<li>An Azure account. If you don&rsquo;t have an Azure subscription, sign up for a <a href="https://azure.microsoft.com/en-us/free/">free account</a>.</li>
</ul>

<p>That was not so bad, was it? Oh, one more thing; in your Azure subscription, you need to have the role of <em>Owner</em> or <em>Contributor</em>.</p>

<h2 id="provision-confluent-cloud">Provision Confluent Cloud</h2>

<p>When we provision Confluent Cloud on Azure, what we do is we are creating a Confluent Cloud organization inside Azure. A Confluent Cloud organization is a resource that provides the mapping between the Azure and Confluent Cloud resources. It&rsquo;s the parent resource for other Confluent Cloud resources.</p>

<p>Within a Confluent organization, you can create multiple environments, clusters, topics, and connectors. The environments, clusters, etc., are created from within Confluent Cloud <em>Software as a Service</em> (SaaS) resources.</p>

<h4 id="create-confluent-cloud-organization">Create Confluent Cloud Organization</h4>

<p>To create a Confluent organization, we must be signed in to your subscription in the <a href="https://portal.azure.com/">Azure Portal</a>.</p>

<p>What you see after you have signed in my vary, but what you want to do is to <em>Create a resource</em>:</p>

<p><img src="/images/posts/ccl-azure-create-resource.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Create Resource</em></p>

<p>I create a resource in Azure by expanding the hamburger menu in the upper left-hand corner of the portal (outlined in blue in <em>Figure 1</em>), and then click on <em>Create a resource</em> (outlined in red in <em>Figure 1</em>). You may see <em>Azure Services</em> in the portal, and underneath <em>Azure Services</em>, you can click on <em>Create a resource</em>, outlined in yellow in <em>Figure 1</em>. Clicking on <em>Create a resource</em> takes you to a page with a search box:</p>

<p><img src="/images/posts/ccl-azure-confl-cloud.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Search for Confluent Cloud</em></p>

<p>You start to write <em>Confluent Clo &hellip;</em> in the search box as you see in <em>Figure 2</em>. Text completion &ldquo;pops up&rdquo; some options where <em>Apache Kafka on Confluent Cloud</em> is one (outlined in red). You choose that one, you hit enter and you see:</p>

<p><img src="/images/posts/ccl-azure-setup-subscribe.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Setup and Subscribe</em></p>

<p>Choosing <em>Apache Kafka on Confluent Cloud</em> as in <em>Figure 2</em> takes you to the page where you start the process of setting up Apache Kafka on Confluent Cloud. When you click on the <em>Setup + subscribe</em> button outlined in red in <em>Figure 3</em> you see something like so:</p>

<p><img src="/images/posts/ccl-azure-create-org2.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Create Organization</em></p>

<p>I mentioned above about Confluent Cloud organizations, and we see in <em>Figure 4</em> the first step in creating an organization. You provide values for the following properties:</p>

<ul>
<li><strong>Subscription</strong>: the Azure subscription to deploy to.</li>
<li><strong>Resource group</strong>: here, you choose an existing resource group. There is the option to create a new resource group, but I got an error when the deployment started when I had chosen to create a new group. The error was along the lines of that the resource group needed a location. So what I do now is to first create a resource group (and define a region) and use that resource group.</li>
<li><strong>Confluent organization name</strong>: This is the name of the Software as a Service resource.</li>
<li><strong>Region</strong>: in what Azure region that you want to place this deployment.</li>
<li><strong>Plan</strong>: the billing plan. I  most instances, this is <em>Pay as you Go</em>.</li>
<li><strong>Billing term</strong>: prefilled based on your chosen <em>Plan</em>.</li>
<li><strong>Price</strong>: as with <em>Billing term</em>, it is prefilled.</li>
</ul>

<p>Having filled in the values for the properties above, you can now click the <em>Review + create</em> button (outlined in red in <em>Figure 4</em>), and you see:</p>

<p><img src="/images/posts/ccl-azure-create-org-validation.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Validate Organization</em></p>

<p>Clicking the <em>Review + create</em> starts a validation of the values you have entered. In <em>Figure 5</em> we see what it looks like when the validation has successfully passed.</p>

<p>We can now go ahead and create and deploy the organization. At the bottom of the page, we see in <em>Figure 5</em> (we only see the top part in <em>Figure 5</em>) is a <em>Create</em> button. Clicking that button starts the deployment. After a while, you see:</p>

<p><img src="/images/posts/ccl-azure-org-complete.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>Deployment Complete</em></p>

<p>As we see in <em>Figure 6</em>, the deployment has now been completed, and everything should be set up. To see that everything is indeed set up, click on the <em>Go to resource</em> button outlined in red in <em>Figure 6</em>. Clicking on that button results in something like so:</p>

<p><img src="/images/posts/ccl-azure-manage-org.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>Manage on Confluent Cloud</em></p>

<p>In <em>Figure 7</em>, we see how we are inside the Confluent organization, and we are more or less done. However, remember how I above mentioned that the organization we just have created is the mapping between Azure and Confluent Cloud resources, and how we use Confluent Cloud resources to create Kafka clusters, Topics, etc. So, to seamlessly move between Azure and Confluent Cloud, we need to enable Single Sign-On (SSO).</p>

<h4 id="single-sign-on-sso">Single Sign-On (SSO)</h4>

<p>Enabling SSO allows us to transparently move from Azure to Confluent Cloud and directly login to Confluent Cloud with an SSO URL.</p>

<p>To enable SSO, we click on the link outlined in red in <em>Figure 7</em>. Clicking on that link gives us this:</p>

<p><img src="/images/posts/ccl-azure-sso-perm.png" alt="" /></p>

<p><strong>Figure 8:</strong> <em>Permissions</em></p>

<p>In <em>Figure 8</em>, we see a request for permissions from Confluent Cloud to do what is outlined in <em>Figure 8</em>. That is the final thing we do in Azure because when we accept the request, we see something like so:</p>

<p><img src="/images/posts/ccl-azure-welcome.png" alt="" /></p>

<p><strong>Figure 9:</strong> <em>Welcome</em></p>

<p>We have now arrived at Confluent Cloud, and the <em>Welcome</em> page, as we see in <em>Figure 9</em>, asks us some questions about what we want to do with Kafka, our experience, etc.  I am not that interested in answering those questions so I just hit the <em>Skip</em> link we see outlined in red in <em>Figure 9</em>, and I see this:</p>

<p><img src="/images/posts/ccl-azure-welcome-done.png" alt="" /></p>

<p><strong>Figure 10:</strong> <em>Ready to Roll</em></p>

<p>Right, we are done with creating the organization, and we have enabled SSO. As we see in <em>Figure 10</em> we can now start doing the &ldquo;cool stuff&rdquo;! However, let us not click on any links on this page, but just close down the page and go back to Azure and our Confluent organization in Azure.</p>

<p>Having enabled SSO, we have three ways of logging into/signing-in to our Confluent Cloud SaaS resources:</p>

<ul>
<li>from inside Azure Portal: by clicking on the <em>Manage on Confluent Cloud</em> link in <em>Figure 7</em> (the one we used to set up SSO).</li>
<li>from the <a href="https://confluent.cloud"><strong>Confluent Cloud</strong> login page</a>, where we use the email and password we use to authenticate against Azure.</li>
<li>use the SSO login URL. You get the URL by right-clicking on the <em>Manage on Confluent Cloud</em> link and choose <em>Copy link</em>.</li>
</ul>

<p>Up until now we have created a Confluent Cloud organization in Azure and registered that organization with Confluent Cloud. It is now time to do the interesting stuff: Kafka Clusters, Topics etc.</p>

<h2 id="create-a-kafka-cluster">Create a Kafka Cluster</h2>

<p>Login to Confluent Cloud by using one of the methods above. After having logged in we see something like so:</p>

<p><img src="/images/posts/ccl-azure-defaukt-environment.png" alt="" /></p>

<p><strong>Figure 11:</strong> <em>Default Environment</em></p>

<p>At the beginning of this post, I said how a Confluent Cloud organization was the parent resource for other Confluent Cloud resources. When we create an organization, an Environment is also created by default (named <code>default</code>), which we see in <em>Figure 11</em>.</p>

<blockquote>
<p><strong>NOTE:</strong> We can change the name of the environment by clicking on the link in <em>Figure 11</em> outlined in yellow.</p>
</blockquote>

<p>An environment is a container for Kafka clusters, and we see in <em>Figure 11</em> a button (outlined in red) <em>Create cluster on my own</em>. When clicking that button we get:</p>

<p><img src="/images/posts/ccl-azure-create-cluster-I.png" alt="" /></p>

<p><strong>Figure 12:</strong> <em>Create Cluster - I</em></p>

<p>In <em>Figure 12</em>, we see the <em>Create cluster</em> page we end up on after clicking the <em>Create cluster on my own</em> in <em>Figure 11</em>. We see how we can choose what cluster type to create, and for our purposes, the <em>Basic</em> type is more than enough. Having selected the cluster type, we are now ready to configure the cluster. Clicking on the button outlined in red in <em>Figure 12</em>: the <em>Begin configuration</em> button, we see this:</p>

<p><img src="/images/posts/ccl-azure-create-cluster-II.png" alt="" /></p>

<p><strong>Figure 13:</strong> <em>Create Cluster - II</em></p>

<p>The first thing to do when configuring a new cluster is what we see in <em>Figure 13</em>, we choose:</p>

<ul>
<li>the region we want the cluster in. A good practice is to choose the same region as our resource group and organization.</li>
<li>the level of availability we want: single zone or multi. In our case, using the <em>Basic</em> cluster type, single-zone is our only choice.</li>
</ul>

<p>Having decided on the <em>Region</em> we click <em>Continue</em> (outlined in red), and we get this:</p>

<p><img src="/images/posts/ccl-azure-create-cluster-III.png" alt="" /></p>

<p><strong>Figure 14:</strong> <em>Create Cluster - III</em></p>

<p>What is left for us to do is to give the cluster a name. In <em>Figure 14</em>, we see how I named it <code>test_cluster_1</code> (outlined in yellow). Then I click on the <em>Launch cluster</em> button, which in <em>Figure 14</em> is outlined in red:</p>

<p><img src="/images/posts/ccl-azure-cluster-overview.png" alt="" /></p>

<p><strong>Figure 15:</strong> <em>Cluster Created</em></p>

<p>The cluster is now created, and we are presented with an overview of the cluster as in <em>Figure 15</em>. For you who have used Confluent Platform, this looks almost like the overview page in Confluent Control Center (the Web UI for Confluent Platform). We see in <em>Figure 15</em>:</p>

<ul>
<li>outlined in blue: a menu covering the cluster&rsquo;s main components; topics, ksqlDB, etc.</li>
<li>red: the link to create and manage topics.</li>
<li>green: to connect to the cluster from the outside world, we need an API key (and a secret). The link here allows us to create that.</li>
<li>yellow: a link to instructions on downloading and installing the Confluent Cloud CLI (CCLI). You can definitely manage your cluster, topics, etc., using the Web UI, but to be more efficient, you want to use the Confluent Cloud CLI.</li>
</ul>

<blockquote>
<p><strong>NOTE:</strong> Above the CCLI box is a link giving examples of how to configure clients. Very useful.</p>
</blockquote>

<p>Speaking about clients: to access the cluster and publish to or consume from a topic you need the address of the cluster. If you click on the <em>Cluster settings</em> link we see in <em>Figure 15</em> you get all sorts of information about the cluster, including the the value of <code>bootstrap.servers</code> that clients need.</p>

<p>OK, so we now have a cluster; let us create a topic.</p>

<h2 id="create-topic-s">Create Topic(s)</h2>

<p>Click on the <em>Topics</em> link outlined in red in <em>Figure 15</em>:</p>

<p><img src="/images/posts/ccl-azure-topics-create-I.png" alt="" /></p>

<p><strong>Figure 16:</strong> <em>Topics</em></p>

<p>What we see in <em>Figure 16</em> should be self-explanatory. It is the page for our topics in this cluster. We don&rsquo;t have any topics yet, so we create one by clicking on the <em>Create topic</em> button outlined in red in <em>Figure 16</em>. Doing that, we see:</p>

<p><img src="/images/posts/ccl-azure-topics-new.png" alt="" /></p>

<p><strong>Figure 17:</strong> <em>New Topic</em></p>

<p>In <em>Figure 17</em>, we see the form for creating a new topic. We fill in the <em>Topic name</em> field outlined in blue, we set the number of partitions we want in the field outlined in yellow, and then we are ready to click the <em>Create with defaults</em> button.</p>

<blockquote>
<p><strong>NOTE:</strong> By default number of partitions is set to 6.</p>
</blockquote>

<p>When we click on the <em>Create with defaults</em> button, the result looks like so:</p>

<p><img src="/images/posts/ccl-azure-topics-topic.png" alt="" /></p>

<p><strong>Figure 18:</strong> <em>Topic Created</em></p>

<p>Yay - we have a topic! In <em>Figure 18</em>, we see an overview of the topic, and we see we have some tabs: <em>Overview</em>, <em>Messages</em>, <em>Schema</em>, and <em>Configuration</em>. I will not go into detail about the various tabs, but - nevertheless - let us have a brief look at the <em>Messages</em> tab</p>

<h4 id="messages">Messages</h4>

<p>Click on the <em>Messages</em> tab and you see something like so:</p>

<p><img src="/images/posts/ccl-azure-topics-messages.png" alt="" /></p>

<p><strong>Figure 19:</strong> <em>Messages</em></p>

<p>As we see in <em>Figure 19</em>, the <em>Messages</em> tab has to do with messages - duh! I mention this tab because it gives us the ability to quickly produce messages to the topic and view messages that have been published to the topic. Being able to publish messages gives us a quick and easy way to ensure the topic is set up correctly. Viewing messages is good because we can quickly ensure messages have arrived in the topic.</p>

<p>To publish, we click on what is outlined in red in <em>Figure 19</em>, and we see the following:</p>

<p><img src="/images/posts/ccl-azure-topics-publish.png" alt="" /></p>

<p><strong>Figure 20:</strong> <em>Publish</em></p>

<p>In <em>Figure 20</em>, we see how a message in JSON format has been generated (you can edit the message as you want), and we publish it by clicking on the <em>Produce</em> button outlined in red. Clicking on the button, you see a notification saying the message is being processed. When the notification disappears, you enter an offset in the field outlined in blue in <em>Figure 19</em>. Enter <code>0</code>, hit return, and you see under the <em>Produce</em> button something like so:</p>

<p><img src="/images/posts/ccl-azure-topics-published=message.png" alt="" /></p>

<p><strong>Figure 21:</strong> <em>Published Message</em></p>

<p>Another Yay! We have a message in the topic, as we see in <em>Figure 21</em>.</p>

<blockquote>
<p><strong>NOTE:</strong> When you are <del>playing around</del> doing tests, you may want to delete all messages from a topic. The only way to do it from the UI is to delete the topic. You then need to re-create it. To delete the topic, you go to the <em>Configuration</em> tab we see in <em>Figure 18</em> (and 19) and click on the <em>Delete topic</em> link in the lower left-hand corner.</p>
</blockquote>

<p>We are almost done. What is left is to create an API key and a secret. We use the API key and the secret to access the cluster from the outside world.</p>

<h2 id="api-access">API Access</h2>

<p>To enable API access, we use the <em>API access</em> link we see outlined in green in <em>Figure 15</em>. Clicking on that link results in this:</p>

<p><img src="/images/posts/ccl-azure-api-access-I.png" alt="" /></p>

<p><strong>Figure 22:</strong> <em>API Access</em></p>

<p>In <em>Figure 22</em> we click on the <em>Create Key</em> button, which is outlined in red, and we get this:</p>

<p><img src="/images/posts/ccl-azure-api-create-key-I.png" alt="" /></p>

<p><strong>Figure 23:</strong> <em>Create API Key</em></p>

<p>An API key can have different scope. In our case, we choose the create the with Global scope as we see in <em>Figure 24</em> and having selected the scope, we click <em>Next</em> (outlined in red in <em>Figure 23</em>):</p>

<p><img src="/images/posts/ccl-azure-api-create-key-II.png" alt="" /></p>

<p><strong>Figure 24:</strong> <em>API Key Created</em></p>

<p>Please read the highlighted section, in <em>Figure 24</em>, carefully!</p>

<p>After creating the key, you see the key and the secret as in <em>Figure 24</em>. However, you will no longer see the secret after clicking the <em>Save</em> button outlined in red. So ensure you have written down the key and the secret somewhere (and remember where you saved it to).</p>

<h2 id="summary">Summary</h2>

<p>This post looked at how to run Apache Kafka in Confluent Cloud, where Azure is our cloud environment.</p>

<p>We saw how we:</p>

<ul>
<li>Create a Confluent Cloud organization from within Azure.</li>
<li>Enable SSO between Azure and Confluent Cloud.</li>
</ul>

<p>The above creates the organization in Azure and &ldquo;registers&rdquo; it with Confluent Cloud. We then:</p>

<ul>
<li>Log into the Confluent Cloud.</li>
<li>Create a Kafka cluster.</li>
<li>In the Kafka cluster, we create one or more topics.</li>
</ul>

<p>To connect to the cluster from the &ldquo;outside&rdquo; world, we let Confluent Cloud generate an API key with a corresponding secret.</p>

<p>That&rsquo;s it!</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 32, 2021]]></title>
    <link href="https://nielsberglund.com/2021/08/08/interesting-stuff---week-32-2021/" rel="alternate" type="text/html"/>
    <updated>2021-08-08T09:09:24+02:00</updated>
    <id>https://nielsberglund.com/2021/08/08/interesting-stuff---week-32-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="big-data-data-analytics">Big Data / Data Analytics</h2>

<ul>
<li><a href="https://medium.com/event-driven-utopia/building-a-low-latency-fitness-leaderboard-with-apache-pinot-40a4da672cf0">Building a Low-Latency Fitness Leaderboard with Apache Pinot</a>. The terms user-facing/site-facing analytics are &ldquo;popping up&rdquo; more and more. When I first heard it, I was pretty confused (pretty typical for me) about what it means - analytics is analytics, after all. But when reading this post, it dawned on me what it is. However, I won&rsquo;t &ldquo;spoil&rdquo; the explanation here. Apart from explaining what user-facing analytics mean, this post covers using Apache Pinot to ingest fitness band events from a Kafka topic and make them available for immediate querying. Very cool!</li>
<li><a href="https://eng.uber.com/orders-near-you/">&lsquo;Orders Near You&rsquo; and User-Facing Analytics on Real-Time Geospatial Data</a>. When it rains, it pours, hey? Another post about user-facing analytics and Apache Pinot. In this post, Uber explains the implementation of the &lsquo;Orders Near You&rsquo; feature and how they generate insights across geospatial data.</li>
<li><a href="https://eng.uber.com/ubers-finance-computation-platform/">Uber&rsquo;s Finance Computation Platform</a>. For a company of Uber&rsquo;s size and scale, it is required to have robust, accurate, and compliant accounting and analytics. The post looks at how they built their own in-house platform - the Finance Computation Platform - to meet their demanding requirements.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/confluent-developer-launches-free-apache-kafka-courses-and-tutorials-online/">The New One-Stop Shop for Learning Apache Kafka</a>. This is awesome, awesome, awesome! Did I say it was awesome? OK, Niels, calm down - what is this? The post announces an all-new website dedicated to Apache Kafka, event streaming, and associated cloud technologies. As the title says, the site is really a one-stop-shop for everything Kafka! Have a look at the various courses they offer - it is a gold mine!</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>Well, apart from spending waaaayyy too much time on <a href="https://developer.confluent.io/"><strong>Confluent Developer</strong></a>, I am prepping for the upcoming <a href="https://dataplatformgeeks.com/dps2021/"><strong>2021 Data Platform Summit</strong></a> where I am doing one conference presentation:</p>

<ul>
<li><strong>How to do Real-Time Analytics Using Apache Kafka and Azure Data Explorer</strong>: This session shows how to do near-real-time analysis on data streaming from Apache Kafka (running on Confluent Cloud in Azure) using Azure Data Explorer.</li>
</ul>

<p>In addition to the session above, I am also doing an eight-hour post-con training class (split over two days):</p>

<ul>
<li><a href="https://dataplatformgeeks.com/dps2021/product/big-data-analytics-with-sql-server-2019-big-data-cluster-by-niels-berglund/"><strong>Big Data &amp; Analytics with SQL Server 2019 Big Data Cluster</strong></a>: This training covers big data, data virtualization and analytics in SQL Server 2019 Big Data cluster. There are still some seats left, so you can <a href="https://dataplatformgeeks.com/dps2021/product/big-data-analytics-with-sql-server-2019-big-data-cluster-by-niels-berglund/">sign up here</a> if you are interested. Apart from getting to know BDC, an added benefit of signing up is getting a free submission to the summit!</li>
</ul>

<p>Lately, I have been investigating SQL Server CDC and the use of Debezium to publish data from SQL Server. For my investigations, I have used Kafka running in Docker. Every time I have set this up, I have struggled with deploying the Debezium SQL Server Connector to the Kafka Connect container. I finally decided to write a blog post about so I have something to go back to for next time, and I published the post yesterday:</p>

<ul>
<li><a href="/2021/08/07/how-to-deploy-the-debezium-sql-server-connector-to-docker/"><strong>How to Deploy the Debezium SQL Server Connector to Docker</strong></a></li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to Deploy the Debezium SQL Server Connector to Docker]]></title>
    <link href="https://nielsberglund.com/2021/08/07/how-to-deploy-the-debezium-sql-server-connector-to-docker/" rel="alternate" type="text/html"/>
    <updated>2021-08-07T06:02:12+02:00</updated>
    <id>https://nielsberglund.com/2021/08/07/how-to-deploy-the-debezium-sql-server-connector-to-docker/</id>
    <content type="html"><![CDATA[<p>I have been doing a couple of conference talks lately (virtual, of course) about streaming data from SQL Server to Kafka. The title of the presentation is <strong>Free Your SQL Server Data With Kafka</strong>.</p>

<p>In the presentation, I talk (and show) various ways of getting data from SQL Server to Kafka. One of the ways I cover is Microsoft CDC, together with Debezium.</p>

<p>When I do the presentation, I always have a SQL Server installed locally, and I run Kafka in Docker. Without fail, every time I set up the environment, I cannot remember how to deploy the Debezium SQL Server Connector into Docker. Therefore I decided to write this post to have something to go back to for next time.</p>

<p></p>

<blockquote>
<p><strong>NOTE:</strong> This post does not cover the intricacies of how to configure Debezium for SQL Server. I leave that for a future post.</p>
</blockquote>

<h2 id="background">Background</h2>

<p>Before diving into how to do this, let us look at the moving parts of this.</p>

<h4 id="kafka-connect">Kafka Connect</h4>

<p>Kafka Connect is a tool for streaming data between Apache Kafka and other systems in a scalable and reliable way. The way you move data between systems and Kafka is using connectors, and there are two flavors of connectors:</p>

<ul>
<li>Source connectors which understand how to interact with the source system send records into Kafka</li>
<li>Sink connectors that propagate records from Kafka topics to other systems.</li>
</ul>

<p>Kafka Connect is a JVM process, and it operates separately from the Kafka Broker. Connectors are <code>.jar</code> files loaded by the connect process. The diagram below shows a high-level overview of what it looks like:</p>

<p><img src="/images/posts/kafka-connect-1.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Kafka Connect Overview</em></p>

<p>In <em>Figure 1</em> we see, (from left to right):</p>

<ul>
<li>Source systems, i.e. systems we want to get data from. These systems can be databases, Hadoop, files, etc.</li>
<li>The Kafka Connect worker with source connectors. The connectors know how to interact with the source system, whether querying a database, using CDC, reading from a filesystem, etc. The connectors publish data to Kafka topics.</li>
<li>The Kafka broker(s). The broker(s) contain topics that are the &ldquo;sinks&rdquo; for the source connectors.</li>
<li>Kafka Connect worker with sink connectors. Source and sink connectors can be in the same Kafka Connect worker. The sink connectors know how to consume events from Kafka topics and ingest them into sink systems.</li>
<li>Sink systems. These are systems we ingest data into. As with source systems, these can be databases, Hadoop, files, etc.</li>
</ul>

<h4 id="debezium">Debezium</h4>

<p>Debezium is an open source distributed platform for change data capture, (I &ldquo;stole&rdquo; the preceding shamelessly from <a href="https://debezium.io/">here</a>). It captures changes in your database(s) and publishes those changes to topics in Kafka.</p>

<blockquote>
<p><strong>NOTE:</strong> Debezium <em>can</em> work without a Kafka cluster, in which case it is embedded in your application, and the application receives the change notifications. Read more about that <a href="https://debezium.io/documentation/reference/1.4/development/engine.html">here</a>.</p>
</blockquote>

<p>Debezium has Kafka Connect connectors for a multitude of source systems. When interacting with Kafka, the connector(s) is deployed to Kafka Connect.</p>

<p>With the above in mind, let us look at how this works with SQL Server.</p>

<h4 id="sql-server-debezium-and-kafka">SQL Server, Debezium, and Kafka</h4>

<p>As I mentioned at the beginning of this post, the aim is to get data out of some table(s) in a database(s) and stream it to a topic(s) in Kafka.</p>

<p>We do not necessarily need to use Debezium as there are other Kafka Connect connectors. We could, for example, use the Confluent SQL Server connector. However, as we want to stream the data in near real-time, with the least amount of work on our side, the Debezium connector is our choice. Coming back to <em>Figure 1</em> it would look something like so:</p>

<p><img src="/images/posts/kafka-connect-cdc.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Kafka Connect SQL Server &amp; Debezium</em></p>

<p>We see in <em>Figure 2</em> how the source system is SQL Server and how the source connector is the Debezium SQL Server connector. In the diagram we publish to one topic as we only retrieve data from one table. If we were to retrieve data from multiple tables, we&rsquo;d publish to multiple topics.</p>

<p>We have several sink connectors reading from our topic and ingest into various sink systems.</p>

<p>Ok, enough background; before we get into the &ldquo;nitty-gritty&rdquo;, let&rsquo;s see what you need if you want to follow along.</p>

<h2 id="pre-reqs-code">Pre-reqs &amp; Code</h2>

<p>There are not many pre-reqs, but here goes:</p>

<ul>
<li><strong>SQL Server</strong>: well, duh - as we want to set up CDC and Debezium to stream data from SQL Server, we would need SQL Server installed somewhere. I have SQL Server 2019 installed on my local dev machine.</li>
<li><strong>Docker Desktop</strong>: another duh - this post is all about how to set up Kafka Connect and Debezium in Docker, so yes - we need Docker Desktop.</li>
</ul>

<h4 id="test-code">Test Code</h4>

<p>I mentioned in the beginning that this post is not about configuring Debezium to read data from SQL Server, so I won&rsquo;t discuss CDC in any detail. However, we need something to test that what we are doing works, so here&rsquo;s some code to set up a database on SQL Server:</p>

<pre><code class="language-sql">USE master;
GO

--  to start from scratch drop the database if exists
IF EXISTS(SELECT * FROM sys.databases WHERE name = 'DebeziumTest')
BEGIN
  ALTER DATABASE DebeziumTest
  SET SINGLE_USER
  WITH ROLLBACK IMMEDIATE;

  DROP DATABASE DebeziumTest;
END
GO

-- create the database
CREATE DATABASE DebeziumTest;
GO

USE DebeziumTest;
GO

-- this statement just if we don't want to drop the db, 
-- but still start over with the table
-- DROP TABLE dbo.tb_CDCTab1;

-- table which we later will CDC enable
CREATE TABLE dbo.tb_CDCTab1 (RowID int identity primary key,
                      Col1 int,
                      Col2 nvarchar(25));
GO

</code></pre>

<p><strong>Code Snippet 1:</strong> <em>DB Objects Creation Script</em></p>

<p>The code in <em>Code Snippet 1</em> creates a database, <code>DebeziumTest</code> and a table, <code>dbo.tb_CDCTab1</code> in the database. Later in the post, we enable the table for CDC. Enabling it allows us to check that our Kafka Connect/Debezium &ldquo;stuff&rdquo; works as expected. So, if you want to follow along, run the script, and after you have run it, ensure you have the database and the table.</p>

<p>Now, let us get into what we are supposed to do; to set this up in Docker.</p>

<h2 id="docker">Docker</h2>

<p>Let us start with getting the necessary Docker images and compose files.</p>

<h4 id="docker-kafka-image">Docker Kafka Image</h4>

<p>There are quite a few Docker images, and Docker composes files around for setting up Kafka and Kafka Connect. The ones I usually use are from Confluent&rsquo;s <a href="https://github.com/confluentinc/cp-all-in-one/"><code>cp-all-in-one</code></a> repository.</p>

<p>Let us get started:</p>

<pre><code class="language-bash">mkdir kafka
cd kafka
git clone https://github.com/confluentinc/cp-all-in-one.git
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Clone the Repo</em></p>

<p>In <em>Code Snippet 2</em>, we create a directory for the repo files and clone the <a href="https://github.com/confluentinc/cp-all-in-one/"><code>cp-all-in-one</code></a>. After cloning, we have a directory named <code>cp-all-in-one</code> under the <code>kafka</code> directory. The <code>cp-all-in-one</code> directory looks like so:</p>

<p><img src="/images/posts/kafka-connect-docker-1.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Kafka Connect SQL Server &amp; Debezium</em></p>

<p>In <em>Figure 3</em>, we see that <code>cp-all-in-one</code> (outlined in red) has some sub-directories. These directories contain Docker Compose files for various setups of Kafka. We are interested in the directory outlined in blue: <code>cp-all-in-one</code>, (yeah I know - the same name as the parent directory).</p>

<p>A quick side note here about <code>cp-all-in-one</code>. This directory contain the image for Confluent Platform, which is the enterprise edition of Confluent. As with most enterprise editions this requires a licennse. However, with the introduction of Confluent Platform 5.2 &ldquo;back in the day&rdquo;, Confluent <a href="https://www.confluent.io/blog/introducing-confluent-platform-5-2">announced</a> that Confluent Platform is &ldquo;free forever&rdquo; on a single Kafka broker! In other words, it is like a &ldquo;Developer Edition&rdquo; of Confluent Platform. The benefit of using Confluent Platform is that you get ALL the goodies, including <strong>Control Center</strong>, which is the WEB UI for Confluent Platform.</p>

<p>When going into that directory, we see a <code>docker-compose.yml</code>. Opening it in an editor, we see the various images deployed as services when running the <code>docker-compose</code> command. From a Kafka Connect perspective, we are interested in the <code>connect</code> service (and image):</p>

<p><img src="/images/posts/kafka-connect-docker-connect-yml.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Docker Compose - Connect Service</em></p>

<p>What <em>Figure 3</em> shows is a &ldquo;condensed&rdquo; part of the <code>yml</code> for the Kafka Connect image. There are three outlined section in the figure:</p>

<ul>
<li>Outlined in green: <code>image</code>, the image the service is built on.</li>
<li>Yellow: <code>CONNECT_REST_PORT</code>, the port to access the service on.</li>
<li>Red: <code>CONNECT_PLUGIN_PATH</code>, the path from where the service loads plugins (connectors).</li>
</ul>

<p>We see in <em>Figure 3</em> how the image is <code>cp-server-connect-datagen</code>. That image is an image containing some &ldquo;base&rdquo; connectors and also tools for generating data. There are other Connect images, and we see one other later in this post. But for now, let us use this image.</p>

<h2 id="run-kafka-connect">Run Kafka &amp; Connect</h2>

<p>Having retrieved the required files as in <em>Code Snippet 2</em> it is time to &ldquo;spin up&rdquo; the Kafka cluster, including Kafka Connect. We do that by <code>cd</code>:ing into the directory where the <code>docker-compose.yml</code> file is and execute: <code>docker-compose up -d</code>.</p>

<p>When running the code you see how Docker is pulling images, starts up the various services, and finally:</p>

<p><img src="/images/posts/kafka-connect-docker-compose-up.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Kafka Started</em></p>

<p>We see in <em>Figure 4</em> how all services have started. Well, that is not exactly true - some services are still &ldquo;spinning&rdquo; up, but after a minute or two, you can browse to the Confluent Control Center and see your Kafka cluster in all its &ldquo;glory&rdquo;:</p>

<p><img src="/images/posts/kafka-connect-control-center.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Confluent Control Center</em></p>

<p>In <em>Figure 5</em>, we see the overview page for the Kafka cluster. Outlined in red at the top, we see how we access it from port 9021 on the box where Docker runs. Outlined in blue, we see that we have one Connect cluster.</p>

<h4 id="installed-connectors">Installed Connectors</h4>

<p>I mentioned above that the Connect image has some connectors installed by default. To see what connectors are pre-installed, we use the Kafka REST API:</p>

<pre><code class="language-bash">GET http://127.0.0.1:8083/connector-plugins
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>GET Connectors</em></p>

<p>We can use whatever tool we are comfortable with to call the REST API (<code>curl</code>, Postman, etc.). Personally, I prefer Postman, and in <em>Code Snippet 3</em> we see how we call into the <code>connector-plugins</code> endpoint and how we use the port I mentioned above: <code>8083</code>:</p>

<p><img src="/images/posts/kafka-connect-get-connectors.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>Installed Connectors</em></p>

<p>Executing the code in <em>Code Snippet 3</em> I get the result we see in <em>Figure 6</em>. We see various Connect connectors, but nothing for SQL Server. So, we need to install it somehow.</p>

<h2 id="installing-debezium-sql-server-connector">Installing Debezium SQL Server Connector</h2>

<p>How do we go about installing a connector into the Connect service? Above I mentioned how a connector is a <code>.jar</code> file, which the service (JVM process) loads. I also mentioned the <code>CONNECT_PLUGIN_PATH</code>, which indicates where the service loads connectors from. So, with that in mind, we can imagine the process to install a connector being:</p>

<ol>
<li>Get the specific connector from &ldquo;somewhere&rdquo;.</li>
<li>Copy it into the path where the service loads connectors from.</li>
<li>Restart the service.</li>
</ol>

<p>If we ran the Kafka cluster as a local install, installing a connector would be as easy as above, but what about installing it when running Kafka in Docker containers?</p>

<p>Well, we could do something similar:</p>

<ol>
<li>Download the connector we want to install (the file is most likely &ldquo;tar&rdquo;:ed).</li>
<li>&ldquo;spin up&rdquo; the Kafka cluster.</li>
<li>Use <code>docker cp</code> to copy the connector into the connect container.</li>
<li>Use <code>docker exec</code> to get to the bash shell in the connect container.</li>
<li>Un-tar the file to the plugin load path.</li>
<li>Back out from the container and <code>docker commit</code> the changes to a new image name.</li>
<li>Tear down the running Kafka cluster: <code>docker-compose down</code>.</li>
<li>Use that image in the docker compose file in place of the &ldquo;original&rdquo; connect image.</li>
</ol>

<p>Yes, we could do something like that, and that is how I did it initially (yeah I know - I am a &ldquo;noob&rdquo;, so sue me). There are however better ways of doing it:</p>

<ul>
<li>Confluent Hub</li>
<li>Create a new image from a <code>Dockerfile</code>.</li>
</ul>

<p>Let us look at the two options.</p>

<h4 id="confluent-hub">Confluent Hub</h4>

<p>Confluent is like the App Store (or NuGet), but for Kafka. The <a href="https://docs.confluent.io/home/connect/confluent-hub/">home page</a> expresses it a lot better than what I can do:</p>

<p><em>Confluent Hub is an online library of pre-packaged and ready-to-install extensions or add-ons for Confluent Platform and Apache Kafka. You can browse the large ecosystem of connectors, transforms, and converters to find the components that suit your needs and easily install them into your local Confluent Platform environment.</em></p>

<p>Using Confluent Hub, you can install Kafka Connect connectors, whether you do it locally or in Docker. This is made possible via the <a href="https://docs.confluent.io/home/connect/confluent-hub/client.html">Confluent Hub Client</a>. The client is part of the Confluent Platform and is located in the <code>/bin</code> directory. If you are not using the Confluent Platform, you can download and install the client locally. Since I am using Confluent Platform, all is good.</p>

<p>Right, so we want to install the Debezium SQL Server connector. Let us go to the hub and look for it. Browse to <a href="https://www.confluent.io/hub/">here</a>, and in the search box, enter &ldquo;SQL Server&rdquo;, followed by a carriage return:</p>

<p><img src="/images/posts/kafka-connect-confl-hub-dbz.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>Debezium SQL Server Connector</em></p>

<p>The result coming back from the search looks like what we see in <em>Figure 7</em>; one entry: <strong>Debezium SQL Server CDC Source Connector</strong>. When you click on the result, you end up at a page with some more information about the connector, and more importantly, the syntax of how to install the connector:</p>

<pre><code class="language-bash">confluent-hub install debezium/debezium-connector-sqlserver:1.6.0
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Install SQL Server Connector</em></p>

<p>Ok, cool - so we see the syntax to install a connector in <em>Code Snippet 4</em>, but this looks suspiciously like how we do it from a bash shell. How do I do it for a Docker container without resorting to the &ldquo;hack&rdquo; I mentioned at the beginning of this section?</p>

<p>Ah, that&rsquo;s where the &ldquo;magic&rdquo; of Docker compose files comes in. It turns out that when you define a container, you can also specify configuration options. One such option is the <code>command</code> option, which allows you to execute arbitrary commands.</p>

<p>Let us edit our <code>docker-compose.yml</code> file and add the command configuration:</p>

<pre><code class="language-bash">connect:
    image: cnfldemos/cp-server-connect-datagen:0.5.0-6.2.0
    
    [snip]

    environment:

      [snip]

      CONNECT_PLUGIN_PATH: &quot;/usr/share/java,/usr/share/confluent-hub-components&quot;
      CONNECT_LOG4J_LOGGERS: org.apache.zookeeper=ERROR,org.I0Itec.zkclient=ERROR,org.reflections=ERROR
    command: 
      - bash 
      - -c 
      - |
        echo &quot;Installing connector plugins&quot;
        confluent-hub install --no-prompt debezium/debezium-connector-sqlserver:latest
        #
        echo &quot;Launching Kafka Connect worker&quot;
        /etc/confluent/docker/run &amp; 
        #
        sleep infinity

</code></pre>

<p><strong>Code Snippet 5:</strong> <em>Installing Connector from Confluent Hub in Container</em></p>

<p>In <em>Code Snippet 5</em>, we see how we use the <code>docker-compose.yml</code> file from before (heavily &ldquo;snipped&rdquo;), but we have added the <code>command</code> configuration option.</p>

<p>In the code, we see how we:</p>

<ul>
<li>&ldquo;spin up&rdquo; the bash shell.</li>
<li>set some options, <code>-c</code> and <code>|</code>.</li>
<li>executing the <code>install</code> command.</li>
<li>making sure we start up the worker process</li>
<li>we finally do <code>sleep infinity</code> to keep the container alive.</li>
</ul>

<p>It is worth noting that instead of a version number of the connector, I say <code>latest</code> to always get the latest release. Having edited the <code>docker-compose.yml</code> file, we now start the cluster: <code>docker-compose up -d</code>.</p>

<blockquote>
<p><strong>NOTE:</strong> Above I say *&hellip; get the latest <strong>release</strong> &hellip;*. What to be aware of is that the Confluent Hub only contains released versions of connectors (AFAIK).</p>
</blockquote>

<p>Wait a little while for the cluster to start up, and then use Postman to retrieve the installed connectors as in <em>Code Snippet 3</em>. Executing the Postman <code>GET</code> command, we see:</p>

<p><img src="/images/posts/kafka-connect-dbz-sql-conn.png" alt="" /></p>

<p><strong>Figure 8:</strong> <em>Installed Debezium SQL Server Connector</em></p>

<p>Yay, the Debezium SQL Server Connector is now deployed to Kafka Connect, as we see in <em>Figure 8</em> (outlined in red). What you see outlined in yellow is the datagen connector which is one of the extra parts of the <code>cp-server-connect-datagen</code> image.</p>

<h4 id="dockerfile">Dockerfile</h4>

<p>Ok, so we have seen how we can install a connector using <code>confluent-hub install</code> in the <code>command</code> option in the <code>docker-compose.yml</code>. That is awesome; however, this requires you to have an internet connection every time you &ldquo;spin up&rdquo; the Kafka cluster.</p>

<p>So, what you can do instead is build your own connect worker image, include the connector(s) you want and use that image in your compose file. To do this we need a base image and a file that tells Docker what to do to build our own image. For the base image, we can definitely use the image we have seen so far, the <code>cp-server-connect-datagen</code> image, but in reality, you want an image containing the bare minimum as the base. For that, we use the <code>cp-server-connect-base</code> image.</p>

<p>I mentioned above that we need a file telling Docker what to do. This file is essentially a build file. It is common practice to name that file <code>Dockerfile</code>. To achieve what we did above in <em>Code Snippet 5</em>, we create an empty file, name it <code>Dockerfile</code> and add the following:</p>

<pre><code class="language-bash">FROM confluentinc/cp-server-connect-base:6.2.0

RUN confluent-hub install --no-prompt \
            debezium/debezium-connector-sqlserver:latest
</code></pre>

<p><strong>Code Snippet 6:</strong> <em>File to Create Image</em></p>

<p>In <em>Code Snippet 6</em>, we see how we first define what image to use, and then we tell Docker we want to run the command to add the connector. Having saved the <code>Dockerfile</code>, we now build the image:</p>

<pre><code class="language-bash">docker build . -t dbz-sql-conn
</code></pre>

<p><strong>Code Snippet 7:</strong> <em>Docker Build</em></p>

<p>The code in <em>Code Snippet 7</em> shows the syntax to build the image, where:</p>

<ul>
<li><code>docker build</code> is the build command.</li>
<li>the <code>.</code> tells docker to pick the file named <code>Dockerfile</code>.</li>
<li>the <code>-t dbz-sql-conn</code> tags the image with a name. Here we can also assign a version number.</li>
</ul>

<p>After having run the code in <em>Code Snippet 7</em>, you can execute <code>docker images</code>, and in the list of images being returned, you should now see the <code>dbz-sql-conn</code>. Notice how it automatically has been assigned the <code>latest</code> tag, as we did not give it a version.</p>

<p>If the Kafka cluster is still up bring it down. Edit the <code>docker-compose.yml</code> file and replace the <code>cnfldemos/cp-server-connect-datagen:0.5.0-6.2.0</code> image with the image we just built: <code>dbz-sql-conn</code>, and bring up the cluster again. We do as we did before; wait a while for the cluster to come up and then use Postman to see the installed connectors. If everything has gone according to plan you now - once again - see the SQL Server connector. This time though, you will not see the datagen connector as it is not part of the <code>cp-server-connect-base</code> image.</p>

<h4 id="pre-release-connector-versions">Pre-Release Connector Versions</h4>

<p>I mentioned previously that Confluent Hub contains released versions of connectors. What if you want to use an Alpha/Beta version of a connector? For example, the latest release of the Debezium SQL Server connector is version 1.6.0, but there is a version 1.7.0 in testing, and I would like to test that version.</p>

<p>First of all, how do you know what versions there are? Well, browse to <a href="https://debezium.io/releases/">Debezium Releases Overview</a>, where you see what version is in development and what version is released:</p>

<p><img src="/images/posts/kafka-connect-dbz-release-overview.png" alt="" /></p>

<p><strong>Figure 9:</strong> <em>Installed Debezium SQL Server Connector</em></p>

<p>Click on the <em>More Info</em> button, which you see outlined in red in <em>Figure 9</em>. That takes you to the <a href="https://debezium.io/releases/1.7/">Debezium Release Series</a> page, where you at the bottom of the page have a <em>Downloads</em> button. Click on that button and then right-click on the connector for the product you want (SQL Server Connector Plug-in), and copy the link. The link for the latest development release for the SQL Server Connector Plug-in at the time of me writing this post (August 2021) is:</p>

<pre><code class="language-bash">https://repo1.maven.org/maven2/io/debezium/ \
       debezium-connector-sqlserver/1.7.0.Alpha1/ \
       debezium-connector-sqlserver-1.7.0.Alpha1-plugin.tar.gz
</code></pre>

<p><strong>Code Snippet 8:</strong> <em>Link to Connector Plugin</em></p>

<p>When you look at <em>Code Snippet 8</em> beware of the line continuations <code>\</code>. As the connector is not available via Confluent Hub, how do we get it?</p>

<p>The answer to that is that nothing much changes. We can either install it via the <code>docker-compose.yml</code> file similar to what we did in <em>Code Snippet 5</em> or what we did in <em>Code Snippet 6</em>. The only difference is that we cannot use <code>confluent-hub install</code>, but we have to:</p>

<ul>
<li>download it using <code>wget</code>.</li>
<li>un-tar it into the plugin path.</li>
</ul>

<p>I will do it by building a new image, and the code in <code>Dockerfile</code> for this looks like so:</p>

<pre><code class="language-bash">FROM confluentinc/cp-server-connect-base:6.2.0

RUN wget https://repo1.maven.org/maven2/io/debezium/ \
        debezium-connector-sqlserver/1.7.0.Alpha1/ \
        debezium-connector-sqlserver-1.7.0.Alpha1-plugin.tar.gz \
    &amp;&amp; tar -xvf ./debezium-connector-sqlserver-1.7.0.Alpha1-plugin.tar.gz \
     -C /usr/share/java/
</code></pre>

<p><strong>Code Snippet 9:</strong> <em>Dockerfile Downloading Connector</em></p>

<p>As with <em>Code Snippet 8</em>, beware of the line continuations in <em>Code Snippet 9</em>. In <em>Code Snippet 9</em>, we see how we download it using <code>wget</code> and then un-tar it into the plugin path. Having edited and saved the <code>Dockerfile</code>, we build it like so:</p>

<pre><code class="language-bash">docker build . -t dbz-sql-conn:1.7.Alpha1
</code></pre>

<p><strong>Code Snippet 10:</strong> <em>Building Alpha Release</em></p>

<p>As we see in <em>Code Snippet 10</em>, I have given the image a tag of <code>1.7.Alpha1</code>. To ensure that it works we:</p>

<ul>
<li>replace the <code>dbz-sql-conn</code> image in the <code>docker-compose.yml</code> file with dbz-sql-conn:1.7.Alpha1</li>
<li>tear down the Kafka cluster</li>
<li>spin up the Kafka cluster again.</li>
</ul>

<p>When we use Postman to get installed connectors, we see the following:</p>

<p><img src="/images/posts/kafka-connect-dbz-sql-conn-alpha.png" alt="" /></p>

<p><strong>Figure 10:</strong> <em>Installed Debezium SQL Server Connector 1.7.0</em></p>

<p>We see in <em>Figure 10</em> how we indeed have installed a pre-release version of the connector - yay!</p>

<h2 id="test">Test</h2>

<p>So far, we have seen that the SQL Server connector is installed, but we have not made sure it does what it is supposed to do, i.e. retrieve data into a Kafka topic.</p>

<p>As I mentioned initially, this post is not about SQL Server CDC or configuring the Debezium connector. In any case, let us quickly go over how to test it works. We start with enabling SQL Server, and we use the database and table we created above:</p>

<pre><code class="language-sql">USE DebeziumTest;
GO
-- before we enable CDC ensure the SQL Server Agent is started
-- we need first to enable CDC on the database
EXEC sys.sp_cdc_enable_db;

-- then we can enable CDC on the table
EXEC sys.sp_cdc_enable_table @source_schema = N'dbo',
                               @source_name   = N'tb_CDCTab1',
                               @role_name = NULL,
                               @supports_net_changes = 0;
</code></pre>

<p><strong>Code Snippet 11:</strong> <em>Enabling Database and Table for CDC</em></p>

<p>The code comments in <em>Code Snippet 10</em> should be pretty self-explanatory. The only thing to think about is that the SQL Server Agent needs to be started for this to work.</p>

<p>Having enabled CDC, we now configure and create the connector instance. We do it using Postman, <code>POST</code>:ing to the <code>connectors</code> endpoint:</p>

<p><img src="/images/posts/kafka-connect-dbz-create-connector.png" alt="" /></p>

<p><strong>Figure 11:</strong> <em>Create Connector</em></p>

<p>In <em>Figure 11</em>, we see a straightforward connector configuration. You can find out more about the various configuration properties <a href="https://debezium.io/documentation/reference/connectors/sqlserver.html#sqlserver-example-configuration">here</a>. When looking at the Kafka topics in Control Center after sending the <code>POST</code> request, you see something like so:</p>

<p><img src="/images/posts/kafka-connect-dbz-topics.png" alt="" /></p>

<p><strong>Figure 12:</strong> <em>Debezium Topics</em></p>

<p>We see in <em>Figure 12</em> how two new topics were automatically created when we created the connector. These topics are Debezium specific topics, and you as a user would not do much with them. When you look at the topics in Kafka at this stage, you do not see any topic related to the table we want to stream data from. That changes as soon as you insert some data into the table:</p>

<pre><code class="language-sql">USE DebeziumTest;
GO

INSERT INTO dbo.tb_CDCTab1(Col1, Col2)
VALUES(1, 'Hello Number 1')
</code></pre>

<p><strong>Code Snippet 12:</strong> <em>Ingest Data</em></p>

<p>When you refresh the Topics page in Control Center after executing the code in <em>Code Snippet 12</em>, you see this:</p>

<p><img src="/images/posts/kafka-connect-dbz-table-topic.png" alt="" /></p>

<p><strong>Figure 13:</strong> <em>Table Topic</em></p>

<p>Cool, in <em>Figure 13</em>, outlined in red, we see a new topic. This is the topic for the table we want to stream data from into Kafka. If you now were to look at the messages in the topic, you would see the data we just inserted. I leave that for you, my dear readers, to do on your own. We have now confirmed that everything works!</p>

<h2 id="summary">Summary</h2>

<p>In this post, I describe what to do if you want to run the Debezium SQL Server connector in a Docker environment.</p>

<p>We started by looking at what Kafka Connect and Debezium is. We said that:</p>

<ul>
<li>Kafka Connect is a JVM process allowing us to stream data between Apache Kafka and other systems. It works by the use of connectors which are <code>.jar</code> files. There are two types of connectors:

<ul>
<li>source connectors that understand how to retrieve data from a system and publish it to Kafka.</li>
<li>sink connectors that read data from Kafka topics and ingests that data into target systems.</li>
</ul></li>
<li>Debezium is a distributed platform with connectors for a multitude of database systems. It uses the underlying system&rsquo;s CDC functionality to capture database changes and publish those changes to topics in Kafka.</li>
</ul>

<p>We then looked at how to deploy a Debezium connector, more specifically the SQL Server connector, to a Kafka Connect Docker container. We saw there are two main ways to get a connector into the Kafka Connect container, and both ways use the Confluent Hub client:</p>

<ul>
<li>In the <code>command</code> option for the Kafka Connect container, run the install command.</li>
<li>Create your own image, and in the <code>Dockerfile</code> file, run the install command.</li>
</ul>

<p>That was about it. Once again, this post was not about how CDC works or the various Debezium configuration options. That may be covered in a future post.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 31, 2021]]></title>
    <link href="https://nielsberglund.com/2021/08/01/interesting-stuff---week-31-2021/" rel="alternate" type="text/html"/>
    <updated>2021-08-01T07:40:28+02:00</updated>
    <id>https://nielsberglund.com/2021/08/01/interesting-stuff---week-31-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/articles/saga-orchestration-outbox/">Saga Orchestration for Microservices Using the Outbox Pattern</a>. The last few weeks, at <a href="/derivco">Derivco</a>, I have been ~playing around~ researching the use of CDC, Debezium and the outbox pattern (a blog post or two may come soon). I&rsquo;ve been looking at it in relation to publishing events from the database. It was then interesting to come across this post discussing CDC and Debezium and how these technologies combined can be used for implementing the SAGA pattern. Very cool!</li>
</ul>

<h2 id="data-architecture">Data Architecture</h2>

<ul>
<li><a href="https://netflixtechblog.com/data-movement-in-netflix-studio-via-data-mesh-3fddcceb1059">Data Movement in Netflix Studio via Data Mesh</a>. I have previously covered posts discussing Data Mesh. In this post, Netflix talks about their Data Mesh. Data Mesh, in this context, is a fully managed, streaming data pipeline product used for enabling Change Data Capture (CDC) use cases. The post is very informative, and there are quite a few concepts worth investigating!</li>
</ul>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://eng.uber.com/pinot-real-time-ingestion/">Pinot Real-Time Ingestion with Cloud Segment Storage</a>. This post, by Uber, discusses how Uber added a deep store to Pinot&rsquo;s real-time ingestion protocol.</li>
<li><a href="https://towardsdatascience.com/getting-started-with-azure-data-explorer-and-azure-synapse-analytics-for-big-data-processing-25500821e370">Getting started with Azure Data Explorer and Azure Synapse Analytics for Big Data processing</a>. Azure Data Explorer is a fully managed data analytics service that can handle large volumes of diverse data from any data source, such as websites, applications, IoT devices, and more. This post looks at leveraging integration between Azure Data Explorer and Azure Synapse for processing data with Apache Spark.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/designing-an-elastic-apache-kafka-for-the-cloud/">Making Apache Kafka Serverless: Lessons From Confluent Cloud</a>. From a developers perspective, serverless in the cloud is awesome and easy to use. However, the system designer and the engineer who has to design and implement a serverless system have challenges. This post starts with looking at the confluent cloud architecture and then dives into how some of the difficulties mentioned above have been overcome.</li>
<li><a href="https://www.confluent.io/blog/from-apache-kafka-to-confluent-cloud-optimizing-for-speed-scale-storage/">Speed, Scale, Storage: Our Journey from Apache Kafka to Performance in Confluent Cloud</a>. Hmm, Confluent Cloud seemed popular this week. This post looks at optimizing Apache Kafka for Confluent Cloud. Even if you are not interested in the cloud, the post is full of good advice and best practices. Oh, and I have to look at the test framework mentioned in the post: <a href="https://github.com/apache/kafka/blob/db3e5e2c0de367ffcfe4078359d6d208ba722581/TROGDOR.md">Trogdor</a>.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
</feed>

