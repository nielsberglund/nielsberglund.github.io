<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Niels Berglund</title>
  <link href="https://nielsberglund.com/atom.xml" rel="self"/>
  <link href="https://nielsberglund.com"/>
  <updated>2021-05-16T08:36:10+02:00</updated>
  <id>https://nielsberglund.com/</id>
  <generator uri="http://gohugo.io/">Hugo</generator>

  
  <entry>
    <title type="html"><![CDATA[test-1]]></title>
    <link href="https://nielsberglund.com/2021/05/16/test-1/" rel="alternate" type="text/html"/>
    <updated>2021-05-16T08:36:10+02:00</updated>
    <id>https://nielsberglund.com/2021/05/16/test-1/</id>
    <content type="html"><![CDATA[<p>This is a test</p>
]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 19, 2021]]></title>
    <link href="https://nielsberglund.com/2021/05/09/interesting-stuff---week-19-2021/" rel="alternate" type="text/html"/>
    <updated>2021-05-09T07:07:52+02:00</updated>
    <id>https://nielsberglund.com/2021/05/09/interesting-stuff---week-19-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="machine-learning">Machine Learning</h2>

<ul>
<li><a href="https://eng.uber.com/optimal-feature-discovery-ml/">Optimal Feature Discovery: Better, Leaner Machine Learning Models Through Information Theory</a>. This post talks about an approach at Uber called Optimal Feature Discovery. It searches for the most compact set of features out of all available at Uber for a given model, both boosting accuracy and reducing feature count at the same time.<br /></li>
</ul>

<h2 id="databases">Databases</h2>

<ul>
<li><a href="https://www.infoq.com/articles/serverless-data-api/">Why a Serverless Data API Might be Your Next Database</a>. This <a href="https://www.infoq.com/">InfoQ</a> article discusses database as a service (DBaaS) and serverless data API for cloud-based data management.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://chathura-ekanayake.medium.com/eda-implementation-integration-scenarios-66895923439">EDA implementation — Integration scenarios</a>. The post linked to is a follow up to <a href="https://chathura-ekanayake.medium.com/applying-event-driven-architecture-in-digital-transformation-projects-acbcb27440af">Applying Event-Driven Architecture in Digital Transformation Projects</a>, which discussed a generic architecture for event-driven architecture (EDA) based systems. This post explores implementation approaches for such event-driven systems by focusing on specific products and their interactions.</li>
<li><a href="https://www.confluent.io/blog/streaming-etl-with-confluent-kafka-message-routing-and-fan-out/">Streaming ETL with Confluent: Routing and Fan-Out of Apache Kafka Messages with ksqlDB</a>. This post looks at how we can use ksqlDB for message routing. I quite liked the post, and I will definitely see if we can implement some of this at <a href="/derivco">Derivco</a>.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 18, 2021]]></title>
    <link href="https://nielsberglund.com/2021/05/02/interesting-stuff---week-18-2021/" rel="alternate" type="text/html"/>
    <updated>2021-05-02T07:55:53+02:00</updated>
    <id>https://nielsberglund.com/2021/05/02/interesting-stuff---week-18-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="sql-server">SQL Server</h2>

<ul>
<li><a href="https://techcommunity.microsoft.com/t5/sql-server/inference-of-ml-models-in-sql-server-via-external-languages/ba-p/2216226">Inference of ML Models in SQL Server via External Languages</a>. SQL Server 2019 introduced the notion of External Languages, whereby we can execute calls against an external language, (Java, Python, etc.), from inside SQL Server. This post looks at scoring of ONNX models where ONXX has been registered as an external language. Very cool!</li>
</ul>

<h2 id="data-analytics">Data Analytics</h2>

<ul>
<li><a href="https://medium.com/apache-pinot-developer-blog/deploying-apache-pinot-at-a-large-retail-chain-42aed2921a38">Deploying Apache Pinot at a Large Retail Chain</a>. The post linked to here looks at the use of Apache Pinot at a large retailer and how it is used to some of the big challenges around data analytics.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/announcing-ksqldb-0-17-0-new-features-and-updates/">Announcing ksqlDB 0.17.0</a>. The title of this post says it all; it looks at the latest release of ksqlDB! Reading the post, there are quite a few interesting new features in this release. Personally, I am quite excited about the ability to do table scans! At <a href="/derivco">Derivco</a>, we have waited for that for a while.</li>
<li><a href="https://www.confluent.io/blog/how-to-survive-a-kafka-outage/">How to Survive a Kafka Outage</a>. Another post where the title says it all. The post looks at various types of potential Kafka outages and options to handle the outages.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 17, 2021]]></title>
    <link href="https://nielsberglund.com/2021/04/25/interesting-stuff---week-17-2021/" rel="alternate" type="text/html"/>
    <updated>2021-04-25T08:21:37+02:00</updated>
    <id>https://nielsberglund.com/2021/04/25/interesting-stuff---week-17-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="data-architecture">Data Architecture</h2>

<ul>
<li><a href="https://lakefs.io/hudi-iceberg-and-delta-lake-data-lake-table-formats-compared/">Hudi, Iceberg and Delta Lake: Data Lake Table Formats Compared</a>. This blog post compares the lake formats Hudi, Iceberg, and Delta Lake on their platform compatibility, performance &amp; throughput, and concurrency. Interesting!</li>
<li><a href="https://medium.com/explorium-ai/benchmarking-sql-engines-for-data-serving-prestodb-trino-and-redshift-1c5f16d6e5da">Benchmarking SQL engines for Data Serving: PrestoDb, Trino, and Redshift</a>. The linked-to post benchmarks the SQL engines, Redshift, Trino &amp; Presto. Read the post for some interesting findings.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://scattered-thoughts.net/writing/2021-q1-roundup/">2021 Q1 roundup</a>. The author of this post is a freelance researcher, and he is doing quite a lot of work related to streaming. This post is a roundup of what he has done during the first quarter of this year. There are some very interesting pieces in there!</li>
<li><a href="https://www.confluent.io/blog/kafka-2-8-0-features-and-improvements-with-early-access-to-kip-500/">What&rsquo;s New in Apache Kafka 2.8</a>. This post, as the title implies, announces the latest version of Apache Kafka: 2.8. Ok, so what is the big deal with that? The big deal is that this version is the first version where you can run Kafka without ZooKeeper! This is not recommended for production, but you can definitely test it out!</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 16, 2021]]></title>
    <link href="https://nielsberglund.com/2021/04/18/interesting-stuff---week-16-2021/" rel="alternate" type="text/html"/>
    <updated>2021-04-18T11:52:42+02:00</updated>
    <id>https://nielsberglund.com/2021/04/18/interesting-stuff---week-16-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="databases">Databases</h2>

<ul>
<li><a href="https://rockset.com/blog/rocksdb-is-eating-the-database-world/">RocksDB Is Eating the Database World</a>. This blog post looks at RocksDB, what it is and how it works. When reading the post, I was surprised by how many distributed databases used RocksDB as the layer to abstract access to local storage.</li>
</ul>

<h2 id="machine-learning-ai">Machine Learning / AI</h2>

<ul>
<li><a href="https://www.infoq.com/news/2021/04/microsoft-zero3-offload">Microsoft Releases AI Training Library ZeRO-3 Offload</a>. This <a href="https://www.infoq.com/">InfoQ</a> article looks at Microsoft&rsquo;s ZeRO-3 Offload, which is an extension of the DeepSpeed AI training library. ZeRO-3 Offload improves memory efficiency while training very large deep-learning models. It allows users to train models with up to 40 billion parameters on a single GPU and over 2 trillion parameters on 512 GPUs. Impressive!</li>
</ul>

<h2 id="azure-sql">Azure SQL</h2>

<ul>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2021/04/12/azure-sql-digital-event-innovate-today-with-azure-sql/">Azure SQL digital event: Innovate today with Azure SQL</a>. The post linked to here is an invitation to an Azure SQL event. The event looks at how to build an effective cloud database management strategy. I will definitely attend!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://youtu.be/-50rsy0qK9M">Kafka Connect 101: Configuration, Connectors, Converters, and Transforms</a>. This is an excellent video doing what the title says; it covers how Kafka Connect works! I found it very informative! Oh, and BTW - Tim and I are not related!</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. Please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me if you have ideas for what to cover.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 15, 2021]]></title>
    <link href="https://nielsberglund.com/2021/04/11/interesting-stuff---week-15-2021/" rel="alternate" type="text/html"/>
    <updated>2021-04-11T10:05:00+02:00</updated>
    <id>https://nielsberglund.com/2021/04/11/interesting-stuff---week-15-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="sql-server-2019-big-data-cluster">SQL Server 2019 Big Data Cluster</h2>

<ul>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2021/04/07/whats-new-with-sql-server-big-data-clusters-cu10-release/">What&rsquo;s new with SQL Server Big Data Clusters—CU10 release</a>. This post announces the release of SQL Server 2019 Big Data Cluster CU10 and some of the new and improved functionality. As soon as I have time, I will install it and &ldquo;take it for a ride&rdquo;.</li>
</ul>

<h2 id="data-architecture">Data Architecture</h2>

<ul>
<li><a href="https://towardsdatascience.com/data-discovery-the-future-of-data-catalogs-for-data-lakes-7b50e2e8cb28">Data Discovery: The Future of Data Catalogs for Data Lakes</a>. The post linked to here discusses how we can prevent our data lakes from becoming data swamps. The key to this is data discovery and data catalogs. I like this post, and it has given me a lot to think about.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://medium.com/apache-pinot-developer-blog/introduction-to-upserts-in-apache-pinot-987c12149d93">Introduction to Upserts in Apache Pinot</a>. In version 0.6 of Apache Pinot, a new feature was made available for stream ingestion, allowing you to upsert events from an immutable log. You may be familiar with upserts from the database world, however in Apache Pinot, an upsert is somewhat different than what you have in a database, and this post looks at what it is and why it is exciting.</li>
<li><a href="https://whylabs.ai/blog/posts/integrating-whylogs-into-your-kafka-ml-pipeline">Integrating whylogs into your Kafka ML Pipeline</a>. WhyLogs is an open-source data quality library that uses advanced data science statistics to log and monitor data used in AI/ML applications. This blog post looks at how we can integrate WhyLogs in Kafka to evaluate, monitor and detect statistical anomalies in streaming data. This is very interesting!</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 14, 2021]]></title>
    <link href="https://nielsberglund.com/2021/04/04/interesting-stuff---week-14-2021/" rel="alternate" type="text/html"/>
    <updated>2021-04-04T12:11:33+02:00</updated>
    <id>https://nielsberglund.com/2021/04/04/interesting-stuff---week-14-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="machine-learning-data-science-ai">Machine Learning / Data Science / AI</h2>

<ul>
<li><a href="https://medium.com/data-for-ai/building-real-time-ml-pipelines-with-a-feature-store-9f90091eeb4">Building Real-Time ML Pipelines with a Feature Store</a>. The term Feature Store is gaining popularity in the Machine Learning world. It is - as the name implies - something that stores feature data. However, it also runs pipelines that transform raw data into feature values, and it serves feature data for training and inference purposes. Most feature stores are batch-oriented, but they must move beyond batch and also become able to handle real-time data. This blog post looks at transitioning from batch to real-time.</li>
<li><a href="https://databricks.com/blog/2021/04/02/data-ai-summit-is-back.html">Data + AI Summit Is Back</a>. This post leads to a link for registration for the North American leg of <a href="https://databricks.com/dataaisummit/north-america-2021">Data + AI Summit</a>. The schedule looks awesome, and I&rsquo;ll definitely register!</li>
</ul>

<h2 id="data-architecture">Data Architecture</h2>

<ul>
<li><a href="https://medium.com/pinterest-engineering/open-sourcing-querybook-pinterests-collaborative-big-data-hub-ba2605558883">Open sourcing Querybook, Pinterest&rsquo;s collaborative big data hub</a>. Pinterest is a data-driven company, and it is more important than ever for teams to be able to compose queries, create analyses, and collaborate with one another. To enable that, Pinterest built Querybook. This post looks at what Querybook is and how they got to the point of open-sourcing it.</li>
<li><a href="https://databricks.com/blog/2021/03/31/top-questions-from-customers-about-delta-lake.html">Top Questions from Customers about Delta Lake</a>. Databricks Delta Lake is a hot topic, and many people have questions about it. This post aims to answer some of those questions.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/kafka-without-zookeeper-a-sneak-peek/">Apache Kafka Made Simple: A First Glimpse of a Kafka Without ZooKeeper</a>. There has been lots of talk about removing ZooKeeper as a dependency for Kafka. Finally, we are almost there, and the upcoming Kafka release will have the ability to run without ZooKeeper - yay! The blog post linked to looks at the implications of the removal and its impact on - among other things - scalability and performance (spoiler alert: improvements!). Very cool &ldquo;stuff&rdquo;!</li>
<li><a href="https://www.confluent.io/blog/monitor-kafka-clusters-with-prometheus-grafana-and-confluent/">Monitoring Your Event Streams: Integrating Confluent with Prometheus and Grafana</a>. Managing and monitoring a system like Kafka, is not and easy feat. But there are help; this is part 1 of a three-part blog series that will explain how to effectively monitor your event streams. This post looks at integration with third party tools such as Prometheus and Grafana.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 13, 2021]]></title>
    <link href="https://nielsberglund.com/2021/03/28/interesting-stuff---week-13-2021/" rel="alternate" type="text/html"/>
    <updated>2021-03-28T09:41:52+02:00</updated>
    <id>https://nielsberglund.com/2021/03/28/interesting-stuff---week-13-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://medium.com/event-driven-utopia/5-reasons-why-you-should-use-microsoft-dapr-to-build-event-driven-microservices-cb2202c579a0">5 Reasons Why You Should Use Microsoft Dapr to Build Event-driven Microservices</a>. Dapr is a portable, event-driven runtime, and it helps developers build event-driven, resilient distributed applications. As the name implies, the post linked to looks at some of the reasons to use <a href="https://dapr.io/">Dapr</a>.</li>
<li><a href="https://www.infoq.com/news/2021/03/alibaba-dapr/">Alibaba Cloud Uses Dapr to Support Its Business Growth</a>. I doubt that the Alibaba crowd read the post above, but they use Dapr extensively. This <a href="https://www.infoq.com/">InfoQ</a> post looks at some of the reasons why Alibaba chose Dapr. Quite interesting read!</li>
<li><a href="http://muratbuffalo.blogspot.com/2021/03/sundial-fault-tolerant-clock.html">Sundial: Fault-tolerant Clock Synchronization for Datacenters</a>. The post linked to here gives an excellent explanation of time synchronization challenges and fundamental techniques to achieve precise time synchronization in data centers.</li>
</ul>

<h2 id="sql-server">SQL Server</h2>

<ul>
<li><a href="https://techcommunity.microsoft.com/t5/sql-server/configure-sql-server-ag-read-scale-for-sql-containers-deployed/ba-p/2224742">Configure SQL Server AG (Read-Scale) for SQL Containers deployed on Kubernetes using Helm</a>. This post looks at how to setup Always On Availability Group (AG) between SQL instances deployed as SQL containers on Kubernetes. As a SQL old-timer, I still can&rsquo;t believe we can run SQL on Linux and in Kubernetes!</li>
</ul>

<h2 id="data-architecture">Data Architecture</h2>

<ul>
<li><a href="https://medium.com/nerd-for-tech/catching-data-in-a-data-mesh-principles-part-i-2b2e11e9e33a">Catching Data In a Data Mesh: Principles (Part I)</a>. This post is part of a series intending to give an in-depth view of how the Data Mesh architectural paradigm helps to build a data platform. This is part one, explaining the thinking about the mesh and the guiding principles for the implementation. After reading this post, be sure to read <a href="https://medium.com/codex/catching-data-in-a-data-mesh-applied-part-ii-114cae4d139a">part two</a> as well.</li>
<li><a href="https://rpradeepmenon.medium.com/making-data-lakehouse-real-yet-effective-f09e84fae0fa">Making Data Lakehouse real yet effective</a>. In previous roundups, I have linked to posts about data warehouses, data lakes, data lakehouses, etc. The post I link to here tries to explore and explain how all this hangs together.</li>
<li><a href="https://towardsdatascience.com/feature-store-data-platform-for-machine-learning-455122c48229">Feature Store: Data Platform for Machine Learning</a>. Feature stores are a relatively recent &ldquo;innovation&rdquo; in the Machine Learning world. A feature store is a data system that runs data pipelines that transform raw data into feature values, stores and manages the feature data, and serves feature data consistently for training and inference purposes. The post I link to here surveys the leading feature stores. As a side note: if you want to read more about feature stores, browse over to <a href="https://www.featurestore.org/">Feature Store for ML</a>.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/real-time-search-and-analytics-with-confluent-cloud-azure-redis-spring-cloud/">Integrating Azure and Confluent: Real-Time Search Powered by Azure Cache for Redis, Spring Cloud</a>. This article looks at an architecture pattern that transforms an existing traditional transaction system into a real-time data processing system. Don&rsquo;t get hung up on the components used (Redis Cache, Spring Cloud), as what the article covers is applicable for different tech-stacks as well.</li>
<li><a href="https://levelup.gitconnected.com/event-driven-architecture-demo-29f5649144b7">Event-Driven Architecture Demo</a>. As the title says, this post looks at a completely event-driven system, incorporating Kafka and GraphQL. Very cool!</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 12, 2021]]></title>
    <link href="https://nielsberglund.com/2021/03/21/interesting-stuff---week-12-2021/" rel="alternate" type="text/html"/>
    <updated>2021-03-21T09:15:37+02:00</updated>
    <id>https://nielsberglund.com/2021/03/21/interesting-stuff---week-12-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<p>.NET</p>

<ul>
<li><a href="https://devblogs.microsoft.com/azure-sql/configurable-retry-logic-for-microsoft-data-sqlclient/">Introducing Configurable Retry Logic in Microsoft.Data.SqlClient v3.0.0-Preview1</a>. This is so cool! One of the harder things to get right when writing applications accessing databases is retries. The post I  link to here discusses the new retry feature in the .NET SqlClient library. Very awesome!</li>
<li><a href="https://devblogs.microsoft.com/dotnet/opentelemetry-net-reaches-v1-0/">OpenTelemetry .NET reaches v1.0</a>. As the title of the post implies, this blog-post discusses version 1 of OpenTelemetry .NET. It is cool to see that .NET supports OpenTelemetry!</li>
</ul>

<h2 id="data-architecture">Data Architecture</h2>

<ul>
<li><a href="https://medium.com/ft-product-technology/financial-times-data-platform-from-zero-to-hero-143156bffb1d">Financial Times Data Platform: From zero to hero</a>. This blog post &ldquo;drills&rdquo; down into how Financial Times created and evolved their data platform.</li>
<li><a href="https://towardsdatascience.com/introducing-dbt-the-etl-elt-disrupter-4351adc34123">Introducing dbt, the ETL &amp; ELT Disrupter</a>. Admittedly, this post&rsquo;s title is somewhat click-bait:y, but the post is interesting nevertheless. It looks at <a href="https://www.getdbt.com/">dbt</a>, (Data Build Tool), which is a command-line data pipeline tool that allows collecting and transforming data for analytics really fast, and really easy!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://rockset.com/blog/building-a-real-time-customer-360-on-kafka-mongodb-and-rockset/">Building a Real-Time Customer 360 on Kafka, MongoDB and Rockset</a>. The post here looks at how one can build a holistic view of a customer, (Customer 360 Profile), and have the view being real-time. Very interesting!</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 11, 2021]]></title>
    <link href="https://nielsberglund.com/2021/03/14/interesting-stuff---week-11-2021/" rel="alternate" type="text/html"/>
    <updated>2021-03-14T07:26:59+02:00</updated>
    <id>https://nielsberglund.com/2021/03/14/interesting-stuff---week-11-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="data-architecture">Data Architecture</h2>

<ul>
<li><a href="https://towardsdatascience.com/building-a-data-platform-in-2021-b759f6470426">Building a Data Platform in 2021</a>. This post looks at building a modern, scalable data platform to power analytics and data science projects. It was a handy read for me, as we are looking at these things in <a href="/derivco">Derivco</a> at the moment.</li>
<li><a href="https://www.theseattledataguy.com/what-is-starburst-data-and-why-you-should-use-it-data-engineering-consulting/">What Is Starburst Data And Why You Should Use It – Data Engineering Consulting</a>. Trino, (the &ldquo;artist&rdquo; formerly known as PrestoDB) is an open-source distributed SQL query engine for running interactive analytic queries against data sources of all sizes ranging from gigabytes to petabytes. As Trino was developed as a bare-bones SQL Engine, you&rsquo;ll have to manage scaling, security, monitoring, and create new connections on your own. That is where Starburst Data comes in, it makes quite a few things a lot easier for using Trino, and this post looks more in detail at what Starburst Data can do.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/real-time-analytics-with-kafka-and-pinot/">Under the Hood of Real-Time Analytics with Apache Kafka and Pinot</a>. Recently I have linked to posts about Apache Pinot, and I do so here again. This post looks at the inner workings of Kafka and Pinot when using them together. Very interesting!</li>
<li><a href="https://eng.uber.com/kafka/">Disaster Recovery for Multi-Region Kafka at Uber</a>. Uber has one of the largest deployments of Apache Kafka in the world, processing trillions of messages and multiple petabytes of data per day. They want to provide a scalable, reliable, performant, and easy-to-use messaging platform on top of Apache Kafka. This article highlights how they solved recovering from disasters like cluster downtime, and it also describes how they built a multi-region Apache Kafka infrastructure.</li>
<li><a href="https://www.confluent.io/blog/integrate-kafka-and-jaeger-for-distributed-tracing-and-monitoring/">Integrating Apache Kafka Clients with CNCF Jaeger at Funding Circle Using OpenTelemetry</a>. A key challenge in a Kafka based microservice architecture is understanding the system as a whole due to the decentralized nature and constant evolution of new and existing services. This post covers the basics for understanding what options are available for Apache Kafka telemetry when it comes to distributed tracing.</li>
<li><a href="https://www.confluent.io/blog/how-to-tune-rocksdb-kafka-streams-state-stores-performance/">How to Tune RocksDB for Your Kafka Streams Application</a>. When building Kafka Streams applications holding state, Kafka Streams uses local state stores that are made fault-tolerant by associated changelog topics stored in Kafka, and RocksDB backs these stores. The blog post linked to covers key concepts that show how Kafka Streams uses RocksDB to maintain its state and how RocksDB can be tuned for Kafka Streams&rsquo; state stores.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 10, 2021]]></title>
    <link href="https://nielsberglund.com/2021/03/07/interesting-stuff---week-10-2021/" rel="alternate" type="text/html"/>
    <updated>2021-03-07T06:44:31+02:00</updated>
    <id>https://nielsberglund.com/2021/03/07/interesting-stuff---week-10-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://devblogs.microsoft.com/dotnet/internals-of-the-poh/">Internals of the POH</a>. A new kind of heap was added in .NET 5, called the POH (Pinned Object Heap). This blog entry explains the internals of it.</li>
</ul>

<h2 id="distributed-systems">Distributed Systems</h2>

<ul>
<li><a href="http://muratbuffalo.blogspot.com/2021/02/foundational-distributed-systems-papers.html">Foundational distributed systems papers</a>. It is well known that if someone wants to develop competence and expertise in their fields, reading technical research papers is - almost - a must. <a href="https://twitter.com/muratdemirbas">Murat</a> has compiled a list of foundational papers in the distributed systems area in the blog post here. Go ahead and read!</li>
</ul>

<h2 id="databases">Databases</h2>

<ul>
<li><a href="https://db.cs.cmu.edu/seminar2021/">VACCINATION DATABASE TECH TALKS - 2021</a>. The &ldquo;Vaccination Database Tech Talks&rdquo; is an on-line seminar series at Carnegie Mellon University with leading developers and researchers of database systems. Each speaker will present the implementation details of their respective systems and examples of the technical challenges that they faced when working with real-world customers. I have had a look at some of them, and they are really interesting!</li>
</ul>

<h2 id="ai-machine-learning">AI/Machine Learning</h2>

<ul>
<li><a href="https://pub.towardsai.net/how-microsoft-icebreaker-addresses-the-cold-start-challenge-in-machine-learning-models-dc68fdb0fee1">How Microsoft Icebreaker Addresses the Cold-Start Challenge in Machine Learning Models</a>. This post looks at a framework for intelligent acquisition of training data. This, in turn, allows the deployment of machine learning models that can operate with little or no-training data.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.kafka-summit.org/">KAFKA SUMMIT 2021</a>. It is this time of the year again; conference time. The page linked to here is the registration page for Kafka Summit 2021. It allows you to register for the three different (virtual) summits this year. I have registered for the <a href="https://www.kafka-summit.org/events/kafka-summit-europe-2021/about">Europe Summit</a>. I suggest you do the same!</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 9, 2021]]></title>
    <link href="https://nielsberglund.com/2021/02/28/interesting-stuff---week-9-2021/" rel="alternate" type="text/html"/>
    <updated>2021-02-28T09:46:34+02:00</updated>
    <id>https://nielsberglund.com/2021/02/28/interesting-stuff---week-9-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI/data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="misc">Misc.</h2>

<ul>
<li><a href="https://netflixtechblog.com/beyond-rest-1b76f7c20ef6">Beyond REST</a>. This <a href="https://netflixtechblog.com/">Netflix</a> blog post discusses how Netflix use <a href="https://dev.to/mrfrontend/graphql-microservices-architecture-by-apollo-1m75">GraphQL microservices</a> as a backend platform, facilitating rapid application development. It looks very interesting. I wonder if we at <a href="/derivco">Derivco</a> could use this?</li>
</ul>

<h2 id="analytics">Analytics</h2>

<ul>
<li><a href="https://medium.com/apache-pinot-developer-blog/building-a-climate-dashboard-with-apache-pinot-and-superset-d3ee8cb7941d">Building a Climate Dashboard with Apache Pinot and Superset</a>. Hmm, somehow, I must have missed this blog post from back in September 2020. Anyway, better late than never. The post discusses how Apache Pinot can easily ingest, query, and visualize millions of events. In this case, the events are climate events, sourced from the NOAA storm database.</li>
<li><a href="https://medium.com/pinterest-engineering/fighting-spam-with-guardian-a-real-time-analytics-and-rules-engine-938e7e61fa27">Fighting spam with Guardian, a real-time analytics and rules engine</a>. The post linked here looks at the evolution of Pinterest&rsquo;s spam-fighting rules and query and what they&rsquo;ve learned throughout the process.</li>
</ul>

<h2 id="data-architecture">Data Architecture</h2>

<ul>
<li><a href="https://www.theseattledataguy.com/how-to-modernize-your-data-architecture-part-1-data-analytics-strategy-consulting/">How To Modernize Your Data Architecture Part 1 – Data Analytics Strategy Consulting</a>. This post is the first in a series about data architecture. It discusses what to avoid when building a data architecture and which questions to ask when building a future data</li>
<li><a href="https://towardsdatascience.com/from-data-lakes-to-data-reservoirs-aa2efebb4f25">From Data Lakes to Data Reservoirs</a>. The post linked to here looks at how you can &ldquo;tame&rdquo; your data lakes. How you can create clean, beautiful, and protected data resources with Apache Spark and Databricks Delta Lake.</li>
<li><a href="https://towardsdatascience.com/the-building-blocks-of-a-modern-data-platform-92e46061165">The Building Blocks of a Modern Data Platform</a>. If you wonder what a &ldquo;modern data platform&rdquo; means, then this post is for you. The post breaks down what a modern data platform means in practice today. This includes the three core characteristics, six fundamental building blocks, and the latest data tools. I found this post extremely valuable.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/running-kafka-at-scale-at-pinterest/">Lessons Learned from Running Apache Kafka at Scale at Pinterest</a>. The post linked to shares how Pinterest runs Kafka and discusses some of the challenges they&rsquo;ve faced and how they addressed them.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 8, 2021]]></title>
    <link href="https://nielsberglund.com/2021/02/21/interesting-stuff---week-8-2021/" rel="alternate" type="text/html"/>
    <updated>2021-02-21T08:56:02+02:00</updated>
    <id>https://nielsberglund.com/2021/02/21/interesting-stuff---week-8-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth that has to do with things that interest me:</p>

<ul>
<li>AI / data science</li>
<li>data in general</li>
<li>data architecture</li>
<li>streaming</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://blog.dapr.io/posts/2021/02/17/announcing-dapr-v1.0/">Announcing Dapr v1.0</a>. As the title implies, this post announces version 1.0 of the Distributed Application Runtime, (Dapr). Dapr lowers the bar for entry to build microservices applications, running on self-hosted infrastructure or Kubernetes.</li>
</ul>

<h2 id="sql-server-big-data-cluster">SQL Server Big Data Cluster</h2>

<ul>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2021/02/16/whats-new-with-sql-server-big-data-clusters/">What&rsquo;s new with SQL Server Big Data Clusters</a>. Microsoft released SQL Server 2019 CU9 recently, and subsequently, there is a CU9 release for SQL Server 2019 Big Data Cluster, (BDC). The post linked to discusses some of the new features in this release. I guess I better upgrade my existing BDC installation and take CU9 &ldquo;for a spin&rdquo;.</li>
</ul>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://eng.uber.com/logging/">Fast and Reliable Schema-Agnostic Log Analytics Platform</a>. This post by Uber looks at their platform for log analytics and how it has evolved. It is a really interesting post, and I think we at <a href="/derivco">Derivco</a> definitely can learn a thing or two from what they have done.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/ksqldb-0-15-features-updates/">Announcing ksqlDB 0.15</a>. I last week&rsquo;s <a href="/2021/02/14/interesting-stuff---week-7-2021/">roundup</a>, I wrote about Confluent Platform 6.1 being released, and with that exciting new ksqlDB features. This post announces a new ksqlDB version and lists some of the features in this version.</li>
<li><a href="https://www.confluent.io/blog/42-ways-zookeeper-removal-improves-kafka/">42 Things You Can Stop Doing Once ZooKeeper Is Gone from Apache Kafka</a>. There has been work underway for quite a time now to remove Kafka&rsquo;s dependency on ZooKeeper. We are getting closer and closer to this being achieved, and the post linked to looks at some of the &ldquo;stuff&rdquo; we no longer need to &ldquo;worry&rdquo; about when the ZooKeeper dependency is gone.</li>
<li><a href="https://www.confluent.io/blog/ksqldb-0-15-reads-more-message-keys-supports-more-data-types/">Keys in ksqlDB, Unlocked</a>. Above, I linked to the post announcing ksqlDB 0.15. The post here looks at one specific new feature in the 0.15 release; support for many more types of data in messages keys, including message keys with multiple columns. Awesome!</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>In last weeks <a href="/2021/02/14/interesting-stuff---week-7-2021/">roundup</a>, I mentioned how I was about to finish a post about installing SQL Server on an Azure VM. Here it is:</p>

<ul>
<li><a href="/2021/02/14/how-to-install-sql-server-on-an-azure-vm/">How To Install SQL Server on an Azure VM</a></li>
</ul>

<p>I am now prepping for a <a href="https://www.meetup.com/Cape-Town-Ms-Dev-User-Group/events/276127124/">user group presentation</a> to talk about SQL Server and Kafka. As part of that prep, I&rsquo;ve played around with Docker, and I may do a post around that.</p>

<p>Oh, speaking about that user group presentation, if you are interested, please <a href="https://www.meetup.com/Cape-Town-Ms-Dev-User-Group/events/276127124/">signup</a>, and join the fun!</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How To Install SQL Server on an Azure VM]]></title>
    <link href="https://nielsberglund.com/2021/02/14/how-to-install-sql-server-on-an-azure-vm/" rel="alternate" type="text/html"/>
    <updated>2021-02-14T09:17:27+02:00</updated>
    <id>https://nielsberglund.com/2021/02/14/how-to-install-sql-server-on-an-azure-vm/</id>
    <content type="html"><![CDATA[<p>A while ago, I wanted to do a quick test on a new SQL installation, and I wanted the SQL installation to be on a &ldquo;pristine&rdquo; server. I was not keen on creating a new virtual machine on my local dev-box, as for that I would need to create a VM image etc., and it seemed like too much hassle for a lazy person like me. The obvious choice then is to do it in the cloud. How hard can that be, what could possibly go wrong?!</p>

<p>It turned out to not be as straight-forward as I thought it would be, but eventually, I managed to get it right. Since I probably need to do it again some time, I thought I&rsquo;d write a post about it, so I have something to go back to. So here we go &hellip;</p>

<p></p>

<h2 id="pre-reqs">Pre-reqs</h2>

<p>The assumption is that you have an Azure subscription. If you do not, you can sign up for a free account <a href="https://azure.microsoft.com/en-gb/free/">here</a>.</p>

<h2 id="create-resource">Create Resource</h2>

<p>When we have an account/subscription we log onto the portal:</p>

<p><img src="/images/posts/azure-vm-sql-portal-1.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Azure Portal</em></p>

<p>After we have logged in to the portal, we see something like in <em>Figure 1</em>. To create anything in Azure, we create a new Azure resource. We do that via the <em>Create a resource</em> menu.  We click on the &ldquo;hamburger&rdquo; menu we see outlined in red in <em>Figure 1</em> to get to that menu. When clicking on the menu, a list is presented where the first item is <em>Create a resource</em>. Click on that, and we see:</p>

<p><img src="/images/posts/azure-vm-sql-create-resource-1.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Create Resource</em></p>

<p>Looking at <em>Figure 2</em>, I do not see anything that really stands out. Sure, we see some items outlined in red in the figure that relates to data and databases. However, when we &ldquo;drill down&rdquo; into those items there is nothing there that refers explicitly to SQL installations on a VM, it is all more related to managed databases.</p>

<p>I guess we could go into the <em>Compute</em> item and deploy a VM. Having done that, we upload the SQL Server install media and do a manual installation of SQL Server on the VM. However, I don&rsquo;t see that being much better than doing it on a VM on my local machine, and I thought there has to be something better.</p>

<h4 id="azure-sql">Azure SQL</h4>

<p>After extensive &ldquo;research&rdquo;, (read Google:ing), I found something that might help:</p>

<p><img src="/images/posts/azure-vm-sql-azure-sql-1.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Azure SQL - I</em></p>

<p>If I enter <em>Azure SQL</em>, in the <em>Search the Marketplace</em> search-box a drop-down list pops up, (or should it be down), and in the list is an entry for <em>Azure SQL</em> as outlined in red in <em>Figure 3</em>. Clicking on that we get something like so:</p>

<p><img src="/images/posts/azure-vm-sql-azure-sql-2.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Azure SQL - II</em></p>

<p>In <em>Figure 4</em>, we see how we can create and manage SQL Server resources, including SQL Server virtual machines, (outlined in red).</p>

<p>So let us click on the <em>Create</em> button which in <em>Figure 4</em> is outlined in yellow:</p>

<p><img src="/images/posts/azure-vm-sql-deploy-option-1.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>SQL Deployment Option</em></p>

<p>After clicking on the <em>Create</em> button, we see a form as in <em>Figure 5</em>, where we have some choices. We are interested in the <em>SQL virtual machine</em> option which is outlined in red. When we click on the drop-down list-box outlined in yellow, we get a list as so:</p>

<p><img src="/images/posts/azure-vm-sql-deploy-options.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>SQL &amp; OS Versions</em></p>

<p>The list in <em>Figure 6</em> shows us some of the various SQL Server and operating system versions we can deploy. We see, outlined in red that we want to deploy SQL Server 2019 Developer Edition on Windows Server 2019.</p>

<blockquote>
<p><strong>NOTE:</strong> I should have mentioned this in the pre-reqs above; you have to have licenses for the OS and SQL edition you choose.</p>
</blockquote>

<p>We choose the version we want to use and click on the <em>Create</em> button.</p>

<h2 id="create-vm">Create VM</h2>

<p>After clicking the <em>Create</em> button we see:</p>

<p><img src="/images/posts/azure-vm-sql-createvm-new.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>Create a VM</em></p>

<p>In <em>Figure 7</em> we see the top part of the <em>Create a virtual machine</em> page. We see, outlined in red, the &ldquo;tabs&rdquo; for setting up different aspects of the VM.</p>

<h4 id="basics">Basics</h4>

<p>We start at the <em>Basics</em> tab and, as the name implies, here we set the base settings for the VM:</p>

<p><img src="/images/posts/azure-vm-sql-createvm-basics.png" alt="" /></p>

<p><strong>Figure 8:</strong> <em>Basic VM Details</em></p>

<p>What <em>Figure 8</em> shows is the main part of the <em>Basics</em> tab. We see how I have filled in the various details for my VM:</p>

<ul>
<li>the subscription I want to use.</li>
<li>what resource group the VM should be placed in. In this case, I have decided to create a new resource group for the VM: <code>rg-sqlvm</code>.</li>
<li>name of the VM.</li>
<li>the SQL Server image is defaulted to what you chose from the list in <em>Figure 6</em>, but you can change it.</li>
<li>the region where you want the VM to be located.</li>
<li>size of the VM.</li>
<li>various account information</li>
</ul>

<p>I have outlined <em>Region</em> and <em>Size</em> in <em>Figure 8</em> because you may get a different price for your VM depending on the region. So if you just want to do a quick test, choose a region with low price.</p>

<p>Having filled in all necessary information, you can now go ahead and click on the <em>Review + create</em> button in the lower left-hand corner in <em>Figure 8</em>. If you were to do that your VM and SQL installation would be created with default settings and the information you entered under the <em>Basics</em> tab.</p>

<p>In my case, I wanted to configure the VM and SQL installation somewhat, and what I wanted to customise was:</p>

<ul>
<li>Disks</li>
<li>SQL Server settings</li>
</ul>

<p>So, let us move on to <em>Disks</em>.</p>

<h4 id="disks">Disks</h4>

<p>I want to customise disks settings because the VM installation uses somewhat expensive Premium SSD disks by default. Since this is just an installation for &ldquo;quick and dirty&rdquo; tests, I want to change to not use the default disks. I click on the <em>Disks</em> tab:</p>

<p><img src="/images/posts/azure-vm-sql-createvm-disks.png" alt="" /></p>

<p><strong>Figure 9:</strong> <em>Disks</em></p>

<p>We see in <em>Figure 9</em> how I have chosen <em>Standard HDD</em> as disk type form my VM. That is the only thing I change related to disks for my installation, and I now go on to SQL settings.</p>

<h4 id="sql-server-settings">SQL Server Settings</h4>

<p>There are a couple of things I want to change related to SQL Settings:</p>

<p><img src="/images/posts/azure-vm-sql-createvm-sql-settings-1.png" alt="" /></p>

<p><strong>Figure 10:</strong> <em>SQL Server Settings - I</em></p>

<p>In <em>Figure 10</em>, outlined in red, we see some of the settings I want to change:</p>

<ul>
<li>I change the networking to be <em>Public</em> to connect directly to SQL Server over port 1433.</li>
<li>I enable SQL authentication. The login name and password default to what you set up for the VM.</li>
</ul>

<p>There is one more SQL setting I want to change:</p>

<p><img src="/images/posts/azure-vm-sql-createvm-sql-settings-2.png" alt="" /></p>

<p><strong>Figure 11:</strong> <em>SQL Server Settings - II</em></p>

<p>Since I am doing some SQL Server Machine Learning Services, we see in <em>Figure 11</em> how I have enabled it.</p>

<p>There is nothing more I want to change, so I click the <em>Review + create</em> button outlined in red in <em>Figure 11</em>.</p>

<h4 id="review-create">Review &amp; Create</h4>

<p>When I click the <em>Review + create</em> button, I am presented with the VM and SQL Server settings. When I am happy with the settings I click <em>Create</em>:</p>

<p><img src="/images/posts/azure-vm-sql-createvm-review-create-1.png" alt="" /></p>

<p><strong>Figure 12:</strong> <em>Create VM - I</em></p>

<p>In <em>Figure 12</em> we see how the deployment has started, and - outlined in yellow - we see the various components being deployed and the state of the deployment. After a while we see something like so:</p>

<p><img src="/images/posts/azure-vm-sql-createvm-review-create-2.png" alt="" /></p>

<p><strong>Figure 13:</strong> <em>Create VM - II</em></p>

<p>The deployment has finished as we see in <em>Figure 13</em>, and we can click the <em>Go to resource</em> button:</p>

<p><img src="/images/posts/azure-vm-sql-createvm-review-create-3.png" alt="" /></p>

<p><strong>Figure 14:</strong> <em>Deployed Resource</em></p>

<p>Clicking the button takes us to the VM and shows information about the VM as we see in <em>Figure 14</em>. What interests us is what we see outlined in red - the public IP address.</p>

<h2 id="connecting-to-sql-server">Connecting to SQL Server</h2>

<p>Having the address, we can connect to the SQL Server. My tool of choice for connecting to SQL is Azure Data Studio, (ADS):</p>

<p><img src="/images/posts/azure-vm-sql-connect-1.png" alt="" /></p>

<p><strong>Figure 15:</strong> <em>Connection Dialog</em></p>

<p>To connect I enter the connection details in the ADS <em>Connection</em> dialog as in <em>Figure 15</em>, and then I click <em>Connect</em>, (outlined in red):</p>

<p><img src="/images/posts/azure-vm-sql-connect-2.png" alt="" /></p>

<p><strong>Figure 16:</strong> <em>SQL Server Overview</em></p>

<p>After I have connected I see in the Server Explorer my new instance, (outlined in red), and an overview of the installed instance, (outlined in yellow).</p>

<p>I can now go ahead and create databases, running queries, etc. like I would do on an on-prem SQL Server!</p>

<h2 id="summary">Summary</h2>

<p>To summarise:</p>

<ul>
<li>Log in to the Azure Portal.</li>
<li>Add a new Azure SQL resource.</li>
<li>Choose the SQL virtual machine deployment option.</li>
<li>Enter the settings for the VM and the SQL Server instance.</li>
<li>Deploy the VM and SQL Server.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 7, 2021]]></title>
    <link href="https://nielsberglund.com/2021/02/14/interesting-stuff---week-7-2021/" rel="alternate" type="text/html"/>
    <updated>2021-02-14T07:28:01+02:00</updated>
    <id>https://nielsberglund.com/2021/02/14/interesting-stuff---week-7-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="data-architecture">Data Architecture</h2>

<ul>
<li><a href="https://alibaba-cloud.medium.com/data-lake-concepts-characteristics-architecture-and-case-studies-28be1b265624">Data Lake: Concepts, Characteristics, Architecture, and Case Studies</a>. This is a long post where the authors try to explain what a Data Lake is, characteristics of a Data Lake, the architecture of a Data Lake, and a lot more. It is an excellent read!</li>
</ul>

<h2 id="big-data-analytics">Big Data Analytics</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/low-latency-high-throughput/">Building Latency Sensitive User Facing Analytics via Apache Pinot</a>. In this <a href="https://www.infoq.com/">InfoQ</a> presentation, the presenter discusses how LinkedIn, Uber and other companies managed to have low latency for analytical database queries despite high throughput.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/introducing-confluent-platform-6-1/">Introducing Confluent Platform 6.1</a>. The post linked to here announces, as the title implies, the 6.1 version of Confluent Platform. There are quite a few new exciting features in this release. What excites me the most are the enhanced functionality of ksqlDB! That is something that will help us at <a href="/derivco">Derivco</a> a lot.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>While writing my blog posts about the open-sourced Python SQL Server language extension, I wanted to install SQL Server on a new, clean server. I decided to do it on an Azure VM instead of &ldquo;messing&rdquo; with VM&rsquo;s on my box.</p>

<p>It was not as straight forward as I thought, so I am now about to finish a post about what I did.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 6, 2021]]></title>
    <link href="https://nielsberglund.com/2021/02/07/interesting-stuff---week-6-2021/" rel="alternate" type="text/html"/>
    <updated>2021-02-07T13:05:26+02:00</updated>
    <id>https://nielsberglund.com/2021/02/07/interesting-stuff---week-6-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://medium.com/apache-pinot-developer-blog/real-time-analytics-with-presto-and-apache-pinot-part-i-cc672caea307">Real-time Analytics with Presto and Apache Pinot — Part I</a>. I have written in previous roundups about Presto, which is now called Trino, and Apache Pinot. The blog post linked to here is the first in a two part series about how to use Presto and Pinot together. The second part is <a href="https://medium.com/apache-pinot-developer-blog/real-time-analytics-with-presto-and-apache-pinot-part-ii-3d09ff937713">here</a>.</li>
</ul>

<h2 id="data-architecture">Data Architecture</h2>

<ul>
<li><a href="https://towardsdatascience.com/the-3-things-to-keep-in-mind-while-building-the-modern-data-stack-5d076743b33a">The 3 Things to Keep in Mind While Building the Modern Data Stack</a>. Building a data stack, never mind a modern data stack, can be confusing and complicated. This post proposes a simplified framework for creating the stack. The post looks at a conceptual model to help us when we pick the tools for the stack. I found the post very informative!</li>
<li><a href="https://towardsdatascience.com/what-is-data-mesh-and-should-you-mesh-it-up-too-364b28fe2ae9">What Is Data Mesh? And Should You Mesh It Up Too?</a>. Recently I have mentioned data meshes quite a lot. Here is another post about data meshes. It looks at what a Data Mesh is, and why more and more companies are looking to implement them.</li>
<li><a href="https://databricks.com/blog/2021/02/04/how-lakehouses-solve-common-issues-with-data-warehouses.html">How Lakehouses Solve Common Issues With Data Warehouses</a>. In <a href="/2021/01/31/interesting-stuff---week-5-2021/">last weeks roundup</a> I linked to a video about data Lakehouses. The post I link to here is the first in a series about Lakehouses, and it is based on <a href="http://cidrdb.org/cidr2021/papers/cidr2021_paper17.pdf">this white-paper</a>. I am certainly looking forward to the other posts in the series.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/consume-avro-data-from-kafka-topics-and-secured-schema-registry-with-databricks-confluent-cloud-on-azure/">Consuming Avro Data from Apache Kafka Topics and Schema Registry with Databricks and Confluent Cloud on Azure</a>. Last <a href="/2021/01/31/interesting-stuff---week-5-2021/">week</a> I posted a <a href="https://azure.microsoft.com/en-us/blog/introducing-seamless-integration-between-microsoft-azure-and-confluent-cloud/">link</a> about integration between Confluent Cloud and Microsoft Azure. I wrote that I hoped to see blog posts from the Confluent guys, (and girls), where they do &ldquo;cool stuff&rdquo; on Azure and not only AWS and Google Cloud. Well ask, and you shall be given! The post linked to here discusses how to configure Azure Databricks to interact with Confluent Cloud so that you can ingest, process, store, make real-time predictions and gain business insights from your data.</li>
<li><a href="https://joshua-robinson.medium.com/simplify-kafka-at-scale-with-confluent-tiered-storage-ae8c1a2c9c80">Simplify Kafka at Scale with Confluent Tiered Storage</a>. In October 2020, Confluent announced Confluent Platform 6.0, and how one of the new features was <a href="https://www.confluent.io/blog/confluent-platform-6-0-delivers-the-most-powerful-event-streaming-platform-to-date/#tired-storage">tiered storage</a>. This post looks at how tiered storage works, how to set it up, and performance implications. Very interesting!</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 5, 2021]]></title>
    <link href="https://nielsberglund.com/2021/01/31/interesting-stuff---week-5-2021/" rel="alternate" type="text/html"/>
    <updated>2021-01-31T12:37:43+02:00</updated>
    <id>https://nielsberglund.com/2021/01/31/interesting-stuff---week-5-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://www.youtube.com/watch?v=T70jTTYhYyM">Intro to Apache Pinot</a>. In <a href="/2021/01/24/interesting-stuff---week-4-2021/">last weeks roundup</a>, I posted a video link about doing real-time analytics using Apache Pinto and Kafka. What I have linked to here is to an awesome video introducing what Pinot is. If you are interested, it is a must-see!</li>
<li><a href="https://www.youtube.com/watch?v=RU2dXoVU8hY">Lakehouse: A New Generation of Open Platforms that Unify Data Warehousing and Advanced Analytics</a>. In some of the previous roundups I have written about Data Meshes, and how the Data Mesh is a hot topic today in the Big Data world. The video I have linked to here discusses another hot topic: the Lakehouse architecture. A Lakehouse is a data management system based on lowcost and directly-accessible storage that also provides traditional analytical DBMS management and performance features.</li>
<li><a href="https://medium.com/expedia-group-tech/a-short-introduction-to-apache-iceberg-d34f628b6799">A Short Introduction to Apache Iceberg</a>. Part of the Lakehouse architecture is the table format. The table format allows for ACID transaction capability as well as data versioning, etc. Some table formats out there are Databricks Delta Lake, Apache Hudi, and Apache Iceberg. The post linked to here looks at Apache Iceberg, and what we can do with it.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://azure.microsoft.com/en-us/blog/introducing-seamless-integration-between-microsoft-azure-and-confluent-cloud/">Introducing seamless integration between Microsoft Azure and Confluent Cloud</a>. Well, I guess the title says it all! We finally have a transparent integration between Azure and Confluent Cloud. Hopefully, we&rsquo;ll now start to see posts from the Confluent guys, (and girls), where they do &ldquo;cool stuff&rdquo; on Azure and not only AWS and Google Cloud.</li>
<li><a href="https://www.youtube.com/watch?v=wjEYH41nvBM">Streaming Machine Learning with Apache Kafka and without another Data Lake by Kai Waehner</a>. Usually, when we do Machine Learning, both training and inference, we use a data lake - perhaps even a Lakehouse as mentioned above. But it&rsquo;s possible to avoid such a data store altogether, using an event streaming architecture. The video linked to explains how this can be achieved leveraging Apache Kafka, Tiered Storage and TensorFlow.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 4, 2021]]></title>
    <link href="https://nielsberglund.com/2021/01/24/interesting-stuff---week-4-2021/" rel="alternate" type="text/html"/>
    <updated>2021-01-24T09:42:57+02:00</updated>
    <id>https://nielsberglund.com/2021/01/24/interesting-stuff---week-4-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="data-architecture">Data Architecture</h2>

<ul>
<li><a href="https://medium.com/codex/how-to-build-a-modern-data-lake-with-minio-db0455eec053">How to Build a Modern Data Lake with MinIO</a>. This is a very &ldquo;cool&rdquo; post looking at creating a &ldquo;poor man&rsquo;s data lake&rdquo;, by using open source technologies. In this case the technologies used are <a href="https://min.io/"><strong>MinIO</strong></a>, and <a href="https://trino.io/"><strong>Trino</strong></a>. MinIO is an object store compatible with S3, and Trino is a distributed SQL query engine, (formerly known as Presto). As I said, a very interesting post! See below for a follow-up post.</li>
<li><a href="https://medium.com/codex/modern-data-platform-using-open-source-technologies-212ba8273eab">Modern Data Platform using Open Source Technologies</a>. This is the follow-up post, mentioned above. This post gives an overview of Trino and MinIO, and it also touches upon some features that they offer when implemented together as a data platform.</li>
<li><a href="https://www.dataengineeringweekly.com/p/data-engineering-weekly-21-metadata">Data Engineering Weekly #21: Metadata Edition</a>. This particular post is from the <a href="https://www.dataengineeringweekly.com/">Data Engineering Weekly</a> newsletter. This edition focuses on recent breakthroughs in metadata management. Very interesting! Oh, and do yourself a favor and subscribe to the newsletter!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/best-kafka-tools-that-boost-developer-productivity/">Helpful Tools for Apache Kafka Developers</a>. What the title says; this post looks at some useful tools for Kafka developers. At <a href="/derivco">Derivco</a> we are using some of these, and the <strong>Kafka Streams Topology Visualizer</strong> is a particular favorite.</li>
<li><a href="https://eng.uber.com/gairos-scalability/">Uber&rsquo;s Real-time Data Intelligence Platform At Scale: Improving Gairos Scalability/Reliability</a>. Gairos is Uber&rsquo;s real-time data processing, storage, and querying platform. This post gives an overview of Gairos and what is done to ensure scalability and reliability. Cool stuff!</li>
<li><a href="https://zoom.us/rec/play/iXyDwNqRjmKQTp7MKkYPp8fiBvW-z84PmDlkkXldu26xMzjuxE7jaAJOvKjF3L1WRHHpXakwp6-ISB8.CVQAVTpF7RWtwneQ?continueMode=true&amp;_x_zm_rtaid=YAm9SyaCSZOQRlCQ3LgfEw.1611288712092.8aa0dac1ffc00a955a58260d99c4945e&amp;_x_zm_rhtaid=745">Using Kafka and Pinot for Real-Time, User-Facing Analytics</a>. This video looks at how <a href="https://pinot.apache.org/">Apache Pinot</a>, and Apache Kafka can work together and enable real-time analytics.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<ul>
<li><a href="/2021/01/24/sql-server-2019-external-libraries-and-your-python-runtime/">SQL Server 2019 External Libraries and Your Python Runtime</a>. I managed to publish this post that I have mentioned in the last couple of weeks roundups. In the post, we look at how we can create external libraries for our Python external language.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SQL Server 2019 External Libraries and Your Python Runtime]]></title>
    <link href="https://nielsberglund.com/2021/01/24/sql-server-2019-external-libraries-and-your-python-runtime/" rel="alternate" type="text/html"/>
    <updated>2021-01-24T06:06:25+02:00</updated>
    <id>https://nielsberglund.com/2021/01/24/sql-server-2019-external-libraries-and-your-python-runtime/</id>
    <content type="html"><![CDATA[<p>The last month or so I have written some blog posts about how Microsoft open-sourced the SQL Server language extensions for R and Python back in September 2020. These language extensions add to the Java extension which was open-sourced in March 2020. My posts have been about bringing your own Python runtime into SQL Server 2019, and the potential pitfalls you may encounter:</p>

<ul>
<li><a href="/2020/12/29/updated-bring-your-own-r--python-runtimes-to-sql-server-extensibility-framework/"><strong>Bring Your Own R &amp; Python Runtimes to SQL Server Extensibility Framework</strong></a></li>
<li><a href="/2021/01/05/updated-write-a-python-3.9-language-extension-for-sql-server-machine-learning-services/"><strong>Write a Python 3.9 Language Extension for SQL Server Machine Learning Services</strong></a></li>
<li><a href="/2021/01/09/solve-python-issues-in-sql-server-machine-learning-services-after-deploying-python-3.9/"><strong>Solve Python Issues in SQL Server Machine Learning Services After Deploying Python 3.9</strong></a></li>
</ul>

<p>I have touched upon the subject of the <code>PYTHONHOME</code> environment variable in the posts, and I have said that it is not needed - <strong>UNLESS</strong> you want to create Python external libraries.</p>

<p>In this post, we look more at <code>PYTHONHOME</code> and creating external libraries.</p>

<p></p>

<h2 id="pre-reqs">Pre-reqs</h2>

<p>I will not list the pre-reqs here, as all the other posts list them.</p>

<p>I just want to mention something important about installing Python on the SQL Server box:</p>

<ul>
<li>run the Python installation as Administrator.</li>
<li>install Python for all users.</li>
</ul>

<p>The reason for mentioning this is that you may get some strange errors when executing if you do not do that.</p>

<h2 id="demo-code">Demo Code</h2>

<p>The starting point for this post is that we have a Python external language created as per:</p>

<ul>
<li>the end of the <a href="/2020/12/29/updated-bring-your-own-r--python-runtimes-to-sql-server-extensibility-framework/"><strong>Bring Your Own R &amp; Python Runtimes to SQL Server Extensibility Framework</strong></a>, in which case it is Python 3.7.</li>
<li>or the Python externa language is based on the <a href="/2021/01/05/updated-write-a-python-3.9-language-extension-for-sql-server-machine-learning-services/"><strong>Write a Python 3.9 Language Extension for SQL Server Machine Learning Services</strong></a> post, where the language is Python 3.9.</li>
</ul>

<p>Furthermore, the expectation is that the language has been created without the <code>PYTHONHOME</code> system environment variable. In this post, I am running against the extension built in the <a href="/2021/01/05/updated-write-a-python-3.9-language-extension-for-sql-server-machine-learning-services/">Python 3.9 post</a>.</p>

<p>As in the other posts, I have created the <code>ExtLangDB</code> database. This is the database the external language above is created in. I also use the &ldquo;built-in&rdquo; Python in SQL Server Machine Learning Services in this database.</p>

<p>Now is a good time to see that everything is set up correctly. To do that we use the same code as in the other posts:</p>

<pre><code class="language-sql">EXEC sp_execute_external_script
@language =N'&lt;lang&gt;',
@script=N'
import pandas as pd
import sys
df = pd.DataFrame(columns=[&quot;Version&quot;])
ver = sys.version
pth = sys.executable
df = df.append({&quot;Version&quot;: ver}, ignore_index=True)
OutputDataSet = df'
WITH RESULT SETS (([Python Version] nvarchar(256)));
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Test Code</em></p>

<p>Run the code in <em>Code Snippet 1</em>, and replace the <code>&lt;lang&gt;</code> placeholder first with <code>Python</code>, and then with the name you gave the external Python language. In my case, it is <code>p39</code>. Both times you run the code, all should work fine.</p>

<h2 id="installing-packages-modules">Installing Packages/Modules</h2>

<p>In R and Python, you install packages/modules typically by being on the machine in question and executing R/Python code:</p>

<pre><code class="language-bash"># R
install.packages(&quot;&lt;some-package&gt;&quot;)

# Python
python.exe -m pip install &lt;some-module&gt;
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Install Packages/Modules</em></p>

<p>In <em>Code Snippet 2</em>, we see one way to install an R package and a Python module.</p>

<p>What about doing if for SQL Server? Sure you can do it that way. However, you would need access to the SQL Server box, and those &ldquo;pesky&rdquo; administrators may not want to give you access to the box the production SQL Server sits on. What could possibly go wrong?!</p>

<p>So, apart from being on the actual SQL Server box itself, we can install packages by connecting to SQL Server from a remote machine and execute. The different ways are:</p>

<ul>
<li>RevoScale: a Microsoft proprietary technology for the Microsoft R and Python SQL Server Machine Learning services. It is exposed via <code>RevoScaleR</code> for R and <code>RevoScalePy</code> for Python. Back in 2018, I wrote about using RevoScaleR for R package installation in the post <a href="/2018/06/30/installing-r-packages-in-sql-server-machine-learning-services---ii/"><strong>Installing R Packages in SQL Server Machine Learning Services - II</strong></a>. You execute using an R or Python client.</li>
<li><a href="https://docs.microsoft.com/en-us/sql/machine-learning/package-management/install-additional-python-packages-on-sql-server?view=sql-server-ver15"><code>sqlmlutils</code></a>: a package which helps you execute R/Python code in SQL Server from any R/Python client.</li>
<li><a href="https://docs.microsoft.com/en-us/sql/t-sql/statements/create-external-library-transact-sql?view=sql-server-ver15"><code>CREATE EXTERNAL LIBRARY</code></a>: this installs an R package or a Python module using T-SQL DDL statements.</li>
</ul>

<p>The ability to using <code>CREATE EXTERNAL LIBRARY</code> for Python was introduced in SQL Server 2019.</p>

<h2 id="external-libraries">External Libraries</h2>

<p>An external library is a package, module, jar file deployed to a specific database using the <code>CREATE EXTERNAL LIBRARY</code> DDL statement. The library is stored in the database and the system DMV <code>sys.external_libraries</code> exposes the installed libraries. The signature for the statement looks like so:</p>

<pre><code class="language-sql">CREATE EXTERNAL LIBRARY library_name  
[ AUTHORIZATION owner_name ]  
FROM (CONTENT = { &lt;file_spec&gt; }  
    [, PLATFORM = &lt;platform&gt; ]) 
WITH ( LANGUAGE = '&lt;language&gt;' )  
[ ; ] 
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Signature CREATE EXTERNAL LIBARY</em></p>

<p>The arguments we see in <em>Code Snippet 3</em> are:</p>

<ul>
<li><code>library_name</code>: A unique name for the package. When I say the package name has to be unique, the unique:ness is based on the name and the principal id under which it is created.</li>
<li><code>owner_name</code>: This optional parameter specifies the name of the user or role that owns the external library.</li>
<li><code>file_spec</code>: The <code>file_spec</code> specifies the package&rsquo;s content for a specific platform, and it can either be in the form of a file location (local path/network path) or a hex literal. If <code>file_spec</code> is a path, then the path needs to be readable by SQL Server. For R/Python and any languages created from the R/Python language extensions, the package/module needs to be inside a zipped archive file.</li>
<li><code>platform</code>: The <code>PLATFORM</code> parameter, which defines the platform for the content of the library. The <code>PLATFORM</code> parameter defaults to the platform SQL Server runs on, (Windows or Linux).</li>
<li><code>language</code>: Specifies the language of the package. This is either R or Python for the SQL Server machine learning languages or the name you created your external language as.</li>
</ul>

<p>Let us see how this works. What we will do is download a Python package that we want to create as an external library. The package is <code>text-tools</code>.</p>

<h4 id="text-tools">text-tools</h4>

<p>The <code>text-tools</code> package contains various tools for manipulating text. To use it, we download the <code>.whl</code> file from <a href="https://pypi.org/project/text-tools/#files">here</a>. After downloading the package, we need to put it into a zipped archive file. My zipped file is named <code>text_tools-1.0.0-py3-none-any.zip</code>. As we want to install from a file path the zip file needs to be placed in a location where SQL Server can read it.</p>

<h4 id="installation">Installation</h4>

<p>Before we do any installation, let&rsquo;s check what external libraries we have in the database already:</p>

<pre><code class="language-sql">USE ExtLangDB
GO

SELECT * FROM sys.external_libraries
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Retrieve External Libraries</em></p>

<p>When I run the code in <em>Code Snippet 4</em> I am not getting any results back as <code>ExtLangDB</code> is a newly created database.</p>

<p>OK then, let&rsquo;s get on with it. We first want to install the <code>text-tools</code> package for the SQL Server Machine Learning Services Python:</p>

<pre><code class="language-sql">CREATE EXTERNAL LIBRARY text_tools
FROM (CONTENT = 'W:\text_tools-1.0.0-py3-none-any.zip')
WITH (LANGUAGE = 'Python');
</code></pre>

<p><strong>Code Snippet 5:</strong> <em>Create text-tools for Python</em></p>

<p>In <em>Code Snippet 5</em> we see how I:</p>

<ul>
<li>give the library the name of <code>text_tools</code>.</li>
</ul>

<blockquote>
<p><strong>NOTE:</strong> When you create the external library for SQL Server Machine Learning Services Python, the name must match the module name you use in your Python script. When you create the library for your &ldquo;own&rdquo; Python, the name does not matter.</p>
</blockquote>

<ul>
<li>indicate that this library is for the &ldquo;built-in&rdquo; Python, (<code>LANGUAGE=Python</code>).</li>
</ul>

<p>After I execute the code in <em>Code Snippet 5</em> I run the code in <em>Code Snippet 4</em>, and now I get:</p>

<p><img src="/images/posts/ext-lib-py-external-library-python.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Python External Library</em></p>

<p>As we see in <em>Figure 1</em> we now have an external library for Python. The question is now if we can use it, and to do that we will use a method in <code>text-tools</code> called <code>find_best_string</code>. The method finds the best matching string in a larger string. It returns either the first match or all matches, and their match value:</p>

<pre><code class="language-sql">EXEC sp_execute_external_script
@language =N'Python',
@script=N'

from text_tools.finders import find_best_string

corpus = &quot;is simply dummy text of the printing and typesetting 
industry. Lorem Ipsum has been the industry''s standard dummy 
text ever since the 1500s.&quot;

query = &quot;Ipsum&quot;

first_match = find_best_string(query, corpus, flex = 2)

print(first_match)
'
</code></pre>

<p><strong>Code Snippet 6:</strong> <em>Execute using Python</em></p>

<p>In <em>Code Snippet 6</em> we see how we:</p>

<ul>
<li>import <code>find_best_string</code> from <code>text_tools.finders</code>.</li>
<li>set the variable <code>corpus</code> to be the string we want to search for.</li>
<li>set <code>query</code> to the string we look for.</li>
</ul>

<p>We then call the method with the <code>query</code>, and <code>corpus</code> as parameters, and the result looks like so:</p>

<p><img src="/images/posts/ext-lib-py-find-best-string-1.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Result</em></p>

<p>In <em>Figure 2</em> (outlined in red), we see that we get back the string that matches the best and the match value, cool!</p>

<p>Now, what if we do this using &ldquo;our own&rdquo; Python. When I say &ldquo;our own&rdquo; I refer to the Python external language mentioned in the <em>Demo Code</em> section above.</p>

<p>I mentioned above how I use Python 3.9 as the external language. If you remember from the <a href="/2021/01/05/updated-write-a-python-3.9-language-extension-for-sql-server-machine-learning-services/">Write a Python 3.9 Language Extension &hellip;</a>, I recompiled the language extension source code against Python 3.9, and then created the language as so:</p>

<pre><code class="language-sql">USE ExtLangDB;
GO

CREATE EXTERNAL LANGUAGE p39
FROM (CONTENT = 'W:\python-lang-extension.zip'
      , FILE_NAME = 'pythonextension.dll');
GO
</code></pre>

<p><strong>Code Snippet 7:</strong> <em>Create External Language</em></p>

<p>We see in <em>Code Snippet 7</em> how I created the language and gave it the name <code>p39</code>. We ensure all works OK by changing the <code>@language</code> parameter in <em>Code Snippet 1</em> to <code>@language=p39</code> and then execute. Yes, it works just fine. To ensure that we have not caused any &ldquo;drama&rdquo; with SQL Server Machine Learning Services Python, we can change back the language to <code>Python</code> and execute. This works fine as well.</p>

<p>As the above worked, let us now create the <code>text-tools</code> package as an external library for our newly created language: <code>p39</code>:</p>

<pre><code class="language-sql">CREATE EXTERNAL LIBRARY text_tools_p39
FROM (CONTENT = 'W:\text_tools-1.0.0-py3-none-any.zip')
WITH (LANGUAGE = 'p39');
</code></pre>

<p><strong>Code Snippet 8:</strong> <em>Create text-tools for p39</em></p>

<p>The code we see in <em>Code Snippet 8</em> looks the same as what we see in <em>Code Snippet 5</em>, except for:</p>

<ul>
<li>the name is now just a string. If I try to name it as in <em>Code Snippet 5</em> I get an error saying there is already an external library with that name and the same owner. As mentioned above, for external languages, we can give the library an arbitrary name.</li>
<li>the <code>LANGUAGE</code> parameter points to the language we created in <code>Code Snippet 7</code>.</li>
</ul>

<p>After we run the code in <em>Code Snippet 8</em> we check that all is OK by executing <code>SELECT * FROM sys.external_libraries</code>:</p>

<p><img src="/images/posts/ext-lib-py-external-library-py39.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>External Libraries</em></p>

<p>It seems that we succeeded in creating the external library as we see, outlined in red, in <em>Figure 3</em> the library we just created. Right, so let&rsquo;s run the code in <em>Code Snippet 6</em>, but use <code>@language=p39</code> instead, and watch the awesomeness:</p>

<p><img src="/images/posts/ext-lib-py-pyhome-error.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>PYTHONHOME Error</em></p>

<p>Eish, the awesomeness turned out to be not so awesome after all. We got a big fat error as in <em>Figure 4</em>. OK, I suppose that if you have read the previous posts, this error might not be that unexpected. This as I wrote about how the <code>PYTHONHOME</code> environment variable is not needed <strong>UNLESS</strong> you create an external library.</p>

<blockquote>
<p><strong>NOTE:</strong> Eish is South African slang, and it can express anything from excitement to horror.</p>
</blockquote>

<p>Oh, you may wonder why we could create the external library, as in <em>Code Snippet 8</em>, but when we tried to run it we failed? The short answer is to think about creating external libraries like you create stored procedures. When you run <code>CREATE PROCEDURE</code> the procedure is &ldquo;registered&rdquo;, it is not until you execute against it, it is being &ldquo;compiled&rdquo;. The same thing is true for external libraries. We will talk about external libraries in more detail and explain what happened here in a future post.</p>

<p>Another question that may &ldquo;pop up&rdquo; is why we could execute using the &ldquo;built-in&rdquo; Python language without having <code>PYTHONHOME</code> set? The short answer is that I have no clue. I assume it has to do with the tight integration between Python and SQL Server.</p>

<h2 id="the-fix">The Fix</h2>

<p>I see <code>PYTHONHOME</code> as &ldquo;the gift that keeps giving&rdquo;, but not in a good way! Let us see what we can do to fix this.</p>

<h4 id="set-the-environment-variable">Set the Environment Variable</h4>

<p>One way to fix this would be to set the <code>PYTHONHOME</code> system environment variable. We create a new system environment variable, we name it <code>PYTHONHOME</code>, and we set the variable value to the Python directory:</p>

<p><img src="/images/posts/byor-r-and-p-pythonhome.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>PYTHONHOME Environment Variable</em></p>

<p>The figure above is from a previous post, where I created <code>PYTHONHOME</code> to point to my Python 3.7 installation. In my case with Python 3.9, I point it to my Python 3.9 installation: <code>C:\Python39</code>. Having done that, I restart the <em>Launchpad</em> service and execute the code again:</p>

<pre><code class="language-bash">STDOUT message(s) from external script: Processing c:\program files
\microsoft sql server\mssql15.inst2\mssql\externallibraries\5\65538
\1\tmp\text_tools-1.0.0-py3-none-any.whl 
Installing collected packages: text-tools 
Successfully installed text-tools-1.0.0

STDOUT message(s) from external script: ('ipsum', 1.0)
</code></pre>

<p><strong>Code Snippet 9:</strong> <em>Result</em></p>

<p>The output from running the code looks something like what we see in <em>Code Snippet 9</em>:</p>

<ul>
<li>first something about where it is installed from. Subsequent executions of the code will not show this.</li>
<li>then the result. The result matches what we see in <em>Figure 2</em>.</li>
</ul>

<p>That&rsquo;s cool - we can use our external library. However, if we now went back and executed any code, for example, what we see in <em>Code Snippet 1</em>, using <code>@language=Python</code> we would fail. We discussed why in the <a href="/2021/01/09/solve-python-issues-in-sql-server-machine-learning-services-after-deploying-python-3.9/"><strong>Solve Python Issues in SQL Server Machine Learning Services After Deploying Python 3.9</strong></a> post.</p>

<p>After we have executed against the external library for the first time, one potential solution is to remove the <code>PYTHONHOME</code> environment variable and restart the <em>Launchpad</em> service. That works but is not very practical or sustainable. So we need something else.</p>

<h4 id="source-code-change">Source Code Change</h4>

<p>As we were getting an error about not finding the environment variable, (<em>Figure 5</em>), we can assume that something in the source code is looking for that particular variable.</p>

<p>In the <a href="/2021/01/05/updated-write-a-python-3.9-language-extension-for-sql-server-machine-learning-services/">Write a Python 3.9 Language Extension &hellip;</a> post, we cloned the GitHub library with the Python language extension source code. Then we recompiled the code to cater for Python 3.9, and that is the language extension used in this post. Having the source code, we can look for where the variable is used and change the name.</p>

<p>If we search for <code>PYTHONHOME</code> in the source code we find it in the method <code>PythonExtensionUtils::GetPathToPython()</code> in the source file <code>PythonExtensionUtils_win.cpp</code>:</p>

<p><img src="/images/posts/ext-lib-py-get-pyhome.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>PYTHONHOME Error</em></p>

<p>The code in <em>Figure 6</em> looks up the environment variable, and we see the variable name outlined in red. We can change that name to something arbitrarily, <code>PYTHONHOME39</code> for example and:</p>

<ul>
<li>recompile the code.</li>
<li>delete the <code>PYTHONHOME</code> variable.</li>
<li>create a new system environment variable named as per what we changed it to in the code.</li>
<li>restart the <em>Launchpad</em> service.</li>
<li>drop the external library.</li>
<li>drop the language.</li>
<li>create the language again based on the recompiled dll.</li>
<li>create the library.</li>
</ul>

<p>When we execute the code after doing the above steps, we get the result as we see in <em>Code Snippet 9</em>, <strong>AND</strong> we can also execute any code using the <code>Python</code> language with no errors. Win!</p>

<p>To change the source code works, but it may be too convoluted - especially if you are not a C++ developer. So let us look at another option.</p>

<h4 id="create-external-language">CREATE EXTERNAL LANGUAGE</h4>

<p>In <em>Code Snippet 7</em> we see how we create the external language, and we have used similar code in the previous posts. It turns out that in the <code>FROM(...)</code> clause, we can have a couple of more parameters other than <code>CONTENT</code> and <code>FILE_NAME</code>. One of those parameters is <code>ENVIRONMENT_VARIABLES</code>. The <code>ENVIRONMENT_VARIABLES</code> parameter allows us to provide a set of environment variables to the external language runtime. If we do this, the environment variable is not in global scope, but bound to that particular language, in that specific database:</p>

<pre><code class="language-sql">USE ExtLangDB;
GO

ALTER EXTERNAL LANGUAGE p39
SET 
(
  CONTENT = 'W:\python-lang-extension.zip'
  , FILE_NAME = 'pythonextension.dll'
  , ENVIRONMENT_VARIABLES = N'{&quot;PYTHONHOME&quot;:&quot;C:\\Python39&quot;}'
);
GO
</code></pre>

<p><strong>Code Snippet 10:</strong> <em>Alter the External Language</em></p>

<p>Instead of doing a <code>CREATE EXTERNAL LANGUAGE</code> and having to drop the external library and the language, we can <code>ALTER</code> the language as in <em>Code Snippet 10</em>. The assumption before we run the code above is that we are back to where we are at <em>Figure 4</em>. We have:</p>

<ul>
<li>created the language as in <em>Code Snippet 7</em>.</li>
<li>created the external library, (<em>Code Snippet 8</em>).</li>
<li>executed the code and received the error as in <em>Figure 4</em>.</li>
</ul>

<p>With the above requirements met, we can now run the code in <em>Code Snippet 10</em>. After running the code, we execute the code in <em>Code Snippet 6</em> with the <code>@language</code> parameter set to <code>p39</code>. And it works! Furthermore, executing with the <code>@language</code> set to <code>Python</code> also works!</p>

<h2 id="summary">Summary</h2>

<p>To create external libraries for external languages, not the &ldquo;built-in&rdquo; Python, the <code>PYTHONHOME</code> systems environment variable needs to be set. Having that variable set causes issues for both the &ldquo;built-in&rdquo; Python and other Python external languages based on earlier/later versions of Python other than the version we created the external library for.</p>

<p>In this post we looked at how we can work around that problem, and we saw following options:</p>

<ul>
<li>create the environment variable but delete it after the first execution of code using the external library. Not practical, nor sustainable.</li>
<li>create a new environment variable with a different name. Change the source code where the variable name is defined to use the new name, and recompile it.</li>
<li>define the variable using the <code>ENVIRONMENT_VARIABLES</code> in <code>CREATE/EDIT EXTERNAL LANGUAGE</code>.</li>
</ul>

<p>Of the three options above, the last seems to be the most practical and least invasive.</p>

<p>Finally, as mentioned previously in the post, a future post or posts will look into how creating external libraries work for external languages.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 3, 2021]]></title>
    <link href="https://nielsberglund.com/2021/01/17/interesting-stuff---week-3-2021/" rel="alternate" type="text/html"/>
    <updated>2021-01-17T07:29:05+02:00</updated>
    <id>https://nielsberglund.com/2021/01/17/interesting-stuff---week-3-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending. Admittedly not much this week but here goes:</p>

<p></p>

<h2 id="data-architecture">Data Architecture</h2>

<ul>
<li><a href="https://thenewstack.io/data-mesh-liberate-business-value-from-data-lakes-data-warehouses/">Data Mesh Liberates Business Value from Data Lakes, Data Warehouses</a>. Recently I have posted quite a bit around data architecture and data meshes. Here is another post about this topic. The post discusses how data meshes can add value to data lakes and traditional data warehouses. This is quite interesting to me as we are looking at this at <a href="/derivco">Derivco</a> right now.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://rmoff.net/2021/01/11/running-a-self-managed-kafka-connect-worker-for-confluent-cloud/">Running a self-managed Kafka Connect worker for Confluent Cloud</a>. Another post that comes at the right time. I have been looking at how to run Kafka in the cloud and use Kafka Connect. The problem is that not all Kafka connectors are cloud-enabled yet. Fortunately, this post comes to help and explains how to, for non cloud-enabled connectors, you can run your own Kafka Connect worker on-prem, which then connects to Confluent Cloud. Awesome!</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>In <a href="/2021/01/10/interesting-stuff---week-2-2021/">last weeks roundup</a>, I mentioned I was looking more into the SQL Server managed language extensions and how some posts would hopefully be forthcoming. Well, I am close to publishing a post around the Python extension and issues around creating external libraries.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
</feed>

