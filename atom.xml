<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Niels Berglund</title>
  <link href="https://nielsberglund.com/atom.xml" rel="self"/>
  <link href="https://nielsberglund.com"/>
  <updated>2021-01-03T13:50:55+02:00</updated>
  <id>https://nielsberglund.com/</id>
  <generator uri="http://gohugo.io/">Hugo</generator>

  
  <entry>
    <title type="html"><![CDATA[Build Boost.Python &amp; Numpy in Windows]]></title>
    <link href="https://nielsberglund.com/2021/01/03/build-boost.python--numpy-in-windows/" rel="alternate" type="text/html"/>
    <updated>2021-01-03T13:50:55+02:00</updated>
    <id>https://nielsberglund.com/2021/01/03/build-boost.python--numpy-in-windows/</id>
    <content type="html"><![CDATA[<p>In this post, we look at how to build <code>Boost.Python</code> and <code>Numpy</code>. We look at it from a perspective where we want to use what we build as part of a bridge between SQL Server 2019 and Python. However, if you are not interested in SQL, the post should still give you some - hopefully - useful information.</p>

<p>Please note that I am a SQL dude, and my knowledge of Boost, Python and Numpy is limited at best. So take this post for what it is; the steps I took to successfully build <code>Boost.Python</code> and <code>Numpy</code> on a Windows box.</p>

<p></p>

<h2 id="background">Background</h2>

<p>In my post, <a href="/2020/12/29/bring-your-own-r--python-runtimes-to-sql-server-extensibility-framework/"><strong>Bring Your Own R &amp; Python Runtimes to SQL Server Extensibility Framework</strong></a> I wrote about how we can use other R and Python runtimes in SQL Server Machine Learning Services than the ones that come &ldquo;out of the box&rdquo;. In the post, I wrote that if you want to bring a Python runtime other than version 3.7.x, (like 3.8, 3.9, etc.), you need to build your own bridge; a SQL Server Python language extension.</p>

<p>A language extension is a C++ dll acting as a bridge between SQL Server and an external runtime - in this case Python. To interact between C++ and Python you often use Boost, and for the SQL Server Python extension, Boost libraries are required.</p>

<h2 id="pre-reqs">Pre-reqs</h2>

<p>What do we need to do this:</p>

<ul>
<li>Python: I have Python 3.9.1 installed together with <code>numpy</code>.</li>
<li>Boost: well, that&rsquo;s fairly obvious. I downloaded Boost 1.75.0 from <a href="https://www.boost.org/users/download/">Boost Downloads</a>.</li>
<li>A C++ compiler, (a <code>toolset</code> in Boost speak). I use the compiler from Visual Studio 2019 - in Boost it is defined as <code>msvc-14.2</code>.</li>
</ul>

<p>Obviously, if you want to use what we do here to build a Python SQL Server Language Extension, you need the source code for the language extensions and SQL Server. That will be covered in a future post.</p>

<p>Now, let&rsquo;s get on with it.</p>

<h2 id="boost">Boost</h2>

<p>Boost is a set of C++ libraries complementing the C++ standard libraries. The Boost libraries provide support for tasks and structures such as linear algebra, pseudorandom number generation, multithreading, image processing, regular expressions, and unit testing.</p>

<p>Boost also allows us to interact between C++ and Python, via <code>Boost.Python</code>. In the Python extension, Boost is used - among other things - to interact with the runtime, execute scripts as well as to interact with <code>numpy</code>.</p>

<p>Most of the Boost libraries are pre-built, however, <code>Boost.Python</code> needs to be built before we can use it. Initially, I thought &ldquo;how hard can it be to build this&rdquo;, well - it turned out a lot more complicated than I imagined. This, in my opinion, is due to that the Boost documentation is less than stellar, which is one big reason I wrote this post.</p>

<h4 id="boost-installation">Boost Installation</h4>

<p>There is no installation file as such, I just unzipped the file I downloaded above to a location on my box: <code>C:\</code>:</p>

<p><img src="/images/posts/byor-python-boost1.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Boost Install</em></p>

<p>We see in <em>Figure 1</em> the Boost installation. The question now is what we do with it? We know from above that we somehow have to build <code>Boost.Python</code>, but what do we build with?</p>

<h4 id="bootstrap">Bootstrap</h4>

<p>It turns out that you have to bootstrap the Boost build engine, <code>Boost.Build</code>. On Windows, you do that by running the <code>bootstrap.bat</code> file, outlined in red in <em>Figure 1</em>:</p>

<pre><code class="language-bash">c:\boost_1_75_0&gt;.\bootstrap.bat vc142
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Bootstrap Boost Build Engine</em></p>

<p>In <em>Code Snippet 1</em> we see how I from command prompt have <code>cd</code>:ed into the Boost directory. I indicate that I want to use the Visual Studio 2019 toolset by defining the <code>vc142</code> flag. When I execute the script, some information is output to the console, and when the script has finished, I see:</p>

<p><img src="/images/posts/byor-python-boost2.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Boost Install</em></p>

<p>In <em>Figure 2</em> I have a couple of things outlined in red and yellow:</p>

<ul>
<li>What is outlined in red is the command you use to build with Boost: <code>b2.exe</code>. That is the executable created by <code>bootstrap.bat</code>.</li>
<li>Outlined in yellow is a configuration file. You configure your builds using - among other things - configuration files which are <code>.jam</code> files. This file <code>project-config.jam</code> is used for project-specific configuration.</li>
</ul>

<p>Let&rsquo;s talk configuration.</p>

<h4 id="configuration">Configuration</h4>

<p>Above I mentioned the <code>project-config.jam</code> file created when you run the bootstrap script. There are two more configuration files used by <code>b2.exe</code>:</p>

<ul>
<li><code>site-config.jam</code>: usually installed and maintained by a system administrator. Not installed by default.</li>
<li><code>user-config.jam</code>: for the user to configure, not installed by default. It usually defines the available compilers and other tools. We&rsquo;ll create the file to indicate what version of Python to compile against.</li>
</ul>

<p>Create a file in your home directory and name it <code>user-config.jam</code>. Edit it to look like so:</p>

<pre><code class="language-py">using python 
   : 3.9
   : C:\\Python39\\python.exe
   : C:\\Python39\\include #directory that contains pyconfig.h
   : C:\\Python39\\libs    #directory that contains python39.lib
   ;
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Configuration File</em></p>

<p>We see in <em>Code Snippet 2</em> how we indicate where to find the Python executable, Python header files, and Python lib files.hr</p>

<blockquote>
<p><strong>NOTE:</strong> Defining the Python version is not really necessary if you have only one Python version installed.</p>
</blockquote>

<p>Now when we have a configuration, it is time to build.</p>

<h4 id="build">Build</h4>

<p>As mentioned above to build, we run <code>b2.exe</code>. If we were to <code>cd</code> into the Boost install directory and just do: <code>b2</code>, then we would build everything - and it would take a while. Here we are only interested in building Python, so we need to limit what <code>b2</code> does.</p>

<p>After browsing for information around Boost, I started with something like so:</p>

<pre><code class="language-bash">b2 --with-python --prefix=c:\\boost175 address-model=64 \
                 variant=release link=static threading=multi \
                 runtime-link=shared install
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Build Take 1</em></p>

<p>Let&rsquo;s look at the code in <em>Code Snippet 3</em>, and see what it means:</p>

<ul>
<li><code>--with-python</code>: limit the build to only build Python. This will also include <code>numpy</code>.</li>
<li><code>--prefix</code>: where to build to. In <em>Code Snippet 3</em> I want everything built to be placed in a root directory: <code>c:\\boost175</code>. Notice for the files to be put into this directory, the <code>install</code> flag needs to be set.</li>
<li><code>address-model=64</code>: specifies if 32-bit or 64-bit code should be generated by the compiler. In my case I want 64-bit.</li>
<li><code>variant=release</code>: specifies release or debug, or both.</li>
<li><code>link=static</code>: defines whether to create <code>static</code> or <code>shared</code> libraries. For the Python extension, we want <code>static</code>. Read more about <code>static</code> vs. <code>shared</code> <a href="https://www.boost.org/doc/libs/1_75_0/libs/python/doc/html/building/choosing_a_boost_python_library_.html">here</a>.</li>
<li><code>threading=multi</code>: threading model.</li>
<li><code>runtime-link=shared</code>: determines if shared or static version of C and C++ runtimes should be used.</li>
<li><code>install</code>: ensures that the built files are put into the <code>--prefix</code> directory.</li>
</ul>

<p>Running the code we see in <em>Code Snippet 3</em> &ldquo;spews&rdquo; out a lot of information to the console, and if something is not working correctly, it can be difficult to see what is going wrong, due to the amount of data being output.</p>

<p>An example of something going wrong was when I initially ran this code on Windows 10; the <code>boost175</code> directory was created as expected. However, when I drilled down in the directory, I saw only a Python lib file, but no Numpy lib file. I knew there should be both Python and Numpy files, so something was clearly not right.</p>

<p>After tearing my hair out for quite a while, I came across the <a href="https://boostorg.github.io/build/manual/develop/index.html">B2 User Manual</a>, and <a href="https://boostorg.github.io/build/manual/develop/index.html#bbv2.overview.invocation.options">invocation options</a>. In there, I found two option flags:</p>

<ul>
<li><code>--debug-configuration</code>: this flag tells <code>b2</code> to produce debug information about the loading of <code>b2</code> and toolset files.</li>
<li><code>-d0</code>: suppresses all informational messages.</li>
</ul>

<pre><code class="language-bash">b2 --with-python --prefix=c:\\boost175 \
                 --debug-configuration \
                 -d0 \
                 address-model=64 \
                 variant=release link=static threading=multi \
                 runtime-link=shared install
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Build Take 2</em></p>

<p>When I ran the build as in <em>Code Snippet 4</em> it produced some useful output:</p>

<p><img src="/images/posts/byor-python-numpy-error.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Numpy Error</em></p>

<p>We see in <em>Figure 3</em> (outlined in red) a bug in Windows 10 2004/20H2, (from build 19041.488), impacting Numpy. It is fixed from build 20270 and upwards. However, that build is still not generally available, (you can get it from Windows Insiders Dev channel). Microsoft estimates a fix will be rolled out sometimes in January 2021. If you are affected by this and you can not get a Windows 10 Dev build  you can solve it by downgrading <code>numpy</code> to version 1.19.3: <code>pip install --upgrade numpy==1.19.3</code>.</p>

<blockquote>
<p><strong>NOTE:</strong> The link <a href="https://developercommunity.visualstudio.com/content/problem/1207405/fmod-after-an-update-to-windows-2004-is-causing-a.html">here</a> has more information about the bug.</p>
</blockquote>

<p>I have since then upgraded to the latest Windows Dev build and when I run the code, everything works fine:</p>

<p><img src="/images/posts/byor-python-lib-files.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Lib Files</em></p>

<p>In <em>Figure 4</em> we see how we have lib files for both Python and Numpy! Success!</p>

<p>When you look at the console output when you do the build you may see something like so:</p>

<p><img src="/images/posts/byor-python-boost-arch.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Boost Address Model &amp; Architecture</em></p>

<p>Hmm, that does not look right. In the code we definitely said <code>address-model=64</code>, but in the output, it says 32 bit. It turns out this is a bug in the build output, so nothing to worry about.</p>

<h2 id="summary">Summary</h2>

<p>We have in this post looked at how to build <code>Boost.Python</code>, and <code>Numpy</code> on a Windows 10 box:</p>

<ol>
<li>Ensure we have Python and Numpy installed.</li>
<li>Download Boost and unzip.</li>
<li>Ensure you have a C++ compiler installed.</li>
<li>Run the <code>bootstrap.bat</code> script, and optionally define the compiler, (<code>toolset</code>).</li>
<li>Execute <code>b2.exe</code> as per <em>Code Snippet 4</em> to build <code>Boost.Python</code> and <code>Numpy</code>.</li>
</ol>

<p>You can now start to use the libs created. In a future post we&rsquo;ll see how to create the SQL Server Python extension, using the files above.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Christmas, New Year, Week 1, 2021]]></title>
    <link href="https://nielsberglund.com/2021/01/03/interesting-stuff---christmas-new-year-week-1-2021/" rel="alternate" type="text/html"/>
    <updated>2021-01-03T07:20:06+02:00</updated>
    <id>https://nielsberglund.com/2021/01/03/interesting-stuff---christmas-new-year-week-1-2021/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This is the &ldquo;roundup&rdquo; of the posts that have been most interesting to me over the Christmas and New Year period 2020, and the first week of 2021.</p>

<p></p>

<h2 id="data-architecture">Data Architecture</h2>

<ul>
<li><a href="https://martinfowler.com/articles/data-mesh-principles.html">Data Mesh Principles and Logical Architecture</a>. The post here is a follow up to <a href="https://martinfowler.com/articles/data-monolith-to-mesh.html">How to Move Beyond a Monolithic Data Lake to a Distributed Data Mesh</a>. It summarizes the data mesh approach by enumerating its underpinning principles, and the high level logical architecture that the principles drive. If you are into data architecture, you have to read this post!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://databricks.com/blog/2020/12/15/handling-late-arriving-dimensions-using-a-reconciliation-pattern.html">Handling Late Arriving Dimensions Using a Reconciliation Pattern</a>. The blog post linked to here looks at a few use cases of late-arriving dimensions and potential solutions to handle it in Apache Spark pipelines.</li>
<li><a href="https://www.confluent.io/blog/introducing-confluent-parallel-message-processing-client/">Introducing the Confluent Parallel Consumer</a>. This blog post looks at a new Kafka consumer client: the <a href="https://github.com/confluentinc/parallel-consumer">Confluent Parallel Consumer</a>. The post covers why a new consumer client is needed and the use cases for this consumer. Very interesting!</li>
<li><a href="https://www.manning.com/books/event-streaming-with-kafka-streams-and-ksqldb">Event Streaming with Kafka Streams and ksqlDB</a>. The link here is to the revised new edition of <strong>Kafka Streams in Action</strong>. It has been expanded to cover more of the Kafka platform used for building event-based applications, including full coverage of ksqlDB. I bought it, and you should buy it as well!</li>
<li><a href="https://www.confluent.io/blog/ksqldb-0-14-0-features-updates/">Announcing ksqlDB 0.14.0</a>. As the title implies, a new version of ksqlDB is out in the wild. This post looks at some of the most notable changes, and new features of this release. Some quite &ldquo;juicy stuff&rdquo; in the release!</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>When I went on leave for Christmas, and New Year I said to myself that I had to get some blog-posts out, and for once my plans came together:</p>

<ul>
<li><a href="/2020/12/21/a-lap-around-sql-server-2019-big-data-cluster-architecture/">A Lap Around SQL Server 2019 Big Data Cluster: Architecture</a>. Finally, finally, finally! This post is a follow on from <a href="/2020/04/26/a-lap-around-sql-server-2019-big-data-cluster-background--technology/">A Lap Around SQL Server 2019 Big Data Cluster: Background &amp; Technology</a>, and it has been in the works for nearly eight months. What can I say? In the post, we look at the architecture of a SQL Server 2019 Big Data Cluster, and the various components of a BDC.</li>
<li><a href="/2020/12/29/bring-your-own-r--python-runtimes-to-sql-server-extensibility-framework/">Bring Your Own R &amp; Python Runtimes to SQL Server Extensibility Framework</a>. In September 2020, Microsoft <a href="https://cloudblogs.microsoft.com/sqlserver/2020/09/24/open-sourcing-the-r-and-python-language-extensions-for-sql-server/">announced</a> that they have open-sourced the R and Python language extensions for SQL Server Machine Learning Services. As a result, we can now bring our own versions of R and Python to SQL Server 2019. In the post linked to I look at how to use a Python runtime with a later version then what is by default shipped in SQL Server Machine Learning Services.</li>
</ul>

<p>So that&rsquo;s what I have done.</p>

<p>I am now working on a couple of posts on how to create your own Python language extension from the open-sourced code Microsoft released. Expect something to be out fairly soon. Yeah, yeah, I know - that&rsquo;s what I said about the Big Data Cluster architecture post as well back in April 2020. I guess we&rsquo;ll see.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Bring Your Own R &amp; Python Runtimes to SQL Server Extensibility Framework]]></title>
    <link href="https://nielsberglund.com/2020/12/29/bring-your-own-r--python-runtimes-to-sql-server-extensibility-framework/" rel="alternate" type="text/html"/>
    <updated>2020-12-29T12:47:45+02:00</updated>
    <id>https://nielsberglund.com/2020/12/29/bring-your-own-r--python-runtimes-to-sql-server-extensibility-framework/</id>
    <content type="html"><![CDATA[<p>Back in the day I wrote quite a few blog posts about <a href="https://nielsberglund.com/categories/sql-server-machine-learning-services/"><strong>SQL Server Machine Learning Services</strong></a> as well as <a href="https://nielsberglund.com/categories/sql-server-extensibility-framework/"><strong>SQL Server Extensibility Framework</strong></a>, and <a href="https://nielsberglund.com/categories/sql-server-extensibility-framework/"><strong>External Languages</strong></a>. <strong>SQL Server Machine Learning Services</strong> is very cool, but a complaint has been that you are restricted in what versions of R and Python you use.</p>

<p>In September 2020, Microsoft <a href="https://cloudblogs.microsoft.com/sqlserver/2020/09/24/open-sourcing-the-r-and-python-language-extensions-for-sql-server/">announced</a> that they have open-sourced the technology behind SQL Server Extensibility Framework. As a result, we can now bring our own versions of R and Python to SQL Server 2019.</p>

<blockquote>
<p><strong>NOTE:</strong> To bring your own R/Python you need SQL Server 2019 CU3 or above.</p>
</blockquote>

<p>In this blog post, we look at how to do that.</p>

<p></p>

<h2 id="background">Background</h2>

<p>In SQL Server 2016, Microsoft introduced SQL Server R Services. From inside SQL Server, that allowed you to call to the R engine via a special procedure, <code>sp_execute_external_script</code>, and execute R scripts.</p>

<p>In SQL Server 2017, Microsoft added Python as an external language and renamed SQL Server R Services to SQL Server Machine Learning Services. As with R, you execute your Python scripts using <code>sp_execute_external_script</code>.</p>

<p>Both the R runtime and the Python runtime are part of the SQL Server install, but they run as an external process. The runtimes are based on the open-source versions, but they have some Microsoft proprietary additions. The communication between SQL Server and the external engine goes over the <em>Launchpad</em> service:</p>

<p><img src="/images/posts/byor-r-and-p-launchpad1.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>External Script and Language</em></p>

<p>We see in <em>Figure 1</em> how:</p>

<ol>
<li>We execute the procedure <code>sp_execute_external_script</code>, notice the <code>@language</code> parameter.</li>
<li>That calls into the <em>Launchpad</em> service.</li>
<li>The <em>Launchpad</em> service passes the script into the relevant launcher based on the <code>@language</code> parameter in <code>sp_execute_external_script</code>. The knowledge of what launcher to call lives inside of the <em>Launchpad</em> service.</li>
<li>The launcher dll, a C++ dll, loads the relevant external engine and passes the script to the engine and executes.</li>
</ol>

<p>If you want to know more about R and Python&rsquo;s implementation in SQL Server, I suggest you look at the link above about <a href="https://nielsberglund.com/categories/sql-server-machine-learning-services/"><strong>SQL Server Machine Learning Services</strong></a>.</p>

<h4 id="java">Java</h4>

<p>In CTP 2.0 of SQL Server 2019, Microsoft made Java publicly available as an external language together with R and Python. Having Java as an external language may not seem that much different from R/Python, but there are some differences:</p>

<ul>
<li>Java is a compiled language, where we call into a specific method. R/Python are scripting languages where we send a script to the engine.</li>
<li>R/Python are part of the SQL Server install, together with launcher dll&rsquo;s and so forth. There is an equivalent of a launcher dll for Java, (<code>javaextension.dll</code>), which calls into the JVM. The difference here between R/Python and Java is that the JVM is not part of the SQL Server install but must be installed separately.</li>
</ul>

<p>Microsoft could have done with the Java integration in SQL Server 2019 to just treat it as R/Python, and &ldquo;hardcode&rdquo; Java as a language in the <em>Launchpad</em> service and let the <em>Launchpad</em> service call the <code>javaextension.dll</code>.</p>

<p>However, Microsoft did not &ldquo;hack&rdquo; the <em>Launchpad</em> service, but what they did was, with the view to &ldquo;properly&rdquo; expose an extensibility framework with multiple external languages, that they introduced a new &ldquo;host&rdquo; for external languages: <code>ExtHost.exe</code>. The <em>Launchpad</em> service calls this host for all languages except the built-in R/Python:</p>

<p><img src="/images/posts/byor-r-and-p-launchpad2.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>External Host</em></p>

<p>In <em>Figure 2</em> we see how:</p>

<ol>
<li>We execute the procedure <code>sp_execute_external_script</code>, with <code>Java</code> being the value of the <code>@language</code> parameter.</li>
<li>The proc calls into the <em>Launchpad</em> service as above.</li>
<li>Since the <code>@language</code> parameter is not <code>R</code> or <code>Python</code>, the <em>Launchpad</em> service call into <code>ExtHost.exe</code>.</li>
<li>The <code>ExtHost.exe</code> calls a well-known entry point in the language extension dll.</li>
<li>The language extension dll loads the external runtime and executes the code.</li>
</ol>

<p>OK, the above does not seem that different from R/Python, but hang on a minute? With R and Python the <em>Launchpad</em> service knows about the launchers, (language extension dll&rsquo;s), what about step 4 above; how does the <em>ExternalHost</em> know which language extension to call?</p>

<h4 id="external-language">External Language</h4>

<p>The answer to the question above is <code>EXTERNAL LANGUAGE</code>. When Microsoft introduced Java, they also introduced the notion of an external language. The external language is the path, (or bytes), to a zip file containing the language extension dll. So in the case of Java, it is the path to the zip file where <code>javaextension.dll</code> is:</p>

<pre><code class="language-sql">CREATE EXTERNAL LANGUAGE Java
FROM (CONTENT = 'W:\javaextension.zip'
      , FILE_NAME = 'javaextension.dll');
GO
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Creating External Language</em></p>

<p>In <em>Code Snippet 1</em>, we set the file name in the <code>FILE_NAME</code> parameter because the zip file may contain multiple files and the file name defines the language extension.</p>

<p>When Microsoft introduced external languages, they also introduced some new system catalog views:</p>

<ul>
<li><code>sys.external_languages</code> - contains a row for each external language in the database.</li>
<li><code>sys.external_language_files</code> - contains a row for each external language extension file in the database.</li>
</ul>

<p>You use the catalog views above to see what external languages exist in a database.</p>

<blockquote>
<p><strong>NOTE:</strong> External languages are database scoped.</p>
</blockquote>

<p>The <a href="https://nielsberglund.com/categories/sql-server-extensibility-framework/"><strong>External Languages</strong></a> link above has links to blog posts to get more information about external languages.</p>

<h4 id="recap">Recap</h4>

<p>So to recap the <em>Background</em>:</p>

<ul>
<li>The R and Python runtimes have Microsoft specific additions, and they are installed together with SQL Server. The launchers are closed source C++ dll&rsquo;s.</li>
<li>The Java language extension, (&ldquo;launcher&rdquo;), is closed source - but you are not tied to a Java version. Well, the version has to be 8+.</li>
<li>When using Java, the language extension needs to be registered and tied to the language.</li>
</ul>

<p>This was the &ldquo;lie of the land&rdquo; up until September this year, (2020).</p>

<h2 id="open-source">Open Source</h2>

<p>When Java became a supported language in SQL Server 2019, Microsoft mentioned that communication between <em>ExternalHost</em> and the language extension should be based on an API, regardless of the external language. The API is the <a href="https://docs.microsoft.com/en-us/sql/language-extensions/reference/extensibility-framework-api?view=sql-server-ver15">Extensibility Framework API for SQL Server</a>. Having an API ensures simplicity and ease of use for the extension developer.</p>

<p>From the paragraph above, one can assume that Microsoft would like to see 3rd party development of language extensions. That assumption turned out to be accurate as, mentioned above, Microsoft open-sourced the <a href="https://github.com/microsoft/sql-server-language-extensions/tree/master/language-extensions/java">Java language extension</a>, together with the <a href="https://github.com/microsoft/sql-server-language-extensions/tree/master/extension-host/include">include files for the extension API</a>, in September 2020! This means that anyone interested can now create a language extension for their own favorite language!</p>

<p>However, open sourcing the Java extension was not the only thing Microsoft did. They also created and open-sourced language extensions for R and Python!</p>

<blockquote>
<p><strong>NOTE:</strong> Microsoft did not open-source the R and Python launcher dll&rsquo;s, but they developed new extensions which do not require the Microsoft implementation of the R and Python runtimes.</p>
</blockquote>

<p>So, if you are on SQL Server 2019 CU3+, you can now bring your own R and Python runtimes to SQL server. In the rest of this post, we look at bringing a Python runtime to SQL Server 2019.</p>

<blockquote>
<p><strong>NOTE:</strong> In this post we bring our own Python runtime, but there is no difference if you want to install an R runtime instead.</p>
</blockquote>

<h2 id="pre-reqs">Pre-reqs</h2>

<p>Before we dive into the &ldquo;nitty-gritty&rdquo;, let&rsquo;s look at what you need in order to bring your own Python runtime to SQL server.</p>

<h4 id="sql-server-2019">SQL Server 2019</h4>

<p>Well, duh - that&rsquo;s quite obvious. However, don&rsquo;t forget you need to be on CU3 or above, (in this post I use CU4). Ensure that <em>Machine Learning Services and Language Extensions</em> are installed, (or part of the installation):</p>

<p><img src="/images/posts/byor-r-and-p-install.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Installation Machine Learning Services and Language Extensions</em></p>

<p>As we will use our own Python runtime, we only check the <em>Machine Learning Services and Language Extensions</em> checkbox during installation or upgrade, as in <em>Figure 3</em>. That ensures that the <em>Launchpad</em> service and <code>ExtHost.exe</code> are installed.</p>

<blockquote>
<p><strong>NOTE:</strong> If you already have installed the Microsoft R/Python runtimes the necessary, Microsoft proprietary, extensions are there. Also, you can run &ldquo;your&rdquo; R/Python runtime side by side with the Microsoft ones.</p>
</blockquote>

<p>If this is a new installation of SQL Server you also need to enable the execution of external scripts:</p>

<pre><code class="language-sql">EXEC sp_configure 'external scripts enabled', 1
RECONFIGURE WITH OVERRIDE;
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Enable Execution of External Scripts</em></p>

<p>As we see in <em>Code Snippet 2</em> we use <code>sp_configure</code> to enable external script execution.</p>

<p>When we use external languages; Java, etc., the languages are databased scoped. So the code we see in <em>Code Snippet 1</em> needs to be run in any database you want to use Java.</p>

<p>Let us create a database to use for our Python runtime:</p>

<pre><code class="language-sql">USE Master;
GO

IF EXISTS(SELECT 1 FROM sys.databases WHERE name = 'ExtLangDB')
BEGIN
  DROP DATABASE 'ExtLangDB'
END
GO

CREATE DATABASE ExtLangDB;
GO
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Creating Database</em></p>

<p>After creating the database, as in <em>Code Snippet 3</em>, we can look at Python&rsquo;s base requirements.</p>

<h4 id="python">Python</h4>

<p>Another duh! But, let us talk about why we would like to use a different runtime. One reason could be using a more recent version of the runtime than the Microsoft provided runtime. I mentioned above that I am using SQL Server 2019 CU4 in this post. When I installed SQL Server, I chose to install the Microsoft provided runtimes for R and Python. I check the version like so:</p>

<pre><code class="language-sql">EXEC sp_execute_external_script
@language =N'Python',
@script=N'
import pandas as pd
import sys
df = pd.DataFrame(columns=[&quot;Version&quot;])
ver = sys.version
df = df.append({&quot;Version&quot;: ver}, ignore_index=True)
OutputDataSet = df'
WITH RESULT SETS (([Python Version] nvarchar(256)));
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Version Check Python</em></p>

<p>In <em>Code Snippet 4</em> I use Python&rsquo;s <code>sys.version</code> to get the version, and I add that to data frame. The data frame is then assigned to the return dataset represented by <code>OutputDataSet</code>. The result when we execute:</p>

<p><img src="/images/posts/byor-r-and-p-python-version-l.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Python Version</em></p>

<p>The version of Python, as we see in <em>Figure 4</em> is 3.7.1. For some reason or another, I would like to use 3.7.9. So we need to ensure we have Python 3.7.9 <a href="https://cloudblogs.microsoft.com/sqlserver/2020/09/24/open-sourcing-the-r-and-python-language-extensions-for-sql-server/">downloaded</a> and installed.</p>

<p>After you have installed Python install the <code>pandas</code> module, as it is not installed by default: <code>pip install pandas</code>.</p>

<h2 id="python-language-extension">Python Language Extension</h2>

<p>As mentioned above, the language extensions for Java, R, and Python are open-sourced, so you can download the source code and build them yourself. However, for now, we&rsquo;ll use <a href="https://github.com/microsoft/sql-server-language-extensions/releases">prebuilt binaries</a>.</p>

<p>Browse to the Python language extension page, and download the version for your platform. For me, it is <code>python-lang-extension-windows.zip</code>.</p>

<blockquote>
<p><strong>NOTE:</strong> On the page there are both release and debug versions of the extension. Use the release version in production!</p>
</blockquote>

<p>Download to somewhere SQL Server has access to. In my case, I downloaded it to the root of my <code>w:\</code> drive.</p>

<p>When the extension is downloaded, it is time to register/install the extension.</p>

<h2 id="installation">Installation</h2>

<p>Before we register the extension, let us see what the zip file contains:</p>

<p><img src="/images/posts/byor-r-and-p-python-pythonextension.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Python Extension</em></p>

<p>When I open the zip file I see it contains one file, <code>pythonextension.dll</code> - as in <em>Figure 5</em>. Knowing the file name is good as we need to supply that when we register the external extension, like in <em>Code Snippet 1</em>.</p>

<p>So, let&rsquo;s do it, let&rsquo;s register the Python extension in the database we created in <em>Code Snippet 3</em>:</p>

<pre><code class="language-sql">USE ExtLangDB;
GO

CREATE EXTERNAL LANGUAGE p379
FROM (CONTENT = 'W:\python-lang-extension-windows.zip'
      , FILE_NAME = 'pythonextension.dll');
GO
</code></pre>

<p><strong>Code Snippet 5:</strong> <em>Create Python Language</em></p>

<p>In <em>Code Snippet 5</em> we see how I:</p>

<ul>
<li>name the language <code>p379</code>. I cannot name it Python, or R, as they are reserved for Microsoft R and Python.</li>
<li>point to the path of the zip file.</li>
<li>say what file in the zip file is the extension dll.</li>
</ul>

<p>After I have executed the code I can check that it succeeded by: <code>SELECT * FROM sys.external_languages</code>:</p>

<p><img src="/images/posts/byor-r-and-p-python-ext-langs.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>External Languages</em></p>

<p>We can see from <em>Figure 6</em>, (outlined in red), that we succeeded in creating our own Python based external language. We also see R and Python as external languages.</p>

<blockquote>
<p><strong>NOTE:</strong> You see R and Python as external languages regardless if you have installed them or not. Even if you haven&rsquo;t ticked the box for <em>Machine Learning Services and Language Extensions</em> checkbox, you will see R and Python.</p>
</blockquote>

<p>Cool, let us execute the code in <em>Code Snippet 4</em>, and see what happens. Before we run the code, change the <code>@language</code> parameter to be <code>p379</code> instead of <code>Python</code>, and then execute.</p>

<p>Hmm, bummer we get a very un-informative error along the lines of:</p>

<p><img src="/images/posts/byor-r-and-p-python-exec-error2.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>Error</em></p>

<p>In <em>Figure 7</em> we see the error message with a result of <code>0x8004004</code>. The <code>HRESULT 0x8004004</code> is <code>E_ABORT</code>, which doesn&rsquo;t really tell us anything - I did say the error was not very informative.</p>

<p>So, it turns out that when installing Python, (as well as R), we need to do some extra steps so the <em>Launchpad</em> service and <em>ExtHost</em> can do its things.</p>

<h4 id="path-permissions">Path &amp; Permissions</h4>

<p>When we execute <code>sp_execute_external_script</code> and after <em>ExternalHost</em> has loaded the python extension dll, the extension needs to know where Python is installed, so it looks for an environment variable named <code>PYTHONHOME</code>. So the first thing we need to do is to create that variable as a system variable:</p>

<p><img src="/images/posts/byor-r-and-p-pythonhome.png" alt="" /></p>

<p><strong>Figure 8:</strong> <em>Create PYTHONHOME</em></p>

<p>In <em>Figure 8</em>, we see how I set the value of <code>PYTHONHOME</code> to where Python is installed. The <em>Launchpad</em> service needs to read and write to the Python directory, so we need to set permissions for that:</p>

<pre><code class="language-bash">icacls &quot;%PYTHONHOME%&quot; /grant &quot;NT Service\MSSQLLAUNCHPAD$INST2&quot;:(OI)(CI)RX /T
</code></pre>

<p><strong>Code Snippet 6:</strong> <em>Grant Permissions to Launchpad</em></p>

<p>We need to run the code in <em>Code Snippet 6</em> from an elevated command prompt. It has to be command prompt, PowerShell will not work. Notice in <em>Code Snippet 6</em> how I define the instance with <code>$instance_name</code>. If you do this for the default instance the command is without <code>$instance_name</code>, like so: <code>icacls &quot;%PYTHONHOME%&quot; /grant &quot;NT Service\MSSQLLAUNCHPAD&quot;:(OI)(CI)RX /T</code>.</p>

<p>Having granted read and execute access to the <em>Launchpad</em> service for the instance, we do the same for the <code>SID</code> <code>S-1-15-2-1</code>:</p>

<pre><code class="language-bash">icacls &quot;%PYTHONHOME%&quot; /grant *S-1-15-2-1:(OI)(CI)RX /T
</code></pre>

<p><strong>Code Snippet 7:</strong> <em>Grant Permissions to ALL APPLICATION PACKAGES</em></p>

<p>The code, which needs to be run from an elevated command prompt,  in <em>Code Snippet 7</em> grants read and execute permissions to <code>ALL APPLICATION PACKAGES</code>.</p>

<blockquote>
<p><strong>NOTE:</strong> You can read more about <code>ALL APPLICATION PACKAGES</code>, and why we need it, <a href="https://docs.microsoft.com/en-us/sql/machine-learning/install/sql-server-machine-learning-services-2019?view=sql-server-ver15">here</a>.</p>
</blockquote>

<p>After you have granted the necessary permissions, restart the <em>Launchpad</em> service, and rerun the code:</p>

<p><img src="/images/posts/byor-r-and-p-python-version-2.png" alt="" /></p>

<p><strong>Figure 9:</strong> <em>Python 3.7.9</em></p>

<p>Wohoo, as we see in <em>Figure 9</em>, it works! We are now using version 3.7.9 of Python!</p>

<blockquote>
<p><strong>NOTE:</strong> If you run the code on a Windows 10 20H2 version you may get an error along the lines of <em>The current Numpy installation (&lt;path to numpy) fails to pass a sanity check due to a bug in the windows runtime</em>. As the error says, it is an issue with Windows and <code>Numpy</code>. The easiest way to fix it is to downgrade <code>Numpy</code> to version <code>1.19.3</code>. You do it like so: <code>pip install --upgrade numpy==1.19.3</code>.</p>
</blockquote>

<p>We are almost ready for a summary, but one last thing.</p>

<h4 id="python-extension-and-python-versions">Python Extension and Python Versions</h4>

<p>Cool, so above we&rsquo;ve seen how I can bring my own Python runtime, and in this case, it was 3.7.9. What about if I wanted a later version, let&rsquo;s say 3.9?</p>

<p>It so happens that I have Python 3.9.0 installed on my machine, so I:</p>

<ul>
<li>change <code>PYTHONHOME</code> to point to where 3.9 is.</li>
<li>apply the necessary permissions for the <em>Launchpad</em> service and <code>ALL APPLICATION PACKAGES</code>.</li>
</ul>

<p>However, when I execute the code as previous, I get the same error as in <em>Figure 7</em>. This is because the Python language extension is Python version-specific, and the release we use here is for Python 3.7.x. For other versions of Python (3.8, 3.9, etc.) you must modify and rebuild the Python Extension binaries. Look out for a future post around targeting different Python versions.</p>

<p>The R and Java extensions are not version specific.</p>

<h2 id="summary">Summary</h2>

<p>In this post, we discussed how Microsoft has open-sourced language extensions for R, Python and Java, which means we can bring our own R and Python runtimes. We mentioned how the language extensions are C++ dll&rsquo;s implementing the <a href="https://docs.microsoft.com/en-us/sql/language-extensions/reference/extensibility-framework-api?view=sql-server-ver15">Extensibility Framework API for SQL Server</a>.</p>

<p>We looked at using a later version of Python, (3.7.9), than the Python version included in SQL Server Machine Learning Services. To use another version, after we have installed the version in question we:</p>

<ul>
<li>download the Python language extension.</li>
<li>create a system environment variable <code>PYTHONHOME</code> pointing to the install directory of the version.</li>
<li>assign read and write permissions for the SQL Server instance-specific <em>Launchpad</em> service, and the <code>ALL APPLICATION PACKAGES</code> group.</li>
<li>create an external language using the <code>CREATE EXTERNAL LANGUAGE</code> syntax, with a unique name, (we cannot name it Python/R).</li>
</ul>

<p>When we&rsquo;ve done the above, we can execute using <code>sp_execute_external_script</code>.</p>

<p>The Python differs from the R and Java extensions in that it is version-specific. The extension we have used here are tied to Python runtimes 3.7.x. For other Python version, we need to modify and rebuild.</p>

<p>Oh, and if you want to bring your own version of R, you do it in the same was we did here with Python.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A Lap Around SQL Server 2019 Big Data Cluster: Architecture]]></title>
    <link href="https://nielsberglund.com/2020/12/21/a-lap-around-sql-server-2019-big-data-cluster-architecture/" rel="alternate" type="text/html"/>
    <updated>2020-12-21T09:18:00+02:00</updated>
    <id>https://nielsberglund.com/2020/12/21/a-lap-around-sql-server-2019-big-data-cluster-architecture/</id>
    <content type="html"><![CDATA[<p>This post is the second in series about <strong>SQL Server 2019 Big Data Cluster</strong> based on a presentation I do: <strong>A Lap Around SQL Server 2019 Big Data Cluster</strong>.</p>

<p>In the first post <a href="/2020/04/26/a-lap-around-sql-server-2019-big-data-cluster-background--technology/"><strong>A Lap Around SQL Server 2019 Big Data Cluster: Background &amp; Technology</strong></a> we looked at - as the title implies - the background of SQL Server 2019 Big Data Cluster, (BDC), and the technology behind it.</p>

<p>In this post, we look at the architecture and components of a BDC.</p>

<p></p>

<p>Before we dive into the architecture, let&rsquo;s refresh our memories around what we covered in the <a href="/2020/04/26/a-lap-around-sql-server-2019-big-data-cluster-background--technology/">previous</a> post.</p>

<h2 id="recap">Recap</h2>

<p>We are getting more and more data, and the data comes in all types and sizes. We need a system to be able to manage, integrate, and analyze all this data. That&rsquo;s where SQL Server comes into the picture.</p>

<p>SQL Server has continuously evolved from its very humble beginnings based on Ashton Tate/Sybase code-base to where it is now:</p>

<ul>
<li>SQL Server in Linux.</li>
<li>SQL Server in Containers.</li>
<li>SQL Server on Kubernetes.</li>
</ul>

<p>With all the capabilities now in SQL Server, it is the ideal platform to handle big data.</p>

<p>The SQL Server itself is not enough to achieve what we want, so in addition to SQL Server a BDC includes quite a few open-source technologies:</p>

<ul>
<li>Apache Spark</li>
<li>Hadoop File System (HDFS)</li>
<li>Influx DB</li>
<li>Graphana</li>
<li>Kibana</li>
<li>Elasticsearch</li>
<li>more &hellip;</li>
</ul>

<p>Oh, and a BDC is not only one SQL server, but quite a few instances. The SQL Server instances are SQL on Linux containers, and the whole BDC are deployed to and runs on Kubernetes (k8s).</p>

<p>We spoke a bit about k8s, and what constitutes a k8s cluster, (nodes, pods, etc.). In the post we tried to illustrate a k8s cluster like so:</p>

<p><img src="/images/posts/bdc-lap-around-bdc-kubernetes-1.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Kubernetes</em></p>

<p>In <em>Figure 1</em> we see some of the parts of a two-node Kubernetes cluster, with a Master node. Later in this post, we talk some more about the Master node, and the role it plays.</p>

<p>In the post, we briefly mentioned how we deploy a BDC, and we said we have essentially two options:</p>

<ul>
<li>Deploy via Python scripts.</li>
<li>Deploy using <strong>Azure Data Studio</strong>.</li>
</ul>

<p>We looked at how to manage and monitor a BDC, and we spoke about the tools for managing and monitoring:</p>

<ul>
<li><code>kubectl</code> - used to manage the Kubernetes cluster the BDC is deployed to.</li>
<li><code>azdata</code> - manage the BDC.</li>
</ul>

<p>In this post, looking at the architecture, we use the tools above, so ensure you have them installed if you want to follow along.</p>

<h2 id="architecture">Architecture</h2>

<p>How can we figure out what the architecture looks like? Well, in the <a href="/2020/04/26/a-lap-around-sql-server-2019-big-data-cluster-background--technology/"><strong>A Lap Around SQL Server 2019 Big Data Cluster: Background &amp; Technology</strong></a> post, we discussed k8s pods and how we could get information about the pod by executing some <code>kubectl</code> commands.</p>

<p>So let us go back to the pod we looked at briefly in the last post: <code>master-0</code>, which is the pod containing the SQL Server master instance. We look at that pod to see if we can get some information from it, which will help us in gaining insight into the architecture of a BDC. The code we use looks like so:</p>

<pre><code class="language-bash">kubectl get pods master-0 -n sqlbdc-cluster -o JSON
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Get Pod Information</em></p>

<p>In <em>Code Snippet 1</em> above we see how we use <code>kubectl get pods</code> and we:</p>

<ul>
<li>Send in the name of the pod we are interested in,</li>
<li>Indicate the k8s namespace the pod is in.</li>
<li>Want the output, <code>-o</code> flag, formatted as JSON.</li>
</ul>

<p>When we execute the code in <em>Code Snippet 1</em> we see something like so:</p>

<p><img src="/images/posts/bdc-lap-around-arch-pods1.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Get Pods</em></p>

<p>The <code>kubectl get pods master-0</code> command returns all information about that particular pod, and in <em>Figure 2</em> we see the first 20 lines or so of the JSON output.</p>

<p>Notice the section outlined in red, the <code>metadata</code> section. This section contains general information about the pod, and if we look closer, we can see three labels outlined in, purple, yellow and green respectively:</p>

<ul>
<li><code>app</code> with a value of <code>master</code>.</li>
<li><code>plane</code> with a value of <code>data</code>.</li>
<li><code>role</code> with a value of <code>master-pool</code>.</li>
</ul>

<p>Maybe these labels would give us some insight if we were to look at all pods? Ok, so let&rsquo;s do that, and we will use some <code>kubectl -o</code> &ldquo;magic&rdquo; to get the information we want:</p>

<pre><code class="language-bash">kubectl get pods -n sqlbdc-cluster \ 
                 -o custom-columns=NAME:.metadata.name, \
                    APP:.metadata.labels.app, \
                    ROLE:.metadata.labels.role,\
                    PLANE:.metadata.labels.plane
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Custom Columns</em></p>

<p>To retrieve the information we want, we use the <code>custom-columns</code> output option. We see in <em>Code Snippet 2</em> how we say we want four columns back: <code>NAME</code>, <code>APP</code>, <code>ROLE, and</code>PLANE`, and what labels those are, (we talk more about labels below). We then execute:</p>

<p><img src="/images/posts/bdc-lap-around-arch-roles-planes.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Pods with Custom Output</em></p>

<p>In <em>Figure 3</em> we see the result from executing the code in <em>Code Snippet 2</em> and we see all pods in the <code>sqlbdc-cluster</code> namespace, i.e. all pods in the BDC. From the <code>PLANE</code> column we see how the BDC has two planes, the control plane and the data plane.</p>

<h4 id="control-data-plane">Control &amp; Data Plane</h4>

<p>Let us make a short diversion here and talk a bit about control and data planes.</p>

<p>In distributed systems/services, we need a way to manage and monitor our services, and that is the role of the control plane. In the previous <a href="/2020/04/26/a-lap-around-sql-server-2019-big-data-cluster-background--technology/">post</a> we spoke about the master node in a k8s cluster and how we interact with the master node for management of the cluster.</p>

<p>However, k8s has no idea about a SQL Server 2019 Big Data Cluster; in which order pods should be deployed etc. This is where the BDC control plane comes in. It knows about the BDC, so whenever there needs to be an interaction between the Kubernetes cluster and the BDC the k8s master node interacts with the BDC&rsquo;s control plane. Take a deployment as an example; when deploying to the BDC, the control plane acts as the coordinator and ensures services, etc., are &ldquo;spun up&rdquo; in the correct order.</p>

<p>That is, however, not the only thing the control plane does. Remember from my previous <a href="/2020/04/26/a-lap-around-sql-server-2019-big-data-cluster-background--technology/">post</a> how we briefly discussed monitoring of a BDC and how we said we use Grafana, and Kibana together with the underlying InfluxDB and EleasticSearch as persistent stores. Well, the control plane is also responsible for monitoring, and in <em>Figure 3</em> you see some examples of this, where there are <code>APP</code>&rsquo;s related to logs and metrics.</p>

<p>The data plane is what we communicate with when working with the BDC, doing queries etc. - the application traffic. In addition to that, the data plane is also responsible for:</p>

<ul>
<li>Routing.</li>
<li>Load balancing.</li>
<li>Observability.</li>
</ul>

<p>Now, knowing a bit about the planes, let us have a look at roles.</p>

<h4 id="role">role</h4>

<p>In Kubernetes, you have the notion of a <code>Role</code>, and that has to do with security: a <code>Role</code> sets permissions within a namespace. The <code>role</code> I refer to here has nothing to do with that. No, a <code>role</code> in this context is a label attributed to a pod in a k8s cluster.</p>

<blockquote>
<p><strong>NOTE:</strong> In <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/">k8s documentation Labels</a> are described as follows: &ldquo;Labels are key/value pairs that are attached to objects, such as pods. Labels are intended to be used to specify identifying attributes of objects that are meaningful and relevant to users, but do not directly imply semantics to the core system. Labels can be used to organize and to select subsets of objects.&rdquo;</p>
</blockquote>

<p>Seeing the description about Labels above, we can deduce that a <code>role</code> describes the &ldquo;role&rdquo; of a Kubernetes component, i.e. what it does or belongs to. With that in mind, we can get some information/insight around the architecture of the BDC from it.</p>

<p>So let us once again look at the pods in the cluster and see what the <code>role</code> labels tell us:</p>

<pre><code class="language-bash">kubectl get pods -n sqlbdc-cluster2 \
                 -o custom-columns=PODNAME:.metadata.name,\
                 ROLE:.metadata.labels.role
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Pods &amp; Roles</em></p>

<p>The code in <em>Code Snippet 3</em> above is almost the same as in <em>Code Snippet 2</em>, but without the <code>APP</code> and <code>PLANE</code> columns. When we execute, we see:</p>

<p><img src="/images/posts/bdc-lap-around-arch-roles2.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Pods with Custom Output</em></p>

<p>In <em>Figure 4</em> we see how the various pods belong to quite few <code>ROLE</code>&rsquo;s. Some of the <code>ROLE</code>&rsquo;s we see are familiar, like: <code>controller</code> and <code>monitoring</code>, and we will not talk about them that much more. However, in <em>Figure 4</em> we also see some <code>ROLE</code>s named <code>xxx-pool</code>. Above we said that the <code>master-0</code> belonged to a role named <code>master-pool</code>, and we see that in <em>Figure 4</em> as well.</p>

<p>It turns out that the functionality, (SQL Server Master, Hadoop, etc.), of a BDC, is split into pools.</p>

<h2 id="pools">Pools</h2>

<p>When looking at <em>Figure 4</em> we see that we have different type of pools, and some of them have more than one pod. The pools are:</p>

<ul>
<li><code>compute-pool</code></li>
<li><code>data-pool</code></li>
<li><code>master-pool</code></li>
<li><code>storage-pool</code></li>
</ul>

<p>Let us look somewhat more in-depth into the pools above.</p>

<h4 id="master-pool">Master Pool</h4>

<p>From above we see how <code>master-0</code> belongs to the <code>master-pool</code>. In the last post as well as in this, we have mentioned how the <code>master-0</code> pod represents the SQL Server master instance, i.e. the &ldquo;normal&rdquo; SQL Server where your OLTP databases sit. So, let us see if we can prove that, by looking at what containers the pod has:</p>

<pre><code class="language-bash">kubectl get pods master-0 -n sqlbdc-cluster2 \
-o custom-columns=PODNAME:.metadata.name,\
ROLE:.metadata.labels.role,\
CONTAINERS:.spec.containers[*].name
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Master Pod Containers</em></p>

<p>The code in <em>Code Snippet 4</em> is almost the same as in <em>Code Snippet 3</em>. The difference is that we also want to see the containers in the pod. When executing, we get:</p>

<p><img src="/images/posts/bdc-lap-around-arch-master-0-containers.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Containers in Master Pod</em></p>

<p>We see in <em>Figure 5</em> that the <code>master-0</code> pod is part of the <code>master-pool</code>, and it consists of three containers. From the <a href="/2020/04/26/a-lap-around-sql-server-2019-big-data-cluster-background--technology/">previous post</a> we already know about <code>collectd</code> and <code>fluentbit</code>. It is the third container, (first in the list), that is interesting - <code>mssql-server</code>, (highlighted in yellow).</p>

<p>To find out some more about the container we change the code in <em>Code Snippet 4</em> to the following:</p>

<pre><code class="language-bash">kubectl get pods master-0 -n sqlbdc-cluster2 \
-o custom-columns=PODNAME:.metadata.name,\
CONTAINERS:.spec.containers[0].name,\
                     IMAGE:.spec.containers[0].image
</code></pre>

<p><strong>Code Snippet 5:</strong> <em>Container Image</em></p>

<p>In the code in <em>Code Snippet 5</em> we see how we retrieve the first container and the first image in the pod. We assume that as the SQL Server container is listed first, (see <em>Figure 5</em>), the container image will also be first. When we execute the code, the result looks like so:</p>

<p><img src="/images/posts/bdc-lap-around-arch-master-0-image.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>Containers in Master Pod</em></p>

<p>What we see in <em>Figure 6</em> is that the SQL Server instance in the master pool is SQL Server 2019 CU8, and it is SQL Server on Linux.</p>

<p>We will see later how there are more SQL Server instances in a BDC, but the master instance is what the user is interacting with. The master instance is also where read-write OLTP or dimensional data is stored, something like so:</p>

<p><img src="/images/posts/bdc-lap-around-arch-master-pool.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>BDC and Master Pool</em></p>

<p>What <em>Figure 7</em> shows us is a partial BDC cluster. The left is the control plane as discussed above and then beside it is the master pool with the one SQL Server instance.</p>

<p>At the bottom - outlined in red - we see a screen which illustrates a user and the interaction with the master instance. We also see a picture showing data stores (outlined in blue). What this means is that in SQL Server 2019, (not only BDC), you can query other data stores outside of SQL Server. This is thanks to Data Virtualization and PolyBase.</p>

<blockquote>
<p><strong>NOTE:</strong> A future post will cover Data Virtualization in SQL Server 2019 BDC.</p>
</blockquote>

<p>So, that is the master pool and the SQL Server master instance, what is next?</p>

<h4 id="compute-pool">Compute Pool</h4>

<p>In <em>Figure 4</em> we see how we have one pod belonging to the compute pool, the <code>compute-0-0</code>. Let us find out what containers are in the pod. We use code similar to <em>Code Snippet 4</em>, and when we execute we see:</p>

<p><img src="/images/posts/bdc-lap-around-arch-compute-containers.png" alt="" /></p>

<p><strong>Figure 8:</strong> <em>Compute Pool Containers</em></p>

<p>Hmm, the compute pool pod looks the same as the master pool pod - a SQL Server instance. If we were to look at the container images, we&rsquo;d see the same as the master instance. So what is this?</p>

<p>As the name implies, the compute pool provides scale-out computational resources for a SQL Server BDC. They are used to offload computational work, or intermediate result sets, from the SQL Server master instance. For you who have worked with PolyBase before it is a fully configured Polybase Scale-Out Group.</p>

<p>The SQL Server instance in the compute pool is - as mentioned before - for computational purposes, not for storing data. The only time there may be data persistence is if it is needed for data shuffling, and in that case, <code>tempdb</code> is used.</p>

<p>If we add the compute pool to what we have in <em>Figure 7</em> we get something like so:</p>

<p><img src="/images/posts/bdc-lap-around-arch-compute-pool.png" alt="" /></p>

<p><strong>Figure 9:</strong> <em>Compute Pool</em></p>

<p>By looking at <em>Figure 9</em> we understand that the compute pool is mostly used when accessing external data, and we see more of this as we go along.</p>

<h4 id="data-pool">Data Pool</h4>

<p>When we look at <em>Figure 4</em> we see we have two pods belonging to the data pool. Let us run the same code as in <em>Figure 8</em> but replace <code>compute-0-0</code> with <code>data-0-0</code> and see what we get:</p>

<p><img src="/images/posts/bdc-lap-around-arch-data-0-containers.png" alt="" /></p>

<p><strong>Figure 10:</strong> <em>Data Pool &amp; Containers</em></p>

<p>In <em>Figure 10</em> we see the same as for the pods in the master and data pools, and if we looked at the second pod it would be the same; one SQL Server instance, together with <code>collectd</code> and <code>fluentbit</code>. So the only difference between the data pool and the other pools is that we have two SQL Server pods in the data pool. Building on the architectural diagram, it looks like so:</p>

<p><img src="/images/posts/bdc-lap-around-arch-data-pool.png" alt="" /></p>

<p><strong>Figure 11:</strong> <em>Data Pool</em></p>

<p>We see the data pool in <em>Figure 11</em>, and how it has the two SQL Server instances mentioned above. The reason it has two is that the data pool acts as a persisting and caching layer of external data. The data pool allows for performance querying of cached data against external data sources and offloading of work.</p>

<p>You ingest data into the data pool using either T-SQL queries or from Spark jobs. When you ingest data into the pool, the data is distributed into shards and stored across all SQL Server instances in the pool.</p>

<blockquote>
<p><strong>NOTE:</strong> The data pool is append only, you cannot edit data in the pool.</p>
</blockquote>

<p>At the beginning of the post, we mentioned Apache Spark and Hadoop, but so far we have only seen SQL Server &ldquo;stuff&rdquo;. Where is Hadoop?</p>

<h4 id="storage-pool">Storage Pool</h4>

<p>In the previous paragraph, we asked where Hadoop comes into the picture, and the answer to that is the storage pool. Let us have a look at one of the pods in the storage pool and see what information we get. We use the same code as for the other pods, and when we execute, we see:</p>

<p><img src="/images/posts/bdc-lap-around-arch-storage-0-containers.png" alt="" /></p>

<p><strong>Figure 12:</strong> <em>Storage Pool &amp; Containers</em></p>

<p>That is interesting! In <em>Figure 12</em> we see the &ldquo;usual suspects&rdquo;; <code>mssql-server</code>, <code>collectd</code>, and <code>fluentbit</code> - but we also see a container we haven&rsquo;t seen before: <code>hadoop</code>.</p>

<p>We have two pods in the storage pool, and the <code>hadoop</code> container in each pod forms part of a Hadoop cluster. The Hadoop container provides persistent storage for unstructured and semi-structured data. Data files, such as Parquet or delimited text, can be stored in the storage pool. Not only is the Hadoop File System, (HDFS), within the container but also Apache Spark:</p>

<p><img src="/images/posts/bdc-lap-around-arch-storage-pool.png" alt="" /></p>

<p><strong>Figure 13:</strong> <em>Storage Pool</em></p>

<p>We have added to our architectural diagram the storage pool cluster as in <em>Figure 13</em>, and at the bottom of the picture, outlined in red, something that looks like files. That represents the ability to mount external HDFS data sources into the storage pool cluster. You access the data via the SQL Server master instance, and PolyBase external tables or you can use the Apache Knox Gateway which sits in the Hadoop name-node: <code>nmnode-0-0</code>, which you see in <em>Figure 4</em>.</p>

<p>You may ask why we have SQL Server instances in the storage pool pods? The Big Data Cluster uses the SQL Servers to optimize the access of the data stored in the HDFS Data Nodes.</p>

<p>We have now looked at the various pools we listed at the beginning of this post, and we should have a relatively good grasp of the architecture of a SQL Server 2019 Big Data Cluster.</p>

<h2 id="applications">Applications</h2>

<p>However, there is one thing more to look at. If we look at <em>Figure 4</em> we see at the very top a pod named <code>appproxy-nsp2m</code>, and its role is <code>proxy</code>. What is this? Well, let us run the same code we have done so many times before and see that containers this pod has:</p>

<p><img src="/images/posts/bdc-lap-around-arch-appproxy-containers.png" alt="" /></p>

<p><strong>Figure 14:</strong> <em>Application Proxy</em></p>

<p>In <em>Figure 14</em> we see that the <code>appproxy-nsp2m</code> has a container named <code>app-service-proxy</code>. This container is used, amongst other things, to enable applications to be deployed to a BDC.</p>

<h4 id="application-pool">Application Pool</h4>

<p>The reason for deploying applications to the BDC is so the applications can benefit from the computational power of the cluster and can access the data that is available on the cluster. Supported runtimes are:</p>

<ul>
<li>R</li>
<li>Python</li>
<li>SSIS</li>
<li>MLeap</li>
</ul>

<p>When we deploy an application to a BDC, it is deployed into the Application Pool:</p>

<p><img src="/images/posts/bdc-lap-around-arch-application-pool.png" alt="" /></p>

<p><strong>Figure 15:</strong> <em>Application Proxy</em></p>

<p>What we see in <em>Figure 15</em> is an example of the application pool where we have a user, outlined in red, interacting with the applications in the pool.</p>

<h2 id="summary">Summary</h2>

<p>This is the second post in a series about SQL Server 2019 Big Data Cluster. In the first post: <a href="/2020/04/26/a-lap-around-sql-server-2019-big-data-cluster-background--technology/">A Lap Around SQL Server 2019 Big Data Cluster: Background &amp; Technology</a> we looked at the reason why SQL Server 2019 Big Data Cluster came about and the tech behind it.</p>

<p>In this post, we looked at the architecture of the BDC. We discussed in this post about:</p>

<ul>
<li>Master Pool: the master instance of SQL Server, which also acts as an entry point into the BDC.</li>
<li>Compute Pool: provides scale-out computational resources for a SQL Server big data cluster.</li>
<li>Data Pool: persistence and caching layer for external data.</li>
<li>Storage Pool: provides persistent storage for unstructured and semi-structured data.</li>
<li>Application Pool: hosts applications running inside the BDC.</li>
</ul>

<p>In future posts, we will look at data virtualization, and how the various pools work.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 50, 2020]]></title>
    <link href="https://nielsberglund.com/2020/12/13/interesting-stuff---week-50-2020/" rel="alternate" type="text/html"/>
    <updated>2020-12-13T09:49:39+02:00</updated>
    <id>https://nielsberglund.com/2020/12/13/interesting-stuff---week-50-2020/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<blockquote>
<p><strong>NOTE:</strong> <em>It is now coming up on Christmas and New Year, and I will take a break with these posts and come back in the beginning of next year.</em></p>
</blockquote>

<p></p>

<h2 id="sql-server-2019-big-data-cluster-bdc">SQL Server 2019 Big Data Cluster (BDC)</h2>

<ul>
<li><a href="https://techcommunity.microsoft.com/t5/sql-server/sql-server-big-data-clusters-cu8-release-surfaces-encryption-at/ba-p/1956946">SQL Server Big Data Clusters CU8 release surfaces Encryption at Rest capabilities and more</a>. The latest cumulative update, (CU8), for SQL Server 2019 BDC includes several fixes, optimizations and adds two main capabilities for SQL Server BDC. This post looks at some of the major improvements, provides additional context to understand the design behind these capabilities better, and points you to relevant resources to learn more and get you started.</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/minibooks/chaos-engineering/">The InfoQ eMag - Real World Chaos Engineering</a>. This <a href="https://www.infoq.com/">InfoQ</a> post links to a download of an &ldquo;eMag&rdquo; around chaos engineering. The eMag pulls together a variety of case studies to show mechanisms by which you can implement chaos engineering.</li>
<li><a href="https://www.infoq.com/news/2020/12/grafana-tempo-distributed-tracin/">Grafana Announces Grafana Tempo, a Distributed Tracing System</a>. The <a href="https://www.infoq.com/">InfoQ</a> article linked to here looks at Grafana Tempo, the distributed tracing backend recently released by Grafana Labs. Grafana Tempo integrates with any existing logging system to create links from trace IDs in log lines, and it only requires object storage like Amazon S3 or Google Cloud Storage (GCS) to operate.</li>
</ul>

<h2 id="data">Data</h2>

<ul>
<li><a href="https://www.theseattledataguy.com/how-can-presto-and-starburst-data-improve-your-data-analytics/">How Can Presto And Starburst Data Improve Your Data Analytics</a>. At <a href="/derivco">Derivco</a> we have started looking at Presto. Presto is an open-source distributed SQL query engine for running interactive analytic queries against data sources of all sizes ranging from gigabytes to petabytes. This post looks more in detail what Presto is and why companies are using it.</li>
<li><a href="https://towardsdatascience.com/deploying-a-python-sql-engine-to-your-cluster-76a590940977">Mix SQL and Machine Learning and leverage your computation cluster</a>. The post linked to above discussed Presto. This post looks at another distributed SQL query engine - <a href="https://nils-braun.github.io/dask-sql/">dask-sql</a>. In the post, the author examines what dask-sql is and how it can be used in machine learning scenarios. Very cool!</li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://medium.com/microsoftazure/no-code-data-enhancement-with-azure-synapse-analytics-and-azure-auto-ml-cb9d97fb0c26">No Code Data Enrichment with Azure Synapse and Azure Machine Learning</a>. This post will walk through how to train and evaluate Azure ML AutoML Regressions model on your data using Azure Synapse Analytics Spark and SQL pools. Quite interesting!</li>
<li><a href="https://www.confluent.io/blog/transactional-machine-learning-with-maads-viper-and-apache-kafka/">Transactional Machine Learning at Scale with MAADS-VIPER and Apache Kafka</a>. The post linked to here shows how transactional machine learning (TML) integrates data streams with automated machine learning (AutoML). Apache Kafka is used as the data backbone, and it allows the creation of a frictionless machine learning process.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.youtube.com/watch?v=KR9yvcNBFIw">Event Streaming Applications with Zero Infrastructure</a>. In this YouTube video, <a href="https://twitter.com/tlberglund">Tim Berglund</a>, (from Confluent), demos how you can quickly spin up new event streaming applications with ksqlDB, Kafka, and connectors, all in a fully managed way on Confluent Cloud.</li>
<li><a href="https://www.confluent.io/blog/kafka-lag-monitoring-and-metrics-at-appsflyer/">Apache Kafka Lag Monitoring at AppsFlyer</a>. One crucial aspect of every distributed system is visibility - how do you see what&rsquo;s going on? In streaming applications, it is vital that we can see if consumers are lagging. The post linked to here looks at how one can implement a system for monitoring lag in Kafka.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 49, 2020]]></title>
    <link href="https://nielsberglund.com/2020/12/06/interesting-stuff---week-49-2020/" rel="alternate" type="text/html"/>
    <updated>2020-12-06T09:06:09+02:00</updated>
    <id>https://nielsberglund.com/2020/12/06/interesting-stuff---week-49-2020/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://www.infoq.com/news/2020/11/microsoft-releases-dotnet-spark/">Microsoft Releases .NET for Apache Spark 1.0</a>. This is an <a href="https://www.infoq.com/">InfoQ</a> article looking at the first major version of .NET for Apache Spark, an open-source package that brings .NET development to the Apache Spark platform. The new release allows .NET developers to write Apache Spark applications using .NET user-defined functions, Spark SQL, and additional libraries such as Microsoft Hyperspace and ML.NET. I can say that the developers here at <a href="/derivco">Derivco</a> are quite excited about this!</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/podcasts/service-mesh-interface/">Michelle Noorali on the Service Mesh Interface Spec and Open Service Mesh Project</a>. The <a href="https://www.infoq.com/">InfoQ</a> podcast linked to here covers quite a few topics: the service mesh interface (SMI) spec, the open service mesh (OSM) project, and the future of application development on Kubernetes.</li>
<li><a href="https://www.infoq.com/articles/microservice-monitoring-right-way/">Monitoring Microservices the Right Way</a>. Another article from <a href="https://www.infoq.com/">InfoQ</a>. The article looks at how recent innovations in open-source time-series databases have improved the scalability of monitoring tools such as Prometheus. These solutions can handle microservices large scale of data while providing metric scraping, querying, and visualization based on Prometheus and Grafana.</li>
<li><a href="https://www.infoq.com/news/2020/12/microservices-strangler-fig/">Migrating a Monolith towards Microservices with the Strangler Fig Pattern</a>. The article linked to here looks at how a company, <a href="https://scholarpack.com">ScholarPack</a>, managed to migrate away from a monolith backend using a Strangler Fig pattern. They applied incremental development and continuous delivery to target customers needs, in the meanwhile strangling their monolith. All this is very interesting for us, <a href="/derivco">Derivco</a>!</li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://engineering.linkedin.com/blog/2020/open-sourcing-dagli">Dagli: Faster and easier machine learning on the JVM, without the tech debt</a>. This post by the LinkedIn engineering team is about the release of Dagli. Dagli is an open source machine learning library for Java (and other JVM languages) that makes it easy to write bug-resistant, readable, modifiable, maintainable, and trivially deployable model pipelines without incurring technical debt.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/apache-kafka-spring-cloud-data-flow-tutorial/">Getting Started with Spring Cloud Data Flow and Confluent Cloud</a>. This blog post gives you the foundation for event streaming and designing and implementing real-time patterns. Using Confluent Schema Registry, ksqlDB, and fully managed Apache Kafka as a service, you can experience clean, seamless integrations with your existing cloud provider. The post also discusses Spring Cloud Data Flow which is a microservices-based toolkit for building streaming and batch data processing pipelines.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 48, 2020]]></title>
    <link href="https://nielsberglund.com/2020/11/29/interesting-stuff---week-48-2020/" rel="alternate" type="text/html"/>
    <updated>2020-11-29T09:01:28+02:00</updated>
    <id>https://nielsberglund.com/2020/11/29/interesting-stuff---week-48-2020/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/netflix-api-graphql-federation/">How Netflix Scales Its API with GraphQL Federation</a>. This is an <a href="https://www.infoq.com/">InfoQ</a> presentation, where the presenters look at how Netflix uses GraphQL to scale its API&rsquo;s. The usage of GraphQL looks very cool, and maybe we can use it at <a href="/derivco">Derivco</a> as well!</li>
<li><a href="https://medium.com/swlh/the-6-things-you-need-to-know-about-event-driven-architectures-38e11fdcb5a">The 6 Things You Need to Know About Event-Driven Architectures</a>. The post linked to is the second in a series about event driven architectures, (the first is <a href="https://medium.com/swlh/the-engineers-guide-to-event-driven-architectures-benefits-and-challenges-3e96ded8568b">here</a>). In this post, the author looks at, and explains, what in his mind are key concepts of event-driven architectures.</li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://towardsdatascience.com/deploy-machine-learning-applications-to-kubernetes-using-streamlit-and-polyaxon-49bf4b963515">Deploy Machine Learning applications to Kubernetes using Streamlit and Polyaxon</a>. This is a step-by-step guide on how to train, analyze, and deploy a containerized Streamlit machine learning application on Kubernetes using Polyaxon.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://netflixtechblog.com/simple-streaming-telemetry-27447416e68f">Simple streaming telemetry</a>. This post looks at the Netflix <code>gnmi-gateway</code> project. <code>gnmi-gateway</code> is a modular, distributed, and highly available service for modern network telemetry via OpenConfig and gNMI. The <code>gnmi-gateway</code> project looks interesting, and I&rsquo;ll forward the post to our network guys.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>If you have not signed up for the <a href="https://dataplatformgeeks.com/dps2020/"><strong>Data Platform Virtual Summit 2020</strong></a> yet, you still have some time!</p>

<p>The <strong>Data Platform Virtual Summit 2020</strong>, (DPS), is a 100% technical learning event with 200 Breakout Sessions, 30 Training Classes, 72 hours of non-stop conference sessions.</p>

<p>DPS 2020 is the largest online learning event on Microsoft Azure Data, Analytics &amp; Artificial Intelligence. Delegates get the recordings at no extra cost, which is quite a wonderful thing. Also, the conference virtual platform looks amazing, <a href="https://www.linkedin.com/posts/amitbansal2010_dps2020-sqlserver-powerbi-activity-6728885748755374080-a8QL/">have a look</a>.</p>

<p>You <a href="https://dataplatformgeeks.com/dps2020/booking/">book here</a>. Oh, and since I am a speaker I get a discount code to hand out to you guys! Use the discount code <strong>DPSSPEAKER</strong> to book your seat at <strong>55%</strong> off.</p>

<p>If you wonder what I am speaking about, this should give you an idea:</p>

<p><img src="/images/posts/Niels_Berglund.jpg" alt="" /></p>

<p>In my talk, I will be talking about Kafka and SQL Server, and various ways we can &ldquo;set our SQL Server data free&rdquo;!</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 47, 2020]]></title>
    <link href="https://nielsberglund.com/2020/11/22/interesting-stuff---week-47-2020/" rel="alternate" type="text/html"/>
    <updated>2020-11-22T08:52:30+02:00</updated>
    <id>https://nielsberglund.com/2020/11/22/interesting-stuff---week-47-2020/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="azure-data-studio">Azure Data Studio</h2>

<ul>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2020/11/16/building-and-sharing-jupyter-books-in-azure-data-studio/">Building and sharing Jupyter Books in Azure Data Studio</a>. We all should know by now that Azure Data Studio allows us to use Jupyter notebooks. This post looks at how we can not only use Jupyter books but also create and share them. Very cool!</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://medium.com/expedia-group-tech/autoscaling-in-kubernetes-a-primer-on-autoscaling-7b8f0f95a928">Autoscaling in Kubernetes: A Primer on Autoscaling</a>. This post is the first in a series looking at application autoscaling in Kubernetes. I was going to write that I really look forward to the second instalment when I realized it already had been <a href="https://medium.com/expedia-group-tech/autoscaling-in-kubernetes-options-features-and-use-cases-c8a6ce145957">published</a>! Awesome!</li>
<li><a href="https://martin.kleppmann.com/2020/11/18/distributed-systems-and-elliptic-curves.html">New courses on distributed systems and elliptic curve cryptography</a>. As the title says; Martin Kleppman of <a href="http://dataintensive.net/">Designing Data-Intensive Applications</a> fame have released some new training courses. I am very interested in the distributed systems course; the videos look awesome! This course is a must for anyone interested in distributed systems!</li>
<li><a href="https://medium.com/swlh/distributed-systems-and-asynchronous-i-o-ef0f27655ce5">Distributed Systems and Asynchronous I/O</a>. The post linked to here looks at how different forms of handling I/O affect the performance, availability, and fault-tolerance of network applications.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://medium.com/@dt_23597/if-youre-using-kafka-with-your-microservices-you-re-probably-handling-retries-wrong-8492890899fa">If Youre Using Kafka With Your Microservices, Youre Probably Handling Retries Wrong</a>. In this excellent article, the author looks at various ways of handling retries in Kafka. The article presents a potential solution together with the downsides of that particular solution. As I said in the beginning - this is an excellent article!</li>
<li><a href="https://www.confluent.io/blog/how-real-time-stream-processing-safely-scales-with-ksqldb/">How Real-Time Stream Processing Safely Scales with ksqlDB, Animated</a>. This post is the third in a series around ksqlDB and how it executes stateless and stateful operations. The two previous posts have looked at a single server setup. This post looks at how stateless and stateful operations work when ksqlDB is deployed with many servers, and more importantly, how it linearly scales the work it is performingeven in the presence of faults.</li>
<li><a href="https://www.confluent.io/blog/using-kafka-ksqldb-kibana-to-stream-data-and-get-real-time-analytics/">Analysing historical and live data with ksqlDB and Elastic Cloud</a>. This is a great post by <a href="https://twitter.com/rmoff">Robin Moffat</a>. He looks at how you can take &ldquo;messy and imperfect&rdquo; data, (think CSV), from a &ldquo;raw data&rdquo; Kafka topic, re-format it, and make it presentable with ksqlDB, push it into another topic, and from there stream it into an analytical dashboard. Awesome stuff!</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>Don&rsquo;t forget Data Platform Summit 2020.</p>

<p><img src="/images/posts/dps_2020.png" alt="" /></p>

<p>I am super excited to be speaking at the <a href="https://dataplatformgeeks.com/dps2020/"><strong>Data Platform Virtual Summit 2020</strong></a>:</p>

<p><img src="/images/posts/Niels_Berglund.jpg" alt="" /></p>

<p>and as you see in the figure above, my presentation is about Kafka and SQL Server.</p>

<p>The <strong>Data Platform Virtual Summit 2020</strong>, (DPS), is a 100% technical learning event with 200 Breakout Sessions, 30 Training Classes, 72 hours of non-stop conference sessions. DPS 2020 is the largest online learning event on Microsoft Azure Data, Analytics &amp; Artificial Intelligence. Delegates get the recordings at no extra cost, which is quite a wonderful thing. Also, the conference virtual platform looks amazing, <a href="https://www.linkedin.com/posts/amitbansal2010_dps2020-sqlserver-powerbi-activity-6728885748755374080-a8QL/">take a look</a>.</p>

<p>If you want to attend and hear industry experts talk about really exciting stuff you can <a href="https://dataplatformgeeks.com/dps2020/booking/">book here</a>. Oh, and the coolest thing is that as I am a speaker I get a discount code to hand out to you guys! Use the discount code <strong>DPSSPEAKER</strong> to book your seat at <strong>55%</strong> off.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 46, 2020]]></title>
    <link href="https://nielsberglund.com/2020/11/15/interesting-stuff---week-46-2020/" rel="alternate" type="text/html"/>
    <updated>2020-11-15T09:31:21+02:00</updated>
    <id>https://nielsberglund.com/2020/11/15/interesting-stuff---week-46-2020/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="misc">Misc.</h2>

<ul>
<li><a href="https://towardsdatascience.com/how-to-integrate-python-and-r-in-visual-studio-code-496a47c90422">How to integrate Python and R in Visual Studio Code</a>. I really like VSCode, but I always have issues with how to configure for Python and R when installing from scratch. So this blog-post comes in real handy, as it explains what one needs to do to get Python and R up and running in VSCode. Awesome!</li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://towardsdatascience.com/synthetic-data-vault-sdv-a-python-library-for-dataset-modeling-b48c406e7398">Synthetic Data Vault (SDV): A Python Library for Dataset Modeling</a>. When doing machine learning/data science, you need realistic data to work with, and that can sometimes be a problem. This post introduces the Synthetic Data Vault, which is a tool to generate complex datasets using statistical &amp; machine-learning models. It looks very interesting!</li>
<li><a href="https://eng.uber.com/metadata-insights-databook/">Turning Metadata Into Insights with Databook</a>. This post looks at Uber&rsquo;s system for handling metadata - Databook.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://medium.com/apache-pinot-developer-blog/using-apache-pinot-and-kafka-to-analyze-github-events-93cdcb57d5f7">Using Apache Pinot and Kafka to Analyze GitHub Events</a>. Apache Pinot is a real-time distributed OLAP datastore, which is used to deliver scalable real-time analytics with low latency. The post I linked to here discusses how to ingest Kafka events into Pinot. What we see in the blog post is very interesting for <a href="/derivco">us</a>, and we will definitely look at it in more detail.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>In <a href="/2020/11/08/interesting-stuff---week-45-2020/">last weeks roundup</a> I wrote about my recording of video for the Data Platform Summit 2020.</p>

<p><img src="/images/posts/dps_2020.png" alt="" /></p>

<p>If I haven&rsquo;t said it before:</p>

<p>I am super excited to be speaking at the <a href="https://dataplatformgeeks.com/dps2020/"><strong>Data Platform Virtual Summit 2020</strong></a>. A 100% technical learning event with 200 Breakout Sessions, 30 Training Classes, 72 hours of non-stop conference sessions  DPS 2020 is the largest online learning event on Microsoft Azure Data, Analytics &amp; Artificial Intelligence. Delegates get the recordings at no extra cost, which is quite a wonderful thing. Also, the conference virtual platform looks amazing, <a href="https://www.linkedin.com/posts/amitbansal2010_dps2020-sqlserver-powerbi-activity-6728885748755374080-a8QL/">take a look</a>.</p>

<p>If you want to attend and hear industry experts talk about really exciting stuff you can <a href="https://dataplatformgeeks.com/dps2020/booking/">book here</a>. Oh, and the coolest thing is that as I am a speaker I get a discount code to hand out to you guys! Use the discount code <strong>DPSSPEAKER</strong> to book your seat at <strong>55%</strong> off.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 45, 2020]]></title>
    <link href="https://nielsberglund.com/2020/11/08/interesting-stuff---week-45-2020/" rel="alternate" type="text/html"/>
    <updated>2020-11-08T10:32:09+02:00</updated>
    <id>https://nielsberglund.com/2020/11/08/interesting-stuff---week-45-2020/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://blog.acolyer.org/2020/11/02/helios-part-ii/">Helios: hyperscale indexing for the cloud &amp; edge (part II)</a>. In last weeks <a href="/2020/11/01/interesting-stuff---week-44-2020/">roundup</a>, I pointed to a post by <a href="https://twitter.com/adriancolyer">Adrian</a>, where he dissected a white-paper about Helios, and I said how I looked forward to part 2. Well, I got what I wanted, and the post I link to here is the follow-up. Enjoy!</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://vlfig.me/posts/microservices">Microservices  architecture nihilism in minimalism&rsquo;s clothes</a>. This post looks at microservices and argues that there is no such thing as a microservice architecture, but software architecture. The author also looks at the size of a microservice and argues that there is no prescribed size of a microservice. I found the post very interesting!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://itnext.io/change-data-capture-with-azure-postgresql-and-kafka-4598dbf0b57a">Change Data Capture with Azure, PostgreSQL, and Kafka</a>. In this blog post, the author looks at how one can use Change Data Capture to stream database modifications from PostgreSQL to Azure Data Explorer, (Kusto), using Apache Kafka. It is an interesting post! As a side note, I must say that Azure Data Explorer looks really interesting!</li>
<li><a href="https://www.confluent.io/blog/pull-queries-in-preview-confluent-cloud-ksqdb/">Announcing Pull Queries in Preview in Confluent Cloud ksqlDB</a>. ksqlDB has two types of queries: push and pull. Push queries allow you subscribe to a query&rsquo;s result as it changes in real-time, whereas with a pull query you fetch the current state of a materialized view. Both types of queries have been in Confluent Platform for a while, but Confluent Cloud has up until now only supported push queries. That changes now, and the post I linked to here discusses more in detail about the support for pull queries in Confluent Cloud.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>Well, it is not so much about what I am doing, as it is of what I have been doing the last couple of days. You who read my blog are probably aware that:</p>

<p><img src="/images/posts/Niels_Berglund.jpg" alt="" /></p>

<p><strong>Figure 1:</strong> <em>SQL Server &amp; Kafka</em></p>

<p>Yes, I am speaking at the conference! My topic is about, as you can see in <em>Figure 1</em>, the various ways we can stream data from SQL Server to Apache Kafka. In a previous roundup, I wrote how I was prepping for the talk. That is all good and well, but a while ago a &ldquo;curve-ball&rdquo; was thrown: the conference is obviously virtual, but: we are not presenting live! Which means that the last few days, I have been recording, and editing the recording. Geez, recording and editing is hard work, compared to &ldquo;just&rdquo; deliver. Well, it is done now, and if you <a href="https://dataplatformgeeks.com/dps2020/booking/https://dataplatformgeeks.com/dps2020/booking/">register</a> you have the chance to see other speakers and me in a recorded fashion.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 44, 2020]]></title>
    <link href="https://nielsberglund.com/2020/11/01/interesting-stuff---week-44-2020/" rel="alternate" type="text/html"/>
    <updated>2020-11-01T11:01:29+02:00</updated>
    <id>https://nielsberglund.com/2020/11/01/interesting-stuff---week-44-2020/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://blog.acolyer.org/2020/10/26/helios-part-1/">Helios: hyperscale indexing for the cloud &amp; edge  part 1</a>. In this post <a href="https://twitter.com/adriancolyer">Adrian</a> from the <a href="https://blog.acolyer.org">morning paper</a> dissects a white-paper about Helios. Helios is a distributed, highly-scalable system used at Microsoft for flexible ingestion, indexing, and aggregation of large streams of real-time data that is designed to plug into relational engines. Adrian is as thorough as usual, and the conclusions he draws are very interesting. I can&rsquo;t wait for part 2.</li>
</ul>

<h2 id="distributed-systems">Distributed Systems</h2>

<ul>
<li><a href="https://medium.com/@polyglot_factotum/how-i-am-learning-distributed-systems-7eb69b4b51bd">How I am learning distributed systems</a>. This post looks, from one person&rsquo;s perspective, how one can learn to design distributed systems. What is interesting in this post is the use of <a href="https://raft.github.io/">Raft</a>, (no, not Raft the game - but the consensus algorithm), as a learning tool. I will definitely point to this post as a learning resource for my developers.</li>
<li><a href="https://thenewstack.io/nginx-steps-into-the-service-mesh-fray-promising-a-simpler-alternative/">NGINX Steps into the Service Mesh Fray Promising a Simpler Alternative</a>. The post linked to here points discusses how NGINX introduces its own service mesh: <a href="https://www.nginx.com/products/nginx-service-mesh">NGINX Service Mesh</a>, (NSM). It promises to be less complicated than ISTIO, so I will definitely have a look.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/how-to-prepare-for-kip-500-kafka-zookeeper-removal-guide/">Preparing Your Clients and Tools for KIP-500: ZooKeeper Removal from Apache Kafka</a>. The Kafka community has for quite a while been talking about removing the dependency of ZooKeeper, (ZK), from Kafka, and it seems we are getting closer. In the post I have linked to here, the author looks at what is needed to do in Kafka consumers so that nothing &ldquo;bad&rdquo; happens when ZK is eventually removed.</li>
<li><a href="https://www.kai-waehner.de/blog/2020/10/27/streaming-machine-learning-kafka-native-model-server-deployment-rpc-embedded-streams/">Streaming Machine Learning with Kafka-native Model Deployment</a>. Kafka is used more and more for real-time machine learning purposes, and we are moving towards Kafka as a native streaming model server. This blog post explores the architectures and trade-offs between various options for model deployment with Kafka.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 43, 2020]]></title>
    <link href="https://nielsberglund.com/2020/10/25/interesting-stuff---week-43-2020/" rel="alternate" type="text/html"/>
    <updated>2020-10-25T09:35:38+02:00</updated>
    <id>https://nielsberglund.com/2020/10/25/interesting-stuff---week-43-2020/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/news/2020/10/kubernetes-chaos-mesh-ga/">Chaos Engineering on Kubernetes : Chaos Mesh Generally Available with v1.0</a>. In <a href="/2020/10/18/interesting-stuff---week-42-2020/">last weeks roundup</a>, I wrote about Kraken, a chaos tool for OpenShift/Kubernetes. This week and in this <a href="https://www.infoq.com/">InfoQ</a> article, we look at another chaos tool; Chaos Mesh.</li>
</ul>

<h2 id="big-data-analytics">Big Data Analytics</h2>

<ul>
<li><a href="https://eng.uber.com/operating-apache-pinot/">Operating Apache Pinot @ Uber Scale</a>. Apache Pinot is a distributed OLAP system designed for performing low latency analytical queries on really Big Data. The post linked to here looks at how Uber is using Pinot, and how the underlying architecture looks like.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/real-time-risk-management-with-kafka-and-event-streaming/">Lessons Learned from Evolving a Risk Management Platform to Event Streaming</a>. This post looks at how a retailer uses Kafka and event streaming to perform risk management. Very cool!</li>
<li><a href="https://netflixtechblog.com/building-netflixs-distributed-tracing-infrastructure-bb856c319304">Building Netflixs Distributed Tracing Infrastructure</a>. This post looks at the underlying architecture/components for Netflix tracing framework <a href="https://netflixtechblog.com/edgar-solving-mysteries-faster-with-observability-e1a76302c71f">Edgar</a>. It is a very interesting and informative post! Let us see if we can do something similar at <a href="/derivco">Derivco</a>.</li>
<li><a href="https://www.confluent.io/blog/bounding-ksqldb-memory-usage/">Bounding ksqlDB Memory Usage</a>. This post which looks at how one can solve unbounded ksqlDB memory growth comes at the right time for us at <a href="/derivco">Derivco</a>, as we have experienced this problem in ksqlDB.</li>
<li><a href="https://www.confluent.io/blog/build-a-intrusion-detection-using-ksqldb/">Intrusion Detection with ksqlDB</a>. We know that ksqlDB can be used for many things and the post linked to here looks at how to use ksqlDB to detect network intrusions. Really, really cool!</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 42, 2020]]></title>
    <link href="https://nielsberglund.com/2020/10/18/interesting-stuff---week-42-2020/" rel="alternate" type="text/html"/>
    <updated>2020-10-18T08:55:21+02:00</updated>
    <id>https://nielsberglund.com/2020/10/18/interesting-stuff---week-42-2020/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>Kafka, Kafka, and Kafka</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.openshift.com/blog/introduction-to-kraken-a-chaos-tool-for-openshift/kubernetes">Introduction to Kraken, a Chaos Tool for OpenShift/Kubernetes</a>. As the title of this post says, it introduces a Kubernetes tool which by you can chaos test your Kubernetes cluster. It looks quite interesting and useful, and I have forwarded the post to the guys at <a href="/derivco">Derivco</a> looking at Kubernetes.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.fullcontact.com/blog/2020/10/08/building-a-lambda-architecture-with-druid-and-kafka-streams/">Building a Lambda Architecture with Druid and Kafka Streams</a>. The post linked to here looks at how to use <a href="https://druid.apache.org/">Druid</a> together with KStreams to build a real-time analytical framework. Really interesting &ldquo;stuff&rdquo;!</li>
<li><a href="https://www.confluent.io/blog/how-real-time-materialized-views-work-with-ksqldb/">How Real-Time Materialized Views Work with ksqlDB, Animated</a>. The post delivers precisely what the title says: it looks at how materialized views work in ksqlDB. I really like the animations in the post, very cool!</li>
<li><a href="https://www.infoq.com/articles/real-time-api-kafka/">Real Time APIs in the Context of Apache Kafka</a>. This <a href="https://www.infoq.com/">InfoQ</a> article by the Kafka Guru <a href="https://twitter.com/rmoff">Robin Moffat</a> looks at how events together with real-time APIs can be used as the foundation for applications which are flexible yet performant; loosely-coupled yet efficient. Apache Kafka offers a scalable event streaming platform with which you can build applications around the powerful concept of events.<br /></li>
<li><a href="https://www.confluent.io/blog/5-things-every-kafka-developer-should-know/">Top 5 Things Every Kafka Developer Should Know</a>. I believe that a developer, to be a good developer, should have an in-depth knowledge of the technologies he/she is working with. If you are using Kafka, then the post linked to here gives you a good start in understanding Kafka.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 41, 2020]]></title>
    <link href="https://nielsberglund.com/2020/10/11/interesting-stuff---week-41-2020/" rel="alternate" type="text/html"/>
    <updated>2020-10-11T08:57:28+02:00</updated>
    <id>https://nielsberglund.com/2020/10/11/interesting-stuff---week-41-2020/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="sql-server-2019-big-data-cluster">SQL Server 2019 Big Data Cluster</h2>

<ul>
<li><a href="https://mohammaddarab.com/stop-and-start-your-aks-big-data-cluster-and-save-money/">Stop and Start your AKS Big Data Cluster and Save $$$</a>. In this post, my good friend <a href="https://twitter.com/mwdarab">Mohammad</a> looks at new functionality in Azure Kubernetes Service, (AKS), whereby you can start and stop Kubernetes clusters at will. Mohammad looks at it from the perspective of SQL Server 2019 Big Data Clusters running in AKS.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://medium.com/data-rocks/kafka-record-tracing-379ed2b0af51">Kafka record tracing</a>. Do you know how long it takes for messages to flow in your data pipeline? Do you know where the bottlenecks are? Why I ask these questions is that in a distributed system, the answers may not always be that easy to find. The post linked to here looks at solutions to these questions, and it looks at distributed tracing for Kafka.</li>
<li><a href="https://www.confluent.io/blog/kafka-cluster-linking-with-confluent-platform/">Introducing Cluster Linking in Confluent Platform 6.0</a>. As the title of this post says, the post is about linking Kafka clusters to each other. From the post: &ldquo;Replicating topics between Kafka clusters has been a long-standing problem thats seen a number of solutions, including MirrorMaker and Confluent Replicator. Although the utility of these projects has come a long way, theyre not without their respective issues.&rdquo;. The issues referred to in the previous sentence, is what the new cluster linking functionality is supposed to fix. It looks very promising, the one question I have right now is if this is a Confluent Platform Enterprise only functionality?</li>
<li><a href="https://medium.com/data-rocks/managing-kafka-connectors-at-scale-using-kafka-connect-manager-kcm-31d887de033c">Managing Kafka Connectors at scale using Kafka Connect Manager</a>. A while back we, (<a href="/derivco">Derivco</a>), looked at using CDC and Debezium to get data out from the database to Kafka. One of the problems we faced was how to manage the various Kafka Connectors, and ensure they were functioning correctly. The post I link to here would have been the perfect solution for us - a framework for managing Kafka Connectors. It would be interesting to know if it is open-sourced.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 40, 2020]]></title>
    <link href="https://nielsberglund.com/2020/10/04/interesting-stuff---week-40-2020/" rel="alternate" type="text/html"/>
    <updated>2020-10-04T15:40:24+02:00</updated>
    <id>https://nielsberglund.com/2020/10/04/interesting-stuff---week-40-2020/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://databricks.com/blog/2020/09/29/diving-into-delta-lake-dml-internals-update-delete-merge.html">Diving Into Delta Lake: DML Internals (Update, Delete, Merge)</a>. Later releases of Delta Lake supports DML, (data manipulation language), statements. The post linked to here demonstrates how to use each of these DML commands, describes what Delta Lake is doing behind the scenes when you run one, and offers some performance tuning tips for each one. Very cool!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/confluent-platform-6-0-delivers-the-most-powerful-event-streaming-platform-to-date/">Introducing Confluent Platform 6.0</a>. As the title of this post says; the post is about the new 6.0 release of Confluent Platform. Reading the post, this release looks huge. There are so many interesting things in there, but the ones that stand out for me are the ksqlDB improvements! For more information about the new ksqlDB functionality, look at the post <a href="https://www.confluent.io/blog/ksqldb-0-12-0-features-updates/">ksqlDB 0.12.0 Introduces Real-Time Query Upgrades and Automatic Query Restarts</a>. This is awesome!</li>
<li><a href="https://www.confluent.io/blog/how-real-time-stream-processing-works-with-ksqldb/">How Real-Time Stream Processing Works with ksqlDB, Animated</a>. While we are on the subject of ksqlDB, this post is doing a great job how ksqlDB works. If you are interested in ksqlDB at all, you must read the post!</li>
<li><a href="https://www.confluent.io/blog/ksqldb-java-client-iot-inspired-demo/">ksqlDB Meets Java: An IoT-Inspired Demo of the Java Client for ksqlDB</a>. Even more ksqlDB. The post linked to here looks at the new ksqlDB Java client. So, instead of using REST API&rsquo;s, you can now do ksqlDB &ldquo;stuff&rdquo; programmatically, using this new client, That is very, very interesting!</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>Yes, what AM I doing? Sometimes, quite often actually, I wonder that myself. Today, however, I do know what I am doing, or more specifically, what I just did! Yesterday, (October 3), I delivered a virtual conference talk at SQLBits about SQL Server and Kafka.</p>

<p>Unfortunately, the talk was pre-recorded, and after I had uploaded the recording, the organizers did some editing. I messed up with what parts of the talk needed editing, so the result was that around a third of the talk was missing. Not good! Anyway, the organizers will do a re-edit and upload so that attendees can view it on demand. If you were one of the attendees, I do apologize again!</p>

<p>Anyway, today I uploaded the slide deck as well as all code for the talk, (including instructions). If you are interested, you can download it from <a href="https://nielsberglund.com/download/set-data-free-kafka.zip">here</a>. If you have questions comments etc., please <a href="mailto:niels.it.berglund@gmail.com">ping me</a>.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 39, 2020]]></title>
    <link href="https://nielsberglund.com/2020/09/27/interesting-stuff---week-39-2020/" rel="alternate" type="text/html"/>
    <updated>2020-09-27T09:41:44+02:00</updated>
    <id>https://nielsberglund.com/2020/09/27/interesting-stuff---week-39-2020/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/monzo-microservices/">Modern Banking in 1500 Microservices</a>. This is an <a href="https://www.infoq.com/">InfoQ</a> presentation where the presenters explain how <a href="https://en.wikipedia.org/wiki/Monzo_(bank)">Monzo</a> builds, operates, observes and maintains the banking infrastructure. They talk about how they compose microservices to add new functionality, deployment and incident tooling, monitoring practices and how they share knowledge effectively. Very interesting!</li>
<li><a href="https://www.infoq.com/podcasts/urban-planning-software-architecture/">Pat Helland on Software Architecture and Urban Planning</a>. The link here is to a podcast by the legendary <a href="https://www.linkedin.com/in/pathelland/">Pat Helland</a> where he talks about the relationship between software architecture and urban planning. Pat explores planning for future growth, regulations/standards, and communication practices that cities - and software architecture - had to evolve to use. He uses these comparisons to distil lessons that architects can use in building distributed systems. If you are only going to listen to one podcast today, this week, month, etc., listen to this one!</li>
</ul>

<h2 id="azure-data-studio">Azure Data Studio</h2>

<ul>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2020/09/22/the-september-2020-release-of-azure-data-studio-is-now-available/">The September 2020 release of Azure Data Studio is now available</a>. As the title of the post says: a new version of Azure Data Studio is out. It has quite a few new features and enhancements, new notebook functionality, support for Kusto Query Language, (KQL), and other stuff. After I have published this post I will download this version and take it through its paces.</li>
</ul>

<h2 id="sql-server">SQL Server</h2>

<ul>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2020/09/24/open-sourcing-the-r-and-python-language-extensions-for-sql-server/">Open sourcing the R and Python language extensions for SQL Server</a>. Wow, this is big! Not only have the SQL Server language extensions for R and Python been open-sourced, but you can now use whatever version of R and Python you want, (well, more or less). Look out for some blog posts from me around this. Here are two links if you are interested in how it worked up until now: <a href="https://nielsberglund.com/categories/sql-server-machine-learning-services/">Sql Server Machine Learning Services</a>, and <a href="https://nielsberglund.com/categories/sql-server-extensibility-framework/">Sql Server Extensibility Framework</a>.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/kafka-devops-with-confluent-kubernetes-and-gitops/">Apache Kafka DevOps with Kubernetes and GitOps</a>. The post linked to here looks at how we can do DevOps with Kafka and Kubernetes.</li>
<li><a href="https://blog.twitter.com/engineering/en_us/topics/infrastructure/2020/streaming-logging-pipeline-of-home-timeline-prediction-system.html">Streaming logging pipeline of Home timeline prediction system</a>. Twitter recently built a streaming data logging pipeline for its home timeline prediction system using Apache Kafka and Kafka Streams to replace the existing offline batch pipeline. This post looks at the architecture and how Twitter did it. In the post, it is mentioned that Twitter had to customise how KStreams left join semantic, and that is covered in the post: <a href="https://www.confluent.io/blog/how-twitter-built-a-machine-learning-pipeline-with-kafka/">Building a Machine Learning Logging Pipeline with Kafka Streams at Twitter</a>.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 38, 2020]]></title>
    <link href="https://nielsberglund.com/2020/09/20/interesting-stuff---week-38-2020/" rel="alternate" type="text/html"/>
    <updated>2020-09-20T08:19:16+02:00</updated>
    <id>https://nielsberglund.com/2020/09/20/interesting-stuff---week-38-2020/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="azure-sql-server">Azure SQL Server</h2>

<ul>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2020/09/17/azure-sql-digital-event-transform-your-applications-with-azure-sql/">Azure SQL digital event: Transform your applications with Azure SQL</a>. This post announces a webinar by <a href="https://twitter.com/bobwardms">Bob Ward</a> about Azure SQL, and how you can get the most out of Azure SQL. As the post says: &ldquo;<em>During this hour-long virtual event, I&rsquo;ll share my advice and guidance on getting the most out of Azure SQL, whether youre looking to migrate from an on-premises SQL Server deployment or exploring ways to utilize the newest Azure SQL service options. You will also hear firsthand experiences and best practices from customers who have become expert Azure SQL users.</em>&rdquo;. Bob is a legend in the SQL Server world, so if you are interested, hurry up and register!</li>
</ul>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://medium.com/superawesome-engineering/how-we-use-apache-druids-real-time-analytics-to-power-kidtech-at-superawesome-8da6a0fb28b1">How we use Apache Druids real-time analytics to power kidtech at SuperAwesome</a>. This post looks at real-time analytics of big data using Kafka and Apache Druid. I like the post a lot!</li>
<li><a href="https://databricks.com/blog/2020/09/15/easily-clone-your-delta-lake-for-testing-sharing-and-ml-reproducibility.html">Easily Clone your Delta Lake for Testing, Sharing, and ML Reproducibility</a>. The post linked to here introduces new functionality in Databricks Delta Lake; the ability to clone tables. This can come in handy for several scenarios: ML result reproducibility, data migration, significant changes to production tables, etc.</li>
</ul>

<h2 id="event-driven-architecture">Event Driven Architecture</h2>

<ul>
<li><a href="https://www.infoq.com/articles/event-driven-finding-seams/">From Monolith to Event-Driven: Finding Seams in Your Future Architecture</a>. An excellent <a href="https://www.infoq.com/">InfoQ</a> article looking at what we can do when we want to migrate from a monolithic architecture to a microservices/domain driven architecture.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://medium.com/faun/event-driven-autoscaling-for-kubernetes-with-kafka-keda-d68490200812">Event-driven Autoscaling for Kubernetes with Kafka &amp; Keda</a>. A very cool post looking at how to scale Kafka consumers on Kubernetes. As I said, very cool!</li>
<li><a href="https://www.confluent.io/blog/stream-data-from-kafka-to-azure-data-explorer/">Streaming Data from Apache Kafka into Azure Data Explorer with Kafka Connect</a>. This blog post shows how to stream events from Apache Kafka on Confluent Cloud on Azure, into Azure Data Explorer, using the Kafka Connect Kusto Sink Connector.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>I am prepping for one of the largest data platform conferences taking place, (virtually), beginning of December this year: <a href="https://dataplatformgeeks.com/dps2020/"><strong>DATA PLATFORM VIRTUAL SUMMIT 2020</strong></a>. I hear you asking: &ldquo;why are you prepping?&rdquo;. Well, that&rsquo;s because:</p>

<p><img src="/images/posts/Niels_Berglund.jpg" alt="" /></p>

<p><strong>Figure 1:</strong> <em>SQL Server &amp; Kafka</em></p>

<p>Yes, I am speaking at the conference! My topic is about, as you can see in <em>Figure 1</em>, the various ways we can stream data from SQL Server to Apache Kafka.</p>

<p>The conference has world class-speakers, (and me), so hurry up and <a href="https://dataplatformgeeks.com/dps2020/booking/">register</a>!</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 37, 2020]]></title>
    <link href="https://nielsberglund.com/2020/09/13/interesting-stuff---week-37-2020/" rel="alternate" type="text/html"/>
    <updated>2020-09-13T09:38:45+02:00</updated>
    <id>https://nielsberglund.com/2020/09/13/interesting-stuff---week-37-2020/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/distributed-video-sharing-demo/">Distributed Programming, Hash Tables, and Fun!</a>. In this <a href="https://www.infoq.com/">InfoQ</a> presentation, the presenters demonstrate how they built a distributed hash-table video-sharing system, the technical hurdles encountered, and the pros/cons of using functional languages to do so. Very interesting!</li>
</ul>

<h2 id="machine-learning">Machine Learning</h2>

<ul>
<li><a href="https://becominghuman.ai/cheat-sheets-for-ai-neural-networks-machine-learning-deep-learning-big-data-678c51b4b463">Cheat Sheets for AI, Neural Networks, Machine Learning, Deep Learning &amp; Big Data</a>. Cheat sheets are good, and I certainly need them. The post linked to gives, according to the post, <em>the most complete list of best AI cheat sheets</em>. I don&rsquo;t know about that, but there are some quite cool cheat sheets in there.</li>
<li><a href="https://dalelane.co.uk/blog/?p=4124">Using TensorFlow to make predictions from Kafka events</a>. This is a very interesting post, looking at how you can use Kafka and TensorFlow together to make machine learning predictions around sensor events.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/prioritize-messages-in-kafka/">Implementing Message Prioritization in Apache Kafka</a>. This post came at the exact right time, as I have been battling with how to implement priority messages in Kafka. In Rabbit, which we used previously, it was easy. In Kafka not so much. So reading about &ldquo;bucketing&rdquo; saved the day!</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>In <a href="/2020/09/06/interesting-stuff---week-36-2020/">last weeks roundup</a> I said how I was going to do a <strong>Derivco Webinar</strong> this week about <strong>SQL Server 2019 Big Data Cluster</strong>. Well, I did it September 10, it went off without a &ldquo;hitch&rdquo;, and - if I can say it myself - it was a lot of fun. If you are interested in seeing it, you find it on the Derivco YouTube channel <a href="https://www.youtube.com/watch?v=EyCsQP5VJhU">here</a>.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 36, 2020]]></title>
    <link href="https://nielsberglund.com/2020/09/06/interesting-stuff---week-36-2020/" rel="alternate" type="text/html"/>
    <updated>2020-09-06T08:40:13+02:00</updated>
    <id>https://nielsberglund.com/2020/09/06/interesting-stuff---week-36-2020/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://medium.com/@madslundt/microservices-with-event-sourcing-using-net-core-33e3074171f5">Microservices with event sourcing using .NET Core</a>. This post looks at how to implement and structure an API using microservices and event sourcing. The article is very interesting, and it definitely gives &ldquo;food for thought&rdquo;!</li>
<li><a href="https://devblogs.microsoft.com/dotnet/introducing-the-half-type/">Introducing the Half type!</a>. The title of the post says it all. Microsoft introduces a new type in .NET 5 - the <code>Half</code> type, which is <code>binary16</code>. The reason for this is that many computation workloads already take advantage of the <code>Half</code> type: machine learning, graphics cards, etc.</li>
</ul>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://medium.com/oreillymedia/introducing-presto-839a26aac724">Introducing Presto</a>. Presto is a distributed SQL query engine, and it is designed to efficiently query data against disparate data sources of all sizes, ranging from gigabytes to petabytes. This post introduces Presto and looks at how Presto works. I find Presto very interesting in that we can use Presto to query Kafka!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/testing-kafka-applications/">Creating a Serverless Environment for Testing Your Apache Kafka Applications</a>. This post looks at how to use the <code>ccloud-stack</code> utility to create a serverless Kafka environment and how to use the Confluent Cloud CLI to provision a fully managed Datagen connector to pre-populate Kafka topics for your applications to use. Stay tuned for a blog-post from me where I test this out!</li>
<li><a href="https://www.confluent.io/blog/confluent-cloud-enables-event-driven-architectures-everywhere-in-azure/">Enabling the Deployment of Event-Driven Architectures Everywhere Using Microsoft Azure and Confluent Cloud</a>. It becomes more and more common that we operate in a hybrid on-prem -&gt; cloud environment. Data migration can be a complex process. This post looks at how to build a persistent bridge from on-premises or other clouds to Microsoft Azure using Confluent Replicator.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>Back in May I did together with <a href="https://charlla.com/">Charl</a> a <a href="/derivco">Derivco</a> webinar around <a href="https://www.youtube.com/watch?v=CSyNiEgEyvY">Kafka</a>. This coming week, (Thursday, Sep. 10), I am doing another webinar - this time about SQL Server 2019 Big Data Cluster. The webinar is open to everyone, and you can sign up <a href="https://derivco.co.za/derivco-webinar-a-lap-around-sql/">here</a>.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 35, 2020]]></title>
    <link href="https://nielsberglund.com/2020/08/30/interesting-stuff---week-35-2020/" rel="alternate" type="text/html"/>
    <updated>2020-08-30T09:45:18+02:00</updated>
    <id>https://nielsberglund.com/2020/08/30/interesting-stuff---week-35-2020/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="sql-server">SQL Server</h2>

<ul>
<li><a href="https://techcommunity.microsoft.com/t5/azure-sql-database/what-is-azure-sql-edge-data-exposed/ba-p/1614877">What is Azure SQL Edge | Data Exposed</a>. This is part one of a three-part video series about Azure SQL Edge. In this part, we get an introduction to Azure SQL Edge and its features.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://databricks.com/blog/2020/08/27/enabling-spark-sql-ddl-and-dml-in-delta-lake-on-apache-spark-3-0.html">Enabling Spark SQL DDL and DML in Delta Lake on Apache Spark 3.0</a>. This post, and video, discuss some of the cool new features in Delta Lake 0.7.0. There are quite a lot of new exciting things in that release!</li>
<li><a href="https://www.confluent.io/blog/kafka-summit-2020-session-highlights/">Best of Kafka Summit 2020 Roundup</a>. So, The Kafka Summit took place August 24 - 25, (virtually), and I managed to attend quite a few sessions. The post linked to here lists some of the &ldquo;must-see&rdquo; sessions. For me, the best session was the keynote the second day by Jay Kreps and Sam Newman. It was awesome!</li>
<li><a href="https://lenses.io/resources/streaming-sql-cheat-sheet-for-apache-kafka/">Streaming SQL Cheat Sheet for Apache Kafka</a>. As the title says, link to a cheat sheet for ksqlDB syntax. I have downloaded the cheat sheet and have it on my desktop so that I can access it easy!</li>
<li><a href="https://www.kafka-streams-book.com/">Mastering Kafka Streams and ksqlDB</a>. The book in the post linked to here promises to be the &ldquo;go-to&rdquo; book for anything KStreams/ksqlDB related. I&rsquo;ll make sure my developers get some copies!</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
</feed>

