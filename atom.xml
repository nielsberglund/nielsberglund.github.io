<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Niels Berglund</title>
  <link href="https://nielsberglund.com/atom.xml" rel="self"/>
  <link href="https://nielsberglund.com"/>
  <updated>2019-11-03T11:07:23+02:00</updated>
  <id>https://nielsberglund.com/</id>
  <generator uri="http://gohugo.io/">Hugo</generator>

  
  <entry>
    <title type="html"><![CDATA[SQL Server 2019 &amp; Java: Parameters]]></title>
    <link href="https://nielsberglund.com/2019/11/03/sql-server-2019--java-parameters/" rel="alternate" type="text/html"/>
    <updated>2019-11-03T11:07:23+02:00</updated>
    <id>https://nielsberglund.com/2019/11/03/sql-server-2019--java-parameters/</id>
    <content type="html"><![CDATA[<p>Microsoft introduced the ability to call Java code from SQL Server in around the SQL Server 2019 CTP 2.0 release, and I have written some posts posts about this:</p>

<ul>
<li><a href="/s2k19_ext_framework_java"><strong>SQL Server 2019 Extensibility Framework &amp; Java</strong></a>.</li>
</ul>

<p>Throughout the CTP releases some functionality has changed, and in CTP 2.5 Microsoft introduced the Java Language Extension SDK. The Java code we write needs now to include the Java SDK and extend an abstract base from the SDK.</p>

<p>With the introduction of the Java Language SDK, came some changes to how you handle method parameters, and I touched upon it briefly in the <a href="/2019/05/26/java--sql-server-2019-extensibility-framework-the-sequel/"><strong>Java &amp; SQL Server 2019 Extensibility Framework: The Sequel</strong></a> post. In this post, I want to look more in detail at parameters and how to handle them.</p>

<p></p>

<blockquote>
<p><strong>DISCLAIMER:</strong> <em>This post contains Java code. I am not a Java guy, in fact, the only Java I have ever written is the code for the SQL Server 2019 Java posts. So, the code is not elegant in any shape or form, and I am absolutely certain it can be done in a much better way. However, this is not about Java as such, but how you call Java code from SQL Server, and what you need to implement on the Java side.</em></p>
</blockquote>

<h2 id="java-code-in-sql-refresh">Java Code in SQL Refresh</h2>

<p>If you have not done any Java code in SQL Server, or at least not recently, here are some posts which introduce you to Java in SQL Server:</p>

<ul>
<li><a href="/2019/03/10/sql-server-2019-java--external-libraries---i/"><strong>SQL Server 2019, Java &amp; External Libraries - I</strong></a>. Part I of a couple of posts where we look at how we can deploy Java code to the database, so it can be loaded from there.</li>
<li><a href="/2019/05/26/java--sql-server-2019-extensibility-framework-the-sequel/"><strong>Java &amp; SQL Server 2019 Extensibility Framework: The Sequel</strong></a>. We look at the implications of the introduction of the Java Language Extension SDK.</li>
<li><a href="/2019/06/06/sql-server-2019-extensibility-framework--external-languages/"><strong>SQL Server 2019 Extensibility Framework &amp; External Languages</strong></a>. We look at SQL Server 2019 Extensibility Framework and Language Extensions.</li>
<li><a href="/2019/08/03/sql-server-2019-ctp3.2--java/"><strong>SQL Server 2019 CTP3.2 &amp; Java</strong></a>. SQL Server 2019 CTP 3.2 and Azule OpenJDK.</li>
<li><a href="/2019/10/26/sql-server-2019--java-null-handling-take-two/"><strong>SQL Server 2019 &amp; Java Null Handling: Take Two</strong></a>. We take a second look at how to handle null values in datasets being passed to and from Java code.</li>
</ul>

<h2 id="pre-req">Pre-req</h2>

<p>If you want to follow along with what we are doing in this post, you need:</p>

<ul>
<li>A SQL Server 2019 CTP 3.0, (or later), instance.</li>
<li>A database where you have registered Java as a language. Read <a href="/2019/06/06/sql-server-2019-extensibility-framework--external-languages/">this post</a> if you are unsure how to register Java as an external language.</li>
<li>The Java Language SDK deployed to the database above. <a href="/2019/05/26/java--sql-server-2019-extensibility-framework-the-sequel/">This post</a> explains how.</li>
<li>Understand how to deploy Java code to the database. The post <a href="/2019/03/10/sql-server-2019-java--external-libraries---i/">here</a> helps with that.</li>
</ul>

<p>You also need an editor with which you can write Java code. I am partial to <strong>Visual Studio Code</strong> myself, and I wrote a post about VS Code and Java <a href="/2019/01/17/sql-server-2019--java-with-visual-studio-code/">here</a>.</p>

<h2 id="parameter-recap">Parameter Recap</h2>

<p>Let us take a look back at parameters to refresh our memory.</p>

<h4 id="bjlsdk-before-java-language-sdk">BJLSDK (Before Java Language SDK)</h4>

<p>Before the introduction of the Java Language SDK, (before CTP 2.5), you called into a method specified in the <code>@script</code> variable of the <code>sp_execute_external_script</code>, (SPEES), procedure. You defined the parameters you wanted to send to the Java code as class variables with the same name as the SQL parameters but without the <code>@</code>:</p>

<pre><code class="language-sql">DECLARE @p1 int = 21;
DECLARE @p2 int = 21;
EXEC sp_execute_external_script
  @language = N'Java'
, @script = N'JavaTest1.adder'
, @params = N'@x int, @y int'
, @x = @p1
, @y = @p2   
GO
</code></pre>

<p><strong>Code Snippet 1</strong> <em>Call from T-SQL to Java with Parameters</em></p>

<p>We see in <em>Code Snippet 1</em> how we want to call into the <code>adder</code> method in the <code>JavaTest1</code> class, passing in two parameters: <code>@x</code>, and <code>@y</code>. When we execute the code the Java C++ language extension gets the parameter values, and looks in the code for two class-level variables named <code>x</code>, and <code>y</code>, and assigns the values to those variables. The <code>adder</code> method then uses <code>x</code>, and <code>y</code>.</p>

<h4 id="ajlsdk-after-java-language-sdk">AJLSDK (After Java Language SDK)</h4>

<p>I mentioned above how a couple of things changed after Microsoft introduced the Java Language SDK. One of them was that you no longer define a method in SPEES&rsquo;s <code>@script</code> parameter. The parameter instead defines a class you want to call into.</p>

<p>The class must extend the <code>AbstractSqlServerExtensionExecutor</code> abstract base class and implement the <code>execute</code> method from the base class. The <code>execute</code> method is the method the Java C++ language extension calls into, and the signature looks like so:</p>

<pre><code class="language-java">public AbstractSqlServerExtensionDataset execute(
                                AbstractSqlServerExtensionDataset input, 
                                LinkedHashMap&lt;String, Object&gt; params) {
  throw new UnsupportedOperationException(&quot;AbstractSqlServerExtensionExecutor 
                                          execute() is not implemented&quot;);
}
</code></pre>

<p><strong>Code Snippet 2</strong> <em>The <code>execute</code> Method</em></p>

<p>From the signature in <em>Code Snippet 2</em> we see how the <code>execute</code> method takes two parameters and has a return type.</p>

<p>To call into Java after the introduction of the Java Language SDK has not changed much:</p>

<pre><code class="language-sql">DECLARE @p1 int = 21;
DECLARE @p2 int = 21;
EXEC sp_execute_external_script
  @language = N'Java'
, @script = N'sql.JavaTest1'
, @params = N'@x int, @y int'
, @x = @p1
, @y = @p2   
GO
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Call from T-SQL to Java with Parameters</em></p>

<p>The only real difference between <em>Code Snippet 3</em> and <em>Code Snippet 2</em> is that the <code>@script</code> parameter is now <code>package.class</code>, instead of <code>class.method</code>.</p>

<p>For the parameters the Java C++ language extension creates a new instance of a <code>LinkedHashMap</code> and populates it with the parameter names, (sans <code>@</code>), and the values.</p>

<h2 id="parameters">Parameters</h2>

<p>Having the recap out of the way, let us see how we handle the parameters in our Java code. Let us first start with some skeleton code:</p>

<pre><code class="language-java">package sql;

import com.microsoft.sqlserver.javalangextension.*;
import java.util.LinkedHashMap;

public class JavaTest1 extends AbstractSqlServerExtensionExecutor {

  public JavaTest1() {
    executorExtensionVersion = SQLSERVER_JAVA_LANG_EXTENSION_V1;
    executorInputDatasetClassName = PrimitiveDataset.class.getName();
    executorOutputDatasetClassName = PrimitiveDataset.class.getName();
  }

  public PrimitiveDataset execute(PrimitiveDataset input, 
                                  LinkedHashMap&lt;String, Object&gt; params) {
    
    return null;

  }
}
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Java Starter Code</em></p>

<p>We see in <em>Code Snippet 4</em> that we have a class, <code>JavaTest1</code>, which extends the <code>AbstractSqlServerExtensionExecutor</code> class. In the constructor, we set some required class members, and the <code>execute</code> method is empty.</p>

<blockquote>
<p><strong>NOTE:</strong> The post <a href="/2019/05/26/java--sql-server-2019-extensibility-framework-the-sequel/"><strong>Java &amp; SQL Server 2019 Extensibility Framework: The Sequel</strong></a> explains about the constructor in <em>Code Snippet 4</em>.</p>
</blockquote>

<p>The code we want to write is to handle the parameters we send in when we execute the code in <em>Code Snippet 3</em>, and in our code, we want to add the two parameters together. With that in mind, our code looks like so:</p>

<pre><code class="language-java">public PrimitiveDataset execute(PrimitiveDataset input, 
                                LinkedHashMap&lt;String, Object&gt; params) {

  int x = (int)params.get(&quot;x&quot;);
  int y = (int)params.get(&quot;y&quot;);
  System.out.printf(&quot;The result of adding %d and %d = %d&quot;, x, y, x + y);
  return null;
}
</code></pre>

<p><strong>Code Snippet 5:</strong> <em>Handle Parameters</em></p>

<p>We see in <em>Code Snippet 5</em> how we handle the parameters that come in from SQL Server:</p>

<ul>
<li>As we said above, the Java C++ language extension populates the <code>LinkedHashMap</code> with the parameters from SQL Server, so we retrieve the <code>x</code> and <code>y</code> using the <code>get</code> method. As the <code>get</code> method returns an object, we cast it to <code>int</code>.</li>
<li>We then add <code>x</code> and <code>y</code> together and we call <code>printf</code> to show the result.</li>
</ul>

<p>After we compile and package the code into a <code>.jar</code> file we deploy to our database:</p>

<pre><code class="language-sql">CREATE EXTERNAL LIBRARY SqlParamLib 
FROM (CONTENT = 'W:\sql-params-1.0.jar')
WITH (LANGUAGE = 'Java');
</code></pre>

<p><strong>Code Snippet 6:</strong> <em>Deploy to the Database</em></p>

<p>From <em>Code Snippet 6</em> we see how I deploy to my SQL Server based on a file-path: <code>W:\sql-params-1.0.jar</code>, and how I indicate that what I deploy is Java code: <code>LANGUAGE = 'Java'</code>. The post <a href="/2019/03/10/sql-server-2019-java--external-libraries---i/"><strong>SQL Server 2019, Java &amp; External Libraries - I</strong></a> covers in detail how to deploy Java code to SQL Server.</p>

<blockquote>
<p><strong>NOTE:</strong> I can use a file path when I deploy as my SQL Server is on my local machine.</p>
</blockquote>

<p>When we have deployed as in <em>Code Snippet 6</em> we execute the code in <em>Code Snippet 3</em>:</p>

<p><img src="/images/posts/sql-2k19-java-params-add.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Add Result</em></p>

<p>In <em>Figure 1</em> we see that our code worked, cool!</p>

<h4 id="null-values">Null Values</h4>

<p>Now then, what if one of the parameters in <em>Code Snippet 3</em> is not <code>DECLARE</code>:ed and set, but instead generated by something, (perhaps a <code>SELECT</code>), and it doesn&rsquo;t have a value:</p>

<pre><code class="language-sql">DECLARE @p1 int;
DECLARE @p2 int;

SELECT @p1 = null, @p2 = 21;

EXEC sp_execute_external_script
  @language = N'Java'
, @script = N'sql.JavaTest1'
, @params = N'@x int, @y int'
, @x = @p1
, @y = @p2   
GO
</code></pre>

<p><strong>Code Snippet 7:</strong> <em>Null Parameter</em></p>

<p>In <em>Code Snippet 7</em> we see how we retrieve the values of the two parameters, and one is null, (no value). Let us see what happens when we execute the code:</p>

<p><img src="/images/posts/sql-2k19-java-params-null.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Null Parameter</em></p>

<p>Hmm, not good, we get a <code>NullPointerException</code> as we see in <em>Figure 2</em>. As a Java developer, this is what you would expect, whereas if we executed similar code in T-SQL, it would work - but the result would be <code>NULL</code>. I wrote <a href="/2019/10/26/sql-server-2019--java-null-handling-take-two/">here</a> about null handling in datasets, and how there is a &ldquo;convenience&rdquo; method <code>getColumnNullMap</code> which indicates null values for a column in the dataset.</p>

<p>Unfortunately, we do not have the same for parameters, so we need to handle it ourselves. Since the parameter value&rsquo;s type in the <code>LinkedHashMap</code> is an <code>Object</code>, we should be able to do a null check quite easily before we do the cast. Alternatively, we can use the primitive type&rsquo;s wrapper class to accomplish the same thing:</p>

<pre><code class="language-java">public PrimitiveDataset execute(PrimitiveDataset input, 
                                LinkedHashMap&lt;String, Object&gt; params) {

  boolean isNull = false;

  Integer x = (Integer)params.get(&quot;x&quot;);
  Integer y = (Integer)params.get(&quot;y&quot;);

  if(x == null || y == null) {
    isNull = true;
  }

  if(isNull) {
    String xStr = x != null ? x.toString() : &quot;null&quot;;
    String yStr = y != null ? y.toString() : &quot;null&quot;;
    System.out.printf(&quot;The result of adding %s and %s = %s&quot;, 
                       xStr, yStr, &quot;null&quot;);
  }
  else {
    System.out.printf(&quot;The result of adding %d and %d = %d&quot;, 
                       x, y, x + y);
  }

  return null;
}
</code></pre>

<p><strong>Code Snippet 8:</strong> <em>Handle Null Parameter</em></p>

<p>In <em>Code Snippet 8</em> we see an example of how we can handle null parameters. As you see in the code snippet, we use the <code>Integer</code> wrapper class for <code>int</code>. Our logic is that if we get a null value, then the result should also be null.</p>

<h4 id="output-parameters">Output Parameters</h4>

<p>In SQL Server, when we execute a procedure, we can return values as output parameters, and the way we indicate that a parameter is for output is by using the <code>OUT</code> statement:</p>

<pre><code class="language-sql">DECLARE @p1 int = 21;
DECLARE @p2 int = 21;
DECLARE @ret int;
EXEC dbo.pr_MyAdder @x = @p1,
                    @y = @p2,
                    @retVal = @ret OUT;
SELECT @ret AS ReturnValue;                    
</code></pre>

<p><strong>Code Snippet 9:</strong> <em>Output Parameters - I</em></p>

<p>We see in <em>Code Snippet 9</em> how we assign <code>@ret</code> as an output parameter and then select the value after the execution of the procedure.</p>

<p>How do we do this in Java? Well, initially output parameters were not supported, but that changed with SQL Server CTP 3.0, (or 3.1). The reason why output parameters were not supported was that the Java C++ language extension did not have an implementation for output parameters. However, as I said above - that changed in CTP 3.0/3.1, and output parameters are now part of the <code>LinkedHashMap</code> parameter.</p>

<p>The Java C++ language extension populates, as I said above, the <code>LinkedHashMap</code> and since it is a reference type when the call returns the Java C++ language extension loops through the parameters and outputs the parameters adorned with <code>OUT</code>, (or <code>OUTPUT</code>).</p>

<p>The code to implement output parameters in our Java code looks like so:</p>

<pre><code class="language-java">public PrimitiveDataset execute(PrimitiveDataset input, 
                                LinkedHashMap&lt;String, Object&gt; params) {

  boolean isNull = false;

  Integer x = (Integer)params.get(&quot;x&quot;);
  Integer y = (Integer)params.get(&quot;y&quot;);
  Integer retVal = (Integer)params.get(&quot;retVal&quot;);

  if(x == null || y == null) {
    isNull = true;
  }

  if(isNull) {
    params.put(&quot;retVal&quot;, null);
  }
  else {
    params.put(&quot;retVal&quot;, x + y);
  }

  return null;
}
</code></pre>

<p><strong>Code Snippet 10:</strong> <em>Output Parameters in Java</em></p>

<p>We see in <em>Code Snippet 10</em> how we have changed the Java code to return the result of the addition of <code>x</code> and <code>y</code> as an output parameter. We retrieve the <code>x</code> and <code>y</code> values as before, and dependent on whether <code>x</code> or <code>y</code> is null or not we either <code>put</code> the <code>retVal</code> parameter as <code>null</code> or the result of the addition.</p>

<blockquote>
<p><strong>NOTE:</strong> It is not necessary to retrieve the <code>retVal</code> parameter as in <em>Code Snippet 10</em> since in this case, it is a pure output parameter. SQL Server, however, allows output parameters both to be used as in as out. So in your Java code, you may get a value coming in for an output parameter.</p>
</blockquote>

<p>The SQL code to execute against the code in <em>Code Snippet 10</em> looks like so:</p>

<pre><code class="language-sql">DECLARE @p1 int = 21;
DECLARE @p2 int = 21;
DECLARE @ret int;
EXEC sp_execute_external_script
  @language = N'Java'
, @script = N'sql.JavaTest1'
, @params = N'@x int, @y int, @retVal int OUT'
, @x = @p1
, @y = @p2  
, @retVal = @ret OUT;

SELECT @ret AS ReturnValue;
GO
</code></pre>

<p><strong>Code Snippet 11:</strong> <em>Output Parameters - II</em></p>

<p>We see in <em>Code Snippet 11</em> how we:</p>

<ul>
<li>Declare a third parameter<code>@ret</code>.</li>
<li>In the <code>@params</code> parameter we define the <code>@retVal</code> parameter.</li>
<li>Assign <code>@ret</code> to <code>@retVal</code>.</li>
</ul>

<p>The result of executing the code in <em>Code Snippet 11</em> is:</p>

<p><img src="/images/posts/sql-2k19-java-params-outputparam.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Output Parameter</em></p>

<p>As we see in <em>Figure 3</em> it works. If we were to send in null for <code>@x</code> or <code>@y</code>, we get back <code>NULL</code> in the <code>SELECT</code>.</p>

<h2 id="summary">Summary</h2>

<p>In this post, we saw how to handle parameters, and we said:</p>

<ul>
<li>The <code>LinkedHashMap</code> parameter in the <code>execute</code> method exposes parameters.</li>
<li>For primitive data type parameters, we can use Java&rsquo;s wrapper classes to handle null values.</li>
<li>The Java C++ language extension handles now output parameters.</li>
<li>We assign a value to an output parameter by doing a <code>put</code>.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 43, 2019]]></title>
    <link href="https://nielsberglund.com/2019/10/27/interesting-stuff---week-43-2019/" rel="alternate" type="text/html"/>
    <updated>2019-10-27T09:15:54+02:00</updated>
    <id>https://nielsberglund.com/2019/10/27/interesting-stuff---week-43-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://mattwarren.org/2019/10/25/Research-based-on-the-.NET-Runtime/">Research based on the .NET Runtime</a>. This post by <a href="https://twitter.com/matthewwarren">Matthew</a> is a collection of Common Language Runtime, (CLR), research papers. It is an impressive list, and if you are interested in CLR, you should read some of the papers.</li>
</ul>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://blog.acolyer.org/2019/10/23/futzing-and-moseying/">Futzing and moseying: interviews with professional data analysts on exploration practices</a>. In this post, <a href="https://twitter.com/adriancolyer">Adrian</a> dissects a white paper from 2018 around what data analysts do when they do Explanatory Data Analysis, (EDA). If you are a data analyst, or if you are interested in data in general, then you should read this.</li>
<li><a href="https://towardsdatascience.com/predicting-customer-churn-with-pyspark-95cd352d393">PySpark &amp; AWS | Predicting Customer Churn</a>. This is a very interesting post where the author takes us through how to work with PySpark on your local computer and then move to AWS to handle large data volumes.</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://medium.com/better-programming/thinking-in-kubernetes-k8s-3770bf14c463">Thinking in Kubernetes (K8s)</a>. Due to what Kubernetes does, and how it does it, starting with Kubernetes may require a mind-shift. This post looks at certain things to look at, and perhaps look at differently when you start your Kubernetes journey.</li>
<li><a href="https://www.infoq.com/presentations/adtech-fraud-detection">High Performance Cooperative Distributed Systems in Adtech</a>. This is an <a href="https://www.infoq.com/">InfoQ</a> presentation where the presenter explores a set of core building blocks exhibited by Adtech platforms and applies them towards building a fraud detection platform. The presenter also touches on critical attributes of system reliability and quality in an Adtech system.</li>
<li><a href="https://itnext.io/tutorial-auto-scale-your-kubernetes-apps-with-prometheus-and-keda-c6ea460e4642">Autoscaling Kubernetes apps with Prometheus and KEDA</a>. There are a couple of ways to scale your application in a Kubernetes cluster. In this post, the author looks at using the Kubernetes Event Driven Autoscaling, (KEDA), Kubernetes operator. What KEDA does is it provides fine-grained autoscaling for event-driven workloads. Very cool article!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/create-dynamic-kafka-connect-source-connectors">4 Steps to Creating Apache Kafka Connectors with the Kafka Connect API</a>. Kafka Connect connectors are used to stream data into Kafka or stream data out of it. In this post, the author discusses how you can create your own connectors using the Kafka Connect API.</li>
</ul>

<h2 id="sql-server-2019">SQL Server 2019</h2>

<ul>
<li><a href="/2019/10/26/sql-server-2019--java-null-handling-take-two/">SQL Server 2019 &amp; Java Null Handling: Take Two</a>. I eventually managed to finish and publish the post I mentioned a couple of weeks ago. Anyway, this post is about how to handle null values passed back and forth between SQL Server and your Java code, after the introduction of the SQL Server Java language SDK.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SQL Server 2019 &amp; Java Null Handling: Take Two]]></title>
    <link href="https://nielsberglund.com/2019/10/26/sql-server-2019--java-null-handling-take-two/" rel="alternate" type="text/html"/>
    <updated>2019-10-26T05:32:54+02:00</updated>
    <id>https://nielsberglund.com/2019/10/26/sql-server-2019--java-null-handling-take-two/</id>
    <content type="html"><![CDATA[<p>You who read my blog know that during the last year, (or so), I have been writing about SQL Server 2019 and the ability to call into Java code from SQL Server:</p>

<ul>
<li><a href="/s2k19_ext_framework_java"><strong>SQL Server 2019 Extensibility Framework &amp; Java</strong></a>.</li>
</ul>

<p>It has been a fascinating &ldquo;journey&rdquo;, since SQL Server 2019 is still in preview, and there have been changes in how you call Java code along the way. In this post, we look at some relatively recent changes to how we handle null values in datasets.</p>

<p></p>

<h2 id="java-code-in-sql-refresh">Java Code in SQL Refresh</h2>

<p>If you have not done any Java code in SQL Server, or at least not recently, here are a couple of posts which introduces you to Java in SQL Server:</p>

<ul>
<li><a href="/2019/03/10/sql-server-2019-java--external-libraries---i/"><strong>SQL Server 2019, Java &amp; External Libraries - I</strong></a>. Part I of a couple of posts where we look at how we can deploy Java code to the database, so it can be loaded from there.</li>
<li><a href="/2019/05/26/java--sql-server-2019-extensibility-framework-the-sequel/"><strong>Java &amp; SQL Server 2019 Extensibility Framework: The Sequel</strong></a>. We look at the implications of the introduction of the Java Language Extension SDK.</li>
<li><a href="/2019/06/06/sql-server-2019-extensibility-framework--external-languages/"><strong>SQL Server 2019 Extensibility Framework &amp; External Languages</strong></a>. We look at SQL Server 2019 Extensibility Framework and Language Extensions.</li>
<li><a href="/2019/08/03/sql-server-2019-ctp3.2--java/"><strong>SQL Server 2019 CTP3.2 &amp; Java</strong></a>. SQL Server 2019 CTP 3.2 and Azule OpenJDK.</li>
</ul>

<h2 id="demo-code">Demo Code</h2>

<h4 id="data">Data</h4>

<p>In today&rsquo;s post, we use some data from the database. The code to insert the data is the same we used in a previous post: <a href="/2018/12/19/sql-server-2019-extensibility-framework--java---null-values/"><strong>SQL Server 2019 Extensibility Framework &amp; Java - Null Values</strong></a>:</p>

<pre><code class="language-sql">USE master;
GO
SET NOCOUNT ON;
GO
DROP DATABASE IF EXISTS JavaNullDB;
GO
CREATE DATABASE JavaNullDB;
GO
USE JavaNullDB;
GO
DROP TABLE IF EXISTS dbo.tb_NullRand10
CREATE TABLE dbo.tb_NullRand10(RowID int identity primary key, 
                          x int, y int, col1 nvarchar(50));
GO
INSERT INTO dbo.tb_NullRand10(x, y, col1)
SELECT TOP(10) CAST(ABS(CHECKSUM(NEWID())) % 14 AS int) 
  , CAST(ABS(CHECKSUM(NEWID())) % 20 AS int)
  , N'Hello ' + CAST(CAST(ABS(CHECKSUM(NEWID())) % 25 AS int) AS nvarchar(50))
FROM sys.objects o1
CROSS JOIN sys.objects o2
CROSS JOIN sys.objects o3
CROSS JOIN sys.objects o4;
GO
UPDATE dbo.tb_NullRand10
  SET y = NULL
WHERE RowId = 3
UPDATE dbo.tb_NullRand10
  SET Col1 = NULL
WHERE RowId = 5    
UPDATE dbo.tb_NullRand10
  SET x = NULL
WHERE RowId = 6 
UPDATE dbo.tb_NullRand10
  SET y = NULL
WHERE RowId = 8  
UPDATE dbo.tb_NullRand10
  SET col1 = NULL
WHERE RowId = 9 
GO
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Create Database Objects</em></p>

<p>We see from *Code Snippet 1 * how we:</p>

<ul>
<li>Create a database: <code>JavaNullDB</code>.</li>
<li>Create a table: <code>dbo.tb_NullRand10</code>.</li>
<li>Insert some data into the table.</li>
</ul>

<p>The data we insert is entirely random, but it gives us something to &ldquo;play around&rdquo; with.</p>

<p>Let us see what the data looks like, by executing: <code>SELECT RowID, x, y FROM dbo.tb_NullRand10;</code>:</p>

<p><img src="/images/posts/sql-2k19-java-null2-result1.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>T-SQL Result</em></p>

<p>As we see in <em>Figure 1</em>, we get back 10 rows, and rows 3, 6, and 8 contains null values.</p>

<h4 id="enable-java-in-the-database">Enable Java in the Database</h4>

<p>To use Java in the database, you need to do a couple of more things:</p>

<ul>
<li>Register Java as an external language in your database. The following post describes what to do: <a href="/2019/06/06/sql-server-2019-extensibility-framework--external-languages/"><strong>SQL Server 2019 Extensibility Framework &amp; External Languages</strong></a>.</li>
<li>Create an external library in the database for the Java SDK. The post <a href="/2019/05/26/java--sql-server-2019-extensibility-framework-the-sequel/"><strong>Java &amp; SQL Server 2019 Extensibility Framework: The Sequel</strong></a> shows you how to do it.</li>
</ul>

<p>Now, when we have a database with some data and Java enabled we, can start.</p>

<h2 id="null-values">Null Values</h2>

<p>In the <a href="/2018/12/19/sql-server-2019-extensibility-framework--java---null-values/">null values post</a> mentioned above, I mentioned that there are differences between SQL Server and Java in how they handle null. So, when we call into Java from SQL Server, we may want to treat null values the same way as we do in SQL Server.</p>

<p>I wrote about this in the <a href="/2018/12/19/sql-server-2019-extensibility-framework--java---null-values/"><strong>SQL Server 2019 Extensibility Framework &amp; Java - Null Values</strong></a> post mentioned above. However, that post was written before SQL Server 2019 CTP 2.5. In CTP 2.5 Microsoft introduced the Java SDK, and certain things changed. Amongst the things that changed is the way we handle nulls when we receive datasets from SQL Server in our Java code.</p>

<p>Let us see how null handling works now post CTP 2.5. We use similar code to what we saw in the <a href="/2018/12/19/sql-server-2019-extensibility-framework--java---null-values/">null post</a> above:</p>

<pre><code class="language-java">public PrimitiveDataset execute(PrimitiveDataset input, 
                               LinkedHashMap&lt;String, Object&gt; params) {
    
  int[] inputCol1 = input.getIntColumn(0);
  int[] inputCol2 = input.getIntColumn(1);
  int[] inputCol3 = input.getIntColumn(2);

  PrimitiveDataset output = new PrimitiveDataset();

  output.addColumnMetadata(0, &quot;RowID&quot;, Types.INTEGER, 0, 0);
  output.addColumnMetadata(1, &quot;x&quot;, Types.INTEGER, 0, 0);
  output.addColumnMetadata(2, &quot;y&quot;, Types.INTEGER, 0, 0);

  output.addIntColumn(0, inputCol1, null);
  output.addIntColumn(1, inputCol2, null);
  output.addIntColumn(2, inputCol3, null);
 
  return output;
}
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Java Input &amp; Output Data</em></p>

<p>The code in <em>Code Snippet 3</em> &ldquo;echoes&rdquo; back what it receives as input dataset and we see how we:</p>

<ul>
<li>Load three arrays with the three columns in the dataset.</li>
<li>Create a new <code>PrimitiveDataset</code> to use as the return type.</li>
<li>Set metadata for the return dataset.</li>
<li>Assign the columns for the return dataset.</li>
<li>Return the dataset.</li>
</ul>

<blockquote>
<p><strong>NOTE:</strong> If you wonder about why the method is named <code>execute</code> and what the <code>PrmitiveDataset</code> is; the post <a href="/2019/05/26/java--sql-server-2019-extensibility-framework-the-sequel/">here</a> explains it.</p>
</blockquote>

<p>After we have compiled and packaged the code into a <code>.jar</code> file we can deploy:</p>

<pre><code class="language-sql">CREATE EXTERNAL LIBRARY SqlNullLib 
FROM (CONTENT = 'W:\sql-null-1.0.jar')
WITH (LANGUAGE = 'Java');
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Deploy Java Code</em></p>

<p>As we see in <em>Code Snippet 3</em> I named my <code>.jar</code> file <code>sql-null-1.0.jar</code> and I deployed it as an external library: <code>SqlNullLib</code>. Since I deploy to a local SQL Server instance, I can use a file location for my <code>.jar</code>.</p>

<blockquote>
<p><strong>NOTE:</strong> The <a href="/2019/03/10/sql-server-2019-java--external-libraries---i/">Java &amp; External Libraries</a> post mentioned above goes into details about external libraries.</p>
</blockquote>

<p>The code to execute looks something like so:</p>

<pre><code class="language-sql">EXEC sp_execute_external_script
  @language = N'Java'
, @script = N'sql.NullValues'
, @input_data_1 = N'SELECT RowID, x, y FROM dbo.tb_NullRand10'
WITH RESULT SETS((RowID int, x int, y int))
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Execute Code</em></p>

<p>We see in <em>Code Snippet 4</em> how we call into the <code>NullValues</code> class in the <code>sql</code> package and how we use the same <code>SELECT</code> statement that generated the resultset we saw in <em>Figure 1</em>. When we execute the code, we see:</p>

<p><img src="/images/posts/sql-2k19-java-null2-result2.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Java Code Result</em></p>

<p>Compare the result we see in <em>Figure 2</em> with the result in <em>Figure 1</em>, and we see the difference in the outlined rows, (3, 6, 8), and the highlighted columns, In <em>Figure 1</em> the columns are <code>NULL</code>, whereas in <em>Figure 2</em> they are <code>0</code>. So why are the columns <code>0</code>?</p>

<p>Well, as we said in the <a href="/2018/12/19/sql-server-2019-extensibility-framework--java---null-values/">null post</a>, this is because the Java language C++ extension converts the null values to the default value for the data type in question.</p>

<blockquote>
<p><strong>NOTE:</strong> The Java language C++ extension is the bridge between SQL Server and your Java code. The <a href="/2019/06/06/sql-server-2019-extensibility-framework--external-languages/"><strong>SQL Server 2019 Extensibility Framework &amp; External Languages</strong></a> post covers it in some detail.</p>
</blockquote>

<p>The question is, why do we care that a null value comes across as zero, at least we do not get a null exception? Let us take a look at the following Java code:</p>

<pre><code class="language-java">public PrimitiveDataset execute(PrimitiveDataset input, 
                                LinkedHashMap&lt;String, Object&gt; params) {
    
  int[] inputCol3 = input.getIntColumn(2);

  int numRows = inputCol3.length;
  int sum = 0;

  for(int x = 0; x &lt; numRows; x++) {
    sum += inputCol3[x];

  }

  double avg = (double)sum / numRows;

  System.out.printf(&quot;Average value of y is: %f&quot;, avg);

  return null;
}

</code></pre>

<p><strong>Code Snippet 5:</strong> <em>Average Value</em></p>

<p>The code in <em>Code Snippet 5</em> expects the same input data as we generated in <em>Code Snippet 4</em>, and it calculates the average value of the <code>y</code> column of that dataset. When we execute the code in <em>Code Snippet 4</em>, after having compiled, packaged and deployed, (comment out <code>WITH RESULT SETS</code>), we see the result as so:</p>

<p><img src="/images/posts/sql-2k19-java-null2-average.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Result of Average Calculation</em></p>

<p>The result in <em>Figure 3</em> looks OK, so let us see what it looks like if we run a similar query in SQL Server:</p>

<pre><code class="language-sql">SELECT CAST(AVG(CAST(y AS DECIMAL(4,2)))AS DECIMAL(4,2))
FROM dbo.tb_NullRand10;
</code></pre>

<p><strong>Code Snippet 6:</strong> <em>T-SQL Average Calculation</em></p>

<p>The result of the query in <em>Code Snippet 6</em> looks like so:</p>

<p><img src="/images/posts/sql-2k19-java-null2-average2.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Result of T-SQL Average Calculation</em></p>

<p>We see in <em>Figure 4</em> how the result of the average calculation, (outlined in red), differs from the Java calculation. The question is why this is; it was the same data in both calculations? Well, was it; we see in <em>Figure 4</em> the highlighted part at the top: &ldquo;Null value is eliminated &hellip;&rdquo;. So what happens is that for certain operations SQL Server eliminates null values, as SQL Server treats nulls as unknown.</p>

<p>As the Java language C++ extension converts nulls, we need to handle it in our Java code.</p>

<h4 id="input-nulls">Input Nulls</h4>

<p>In the previous <a href="/2018/12/19/sql-server-2019-extensibility-framework--java---null-values/">null post</a> we saw that, when we want to handle null input, we use a required class level variable <code>inputNullMap</code>, which the Java language extension populates &ldquo;automagically&rdquo;. However, after the introduction of the Java language SDK, this variable is not required any more. Even if you declared it, the Java language extension does not populate it.</p>

<p>So how do we then figure out whether a column has a null value? Well, since the Java language extension passes data into the <code>execute</code> method via the <code>PrimitiveDataSet</code> class, let us have a look at the base class for <code>PrimitiveDataSet</code>: <code>AbstractSqlServerExtensionDataset</code>:</p>

<pre><code class="language-java">public class AbstractSqlServerExtensionDataset {
  /**
   * Column metadata interfaces
   */
  public void addColumnMetadata(int columnId, String columnName, int columnType, 
                                int precision, int scale) {
    throw new UnsupportedOperationException(&quot;addColumnMetadata is not implemented&quot;);
  }

  ...

  public boolean[] getColumnNullMap(int columnId) {
    throw new UnsupportedOperationException(&quot;getColumnNullMap is not implemented&quot;);
  }

  ...
}
</code></pre>

<p><strong>Code Snippet 7:</strong> <em>AbstractSqlServerExtensionDataset</em></p>

<p>We see in <em>Code Snippet 7</em> how the <code>AbstractSqlServerExtensionDataset</code> has a section for metadata, and in that section is a method: <code>getColumnNullMap</code>. The method takes an integer as an input parameter, and it returns an array of type <code>boolean</code>.</p>

<blockquote>
<p><strong>NOTE:</strong> The Java SDK is open source, and you find it <a href="https://github.com/microsoft/sql-server-language-extensions">here</a>.</p>
</blockquote>

<p>This is what happens when the Java language C++ extension populates the dataset which is used as an input parameter:</p>

<ul>
<li>The extension creates a <code>boolean</code> array for each non-nullable Java datatype columns.</li>
<li>The extension loops each row for each column in the dataset.</li>
<li>Where there is a null value, for a primitive data type, the extension assigns the default value of the data type to that column.</li>
<li>When the extension comes across a null value in a non-nullable Java data type column, it sets the boolean array value to <code>true</code> in the column array.</li>
</ul>

<p>With this in mind we can change the code in <em>Code Snippet 5</em> to handle null values, or rather handle values in the dataset that originates from a SQL Server null value:</p>

<pre><code class="language-java">public PrimitiveDataset execute(PrimitiveDataset input, 
                                LinkedHashMap&lt;String, Object&gt; params) {
    
  int[] inputCol3 = input.getIntColumn(2);

  int numRows = inputCol3.length;
  boolean[] nullMap = input.getColumnNullMap(2);
  int nonNull= 0;
  int sum = 0;

  for(int x = 0; x &lt; numRows; x++) {
    if(!nullMap[x]) {
      nonNull ++;
      sum += inputCol3[x];
    }

  }

  double avg = (double)sum / nonNull;

  System.out.printf(&quot;Average value of y is: %f&quot;, avg);

  return null;
}
</code></pre>

<p><strong>Code Snippet 8:</strong> <em>Null Handling</em></p>

<p>So, in <em>Code Snippet 8</em> we see how we:</p>

<ul>
<li>Get the column we want to create the average over.</li>
<li>Use <code>getColumnNullMap</code> to retrieve the null map for the column we use for the calculation.</li>
<li>In the <code>for</code> loop check whether the column value is null or not. If it is not null, we include the value and increase the row count.</li>
<li>Finally do the average calculation.</li>
</ul>

<p>The result when executing the code in <em>Code Snippet 4</em> against our new code looks like so:</p>

<p><img src="/images/posts/sql-2k19-java-null2-average3.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>New Java Average Calculation</em></p>

<p>We see in <em>Figure 5</em> how our Java calculation now gives the same result as the T-SQL calculation. Awesome!</p>

<h4 id="output-nulls">Output Nulls</h4>

<p>We have now seen how to use <code>getColumnNullMap</code> to distinguish input values that come in as <code>NULL</code> from SQL Server, which the Java language C++ extension converts to the default value for the Java data type.</p>

<p>What about if we need to return null values to SQL Server in a return dataset, but the Java data type is non-nullable? I.e. we receive data in the input dataset, and some column values for a non-nullable Java type are null when passed in from SQL Server. If we wanted to, for example, add the column to another column, the sum should be <code>NULL</code> if we were to handle it the same way as SQL Server does.</p>

<p>So how do we indicate to SQL Server that a column value is null, even though it has a value in Java? Let us go back to <em>Code Snippet 2</em> where we discussed how to return data to SQL Server from Java code. After &ldquo;newing&rdquo; up an instance of <code>PrimitiveDataset</code>, we defined the metadata for the columns via the <code>addColumnMetadata</code> method. We then added the row arrays for the columns through the <code>add*Type*Column</code>, (in our case it was <code>addIntColumn</code>), and it is in that method the &ldquo;secret&rdquo; to null values lies. Let us go back to <code>AbstractSqlServerExtensionDataset</code> and look at the signature for <code>addIntColumn</code>:</p>

<pre><code class="language-java">public class AbstractSqlServerExtensionDataset {
  
  /**
   * Adding column interfaces
   */
  public void addIntColumn(int columnId, int[] rows, 
                           boolean[] nullMap) {
    throw new UnsupportedOperationException(...);
  }
}
</code></pre>

<p><strong>Code Snippet 9:</strong> <em>Add Column Method</em></p>

<p>Look in <em>Code Snippet 9</em> at the third parameter in the add method. See how it takes a <code>boolean</code> array, and how the name &ldquo;gives it away&rdquo;: <code>nullMap</code>. If we look at other methods for non-nullable Java types, we see that all of them have this parameter, whereas add methods for types that are nullable do not have it.</p>

<p>So for non-nullable types, we define <code>boolean</code> arrays, and in those arrays, we indicate what row value(s) is <code>null</code>. Let us see an example:</p>

<pre><code class="language-java">public PrimitiveDataset execute(PrimitiveDataset input, 
                                LinkedHashMap&lt;String, Object&gt; params) {
    
  PrimitiveDataset output = new PrimitiveDataset();

  int numRows = 5;

  output.addColumnMetadata(0, &quot;RowID&quot;, Types.INTEGER, 0, 0);
  output.addColumnMetadata(1, &quot;IntCol&quot;, Types.INTEGER, 0, 0);
  output.addColumnMetadata(2, &quot;StringCol&quot;, Types.NVARCHAR, 256, 0);

  int[] rowIdRows = new int[numRows];
  int[] intColRows = new int[numRows];
  String[] stringColRows = new String[numRows];

  boolean[] intColNullMap = new boolean[numRows];

  for(int x = 0; x &lt; numRows; x++){
    rowIdRows[x] = x+1;
    if(x % 2 ==0) {
      intColRows[x] = 0;
      intColNullMap[x] = true;
    }
    else {
      intColRows[x] = x + 2;
      intColNullMap[x] = false;
    }
    if(x % 3 ==0) {
      stringColRows[x] = null;
    }
    else {
      stringColRows[x] = &quot;Hello number: &quot; + (x + 1);
    }
  }

  output.addIntColumn(0, rowIdRows, null);
  output.addIntColumn(1, intColRows, intColNullMap);
  output.addStringColumn(2, stringColRows);
 
  return output;
}
</code></pre>

<p><strong>Code Snippet 10:</strong> <em>Return Dataset</em></p>

<p>What we see in <em>Code Snippet 10</em> is a somewhat contrived example where we return a dataset in which we want certain column values to be <code>NULL</code> in SQL Server. We see how we:</p>

<ul>
<li>Create metadata for the dataset.</li>
<li>Create arrays for the individual rows.</li>
<li>Create a null map for one of the integer columns.</li>
<li>In the <code>for</code> loop add values to the arrays, and based on some modulus operations emulate that some values are null.</li>
<li>Add the arrays to the columns, and for the second integer column, we also add the null map.</li>
<li>Finally return the dataset.</li>
</ul>

<p>A couple of things to notice in the code in <em>Code Snippet 10</em>:</p>

<ul>
<li>A null-map is not required for a non-nullable data type columns if the values are not null.</li>
<li>We do not need a null-map for nullable data type columns.</li>
</ul>

<p>To see that our code works we use following code:</p>

<pre><code class="language-sql">EXEC sp_execute_external_script
  @language = N'Java'
, @script = N'sql.NullValues'
WITH RESULT SETS((RowID int, IntCol int, StringCol nvarchar(256)))
</code></pre>

<p><strong>Code Snippet 11:</strong> <em>Execute T-SQL</em></p>

<p>The result we get when executing the code in <em>Code Snippet 11</em>, looks like so:</p>

<p><img src="/images/posts/sql-2k19-java-null2-return-null.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>Return Dataset with Null Values</em></p>

<p>When we look at <em>Figure 6</em> we see that our code worked, and how the Java C++ language extension converted the <code>0</code> values in the integer column to <code>NULL</code> based on the null map.</p>

<h2 id="summary">Summary</h2>

<p>In this post, we discussed how to handle null values in datasets passed into our Java code from SQL Server, and from our Java code back to SQL Server. I wrote about null handling in a previous <a href="/2018/12/19/sql-server-2019-extensibility-framework--java---null-values/">post</a>, but since that post, Microsoft introduced the Java language SDK for SQL Server, and null handling has changed.</p>

<p>We care about null values because, in SQL Server, all data types are nullable, whereas, in Java, that is not the case. In Java, like in .NET, primitive types, (<code>int</code>, etc.), cannot be null. The Java C++ language extension handles the mismatch between nullable in SQL Server and non-nullable in Java, whereby it converts null values in data from SQL Server to the data type&rsquo;s default value for Java. Going the other way, from Java to SQL Server, the C++ language extension converts the values supposed to be null to actual null values.</p>

<p>The way we handle null values after the introduction of the Java language SDK is that we:</p>

<ul>
<li>For input data, and each non-nullable column, we call <code>getColumnNullMap</code> and pass in the columns ordinal position. We then handle the values where the null map indicates a null value.</li>
<li>For output data we create <code>boolean</code> arrays for the columns which should contain null values in SQL Server. We pass the array in as a parameter to the <code>add*TypeName*Column</code> method.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 42, 2019]]></title>
    <link href="https://nielsberglund.com/2019/10/20/interesting-stuff---week-42-2019/" rel="alternate" type="text/html"/>
    <updated>2019-10-20T08:40:13+02:00</updated>
    <id>https://nielsberglund.com/2019/10/20/interesting-stuff---week-42-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://levelup.gitconnected.com/net-core-worker-service-as-windows-service-or-linux-daemons-a9579a540b77">.NET Core Worker Service as Windows Service or Linux Daemons</a>. This post discusses a new .NET Core 3 application template called &ldquo;Worker Service&rdquo;. The template makes it easy to write Windows Services or Linux Daemons.</li>
</ul>

<h2 id="sql-server">SQL Server</h2>

<ul>
<li><a href="https://techcommunity.microsoft.com/t5/SQL-Customer-Success-Engineering/JSON-in-your-Azure-SQL-Database-Let-s-benchmark-some-options/ba-p/909131">JSON in your Azure SQL Database? Letâ€™s benchmark some options!</a>. This Microsoft post discusses and benchmarks options on how to handle JSON in SQL Server.</li>
<li><a href="https://techcommunity.microsoft.com/t5/SQL-Server/Optimize-OLTP-Performance-with-SQL-Server-on-Azure-VM/ba-p/916794">Optimize OLTP Performance with SQL Server on Azure VM</a>. This article shares recommendations on how to optimize SQL Server performance on Azure VMs. The article is based on performance testing with a somewhat scaled-down <a href="http://www.tpc.org/tpce/default.asp">TPC-E</a> benchmark.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://rmoff.net/2019/10/15/skipping-bad-records-with-the-kafka-connect-jdbc-sink-connector/">Skipping bad records with the Kafka Connect JDBC sink connector</a>. This post by the Kafka guru <a href="https://twitter.com/rmoff">Robin Moffat</a> discusses how to handle bad records when using the JDBC sink connector in Kafka Connect. Very interesting!</li>
<li><a href="https://yokota.blog/2018/11/19/kcache-an-in-memory-cache-backed-by-kafka/">KCACHE: AN IN-MEMORY CACHE BACKED BY KAFKA</a>. This post details how you can have an in-memory cache backed by Kafka. Excellent!</li>
<li><a href="https://rmoff.net/2019/10/16/using-kafka-connect-and-debezium-with-confluent-cloud/">Using Kafka Connect and Debezium with Confluent Cloud</a>. This is yet another awesome post by <a href="https://twitter.com/rmoff">Robin</a>. One where he discusses how you can use Debezium together with Confluent Cloud.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 41, 2019]]></title>
    <link href="https://nielsberglund.com/2019/10/13/interesting-stuff---week-41-2019/" rel="alternate" type="text/html"/>
    <updated>2019-10-13T16:09:41+02:00</updated>
    <id>https://nielsberglund.com/2019/10/13/interesting-stuff---week-41-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="data-science">Data Science</h2>

<ul>
<li><a href="https://databricks.com/blog/2019/10/09/democratizing-financial-time-series-analysis-with-databricks.html">Democratizing Financial Time Series Analysis with Databricks</a>. This is a very interesting post, in that it discusses how to develop financial time series analysis faster using Apache Spark, (well actually Databricks), and Koalas.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.ververica.com/blog/announcing-stateful-functions-distributed-state-uncomplicated">Announcing Stateful Functions: Distributed State, Uncomplicated</a>. This post from Ververica, (previously dataArtisans), discusses Stateful Functions which is an open-source framework that reduces the complexity of building and orchestrating distributed stateful applications at scale.</li>
<li><a href="https://www.confluent.io/blog/kafka-summit-san-francisco-2019-session-videos">Kafka Summit San Francisco 2019 Session Videos</a>. The <a href="https://kafka-summit.org/events/kafka-summit-san-francisco-2019/">Kafka Summit San Francisco 2019</a> finished last week, (October 1). The organizers have done a tremendous job and managed to put all session videos online. The post I linked to has links videos for all sessions, as well as a top ten list of sessions. Next weekend will be a Kafka Summit videos binge for me!</li>
<li><a href="https://www.infoq.com/presentations/event-driven-benefits-pitfalls">Opportunities and Pitfalls of Event-driven Utopia</a>. This is an <a href="https://www.infoq.com/">InfoQ</a> presentation where the presenter goes over the concepts, the advantages, and the pitfalls of event-driven utopia. He shares real-life stories or points to source code examples.</li>
<li><a href="https://rmoff.net/2019/10/07/kafka-connect-and-elasticsearch/">Kafka Connect and Elasticsearch</a>. This post by <a href="https://twitter.com/rmoff">Robin Moffat</a> discusses recent changes in Elasticsearch and the Kafka Connector for Elastic and what you can do to fix some of the errors you may encounter due to the changes.</li>
</ul>

<h2 id="wind-what-is-niels-doing">WIND (What Is Niels Doing)</h2>

<p>Towards the end of last year, I wrote a post about how to handle null values when calling from SQL Server into Java, (and the reverse). Since then, Microsoft released the <a href="/2019/05/26/java--sql-server-2019-extensibility-framework-the-sequel/">Java Extensibility SDK</a>, and some things changed. I am now working on a follow-up post where I look at how to handle null values post the Java Extensibility SDK. Expect it to be published in a week or so.</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 40, 2019]]></title>
    <link href="https://nielsberglund.com/2019/10/06/interesting-stuff---week-40-2019/" rel="alternate" type="text/html"/>
    <updated>2019-10-06T08:09:53+02:00</updated>
    <id>https://nielsberglund.com/2019/10/06/interesting-stuff---week-40-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/evolutionary-architecture">Evolutionary Architecture</a>. This is an <a href="https://www.infoq.com/">InfoQ</a> presentation where the presenter discusses traditional approaches of evolutionary architecture showing how to use fitness functions and transition to an evolutionary architecture even in the face of legacy systems.</li>
<li><a href="https://www.infoq.com/news/2019/09/cqrs-event-sourcing-production">Day Two Problems When Using CQRS and Event Sourcing</a>. This is a summary of an <a href="https://www.infoq.com/">InfoQ</a> presentation where the presenter shared his experience running and evolving CQRS and event-sourced applications in production.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/kafka-made-serverless-with-confluent-cloud">Free Apache Kafka as a Service with Confluent Cloud</a>. The <a href="https://kafka-summit.org/events/kafka-summit-san-francisco-2019">Kafka Summit 2019</a> was held earlier this week, and this is a post about an announcement that was made about how Confluent is now offering Confluent Cloud for free. Well, for free is not exactly true, but you get $50 off the bill each month for the first three months after you have signed up. This may not sound like much, but it goes a long way. I have signed up, so expect some posts about Kafka in the cloud going forward.</li>
<li><a href="https://www.infoq.com/presentations/stream-analysis-fp">Real-Time Stream Analysis in Functional Reactive Programming</a>. This <a href="https://www.infoq.com/">InfoQ</a> presentation discusses a reactive approach to application design, and how to account for handling events in near real-time employing the Functional Reactive Programming paradigm.</li>
<li><a href="https://www.youtube.com/watch?reload=9&amp;v=4QoCbhsQeyE&amp;feature=">Jay Kreps, Confluent | Kafka Summit SF 2019 Keynote</a>. Above I mentioned <a href="https://kafka-summit.org/events/kafka-summit-san-francisco-2019">Kafka Summit 2019</a>, and the YouTube video in the link is <a href="https://www.linkedin.com/in/jaykreps/">Jay Kreps</a> keynote address. In the keynote, he explains modern stream processing, real-time databases, KSQL, and other interesting &ldquo;stuff&rdquo;.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 39, 2019]]></title>
    <link href="https://nielsberglund.com/2019/09/29/interesting-stuff---week-39-2019/" rel="alternate" type="text/html"/>
    <updated>2019-09-29T13:17:50+02:00</updated>
    <id>https://nielsberglund.com/2019/09/29/interesting-stuff---week-39-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://devblogs.microsoft.com/dotnet/announcing-net-core-3-0/">Announcing .NET Core 3.0</a>. The title says it all, .NET Core 3.0 is now released! Go and <a href="https://dotnet.microsoft.com/download/dotnet-core/3.0">get it</a>!</li>
<li><a href="https://mattwarren.org/2019/09/26/Stubs-in-the-.NET-Runtime/">&ldquo;Stubs&rdquo; in the .NET Runtime</a>. This post is another performance related post by <a href="https://twitter.com/matthewwarren">Matthew</a>. In the post, he discusses &ldquo;Stubs&rdquo; which is used in the .NET framework to provide a level of indirection. The post explores what they are, how they work, and why they&rsquo;re needed.</li>
</ul>

<h2 id="data">Data</h2>

<ul>
<li><a href="https://www.theseattledataguy.com/why-do-you-need-data-engineers-and-what-do-they-do/">Why Do You Need Data Engineers And What Do They Do?</a>. This is an interesting post discussing the importance of data engineers.</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.tbray.org/ongoing/When/201x/2019/09/25/On-Sharding">On Sharding</a>. This post by <a href="https://en.wikipedia.org/wiki/Tim_Bray">Tim Bray</a>, (of XML fame), discusses different types of sharding in high traffic scenarios. It is always interesting to read what Tim has to say, as he has tremendous experience in all aspects of computing.</li>
<li><a href="https://levelup.gitconnected.com/components-of-kubernetes-architecture-6feea4d5c712">Components of Kubernetes Architecture</a>. I can warmly recommend this post to anyone interested in Kubernetes. The post gives a handy overview of the components of Kubernetes.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/every-company-is-becoming-software">Every Company is Becoming <del>a</del> Software <del>Company</del></a>. In this blog post, <a href="https://www.linkedin.com/in/jaykreps/">Jay Kreps</a>, co-founder and CEO of Confluent, makes the point that not only businesses use more software, but that, increasingly, a business is defined in software. He goes on to discuss how Apache Kafka can help companies to re-architect themselves around event streaming, which he sees as the future.</li>
<li><a href="https://www.confluent.io/blog/analytics-with-apache-kafka-and-rockset">Real-Time Analytics and Monitoring Dashboards with Apache Kafka and Rockset</a>. This blog post looks at how we can build analytics and monitoring on top of Apache Kafka using <a href="https://rockset.com/">Rockset</a>. This is something I want to look into for a couple of our <a href="/derivco">Derivco</a> projects.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 38, 2019]]></title>
    <link href="https://nielsberglund.com/2019/09/22/interesting-stuff---week-38-2019/" rel="alternate" type="text/html"/>
    <updated>2019-09-22T17:01:04+02:00</updated>
    <id>https://nielsberglund.com/2019/09/22/interesting-stuff---week-38-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="databases">Databases</h2>

<ul>
<li><a href="https://blog.acolyer.org/2019/09/20/even-more-amazing-papers-at-vldb/">Even more amazing papers at VLDB 2019 (that I didn&rsquo;t have space to cover yet)</a>. The <a href="https://en.wikipedia.org/wiki/International_Conference_on_Very_Large_Data_Bases">VLDB (Very Large Databases)</a> was held at the end of August, and as usual, quite a lot of white papers were presented at the conference. <a href="https://twitter.com/adriancolyer">Adrian</a> has during the last week, dissected some of them. In this post, he lists some of the papers he didn&rsquo;t dissect but still finds interesting. There are some nuggets in the list!</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/news/2019/09/maesh-kube-service-mesh">Introducing Maesh: A Service Mesh for Kubernetes</a>. This article is an <a href="https://www.infoq.com/">InfoQ</a> article about Maesh. Maesh is an open source service mesh written in Golang and built on top of the reverse proxy and load balancer Traefik. Maesh promises to provide a lightweight service mesh solution that is easy to get started with and to roll out across a microservice application.</li>
<li><a href="https://medium.com/better-programming/essential-kubernetes-resources-2ccb250bcf44">Essential Kubernetes Resources</a>. This post covers a list of Kubernetes resources related to security, resilience, and availability best practices.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/multi-region-data-replication">Built-In Multi-Region Replication with Confluent Platform 5.4-preview</a>. This post covers new functionality in Confluent Platform 5.4: the ability to replicate data across data center regions. Very interesting!</li>
<li><a href="https://www.buzzsprout.com/186154/1720903-kip-500-apache-kafka-without-zookeeper-ft-colin-mccabe-and-jason-gustafson">KIP-500: Apache Kafka Without ZooKeeper ft. Colin McCabe and Jason Gustafson</a>. The link here is to a podcast, where <a href="https://twitter.com/tlberglund">Tim Berglund</a> talks with a couple of Kafka engineers and discusses new functionality coming to Kafka where Kafka do not need ZooKeeper any more.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 37, 2019]]></title>
    <link href="https://nielsberglund.com/2019/09/15/interesting-stuff---week-37-2019/" rel="alternate" type="text/html"/>
    <updated>2019-09-15T16:25:58+02:00</updated>
    <id>https://nielsberglund.com/2019/09/15/interesting-stuff---week-37-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<p>This week I do not have that much to share, partly because I have been occupied with writing a blog post about deploying <strong>SQL Server 2019 Big Data Cluster</strong> using <strong>Azure Data Studio</strong> (see below). It has also been <strong>SQL Saturday</strong> which I have been &ldquo;prepping&rdquo; for.</p>

<h2 id="data-science-machine-learning">Data Science / Machine Learning</h2>

<ul>
<li><a href="https://blog.acolyer.org/2019/09/09/ms-approx-query/">Experiences with approximating queries in Microsoft&rsquo;s production big-data clusters</a>. This is a dissection, by <a href="https://twitter.com/adriancolyer">Adrian</a>, of a white paper about approximating queries at Microsoft. Approximation of queries is used when you want to run analysis / OLAP queries against massive datasets where a query could potentially run for hours. By using approximation the time to run the query is reduced significantly.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/event-streaming-reflections-as-confluent-turns-five-part-1">Reflections on Event Streaming as Confluent Turns Five â€“ Part 1</a>. This is a blog post by <a href="https://twitter.com/tlberglund">Tim Berglund</a>, (awesome name by the way), where he looks back at how Apache Kafka and the Confluent Platform has changed the way we build event-driven systems. Happy 5th Birthday to Confluent!</li>
</ul>

<h2 id="sql-server-2019-big-data-cluster">SQL Server 2019 Big Data Cluster</h2>

<ul>
<li><a href="/2019/09/11/install-sql-server-2019-big-data-cluster-using-azure-data-studio/">Install SQL Server 2019 Big Data Cluster using Azure Data Studio</a>. I had to tear down the <strong>SQL Server 2019 Big Data Cluster</strong> <a href="https://twitter.com/adriancolyer">Andrew</a> and me used for our workshop <strong>A Day of SQL Server 2019 Big Data Cluster</strong> in Johannesburg and rebuild it for the Cape Town leg of SQL Saturday. While I did the rebuild, I thought it would be a good idea to document what I did, and this blog post is the result.</li>
</ul>

<h2 id="sql-saturday">SQL Saturday</h2>

<p>The South African leg of SQL Saturday is done and dusted. We were in Cape Town yesterday, (Saturday, September 14), and I delivered two conference talks, (in addition to Andrew&rsquo;s and mine workshop mentioned above):</p>

<ul>
<li><a href="/download/s2k19-bdc-overview.pdf">A Lap Around SQL Server 2019 Big Data Cluster</a>: The new release of SQL Server; SQL Server 2019 includes Apache Spark and Hadoop Distributed File System (HDFS) for scalable compute and storage. This new architecture that combines together the SQL Server database engine, Spark, and HDFS into a unified data platform is called a â€œbig data cluster.â€
This session gives you an overview of what a SQL Server Big Data Cluster is, and what you can do with it.</li>
<li><a href="/download/pirate-snake-coffee.pdf">What is the PiRate, Snake, and Cup of Coffee Doing in My Database?</a>: In this session we looked at the SQL Server Extensibility Framework, and we saw how we can call out to external languages from inside SQL Server. We looked at R, Python and Java, and what we can do from SQL Server having access to those languages.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Install SQL Server 2019 Big Data Cluster using Azure Data Studio]]></title>
    <link href="https://nielsberglund.com/2019/09/11/install-sql-server-2019-big-data-cluster-using-azure-data-studio/" rel="alternate" type="text/html"/>
    <updated>2019-09-11T06:12:12+02:00</updated>
    <id>https://nielsberglund.com/2019/09/11/install-sql-server-2019-big-data-cluster-using-azure-data-studio/</id>
    <content type="html"><![CDATA[<p>I wrote a <a href="/2018/11/10/sql-server-2019-big-data-cluster-on-azure-kubernetes-service/">blog post</a> back in November 2018, about how to install and deploy <strong>SQL Server 2019 Big Data Cluster</strong> on Azure Kubernetes Service. Back then <strong>SQL Server 2019 Big Data Cluster</strong> was in private preview, (CTP 2.1 I believe), and you had to sign up, to get access to the &ldquo;bits&rdquo;. Well, you did not really get any &ldquo;bits&rdquo;; what you did get was access to Python deployment scripts.</p>

<p>Now, September 2019, the BDC is in public preview (you do not have to sign up), and it has reached Release Candidate (RC) status, RC 1. The install method has changed, or rather, in addition to installing via deployment scripts, you can now also install using <strong>Azure Data Studio</strong> deployment notebooks, and that is what this blog post is about.</p>

<p>I install it on Azure, and since I am in South Africa, I use one of the South African hosting locations.</p>

<p></p>

<h2 id="pre-reqs">Pre-Reqs</h2>

<p>To deploy using <strong>Azure Data Studio</strong>, (ADS), you need ADS (duh - see below), but you also need some other things.</p>

<h4 id="azure-subscription">Azure Subscription</h4>

<p>If you want to install on <strong>Azure Kubernetes Service</strong>, (AKS), you need an Azure subscription. If you do not have one you can sign up for a free trial subscription <a href="https://azure.microsoft.com/en-us/free/">here</a>.</p>

<p>Of course you do not need to install it on AKS, you can install it on basically any Kubernetes cluster</p>

<h4 id="python">Python</h4>

<p>Well, Python is not a tool as such, but you need Python installed on the machine you install from, as the ADS deployment runs some Python scrips. You need Python3, and on my machine, I have Python 3.7.3. Ensure that Python is on the <code>PATH</code>.</p>

<h4 id="azdata">azdata</h4>

<p><code>azdata</code> is a Python command-line tool replacing <code>mssqlctl</code>. It enables cluster administrators to bootstrap and manages the big data cluster via REST APIs.</p>

<p>There are a couple of steps to install it:</p>

<ul>
<li>If you have <code>mssqlctl</code> installed you need to uninstall it:</li>
</ul>

<pre><code class="language-bash">$ pip3 uninstall -r https://private-repo.microsoft.com/ \
                           python/ctp3.1/mssqlctl/requirements.txt
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Uninstall <code>mssqlctl</code></em></p>

<p>In <em>Code Snippet 1</em> above I have inserted a line continuation (<code>\</code>) to make the code fit the page.</p>

<ul>
<li>If you have deployed CTP 3.2 of the BDC, then you need to uninstall that version of <code>azdata</code>:</li>
</ul>

<pre><code class="language-bash">pip3 uninstall -r https://azdatacli.blob.core.windows.net/ \
                  python/azdata/2019-ctp3.2/requirements.txt
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Uninstall <code>azdata</code></em></p>

<p>In <em>Code Snippet 2</em> you see how the uninstall command takes the version of <code>azdata</code> to uninstall: <em><code>2019-ctp3.2</code></em>. If you have the 3.2 version installed you need to confirm when you run the code in <em>Code Snippet 2</em> that you want to remove some installed components:</p>

<p><img src="/images/posts/inst-bdcrc1-uninst-azdata.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Confirm Uninstall <code>azdata</code></em></p>

<p>Just click <code>y</code> when asked to proceed.</p>

<blockquote>
<p><strong>NOTE:</strong> The biggest issue causing errors in a BDC deployment, by far, is using an older version of <code>azdata</code>. So please, do not be &ldquo;that guy&rdquo; (or girl) - make sure you uninstall <code>azdata</code> if you have an earlier version. In fact, before a deployment, always uninstall <code>azdata</code> followed by an install, (see below).</p>
</blockquote>

<ul>
<li>You need the latest version of the Python <code>requests</code> package installed:</li>
</ul>

<pre><code class="language-bash">$ pip3 install -U requests
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Install/Upgrade <code>requests</code></em></p>

<ul>
<li>When you have executed the code in <em>Code Snippet 3</em> you can install <code>azdata</code>:</li>
</ul>

<pre><code class="language-bash">$ pip3 install -r https://aka.ms/azdata
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Installing <code>azdata</code></em></p>

<p>After executing the code in <em>Code Snippet 4</em> you can go ahead and install the other tools needed.</p>

<h4 id="kubectl">kubectl</h4>

<p>The <code>kubectl</code> tool is a Kubernetes command-line tool, and it allows you to run commands against Kubernetes clusters. You can use <code>kubectl</code> to deploy applications, inspect and manage cluster resources, and view logs.</p>

<p>You can install <code>kubectl</code> in different ways, and I installed it from <a href="https://chocolatey.org/packages/kubernetes-cli">Chocolatey</a>: <code>choco install kubernetes-cli</code>.</p>

<h4 id="azure-cli">Azure CLI</h4>

<p>The Azure CLI is Microsoft&rsquo;s cross-platform command-line experience for managing Azure resources, and you install it on your local machine. You find install links for Azure CLI <a href="https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest">here</a>.</p>

<h2 id="azure-data-studio">Azure Data Studio</h2>

<p>Since this post is about installing and deploying a BDC using <strong>Azure Data Studio</strong>, you also need ADS. You may already have ADS installed, but to be able to install and deploy to the release candidate of BDC you need a specific install. If you have already installed ADS, this ADS version installs side-by-side with existing ADS installations.</p>

<p>The install link to the ADS RC version is <a href="https://aka.ms/azuredatastudio-rc">here</a>.</p>

<h4 id="sql-server-2019-preview">SQL Server 2019 (Preview)</h4>

<p>In addition to ADS, you also need the <strong>SQL Server 2019 (Preview)</strong> extension, which you install after installing ADS.</p>

<p>As opposed to other ADS extensions, you need to download the extension to your machine before you can install it. You download it from <a href="https://docs.microsoft.com/en-us/sql/azure-data-studio/sql-server-2019-extension">here</a>. After download, you install it from the <strong>File</strong> menu, and the <strong>Install Extension from VSIX Package</strong> item:</p>

<p><img src="/images/posts/inst-bdcrc1-inst-extension.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Install Extension</em></p>

<p>In <em>Figure 2</em> you see the <strong>File</strong> menu (outlined in blue), and the <strong>Install Extension from VSIX Package</strong> item outlined in red.</p>

<h4 id="azure-data-studio-notebooks">Azure Data Studio Notebooks</h4>

<p>I mentioned above that you deploy the BDC using ADS deployment <em>Notebooks</em>. You may ask yourself what an <strong>Azure Data Studio Notebook</strong> is? Well, Notebooks come from the Data Science world where a Notebook can contain live code, equations, visualizations and narrative text. It is a tool for teaching or sharing information between people. A notebook makes it easy to link lots of docs and code together.</p>

<p>When Microsoft developed ADS, the embedded the <a href="https://jupyter.org/">Jupyter</a> service in ADS, which enables ADS to run Notebooks. When you talk about Notebooks, you also talk about <em>Kernels</em>. A <em>Kernel</em> is the programming language you can write and execute code in, in the <em>Notebook</em>:</p>

<p><img src="/images/posts/inst-bdcrc1-ads-notebook-kernels.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Notebook Kernels</em></p>

<p>The drop-down you see in <em>Figure 3</em> shows the <em>Kernels</em> ADS supports. When you deploy, you use the <em>Python 3</em> kernel.</p>

<p>If you have not used Python Notebooks before in ADS, you need to configure Python for use with Notebooks. You enter <strong>Ctrl+Shift+P</strong> to open the command palette, and you search for <em>Configure Python</em>:</p>

<p><img src="/images/posts/inst-bdcrc1-ads-configure-notebooks.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Configure Notebooks</em></p>

<p>In <em>Figure 4</em> you see the command palette, and you choose <em>Configure Python for Notebooks</em>, and follow the instructions.</p>

<p>When you have configured Python for the notebooks, you are ready to deploy the BDC.</p>

<h2 id="deployment-settings">Deployment Settings</h2>

<p>The first step in the deployment is to configure settings which the notebook use.</p>

<p>After you launch ADS, you click on the <strong>Connections</strong> icon in the top of the <em>Activity</em> bar (leftmost panel in ADS). That opens the sidebar where you can see your connections. Click on the ellipsis, (the three dots &ldquo;&hellip;&rdquo;), to the right in the top panel of the sidebar:</p>

<p><img src="/images/posts/inst-bdcrc1-ads-deploy.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Connections</em></p>

<p>You see in <em>Figure 5</em>:</p>

<ul>
<li>Connections icon outlined in yellow.</li>
<li>The three dots in the connections panel outlined in blue.</li>
<li>The pop-up menu items when clicking on the three dots.</li>
</ul>

<p>You also see in <em>Figure 5</em> that the pop-up menu gives you choices for SQL Server deployments. To deploy a BDC, you click on the item outlined in red:</p>

<p><img src="/images/posts/inst-bdcrc1-ads-deploy-options1.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>Connections</em></p>

<p>When you click on the *Deploy SQL Server big data cluster you see something like in <em>Figure 6</em>: the <em>Select the deployment options</em> dialog. You see in the dialog what you can deploy, and what options you have.</p>

<p>You choose <em>SQL Server big data cluster</em>, (outlined in blue), the version <em>SQL Server 2019 RC big data cluster</em>, (outlined in yellow), and the target to deploy to: <em>New Azure Kubernetes Service cluster</em>. When you click <strong>Select</strong> in the dialog, you see a new dialog:</p>

<p><img src="/images/posts/inst-bdcrc1-ads-new-cluster.png" alt="" /></p>

<p><strong>Figure 7:</strong> <em>New AKS Cluster</em></p>

<p>You see in Figure 7* a settings dialog for the creation and deployment of your new cluster. All of the settings are relatively self-explanatory. However, there are two where you may not know how to retrieve them:</p>

<ul>
<li>Subscription id - you can have multiple subscriptions in Azure. This defines under which subscription to create the cluster. If you only have one subscription you leave this as is. If you have more subscriptions see below.</li>
<li>Region - in what Azure region your cluster should be created.</li>
</ul>

<h4 id="subscription-id">Subscription id</h4>

<p>You retrieve subscription information either by using the Azure portal or logging in via the Azure CLI. I prefer to log-in using Azure CLI:</p>

<pre><code class="language-bash">$ az login
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Login to Azure</em></p>

<p>When I execute the code in <em>Code Snippet 4</em> a tab opens in my browser, and I see a dialog that asks me to pick an account to log in to Azure with:</p>

<p><img src="/images/posts/sql_2k19_bdc_az_login.png" alt="" /></p>

<p><strong>Figure 8:</strong> <em>Azure Login</em></p>

<p>I choose the account from what I see in <em>Figure 8</em>, and after a little while, I see in the browser a success message:</p>

<p><img src="/images/posts/sql_2k19_bdc_az_login_success.png" alt="" /></p>

<p><strong>Figure 9:</strong> <em>Azure Login Success</em></p>

<p>At the same time as the success message in <em>Figure 9</em>, the code in <em>Code Snippet 4</em> returns with information what subscriptions I have access to in Azure:</p>

<p><img src="/images/posts/sql_2k19_bdc_az_login_return.png" alt="" /></p>

<p><strong>Figure 10:</strong> <em>Azure Login Return</em></p>

<p>As we see in <em>Figure 10</em>, I have access to multiple subscriptions, and I enter the id for the subscription I want to use in the <em>Subscription id</em> field.</p>

<h4 id="regions">Regions</h4>

<p>To see a list of Azure regions you execute <code>az account list-locations</code>:</p>

<p><img src="/images/posts/inst-bdcrc1-azure-regions.png" alt="" /></p>

<p><strong>Figure 11:</strong> <em>Azure Regions</em></p>

<p>When you execute the code above you get back a list of all regions you have access to and what you enter in the <em>Region</em> is the <code>name</code> property of your chosen region. In my case, I choose <code>southafricnorth</code>. When you have the necessary information, you set the various settings. For me the settings dialog looks like so:</p>

<p><img src="/images/posts/inst-bdcrc1-cluster-settings.png" alt="" /></p>

<p><strong>Figure 12:</strong> <em>Azure Regions</em></p>

<p>It is worth noting that I have changed the <em>VM size</em>, and <em>VM count</em> from its default of <code>Standard_E4s_v3</code>, and <code>5</code> to <code>Standard_B8ms</code> and <code>3</code>. Reason for this is that having fewer nodes cuts down on install time. The thing to bear in mind here is that a BDC deployment requires at a minimum around 24 hard disks altogether in your cluster, and each VM has a set number of disks. In my case, each <code>Standard_B8ms</code> VM has 16 disks so I should be good (3 * 16).</p>

<h2 id="deployment-notebook">Deployment Notebook</h2>

<p>With the settings set you now click <strong>Open Notebook</strong>:</p>

<p><img src="/images/posts/inst-bdcrc1-deploy-notbook1.png" alt="" /></p>

<p><strong>Figure 13:</strong> <em>Deploy Notebook</em></p>

<p>In <em>Figure 13</em> you see the opened deployment notebook. When you scroll through the notebook, you see the various stages of the deployment, and what it does in each stage:</p>

<ul>
<li>Check dependencies.</li>
<li>Required information.</li>
<li>Azure settings.</li>
<li>Default settings.</li>
<li>Login to Azure.</li>
<li>Set active Azure subscription.</li>
<li>Create Azure resource group.</li>
<li>Create AKS cluster.</li>
<li>Set the new AKS cluster as current context.</li>
<li>Create a deployment configuration file.</li>
<li>Create SQL Server 2019 big data cluster.</li>
<li>Login to SQL Server 2019 big data cluster.</li>
<li>Show SQL Server 2019 big data cluster endpoints.</li>
<li>Connect to master SQL Server instance in Azure Data Studio.</li>
</ul>

<p>To do the deployment, you can now either run each cell independently by clicking on the cell and hit F5 or click on the <strong>Run Cells</strong> command at the top of the notebook. In either case, you see what command the cell executes as well as the result:</p>

<p><img src="/images/posts/inst-bdcrc1-cell-output.png" alt="" /></p>

<p><strong>Figure 14:</strong> <em>Cell Output</em></p>

<p>What you see in <em>Figure 14</em> is the output from creating the Azure resource group.</p>

<p>Be aware that the deployment takes a while, and especially the stage <em>Create SQL Server 2019 big data cluster</em>. Eventually, the deployment finishes, and you get an output from the cell <em>Show SQL Server 2019 big data cluster endpoints</em>:</p>

<p><img src="/images/posts/inst-bdcrc1-endpoints.png" alt="" /></p>

<p><strong>Figure 15:</strong> <em>Cell Output</em></p>

<p>The BDC exposes external endpoints for various services, and those are the ones you see in <em>Figure 15</em>. It is beyond the scope of this post to discuss what all those endpoints are, but the one outlined in red is the endpoint for the SQL Server master instance.</p>

<p>To connect to the master instance, you create a new connection:</p>

<p><img src="/images/posts/inst-bdcrc1-ads-connect.png" alt="" /></p>

<p><strong>Figure 16:</strong> <em>New Connection</em></p>

<p>When you click the <em>New Connection</em> icon in the <em>Servers</em> panel in the sidebar that you see in <em>Figure 16</em>, a <em>Connection</em> dialog &ldquo;pops up&rdquo;:</p>

<p><img src="/images/posts/inst-bdcrc1-ads-new-connection.png" alt="" /></p>

<p><strong>Figure 17:</strong> <em>Connection Dialog</em></p>

<p>In the <em>Connection</em> dialog you enter the details for your connection, including the IP address for the SQL Server master instance, and then you click <strong>Connect</strong>:</p>

<p><img src="/images/posts/inst-bdcrc1-ads-connection.png" alt="" /></p>

<p><strong>Figure 18:</strong> <em>Connected</em></p>

<p>As you see in <em>Figure 18</em>, you are now connected.</p>

<h2 id="summary">Summary</h2>

<p>In this post you saw how you can deploy a *<em>SQL Server 2019 Big Data Cluster</em> using <strong>Azure Data Studio</strong> and notebooks.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 36, 2019]]></title>
    <link href="https://nielsberglund.com/2019/09/08/interesting-stuff---week-36-2019/" rel="alternate" type="text/html"/>
    <updated>2019-09-08T20:23:48+02:00</updated>
    <id>https://nielsberglund.com/2019/09/08/interesting-stuff---week-36-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/microservices-ddd-bounded-contexts">PracticalDDD: Bounded Contexts + Events =&gt; Microservices</a>. An <a href="https://www.infoq.com/">InfoQ</a> presentation discussing the intersection of DDD as a software discipline with Messaging as a technology counterpart. DDD allows us to move faster and write high-quality code. When we start to use the technology of messaging to communicate between clean and well-defined bounded contexts, we get to remove temporal coupling. We now have microservices that are built for autonomy from the ground up. A fascinating presentation!</li>
<li><a href="https://www.infoq.com/presentations/uber-microservices-distributed-tracing">Conquering Microservices Complexity @Uber with Distributed Tracing</a>. Another <a href="https://www.infoq.com/">InfoQ</a> presentation. The presenter presents a methodology that uses data mining to learn the typical behavior of the system from massive amounts of distributed traces, compares it with pathological behavior during outages, and uses complexity reduction and intuitive visualizations to guide the user towards actionable insights about the root cause of the outages. This is a very, very interesting presentation!</li>
<li><a href="https://blog.acolyer.org/2019/09/04/slog/">SLOG: serializable, low-latency, geo-replicated transactions</a>. In this white paper, dissection, <a href="https://twitter.com/adriancolyer">Adrial</a> looks at SLOG. SLOG is a system which leverages physical region locality in an application workload to achieve: strict serializability, low-latency writes, and high transactional throughput, while also supporting online consistent dynamic &ldquo;re-mastering&rdquo; of data as application patterns change over time.</li>
</ul>

<h2 id="vs-code">VS Code</h2>

<ul>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2019/09/03/visual-studio-code-develop-pyspark-jobs-for-sql-server-2019-big-data-clusters/">Visual Studio Code: Develop PySpark jobs for SQL Server 2019 Big Data Clusters</a>. The linked blog post announces a new VS Code extension. The extension allows you, amongst other things, to deploy PySpark applications to SQL Server 2019 Big Data Cluster. This is an extension anyone developing Python applications need to have a look at.</li>
</ul>

<h1 id="sql-saturday">SQL Saturday</h1>

<p>I have just come back from SQL Saturday in Johannesburg where I delivered a workshop about SQL Server 2019 Big Data Cluster with a colleague of mine; <a href="https://twitter.com/datawookie">Andrew Collier</a>. I also delivered two presentations:</p>

<ul>
<li><strong>What is the PiRate, Snake, and Cup of Coffee Doing in My Database?</strong>.</li>
<li><strong>A Lap Around SQL Server Big Data Cluster</strong>.</li>
</ul>

<p>Next week I do the big data cluster workshop with <a href="https://twitter.com/datawookie">Andrew</a>, <a href="https://www.quicket.co.za/events/80996-sqlsaturday-cape-town-2019-precon-with-neils-berglund-mvp-and-andrew-collier-sq/#/"><strong>A Day of SQL Server 2019 Big Data Cluster</strong></a>, on Friday (Sep., 13), and two presentations on Saturday (Sep., 14). I believe the presentation will be the same as in Johannesburg:</p>

<ul>
<li><strong>What is the PiRate, Snake, and Cup of Coffee Doing in My Database?</strong>. A talk about the SQL Server Extensibility Framework and how you can use R, Python and Java from inside SQL Server.</li>
<li><strong>A Lap Around SQL Server Big Data Cluster</strong>. This session gives you an overview of what a SQL Server Big Data Cluster is, and what you can do with it. We look at the various components, (Kubernetes, Spark, HDFS, PolyBase, etc.), and what you can do with them.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 35, 2019]]></title>
    <link href="https://nielsberglund.com/2019/09/01/interesting-stuff---week-35-2019/" rel="alternate" type="text/html"/>
    <updated>2019-09-01T19:40:31+02:00</updated>
    <id>https://nielsberglund.com/2019/09/01/interesting-stuff---week-35-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://medium.com/microsoft-open-source-stories/how-microsoft-rewrote-its-c-compiler-in-c-and-made-it-open-source-4ebed5646f98">How Microsoft rewrote its C# compiler in C# and made it open source</a>. TThis is a fascinating post by Mads Torgersen, where he discusses Roslyn, the open-source compiler for C# and VB.NET.</li>
</ul>

<h2 id="big-data">Big Data</h2>

<ul>
<li><a href="https://buckwoody.wordpress.com/2019/08/26/big-data-is-just-data/">Big Data is just Data</a>. In this blog post <a href="https://twitter.com/BuckWoodyMSFT">Buck</a> discusses what happened to <strong>Big Data</strong>, did it die or what - as no one talks about it any more? The post gives one food for thoughts! Oh, and I am so excited to get to meet Buck at the SQL Saturday&rsquo;s in <a href="https://www.sqlsaturday.com/903/EventHome.aspx">Johannesburg</a> and <a href="https://www.sqlsaturday.com/897/EventHome.aspx">Cape Town</a> the next two weekends!</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/news/2019/08/octant-kubernetes-dashboard">Octant: Local and Real-Time Dashboard for Kubernetes Workloads</a>. This <a href="https://www.infoq.com/">InfoQ</a> article is about Octant, a tool to help developers understand how their applications are running in a Kubernetes cluster. Developers can graphically visualize Kubernetes objects dependencies, forward local ports to a running pod, inspect pod logs, and navigate through different clusters.</li>
<li><a href="https://www.infoq.com/articles/cellery-code-first-kubernetes">Cellery: A Code-First Approach to Deploy Applications on Kubernetes</a>. This another Kubernetes article from <a href="https://www.infoq.com/">InfoQ</a>. The article explains Cellery, which is a code-first approach to building, integrating, running, and managing composite applications on Kubernetes, using a cell-based architecture.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/kafka-graph-visualizations">Using Graph Processing for Kafka Stream Visualizations</a>. This is a very, very cool article discussing how we can match up Kafka with graph databases, (in this case Neo4J), to visualize our streaming data and gain insight from the data.</li>
</ul>

<h1 id="sql-saturday">SQL Saturday</h1>

<p>It is early September, and that means that SQL Saturday is just around the corner. In fact, the Johannesburg leg of SQL Saturday is next weekend. I am fortunate enough to present in both Johannesburg and Cape Town.</p>

<p>Me together with a colleague - [Andrew Collier][acoll] - delivers a one day workshop: <strong>A Day of SQL Server 2019 Big Data Cluster</strong>, where we drill into - you guessed it - SQL Server 2019 Big Data Cluster. We do that Friday, September 6 in Johannesburg, and Friday, September 13 in Cape Town.</p>

<p>If you are interested to understand what SQL Server 2019 Big Data Cluster is all about, please register for Johannesburg <a href="https://www.quicket.co.za/events/81482-a-day-of-sql-server-2019-big-data-cluster-with-neils-berglund-and-andrew-collier#/">here</a>, or Cape Town <a href="https://www.quicket.co.za/events/80996-sqlsaturday-cape-town-2019-precon-with-neils-berglund-mvp-and-andrew-collier-sq/#/">here</a>. It will be a blast - I promise!</p>

<p>In Johannesburg, I deliver on Saturday, (September 7), two talks:</p>

<ul>
<li><a href="https://www.sqlsaturday.com/903/Sessions/Details.aspx?sid=95909">What is the PiRate, Snake, and Cup of Coffee Doing in My Database?</a>. A talk about the SQL Server Extensibility Framework and how you can use R, Python and Java from inside SQL Server.</li>
<li><a href="https://www.sqlsaturday.com/903/Sessions/Details.aspx?sid=95908">A Lap Around SQL Server Big Data Cluster</a>. This session gives you an overview of what a SQL Server Big Data Cluster is, and what you can do with it. We look at the various components, (Kubernetes, Spark, HDFS, PolyBase, etc.), and what you can do with them.</li>
</ul>

<p>In Cape Town, Saturday, September 14, I also deliver two talks:</p>

<ul>
<li><a href="https://www.sqlsaturday.com/897/Sessions/Details.aspx?sid=95915">Set Your SQL Server Data Free with the Use of Kafka</a>. In this talk, we look at how we can stream data from SQL Server to the de facto standard for streaming: Apache Kafka. We look at tools like Kafka Connect and external languages, and after the session, we should have a good understanding of various ways we can &ldquo;set the data free&rdquo;.</li>
<li><a href="https://www.sqlsaturday.com/897/Sessions/Details.aspx?sid=95916">SQL Server Machine Learning Services &amp; External R/Python Packages</a>. SQL Server Machine Learning Services supports both R and Python, and they come with all the regular packages installed. However, there are a lot more packages available &ldquo;out in the wild&rdquo;. In this session, you learn various ways you can install both R and Python packages in SQL Server instances.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 34, 2019]]></title>
    <link href="https://nielsberglund.com/2019/08/25/interesting-stuff---week-34-2019/" rel="alternate" type="text/html"/>
    <updated>2019-08-25T19:51:11+02:00</updated>
    <id>https://nielsberglund.com/2019/08/25/interesting-stuff---week-34-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="misc">Misc.</h2>

<ul>
<li><a href="https://medium.com/better-programming/what-to-do-when-youve-inherited-dysfunctional-code-e09822656b3a">What to Do When You&rsquo;ve Inherited Dysfunctional Code</a>. This is a very interesting post about what you can do when you have inherited &ldquo;crap&rdquo; code. Code that makes no logical sense and is coded in a way that resembles a drunk and disoriented spider trying to build a web.</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/state-serverless-computing">The State of Serverless Computing</a>. This is an <a href="https://www.infoq.com/">InfoQ</a> presentation where the presenter briefly discusses both the benefits and shortcomings of existing serverless offerings. He then projects forward to the future and highlights challenges that must be overcome to realize truly general-purpose serverless computing, as well as our efforts to get there.</li>
<li><a href="https://www.infoq.com/presentations/multi-tenancy-kubernetes">Multi-Tenancy in Kubernetes</a>. This is another <a href="https://www.infoq.com/">InfoQ</a> presentation. In this presentation the presenter discusses both the mechanics and the implications of cluster sharing on cost, isolation, and operational efficiency, including use cases, even challenging ones.</li>
</ul>

<h2 id="sql-server-2019">SQL Server 2019</h2>

<ul>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2019/08/21/sql-server-2019-release-candidate-is-now-available/">SQL Server 2019 release candidate is now available</a>. Boys and girls, start your engines - we are getting closer. Closer to the release of SQL Server 2019! This post announces, as the title implies, that RC1 of SQL Server 2019 has been released, go and get it! Worth noting, however, is that SQL Server 2019 Big Data Cluster has not reached RC1 stage yet. RC1 for SQL Server 2019 Big Data Cluster will come a little bit later.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/transactional-systems-with-apache-kafka">Building Transactional Systems Using Apache Kafka</a>. This article presents an event-based architecture that retains most transactional properties as provided by an RDBMS while leveraging Apache KafkaÂ® as a scalable and highly available single source of truth.</li>
</ul>

<h2 id="sql-saturday">SQL Saturday</h2>

<p>It is late August, and I am in a state of panic! Why, well - the conference season is upon us, and I have months, and months ago, submitted some talks and workshops - and now it is time to get those talks and workshops ready. In other words, I am burning the midnight oil.</p>

<p>Me together with a colleague - <a href="https://twiter.com/datawookie">Andrew Collier</a> - delivers a one day workshop: <strong>A Day of SQL Server 2019 Big Data Cluster</strong>, where we drill into - you guessed it - SQL Server 2019 Big Data Cluster. We do that Friday, September 6 in Johannesburg, and Friday, September 13 in Cape Town.</p>

<p>If you are interested to understand what SQL Server 2019 Big Data Cluster is all about, please register for Johannesburg <a href="https://www.quicket.co.za/events/81482-a-day-of-sql-server-2019-big-data-cluster-with-neils-berglund-and-andrew-collier#/">here</a>, or Cape Town <a href="https://www.quicket.co.za/events/80996-sqlsaturday-cape-town-2019-precon-with-neils-berglund-mvp-and-andrew-collier-sq/#/">here</a>. It will be a blast - I promise!</p>

<p>In Johannesburg, I deliver on Saturday, (September 7), two talks:</p>

<ul>
<li><a href="https://www.sqlsaturday.com/903/Sessions/Details.aspx?sid=95909">What is the PiRate, Snake, and Cup of Coffee Doing in My Database?</a>. A talk about the SQL Server Extensibility Framework and how you can use R, Python and Java from inside SQL Server.</li>
<li><a href="https://www.sqlsaturday.com/903/Sessions/Details.aspx?sid=95908">A Lap Around SQL Server Big Data Cluster</a>. This session gives you an overview of what a SQL Server Big Data Cluster is, and what you can do with it. We look at the various components, (Kubernetes, Spark, HDFS, PolyBase, etc.), and what you can do with them.</li>
</ul>

<p>For Cape Town, the Saturday schedule is not ready yet, but I have no doubt that I will deliver one or two talks.</p>

<p>I hope to see you there!</p>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 33, 2019]]></title>
    <link href="https://nielsberglund.com/2019/08/18/interesting-stuff---week-33-2019/" rel="alternate" type="text/html"/>
    <updated>2019-08-18T16:08:06+02:00</updated>
    <id>https://nielsberglund.com/2019/08/18/interesting-stuff---week-33-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="net">.NET</h2>

<ul>
<li><a href="https://devblogs.microsoft.com/dotnet/announcing-net-core-3-0-preview-8/">Announcing .NET Core 3.0 Preview 8</a>. We are getting close to an official release of .NET Core 3.0. As you can see from the title of the linked post, Microsoft just released Preview 8 of .NET Core 3.0.</li>
</ul>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://www.infoq.com/articles/kubernetes-workloads-serverless-era/">Kubernetes Workloads in the Serverless Era: Architecture, Platforms, and Trends</a>. An <a href="https://www.infoq.com/">InfoQ</a> article about how microservices architecture has evolved into cloud-native architecture, where many of the infrastructure concerns are provided by Kubernetes in combination with additional abstractions provided by service mesh and serverless frameworks.</li>
<li><a href="https://www.infoq.com/presentations/monolith-observable-microservices-ddd">From Monolith to Observable Microservices using DDD</a>. This is an <a href="https://www.infoq.com/">InfoQ</a> presentation about how to move from a monolith to microservices applying Domain-Driven Design principles. Additionally, the presenter explains how to better maintain microservices in production by making them observable.</li>
<li><a href="https://www.infoq.com/articles/twelve-testing-techniques-microservices-intro">Testing Microservices: Overview of 12 Useful Techniques - Part 1</a>. Yet another <a href="https://www.infoq.com/">InfoQ</a> post. This is the first in a series about how to test microservices. An excellent post!</li>
</ul>

<h2 id="machine-learning-data-science">Machine Learning / Data Science</h2>

<ul>
<li><a href="https://medium.com/better-programming/fundamentals-of-time-series-data-and-forecasting-15e9490b2618">Fundamentals of Time Series Data and Forecasting</a>. This article explores the fundamentals of time series data. It talks about how very simple forecasting methods work. Plus, the article describes the most common patterns found in time-series data. I found this article very informative!</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/kafka-connect-improvements-in-apache-kafka-2-3">Kafka Connect Improvements in Apache Kafka 2.3</a>. Apache Kafka 2.3 was released recently together with Confluent Platform 5.3. The release has some improvements of Kafka Connect. In this blog post, <a href="https://twitter.com/rmoff">Robin Moffat</a>, covers the - in his mind - most exciting improvements.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 32, 2019]]></title>
    <link href="https://nielsberglund.com/2019/08/11/interesting-stuff---week-32-2019/" rel="alternate" type="text/html"/>
    <updated>2019-08-11T16:07:14+02:00</updated>
    <id>https://nielsberglund.com/2019/08/11/interesting-stuff---week-32-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="sql-server">SQL Server</h2>

<ul>
<li><a href="https://techcommunity.microsoft.com/t5/SQL-Server/Introducing-Distributed-transaction-functionality-on-SQL-Server/ba-p/786632">Introducing Distributed transaction functionality on SQL Server 2017 on Linux starting with CU16</a>. When Microsoft introduced SQL Server on Linux, distributed transactions were not supported, as it required windows components. This blog post announces that Microsoft has managed to integrate the Distributed Transaction Coordinator with SQLPAL for SQL Server 2017 CU16, which means that SQL Server 2017 CU16+ now supports distributed transactions even on Linux.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.confluent.io/blog/kafka-ksql-udf-udaf-with-maven-made-easy">KSQL UDFs and UDAFs Made Easy</a>. This blog post shows how easy it is to create custom KSQL user defined functions, and user defined aggregate functions with the new Maven archetype for UDFs/UDAFs.</li>
<li><a href="https://sbg.technology/2019/08/09/automated-prize-draws-kafka/">Automated Prize Draws with Kafka</a>. This is an excellent article covering how Sky Betting &amp; Gaming uses Kafka in one of their promotional applications.</li>
<li><a href="https://www.confluent.io/blog/announcing-apache-kafka-tutorials">Announcing Tutorials for Apache Kafka</a>. This is an announcement of Kafka Tutorials: a collection of common event streaming use cases, with each tutorial featuring an example scenario and several complete code solutions. I so have bookmarked the site!</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 31, 2019]]></title>
    <link href="https://nielsberglund.com/2019/08/04/interesting-stuff---week-31-2019/" rel="alternate" type="text/html"/>
    <updated>2019-08-04T11:15:18+02:00</updated>
    <id>https://nielsberglund.com/2019/08/04/interesting-stuff---week-31-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="https://medium.com/@aptxkid/paxos-at-its-heart-is-very-simple-b6a0eafbeb50">Paxos at its heart is very simple</a>. This is a post where the author tries to explain <a href="https://en.wikipedia.org/wiki/Paxos_(computer_science)">Paxos</a> in simple terms. In my opinion, the author has made an excellent job of it.</li>
</ul>

<h2 id="data-science-machine-learning">Data Science / Machine Learning</h2>

<ul>
<li><a href="https://towardsdatascience.com/the-5-basic-statistics-concepts-data-scientists-need-to-know-2c96740377ae">The 5 Basic Statistics Concepts Data Scientists Need to Know</a>. Machine Learning is not <a href="https://towardsdatascience.com/no-machine-learning-is-not-just-glorified-statistics-26d3952234e3">glorified statistics</a>, but by understanding statistics, we can get a better understanding of what our Machine Learning / Data Science algorithms show us.</li>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2019/07/30/microsoft-machine-learning-server-9-4-is-now-available/">Microsoft Machine Learning Server 9.4 is now available</a>. The title says it all, Microsoft has now released Machine Learning Server 9.4, and it is based on Microsoft R Open 3.5.2 and Python 3.7.1. This means that we should be able to update R and Python in SQL Server Machine Learning Services.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/linkedin-streams-brooklin/">A Dive Into Streams @LinkedIn With Brooklin</a>. This is an <a href="https://www.infoq.com/">InfoQ</a> presentation where the presenter talks about Brooklin â€“ Linked In&rsquo;s managed data streaming service that supports multiple pluggable sources and destinations, which can be data stores or messaging systems. The presenter also dives deeper into Brooklin&rsquo;s architecture and use cases, as well as Linked In&rsquo;s future plans for Brooklin.</li>
<li><a href="https://www.confluent.io/blog/introducing-confluent-platform-5-3">Introducing Confluent Platform 5.3</a>. This is another post where the title says it all; Confluent has released Confluent PLatform 5.3. There are quite a few new features in this release. The ones that excite me the most are: Confluent Operator, Role Based Access Control, and Secret Protection - encrypting secrets within configuration files.<br /></li>
<li><a href="https://www.confluent.io/blog/building-shared-state-microservices-for-distributed-systems-using-kafka-streams">Building Shared State Microservices for Distributed Systems Using Kafka Streams</a>. This is an excellent article discussing how to use Kafka Streams to build shared state microservices that serve as fault-tolerant, highly available single sources of truth about the state of objects in a system. This article came at the right time as we are looking at that in <a href="/derivco">Derivco</a> at the moment!</li>
</ul>

<h2 id="sql-server-2019">SQL Server 2019</h2>

<ul>
<li><a href="/2019/08/03/sql-server-2019-ctp3.2--java/">SQL Server 2019 CTP3.2 &amp; Java</a>. Each new SQL Server CTP release seems to introduce new &ldquo;stuff&rdquo; for the Java language extension. In this post, I look at the inclusion of the Azul OpenJDK in SQL Server 2019 CTP 3.2, and what it introduces.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SQL Server 2019 CTP3.2 &amp; Java]]></title>
    <link href="https://nielsberglund.com/2019/08/03/sql-server-2019-ctp3.2--java/" rel="alternate" type="text/html"/>
    <updated>2019-08-03T06:55:33+02:00</updated>
    <id>https://nielsberglund.com/2019/08/03/sql-server-2019-ctp3.2--java/</id>
    <content type="html"><![CDATA[<p>It seems that for each new SQL Server 2019 CTP release, there are changes to the Java extensions, and CTP 3.2 is no exception. Well, that is not exactly true as in CTP 3.2 release the changes are not about the extension and how we write code, but Java itself.</p>

<p>One of the <a href="https://cloudblogs.microsoft.com/sqlserver/2019/07/24/free-supported-java-in-sql-server-2019-is-now-available/">announcements</a> of what is new in CTP 3.2 was that SQL Server now includes <a href="https://www.azul.com/">Azul System&rsquo;s</a> Zulu Embedded right out of the box for all scenarios where we use Java in SQL Server, including Java extensibility.</p>

<p>So, in this post, we look at the impact, (if any), this has to how we use the Java extensibility framework in SQL Server 2019.</p>

<p></p>

<p>First of all, I find Microsoft&rsquo;s change of stance related to Java very interesting as I remember back in the day the <a href="https://en.wikipedia.org/wiki/Microsoft_Java_Virtual_Machine">battle</a> between Sun and Microsoft regarding Java and the subsequent Java vs .NET &ldquo;war&rdquo;.</p>

<blockquote>
<p><strong>NOTE:</strong> Around <sup>1995</sup>&frasl;<sub>96</sub> I attended a Microsoft event in London where Microsoft introduced what was to be Visual J++. It went under the code name &ldquo;Jakarta&rdquo;. The MS people presenting at the conference explained why they chose that name: <em>Jakarta is the capital of Java</em>. That was not entirely true as Jakarta is the capital of Indonesia, but Java is the largest island, so&hellip;</p>
</blockquote>

<p>Anyway, let us get on with it, and see what happens when we install SQL Server 2019 CTP 3.2.</p>

<h2 id="installation">Installation</h2>

<p>You may remember from <a href="/2018/12/02/sql-server-2019-extensibility-framework--java---hello-world/">SQL Server 2019 Extensibility Framework &amp; Java - Hello World</a> that to enable Java in SQL Server 2019; you have to - during installation - choose the <em>Machine Learning Services</em> feature:</p>

<p><img src="/images/posts/sql_2k19_java_intro_install1.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Enable In-Database Machine Learning</em></p>

<p>In <em>Figure 1</em> we have chosen <em>Machine Learning Services</em> which ensures that the required components, (<em>Launchpad</em>, and so on), gets installed. In pre-CTP 3.2, we had to - in addition to choosing <em>Machine Learning Services</em> - also ensure that Java is installed on the box where we install the SQL Server instance.</p>

<blockquote>
<p><strong>NOTE:</strong> We also need to do other &ldquo;stuff&rdquo; if we want to enable Java, and I cover that below.</p>
</blockquote>

<p>When we install SQL Server CTP 3.2, we see something like so:</p>

<p><img src="/images/posts/s2k19_ctp32_java.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Java Feature Selection</em></p>

<p>What we see in <em>Figure 2</em> is:</p>

<ul>
<li>The <em>Machine Learning Services</em> feature is now named <em>Machine Learning Services and Language Extensions</em>.</li>
<li>Java appears as a feature in the same way as R/Python.</li>
</ul>

<p>We choose Java as a feature, as in <em>Figure 2</em>, and after feature rules and instance configuration, we come to a new dialog; <em>Java Install Location</em>:</p>

<p><img src="/images/posts/s2k19_ctp32_java2.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Java Install Location</em></p>

<p>In the dialog, we see in <em>Figure 3</em>, how we have the ability to choose to install the Azule Zulu OpenJDK which is part of the SQL Server install. We choose the OpenJDK and continue with the installation.</p>

<p>Eventually the installation finish, and if we look in the installation path of the instance we just installed, we see:</p>

<p><img src="/images/posts/s2k19_ctp32_openjdk.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Azule Zulu OpenJDK</em></p>

<p>If we drill down in the <code>AZUL-OpenJDK-JRE</code> directory, we see it is the directory for the Java JRE. So we now have Java installed, and theoretically, we should be able to execute Java code, exactly as we execute R/Python?</p>

<h2 id="post-install">Post Install</h2>

<p>Well, to execute Java code is not as straightforward as in the previous paragraph even though Java is now installed. We still have to do some things:</p>

<ul>
<li>Enable external scripts: <code>EXEC sp_configure 'external scripts enabled', 1</code></li>
<li>Register Java as an external language in the database from where you want to execute your code: <code>CREATE EXTERNAL LANGUAGE ...</code>. Read more about it in <a href="/2019/06/06/sql-server-2019-extensibility-framework--external-languages/">SQL Server 2019 Extensibility Framework &amp; External Languages</a>.</li>
<li>Deploy the Java external language extension SDK to the database above: <code>CREATE EXTERNAL LIBRARY ...</code>. Refer to <a href="/2019/05/26/java--sql-server-2019-extensibility-framework-the-sequel/">Java &amp; SQL Server 2019 Extensibility Framework: The Sequel</a> for more about that.</li>
</ul>

<p>When the above is done, we deploy our code either via <code>CREATE EXTERNAL LIBRARY ...</code>, or have our code available in the <code>CLASSPATH</code>. Having done that we should be able to execute:</p>

<pre><code class="language-sql">DECLARE @p1 int = 21;
DECLARE @p2 int = 21;
EXEC sp_execute_external_script
  @language = N'Java'
, @script = N'sql.JavaTest1'
, @params = N'@x int, @y int'
, @x = @p1
, @y = @p2   
GO
</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Execute Java Code</em></p>

<p>In <em>Code Snippet 1</em> we see that we want to <code>EXEC sp_execute_external_script</code> against some Java code deployed as a <code>.jar</code>, (this is the same code as in the <a href="/2019/06/06/sql-server-2019-extensibility-framework--external-languages/">external languages</a> post). However, when we execute the result is like so:</p>

<p><img src="/images/posts/s2k19_ctp32_exec1.png" alt="" /></p>

<p><strong>Figure 5:</strong> <em>Missing Environment Variable</em></p>

<p>So even though we explicitly said during the SQL installation that we want to use the included Java and it got installed, we still need to set the <code>JRE_HOME</code> variable. The installation does not do that for us.</p>

<p>After we have created the <code>JRE_HOME</code> environment variable and set it to point to the <code>AZUL-OpenJDK-JRE</code> directory we see in <em>Figure 4</em>, we need to restart the <em>Launchpad</em> service, and then we can execute the code in <em>Code Snippet 1</em> again:</p>

<p><img src="/images/posts/s2k19_ctp32_exec2.png" alt="" /></p>

<p><strong>Figure 6:</strong> <em>Success</em></p>

<p>Now everything works as we see in <em>Figure 6</em>.</p>

<h2 id="scenarios">Scenarios</h2>

<p>Even though we explicitly choose the included Java in the installation, we still need to set up Java as in pre-CTP 3.2, so what does this give us?</p>

<p>It gives us peace of mind! Remember that Oracle is phasing out free support for Java, and I wonder how interested they are in solving issues with Java on SQL Server. If we use open source OpenJDK, we rely on the community for support, which may not be ideal in an enterprise production environment. So by using the included Java, we are guaranteed enterprise support - peace of mind!</p>

<p>Does this then mean that we need to use the included Java? Not at all! Below follows some scenarios/options. All require what we did above; create external language, deploy the SDK, deploy your code:</p>

<ul>
<li>Enable <em>Machine Learning Services and Language Extensions</em> only, (no Java): Ensure that your <code>JRE_HOME</code> environment variable points to whatever Java JRE you want to use.</li>
<li>Enable <em>Machine Learning Services and Language Extensions</em> including Java, (as per above): The <code>JRE_HOME</code> variable can point to some other Java JRE than Azul, even though the Azul JDK gets installed.</li>
</ul>

<p>What happens if I enable Java, but instead of choosing the included JRE as in <em>Figure 3</em>, I choose the second option: I provide the location of a different JRE? For Java in <em>Machine Learning Services and Language Extensions</em> that has no effect. You need to, as in all other scenarios, point the <code>JRE_HOME</code> variable to a valid JRE directory.</p>

<blockquote>
<p><strong>NOTE:</strong> It is good practice to whenever you change, (or first set), the <code>JRE_HOME</code> variable to restart the <em>Launchpad</em> service.</p>
</blockquote>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 30, 2019]]></title>
    <link href="https://nielsberglund.com/2019/07/28/interesting-stuff---week-30-2019/" rel="alternate" type="text/html"/>
    <updated>2019-07-28T09:40:56+02:00</updated>
    <id>https://nielsberglund.com/2019/07/28/interesting-stuff---week-30-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="distributed-computing">Distributed Computing</h2>

<ul>
<li><a href="http://muratbuffalo.blogspot.com/2019/07/dissecting-performance-bottlenecks-of.html">Dissecting performance bottlenecks of strongly-consistent replication protocols</a>. This is a post by <a href="https://twitter.com/muratdemirbas">Murat</a> about one of his white papers. The post dissects the white paper which discusses the performance of Paxos protocols. If you are into distributed computing and consensus protocol, this is for you.</li>
<li><a href="https://goto.docker.com/virtual-event-docker-enterprise-3.0.html">Drive High-Velocity Innovation with Docker Enterprise 3.0</a>. Earlier this week <a href="https://blog.docker.com/2019/07/announcing-docker-enterprise-3-0-ga/">Docker announced general availability</a> of Docker Enterprise 3.0. The post I link to is registration for a 5-part virtual event to get a deep dive on new features and enhancements in Docker Enterprise 3.0.</li>
</ul>

<h2 id="streaming">Streaming</h2>

<ul>
<li><a href="https://www.infoq.com/presentations/kafka-distributed-deployments">Streaming Log Analytics with Kafka</a>. This is an <a href="https://www.infoq.com/">InfoQ</a> presentation where the presenter discusses how and why they use Kafka internally and demos how they utilize it as a straightforward event-sourcing model for distributed deployments.</li>
<li><a href="https://www.confluent.io/blog/fault-tolerance-distributed-systems-tracing-with-apache-kafka-jaeger">Fault Tolerance in Distributed Systems: Tracing with Apache Kafka and Jaeger</a>. This is the second post about distributed tracing with Kafka and Jaeger. This post looks at how to make Jaeger fault-tolerant, i.e. how to safeguard against Jaeger going down.</li>
</ul>

<h2 id="sql-server-2019">SQL Server 2019</h2>

<ul>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2019/07/24/sql-server-2019-community-technology-preview-3-2-is-now-available/">SQL Server 2019 community technology preview 3.2 is now available</a>. I guess the title of the post says it all; CTP 3.2 is released. One of the big &ldquo;things&rdquo; in this release is that the SQL Server 2019 Big Data Cluster (BDC) is now in public preview. What this means is that you no longer need to sign up for the preview, but you can deploy directly. Look out for an upcoming blog post from me regarding this.</li>
<li><a href="https://cloudblogs.microsoft.com/sqlserver/2019/07/24/free-supported-java-in-sql-server-2019-is-now-available/">Free supported Java in SQL Server 2019 is now available</a>. Wow, &ldquo;the times they are a changin&rsquo;&rdquo;, as <a href="https://www.youtube.com/watch?v=e7qQ6_RV4VQ">Bob Dylan sang</a>! I remember years and years ago, the battle between Microsoft and Sun regarding <a href="https://en.wikipedia.org/wiki/Microsoft_Java_Virtual_Machine">Java</a>. Who would then have thought that come 2019, Microsoft would include Java in one of Microsoft&rsquo;s flagship products: SQL Server 2019. Very cool!</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interesting Stuff - Week 29, 2019]]></title>
    <link href="https://nielsberglund.com/2019/07/21/interesting-stuff---week-29-2019/" rel="alternate" type="text/html"/>
    <updated>2019-07-21T06:16:02+02:00</updated>
    <id>https://nielsberglund.com/2019/07/21/interesting-stuff---week-29-2019/</id>
    <content type="html"><![CDATA[<p>Throughout the week, I read a lot of blog-posts, articles, and so forth, that has to do with things that interest me:</p>

<ul>
<li>data science</li>
<li>data in general</li>
<li>distributed computing</li>
<li>SQL Server</li>
<li>transactions (both db as well as non db)</li>
<li>and other &ldquo;stuff&rdquo;</li>
</ul>

<p>This blog-post is the &ldquo;roundup&rdquo; of the things that have been most interesting to me, for the week just ending.</p>

<p></p>

<h2 id="data-science-machine-learning">Data Science / Machine Learning</h2>

<ul>
<li><a href="https://devblogs.microsoft.com/dotnet/announcing-ml-net-1-2-and-model-builder-updates-machine-learning-for-net/">Announcing ML.NET 1.2 and Model Builder updates (Machine Learning for .NET)</a>. So, <a href="https://dotnet.microsoft.com/apps/machinelearning-ai/ml-dotnet">ML.NET</a> has come a long way since its inception, and I have covered it on and off in these roundups during the last year or so. In the linked blog post, Microsoft introduces ML.NET 1.2, with some cool new features. I particularly like the time series support and the updates to Model Builder.</li>
</ul>

<h2 id="misc">Misc.</h2>

<ul>
<li><a href="https://medium.com/it-dead-inside/implementing-new-tech-in-your-company-is-tougher-than-you-think-e5ff7f887bff">Implementing new tech in your company is tougher than you think</a>. The topic of this blog post is close to my heart; getting new technologies into a company. The post talks about some of the &ldquo;barriers to entry&rdquo; when it comes to new tech. Well worth a read!</li>
<li><a href="https://medium.com/@rusty.alderson/enterprise-data-architecture-c5c579b54abe">Enterprise Data Architecture</a>. This post is a white paper about data architecture and data architects. It tries to define what data architecture is and what it means to be a data architect. Very interesting!</li>
</ul>

<h2 id="sql-server-2019">SQL Server 2019</h2>

<ul>
<li><a href="/2019/07/16/set-your-sql-server-data-free-with-kafka-extensibility-framework/">Set Your SQL Server Data Free with Kafka: Extensibility Framework</a>. In this post by yours truly I look at how we can push data from SQL Server 2019 to Apache Kafka by the use of SQL Server Extensibility Framework, and the Java language extension.</li>
</ul>

<h2 id="finally">~ Finally</h2>

<p>That&rsquo;s all for this week. I hope you enjoy what I did put together. If you have ideas for what to cover, please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Set Your SQL Server Data Free with Kafka: Extensibility Framework]]></title>
    <link href="https://nielsberglund.com/2019/07/16/set-your-sql-server-data-free-with-kafka-extensibility-framework/" rel="alternate" type="text/html"/>
    <updated>2019-07-16T05:10:24+02:00</updated>
    <id>https://nielsberglund.com/2019/07/16/set-your-sql-server-data-free-with-kafka-extensibility-framework/</id>
    <content type="html"><![CDATA[<p>As many of you may know, (or not), is that my background is SQL Server. Ever since I started programming, SQL Server has been my &ldquo;trusty companion&rdquo;, and my belief is that if you don&rsquo;t have SQL Server as a backend, then there is something wrong. At work, (<a href="/derivco">Derivco</a>), it is the same thing, and we are jokingly saying that we do not have business logic in the database, we have full-blown applications!</p>

<p>However, both me personally and at work, we do realise the value of streaming data; for real-time processing as well as to distribute data without having to rely on replication. In the ideal world, we would change the applications/systems that are the source of the data to both publish the data as event streams as well as persisting the data to the database. However, it may not be possible to change those applications/systems - at least not in the time frame we would like. So what we want to do is to use the database as the source of the data, but treat the data, not as rows in a database but, as streaming events.</p>

<p>This is the first post in a &ldquo;mini&rdquo; series where we look at how we can do what is outlined above. In this post, we look at how to use the <strong>SQL Server Extensibility Framework</strong>, and more specifically the Java language extension to solve the issue.</p>

<p></p>

<blockquote>
<p><strong>NOTE:</strong> What we do in this post requires SQL Server 2019 CTP 3.0+.</p>
</blockquote>

<h2 id="scenario-code">Scenario &amp; Code</h2>

<p>In this post, we look at a scenario somewhat like what we have at <a href="/derivco">Derivco</a>. We have online Casino gameplay where the user, (player), plays Casino games, (slots, table games, etc.), on his PC, tablet or mobile. The game play is persisted to a SQL Server database:</p>

<pre><code class="language-sql">USE master;
GO

DROP DATABASE IF EXISTS GamePlayDB;
GO

CREATE DATABASE GamePlayDB;
GO

USE GamePlayDB;
GO

CREATE TABLE dbo.tb_GamePlay
(
  RowID bigint identity PRIMARY KEY,
  UserID int NOT NULL,
  GameID int NOT NULL,
  WagerAmount decimal(10, 2) NOT NULL,
  PayoutAmount decimal(10, 2) NOT NULL,
  EventTime datetime2 NOT NULL
);
GO

</code></pre>

<p><strong>Code Snippet 1:</strong> <em>Game Play Database</em></p>

<p>In <em>Code Snippet 1</em> we see some SQL code which creates:</p>

<ul>
<li>The database <code>GamePlayDB</code>.</li>
<li>A table, <code>dbo.tb_GamePlay</code>, to persist each outcome of a spin, hand, etc.</li>
</ul>

<blockquote>
<p><strong>NOTE:</strong> The code is as simple it can be, this to concentrate on the important parts.</p>
</blockquote>

<p>When the player spins, etc., the gameplay goes via various services, and finally, the last service in the call-chain calls a stored procedure which persists the outcome:</p>

<pre><code class="language-sql">CREATE PROCEDURE dbo.pr_LogWager @UserID INT,
                                 @GameID INT,
                                 @WagerAmount decimal(10, 2),
                                 @PayoutAmount decimal(10, 2),
                                 @EventTime datetime2
AS
BEGIN
  IF(@EventTime IS NULL)
  BEGIN
    SET @EventTime = SYSUTCDATETIME();
  END
  BEGIN TRY
    BEGIN TRAN
      INSERT INTO dbo.tb_GamePlay(UserID, GameID, WagerAmount, 
                                  EventTime, PayoutAmount)
      VALUES(@UserID, @GameID, @WagerAmount, @PayoutAmount, EventTime);
      --do more tx &quot;stuff&quot; here
    COMMIT TRAN;

  END TRY
  BEGIN CATCH
    ROLLBACK TRAN;
  END CATCH

END  
GO
</code></pre>

<p><strong>Code Snippet 2:</strong> <em>Game Play Procedure</em></p>

<p>We see how the procedure in <em>Code Snippet 2</em> takes relevant gameplay details and inserts them into the <code>dbo.tb_GamePlay</code> table.</p>

<p>In our scenario, we want to stream the individual gameplay events, but we cannot alter the services which generate the gameplay. We instead decide to generate the event from the database using, as we mentioned above, the SQL Server Extensibility Framework.</p>

<h2 id="sql-server-extensibility-frameworkback">SQL Server Extensibility FrameworkBack</h2>

<p>Together with the release of SQL Server 2016, Microsoft introduced the feature to be able to execute R script code from inside SQL Server against an external R engine. SQL Server 2017 added the ability to execute Python code against an external Python environment.</p>

<p>A SQL Server framework enables the ability to call R/Python: the <strong>SQL Server Extensibility Framework</strong>, and you can read more about it in my blog post <a href="/2019/06/06/sql-server-2019-extensibility-framework--external-languages/">SQL Server 2019 Extensibility Framework &amp; External Languages</a>. For SQL Server 2019, Java is available as an external language, and that is what we use in this post.</p>

<p>If you are unsure about what I talk about here are some blog posts that may help:</p>

<ul>
<li><a href="/2018/03/07/microsoft-sql-server-r-services---sp_execute_external_script---i/">Microsoft SQL Server R Services: sp_execute_external_script - I</a> - When calling out to R/Python/Java you do it from a specific procedure: <code>sp_execute_external_script</code>. This is the first post of three looking at <code>sp_execute_external_script</code>.</li>
<li><a href="/2018/03/11/microsoft-sql-server-r-services---sp_execute_external_script---ii/">Microsoft SQL Server R Services - sp_execute_external_script - II</a> - The second post about <code>sp_execute_external_script</code>.</li>
<li><a href="/2018/03/21/microsoft-sql-server-r-services---sp_execute_external_script---iii/">Microsoft SQL Server R Services: sp_execute_external_script - III</a>  - The third post about <code>sp_execute_external_script</code>.</li>
<li><a href="/s2k19_ext_framework_java">SQL Server 2019 Extensibility Framework &amp; Java</a> - A series of posts discussing how to write Java code so we can call it from SQL server.</li>
</ul>

<h2 id="solution">Solution</h2>

<p>As we mentioned above, we want to generate the wager &ldquo;event&rdquo; from inside the database, and there are a couple of ways we can do that:</p>

<ul>
<li><a href="https://docs.microsoft.com/en-us/sql/relational-databases/track-changes/about-change-data-capture-sql-server?view=sql-server-2017">Change Data Capture</a> - CDC captures insert, update, and delete activity that is applied to a SQL Server table, and makes the details of the changes available to consumers.</li>
<li><a href="https://docs.microsoft.com/en-us/sql/relational-databases/track-changes/about-change-tracking-sql-server?view=sql-server-2017">Change Tracking</a> - CT tracks and makes available rows that change in SQL Server tables. The difference
between CT and CDC is that CT does not capture the changed values.</li>
</ul>

<p>Both CDC, and CT can be used to get data from SQL Server to Kafka, and we will cover that in future posts. In this post however, we look at doing the event generation in another way: hook-points in stored procedures.</p>

<p>The idea with stored procedure hook-points is that at a place in a stored procedure, a code block is inserted, (hooked in), and this code block executes some code. In this case, it publishes a message to Kafka.</p>

<blockquote>
<p><strong>NOTE:</strong> With the definition of a hook-point above, you may ask what the difference is between a hook-point and an ordinary procedure call? There is no real difference; the way I see it is that the hook-point executes code which is, to a large degree, un-related to what the procedure does. Oh, and the name &ldquo;hook-point&rdquo; sounds cool.</p>
</blockquote>

<p>So, where do we insert the hook-point in the procedure we see in <em>Code Snippet 2</em>? As the <code>pr_LogWager</code> is transactional, we insert the hook-point after the commit, so we do not publish any messages for rolled back wagers. The implication of publishing the event/message after the last commit is that if you have several stored procedures calling each other, the hook-point may be in the beginning of the call-chain.</p>

<p>Now we know where the hook-point should be, but what should it do? Obviously, it should publish to Kafka, but what should it publish? In this case, it should publish an event, so the hook-point also needs to generate the event. What the event should look like is very much up to you, for now, let us assume the event looks something like so:</p>

<pre><code class="language-json">{
   &quot;$schema&quot;: &quot;http://json-schema.org/draft-04/schema#&quot;,
   &quot;title&quot;: &quot;Wager&quot;,
   &quot;description&quot;: &quot;A placed wager&quot;,
   &quot;type&quot;: &quot;object&quot;,
  
   &quot;properties&quot;: {
  
      &quot;eventTypeId&quot;: {
         &quot;description&quot;: &quot;Unique identifier for the event type&quot;,
         &quot;type&quot;: &quot;integer&quot;
      },

      &quot;userId&quot;: {
         &quot;description&quot;: &quot;The unique identifier for a user&quot;,
         &quot;type&quot;: &quot;integer&quot;
      },

      &quot;userId&quot;: {
         &quot;description&quot;: &quot;The unique identifier for a game&quot;,
         &quot;type&quot;: &quot;integer&quot;
      },

      &quot;wagerAmount&quot;: {
         &quot;description&quot;: &quot;Amount wagered&quot;,
         &quot;type&quot;: &quot;number&quot;
      },

      &quot;payoutAmount&quot;: {
         &quot;description&quot;: &quot;Amount paid out (win)&quot;,
         &quot;type&quot;: &quot;number&quot;
      },
      
      &quot;eventTime&quot;: {
         &quot;description&quot;: &quot;Time when the wager happened&quot;,
         &quot;type&quot;: &quot;string&quot;,
         &quot;format&quot;: &quot;date-time&quot;
      },
    
     
   },
  
   &quot;required&quot;: [&quot;eventTypeId&quot;, &quot;userId&quot;, &quot;userId&quot;, 
                &quot;wagerAmount&quot;, &quot;payoutAmount&quot;]
}
</code></pre>

<p><strong>Code Snippet 3:</strong> <em>Wager Schema</em></p>

<p>We see in <em>Code Snippet 3</em> how the schema for the event looks very much like what we persist to the table in our stored procedure. The only difference is that we also define an <code>eventTypeId</code>, which we can use when we do stream processing to filter out various types of events.</p>

<p>Even though the hook-point can be just a few lines of T-SQL code, best practice is to define an event-specific stored procedure:</p>

<pre><code class="language-sql">CREATE PROCEDURE dbo.pr_GenerateAndPublishWagerEvent @UserID INT,
                                          @GameID INT,
                                          @WagerAmount decimal(10, 2),
                                          @PayoutAmount decimal(10, 2),
                                          @EventTime datetime2
AS
BEGIN
  DECLARE @msg nvarchar(max);

  --generate the event
  SET @msg =  (SELECT 1500 AS eventTypeId,
                 @UserId AS userId,
                 @GameId AS gameId,
                 @WagerAmount AS wagerAmount,
                 @PayoutAmount AS payoutAmount,
                 @EventTime AS eventTime
     FOR JSON PATH, WITHOUT_ARRAY_WRAPPER);
  -- call &quot;something&quot; to publish to Kafka
END
</code></pre>

<p><strong>Code Snippet 4:</strong> <em>Hook Point Procedure</em></p>

<p>The code we see in <em>Code Snippet 4</em> is the start of the hook-point procedure. We see how the procedure generates the wager event by using the T-SQL <code>FOR JSON</code> syntax. A placeholder for the publish call follows the creation of the event.</p>

<blockquote>
<p><strong>NOTE:</strong> when you look at the hook-point procedure it may seem like overengineering as the call is basically a pass-through from the <code>dbo.pr_LogWager</code> procedure. The reason we have a specific procedure is that in the &ldquo;real world&rdquo; you most likely do more things inside the procedure.</p>
</blockquote>

<p>So, the placeholder for the publish call; that is where we call into some Java code that publishes to Kafka. Before we look at the Java code, let us see what the Kafka setup should look like.</p>

<h2 id="kafka">Kafka</h2>

<p>I have assumed in this post that we have Kafka installed &ldquo;somewhere&rdquo;, and that we can connect to it. If that is not the case, have a look at the <a href="/2019/06/18/confluent-platform--kafka-for-a-.net-developer-on-windows/">Confluent Platform &amp; Kafka for a .NET Developer on Windows</a> post to see how you install Kafka in a Docker container.</p>

<p>Now, when we have Kafka installed, let us create two topics:</p>

<ul>
<li><code>testTopic</code> - as the name implies, it is a topic for test. We use it initially just to make sure our Java code works. Create it with 1 partition.</li>
<li><code>wagers</code> - this is the topic to where we publish wagers. We create it with 4 partitions.</li>
</ul>

<blockquote>
<p><strong>NOTE:</strong> In a future post we will talk more about partitions, and what role they play.</p>
</blockquote>

<p>As I did in the <a href="/2019/06/18/confluent-platform--kafka-for-a-.net-developer-on-windows/">post</a> mentioned above, I create the topics using <em>Control Center</em>:</p>

<p><img src="/images/posts/sql_kafka_extlang_topics.png" alt="" /></p>

<p><strong>Figure 1:</strong> <em>Create Wagers Topic</em></p>

<p>In <em>Figure 1</em> we see how I create the <code>wagers</code> topic in <em>Control Center</em>&rsquo;s <em>New topic</em> screen.</p>

<blockquote>
<p><strong>NOTE:</strong> If you don&rsquo;t have <em>Control Center</em> you can create topics via the command line, using the <code>kafka-topics --create ...</code> statement.</p>
</blockquote>

<p>When we have the two topics, let us write some Java code.</p>

<h2 id="java">Java</h2>

<blockquote>
<p><strong>NOTE:</strong> I am by no means a Java developer, so I apologize in advance for simplistic and naive code. Furthermore, the code is definitely not production code; no error handling, etc., so use it on your own risk.</p>
</blockquote>

<p>For this blog post, I use <em>VS Code</em> together with the <em>Maven</em> extension. You can read more about that in the <a href="/2019/01/17/sql-server-2019--java-with-visual-studio-code/">SQL Server 2019 &amp; Java with Visual Studio Code</a> post.</p>

<p>To begin with, let us:</p>

<ul>
<li>Create an empty folder where you want your Java project.</li>
<li>Open <em>VS Code</em> and open the folder you created above.</li>
<li>Create a new <em>Maven</em> project, using the archetype <code>maven-archetype-quickstart</code>.</li>
</ul>

<p>During the creation of the project, you are asked for some properties of the project: <code>groupId</code>,  <code>artifactId</code>, <code>version</code>, and <code>packageId</code>. In my project, I set them to:</p>

<ul>
<li><code>groupId</code>: <code>com.nielsberglund.sqlserver</code>.</li>
<li><code>artifactId</code>: <code>SqlToKafka</code>.</li>
<li><code>version</code>: <code>1.0</code>.</li>
<li><code>package</code>: <code>kafkapublish</code>.</li>
</ul>

<p>To be able to publish to Kafka, we need a Java Kafka client, and we use the native client from <code>org.apache.kafka</code>: <code>kafka-clients</code>. To use the client, we need to add it as a dependency in the projects <code>pom.xml</code> file:</p>

<pre><code class="language-xml">&lt;dependencies&gt;
  &lt;dependency&gt;
    &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;
    &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;
    &lt;version&gt;2.3.0&lt;/version&gt;
  &lt;/dependency&gt;
  ...
&lt;/dependencies&gt;
</code></pre>

<p><strong>Code Snippet 5:</strong> <em>Kafka Clients Dependencies</em></p>

<p>We see in <em>Code Snippet 5</em> the dependency we added to the <code>pom.xml</code> file. We are now ready to start to write some code.</p>

<p>To begin with, we create a very basic method which publishes to Kafka:</p>

<pre><code class="language-java">package kafkapublish;
import java.util.Properties;

//import necessary Kafka packages
import org.apache.kafka.clients.producer.Producer;
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerRecord;

public class SqlKafka 
{
  public static void main( String[] args )
  {
      SqlKafka sq = new SqlKafka();
      sq.publishToKafka();
  }

  public void publishToKafka() {
      String topicName = &quot;testTopic&quot;;

      Properties config = new Properties();
      config.put(&quot;client.id&quot;, &quot;1&quot;);
      config.put(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;);
      config.put(&quot;acks&quot;, &quot;all&quot;);

      config.put(&quot;key.serializer&quot;, \
      &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);
      config.put(&quot;value.serializer&quot;, \
      &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);

      Producer&lt;String, String&gt; producer = \
           new KafkaProducer&lt;String, String&gt;(config);

      for (int i = 0; i &lt; 10; i++) {
          producer.send(new ProducerRecord&lt;String, String&gt; \
          (topicName, null, String.format(&quot;Hello number: %s&quot;, i)));
      }
      producer.close();
  }
}
</code></pre>

<p><strong>Code Snippet 6:</strong> <em>Simple Publish</em></p>

<p>The code we see in <em>Code Snippet 6</em> is in the project just for us to make sure that we can publish to the broker, and has nothing to do with SQL Server. In the <code>publishToKafka</code> method, we see how we:</p>

<ul>
<li>Set some properties, amongst them are the brokers, (<code>bootstrap.servers</code>), we connect to.</li>
<li>Set the serializers we use.</li>
<li>Create a <code>Producer</code> instance.</li>
<li>Call <code>send</code> on the <code>producer</code> instance.</li>
</ul>

<p>To check that everything works we &ldquo;spin up&rdquo; a <code>bash</code> shell in the Kafka broker instance. If you run this in Docker you:</p>

<ul>
<li>Do a <code>Docker exec -it &lt;kafka_instance_name&gt; bash</code>.</li>
<li>From the command prompt in the container you <code>cd</code> into the <code>/usr/bin/</code> directory.</li>
</ul>

<p>When you are in the <code>/usr/bin/</code> directory you start up a Kafka console consumer like so:</p>

<pre><code class="language-bash">$ ./kafka-console-consumer --bootstrap-server broker:29092 \
                           --topic testTopic
</code></pre>

<p><strong>Code Snippet 7:</strong> <em>Kafka Console Consumer</em></p>

<p>You may ask yourself why, in the code in <em>Code Snippet 6</em>, <code>bootstrap.servers</code> is <code>localhost:9092</code>, whereas in <em>Code Snippet 7</em> it is <code>broker:29092</code>; different host-name as well as port number? That is because we run in a Docker container where we have different listeners for internal network connections compared to external (from the host machine or other machines).</p>

<blockquote>
<p><strong>NOTE:</strong> <a href="https://twitter.com/rmoff">Robin Moffat</a>, who is a Kafka guru, has written a blog post about port addresses and listeners: <a href="https://rmoff.net/2018/08/02/kafka-listeners-explained/">Kafka Listeners - Explained</a>. If you are interested in Kafka, you should read that post, and whatever else Robin publishes. He knows his stuff!</p>
</blockquote>

<p>Anyway, you execute the code in <em>Code Snippet 7</em>. When you subsequently run the Java code in <em>Code Snippet 6</em>, the output in the terminal window where you run <code>kafka-console-consumer</code> looks like so:</p>

<p><img src="/images/posts/sql_kafka_extlang_output1.png" alt="" /></p>

<p><strong>Figure 2:</strong> <em>Consume Output</em></p>

<p>We see in <em>Figure 2</em> the commands we used to run the containers <code>bash</code> shell, together with the <code>kafka-console-consumer</code> command, and the output, (&ldquo;Hello number: <em>n</em>&rdquo;), which we receive after executing the Java code. That&rsquo;s cool - our simple code works, but - once again - the code is very simple, and we definitely cannot use it, as is in SQL Server. Let us see what we need to do to make this work from SQL.</p>

<h2 id="java-code-sql-server">Java Code &amp; SQL Server</h2>

<p>This post is not about how to write Java code so it can be used from SQL Server, read my <a href="/s2k19_ext_framework_java">SQL Server 2019 Extensibility Framework &amp; Java</a> series for that. There is one thing however that is important that I want to re-iterate, and that is the <a href="https://docs.microsoft.com/en-us/sql/language-extensions/how-to/extensibility-sdk-java-sql-server">Microsoft Extensibility SDK for Java</a>, which is required if we want to write Java code for SQL Server. The SDK was introduced together with SQL Server 2019 CTP 2.5, (I am now on CTP 3.1). I wrote about the SDK in <a href="/2019/05/26/java--sql-server-2019-extensibility-framework-the-sequel/">Java &amp; SQL Server 2019 Extensibility Framework: The Sequel</a>. One difference between CTP 2.5 and CTP 3.1 is that the SDK is now part of the SQL Server distribution, so you do not need to download it. You find it at: <code>\&lt;path_to_instance_install&gt;\MSSQL\Binn\mssql-java-lang-extension.jar</code>.</p>

<p>What we need to do is to add the SDK as a dependency for our project. If you use <em>VS Code</em> and <em>Maven</em> I covered how to do it in the <a href="/2019/05/26/java--sql-server-2019-extensibility-framework-the-sequel/">Java &amp; SQL Server 2019 Extensibility Framework: The Sequel</a>. A short recap:</p>

<ul>
<li>Create a new directory in <em>Maven</em>&rsquo;s local repository directory. For me, on Windows, it is <code>%USERPROFILE%\.m2\repository</code>.</li>
<li>Create a subdirectory of the new directory, named <code>mssql-java-lang-extension</code>.</li>
<li>Create a subdirectory of <code>mssql-java-lang-extension</code>, and name it as a version number. (<code>1.0</code> for example).</li>
<li>Copy <code>mssql-java-lang-extension.jar</code> to the &ldquo;version&rdquo; directory and add the &ldquo;version&rdquo; number to the <code>.jar</code> file like so: <code>mssql-java-lang-extension-1.0.jar</code>:</li>
</ul>

<p><img src="/images/posts/sql_2k19_java_sdk_dep_hierarch.png" alt="" /></p>

<p><strong>Figure 3:</strong> <em>Folder Hierarchy Dependency</em></p>

<p>In <em>Figure 3</em> we see the &ldquo;layout&rdquo; of the local <em>Maven</em> repository after I have set it up for the SDK dependency, and we see how I named the top-level directory <code>nielsb</code>. Outlined in blue we see the different folders below<code>..\m2\repository</code>, and the outline in red shows the renamed SDK file. We can now add the dependency to the <code>pom.xml</code> file:</p>

<pre><code class="language-xml">&lt;dependencies&gt;
  &lt;dependency&gt;
    &lt;groupId&gt;nielsb&lt;/groupId&gt;
    &lt;artifactId&gt;mssql-java-lang-extension&lt;/artifactId&gt;
    &lt;version&gt;1.0&lt;/version&gt;
  &lt;/dependency&gt;
  ...
&lt;/dependencies&gt;
</code></pre>

<p><strong>Code Snippet 8:</strong> <em>Java SDK Dependency</em></p>

<p>By having added the dependency to the <code>pom.xml</code> file, we can now reference the SDK in our code:</p>

<pre><code class="language-java">package kafkapublish;

...

import com.microsoft.sqlserver.javalangextension.*;
import java.util.LinkedHashMap;

public class SqlKafka  extends AbstractSqlServerExtensionExecutor
{
  ...
}
</code></pre>

<p><strong>Code Snippet 9:</strong> <em>Extending the SqlKafka Class</em></p>

<p>What we see in <em>Code Snippet 9</em> is how I <code>import</code> all classes in <code>com.microsoft.sqlserver.javalangextension</code>, and how I subsequently extend the <code>SqlKafka</code> class with <code>AbstractSqlServerExtensionExecutor</code>. Oh, I also <code>import</code> <code>java.util.LinkedHashMap</code>, which I use later.</p>

<p>We know from <a href="/2019/05/26/java--sql-server-2019-extensibility-framework-the-sequel/">Java &amp; SQL Server 2019 Extensibility Framework: The Sequel</a> that when we call Java code from SQL Server, our code needs to implement the <code>execute</code> method from the <code>AbstractSqlServerExtensionExecutor</code> class, and that the method looks like so:</p>

<pre><code class="language-java">public AbstractSqlServerExtensionDataset execute(
               AbstractSqlServerExtensionDataset input, 
               LinkedHashMap&lt;String, Object&gt; params) {...}
</code></pre>

<p><strong>Code Snippet 10:</strong> <em>Execute Method</em></p>

<p>In <em>Code Snippet 10</em> we see how the method expects two parameters: a dataset, and a map of strings and objects. That is quite useful for us, as, even though right now the publish method in our code has hardcoded values for broker, topic, message, and so on; in a &ldquo;real world&rdquo; scenario we do do not publish to just one topic, and we may publish the same event to different brokers.</p>

<p>So, the way we code it is that we expect the dataset to contain a broker address, (including port-number), topic, and partition value. We pass in the event, (message), as a parameter, part of the <code>LinkedHashMap</code>. That way, we do not duplicate the message if we publish to multiple brokers:</p>

<pre><code class="language-java">public PrimitiveDataset execute(PrimitiveDataset input, 
                               LinkedHashMap&lt;String, 
                               Object&gt; params) 
{  
    String[] brokers = input.getStringColumn(0);
    String[] partitions = input.getStringColumn(1);
    String[] topics = input.getStringColumn(2);
    String message = (String)params.get(&quot;msg&quot;);

    int rowCount = brokers.length;

    for(int i= 0; i &lt; rowCount; i++)
    {
      //grab the column values  
      String broker = (String)brokers[i];
      String partition = (String)partitions[i];
      String topic = (String)topics[i];
      
      sqlPublishToKafka(topic, partition, broker, message);
    }
    return null;
}

public void sqlPublishToKafka(String topic, String partition, String broker, String message) 
{
    
    Properties config = new Properties();
    config.put(&quot;client.id&quot;, &quot;1&quot;);
    config.put(&quot;bootstrap.servers&quot;, broker);
    config.put(&quot;acks&quot;, &quot;all&quot;);

    config.put(&quot;key.serializer&quot;, \
    &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);
    config.put(&quot;value.serializer&quot;, \
    &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);

    Producer&lt;String, String&gt; producer = new KafkaProducer&lt;String, String&gt;(config);
    producer.send(new ProducerRecord&lt;String, String&gt;(topic, partition, message));
    producer.close();
}

</code></pre>

<p><strong>Code Snippet 11:</strong> <em>The Execute Method</em></p>

<p>We see in <em>Code Snippet 11</em> how I have implemented the <code>execute</code> method, and how I, in that method, handle the incoming parameters and subsequently call into the new <code>sqlPublishToKafka</code> method.</p>

<p>The Java language extension has some requirements on the code, which means we have to set some member variables in the class constructor like so:</p>

<pre><code class="language-java">public class SqlKafka  extends AbstractSqlServerExtensionExecutor
{

  public SqlKafka() 
  {
    executorExtensionVersion = SQLSERVER_JAVA_LANG_EXTENSION_V1;
    executorInputDatasetClassName = PrimitiveDataset.class.getName();
    executorOutputDatasetClassName = PrimitiveDataset.class.getName();
  }

  public PrimitiveDataset execute(PrimitiveDataset input, 
                                  LinkedHashMap&lt;String, Object&gt; params) 
  { ... }

  public void sqlPublishToKafka(String topic, String partition, 
                                String broker, String message) 
  { ... }
}
</code></pre>

<p><strong>Code Snippet 12:</strong> <em>Variables Required by Language Extension</em></p>

<p>You can read more about the required variables we see in <em>Code Snippet 12</em> in the <a href="/2019/05/26/java--sql-server-2019-extensibility-framework-the-sequel/">Java &amp; SQL Server 2019 Extensibility Framework: The Sequel</a> post.</p>

<p>Normally we can now build the application and create a <code>.jar</code> file out of it, to use later. If you use <em>VS Code</em> together with <em>Maven</em> see my post <a href="/2019/01/17/sql-server-2019--java-with-visual-studio-code/">SQL Server 2019 &amp; Java with Visual Studio Code</a> if you are unsure how to create a <code>.jar</code> file.</p>

<p>In the previous paragraph, I wrote &ldquo;normally&rdquo;, because when we want to use our code from SQL Server, it is not as straightforward as to just create a <code>.jar</code> file. As the <code>.jar</code> file will be deployed to SQL Server we need to ensure that all dependencies also are included, i.e., we need an &ldquo;uber jar&rdquo;.</p>

<p>To create this &ldquo;uber jar&rdquo;, (at least in <em>VS Code</em> and <em>Maven</em>), we use a <em>Maven</em> plugin <code>maven-shade-plugin</code> which is part of the <code>org.apache.maven.plugins</code> group, and we add the plugin to the <code>pom.xml</code> file. Let us see how and where to place it.</p>

<p>When we create a <em>Maven</em> project, the <code>pom.xml</code> file looks something like so:</p>

<p><img src="/images/posts/ql_kafka_extlang_maven_plugins1.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Build Section</em></p>

<p>We see in <em>Figure 4</em> that there is a <code>&lt;build&gt;</code> section in the <code>pom.xml</code> file, and it is in that section we add the plugin. In fact, for this project, we do not need anything else in the <code>&lt;build&gt;</code> section apart from our plugin. So we replace the whole existing <code>&lt;build&gt;</code> section with the following:</p>

<pre><code class="language-xml">&lt;build&gt;
  &lt;plugins&gt;
    &lt;plugin&gt;
      &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
      &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;
      &lt;version&gt;3.2.1&lt;/version&gt;
      &lt;configuration&gt;
        &lt;createDependencyReducedPom&gt;false&lt;/createDependencyReducedPom&gt;
        &lt;artifactSet&gt;
          &lt;excludes&gt;
            &lt;exclude&gt;nielsb:*&lt;/exclude&gt;  
          &lt;/excludes&gt;
        &lt;/artifactSet&gt;
      &lt;/configuration&gt;
      &lt;executions&gt;
        &lt;execution&gt;
          &lt;phase&gt;package&lt;/phase&gt;
          &lt;goals&gt;
            &lt;goal&gt;shade&lt;/goal&gt;
          &lt;/goals&gt;
        &lt;/execution&gt;
      &lt;/executions&gt;
    &lt;/plugin&gt;
  &lt;/plugins&gt;
&lt;/build&gt;
</code></pre>

<p><strong>Code Snippet 13:</strong> <em>New Build Section</em></p>

<p>Notice in <em>Code Snippet 13</em> how there is an <code>&lt;excludes&gt; section containing</code><exclude>nielsb:*</exclude><code>. That</code>nielsb` refers to the directory we added for the Java SDK dependency, (<em>Figure 3</em>). What we say here is that we do not want to include the Java SDK as a dependency in our &ldquo;uber jar&rdquo;, as we later deploy the SDK standalone, and having two SDK&rsquo;s deployed to the same database cause bad things to happen.</p>

<p>Having done this, we:</p>

<ul>
<li>Save the <code>pom.xml</code> file.</li>
<li>Compile the project.</li>
<li>Create the <code>.jar</code> via the <code>package</code> command.</li>
</ul>

<p>The created <code>.jar</code> file is in the project&rsquo;s <code>target</code> directory, and &ldquo;weighs&rdquo; in at around 9Mb. Later we see how we use this <code>.jar</code> file, but let us now go back to the database.</p>

<h2 id="sql">SQL</h2>

<p>Having a <code>.jar</code> file for our application means that we can deploy it to the database and test and see what happens. However, there are a couple of things we need to do before that.</p>

<p>Remember from the code in <em>Code Snippet 11</em> how we send in a dataset with the relevant information about where to publish a message to. That information comes from tables in the database. Let us create the tables:</p>

<pre><code class="language-sql">CREATE TABLE dbo.tb_KafkaCluster
(
  ClusterID int,
  BootstrapServers nvarchar(4000) NOT NULL,
  [Description] nvarchar(4000) NOT NULL,
  CONSTRAINT [pk_KafkaCluster] PRIMARY KEY
  (ClusterID)
);
GO

CREATE TABLE dbo.tb_KafkaEventSubscriber
(
  SubscriberID int identity,
  EventID int NOT NULL,
  ClusterID int NOT NULL,
  Topic nvarchar(256) NOT NULL,
  CONSTRAINT [pk_KafkaEventSubscriber] PRIMARY KEY
  (EventID, ClusterID, Topic),
  CONSTRAINT [fk_KafkaEventSubscriber_ClusterID] 
  FOREIGN KEY (ClusterID)
  REFERENCES dbo.tb_KafkaCluster(ClusterID)
);
GO
</code></pre>

<p><strong>Code Snippet 14:</strong> <em>Routing Tables</em></p>

<p>The code in <em>Code Snippet 14</em> creates two tables which will contain information about who wants the different events, and where to send it to. Notice the <code>BootstrapServers</code> column in the <code>dbo.tb_KafkaCluster</code> table. That column contains the bootstrap servers as a comma-delimited string.</p>

<blockquote>
<p><strong>NOTE:</strong> The setup above is very simplistic. We should have a tables for different type of events, and with foreign key references to that table. Potentially also a table for topics. The <code>BootstrapServers</code> column is also a shortcut. We should have a separate table for the brokers with foreign key reference to the cluster table. However, for this blog-post, this is enough.</p>
</blockquote>

<p>When we have created the tables in <em>Code Snippet 14</em> we insert some data:</p>

<pre><code class="language-sql">INSERT INTO dbo.tb_KafkaCluster(ClusterID, BootstapServers, 
                               [Description])
VALUES(1, 'localhost:9092', 'First cluster');

INSERT INTO dbo.tb_KafkaEventSubscriber(EventID, ClusterID, 
                                        Topic)
VALUES (1, 1, 'testTopic'),
       (1500, 1, 'wagers');
</code></pre>

<p><strong>Code Snippet 15:</strong> <em>Insert Data</em></p>

<p>As I run both SQL Server and Kafka on my local machine, I set the <code>BootstapServers</code> value to <code>localhost:9092</code> which we see in <em>Code Snippet 15</em>.</p>

<p>In the hook-point procedure, (<em>Code Snippet 4</em>), we can publish straight to Kafka, but that means we need the same publishing code in each event type&rsquo;s hook-point procedure. So let us instead create a procedure which does the actual publish to Kafka:</p>

<pre><code class="language-sql">CREATE PROCEDURE dbo.pr_PublishToKafka @EventID int,
                                       @PartitionValue nvarchar(50),
                                       @EventMessage nvarchar(max)
AS
BEGIN

  IF(@PartitionValue = '')
  BEGIN
    SET @PartitionValue = NULL
  END

  EXEC sp_execute_external_script 
    @language = N'Java',
    @script = N'kafkapublish.SqlKafka',
    @input_data_1 = N'SELECT kc.BootstrapServers, @partVal, 
                             kes.Topic
                      FROM dbo.tb_KafkaCluster kc
                      JOIN dbo.tb_KafkaEventSubscriber kes
                        ON kc.ClusterID = kes.ClusterID
                      WHERE kes.EventID =  @eventID ',
    @params = N'@msg nvarchar(max), @eventID int, 
                @partVal nvarchar(50)',
    @eventID = @EventID,
    @partVal = @PartitionValue,
    @msg = @EventMessage;
END
GO
</code></pre>

<p><strong>Code Snippet 16:</strong> <em>Procedure to Publish</em></p>

<p>In <em>Code Snippet 16</em> we see how we pass in parameters for what type of event it is, the partition value, and the event message. To publish to Kafka we use the procedure <code>sp_execute_external_script</code>, and in the procedure we:</p>

<ul>
<li>Set the language to Java.</li>
<li>Set the <code>@script</code> parameter to our package and class name.</li>
<li>Retrieve the broker and topic information for the event type, together with the partition value. This the <code>SELECT</code> statement in the <code>@input_data_1</code> oarameter.</li>
<li>Define parameters that we use in the <code>SELECT</code>, and also by our Java code, (the <code>@msg</code> parameter).</li>
</ul>

<p>Now we are almost done, and we alter the procedures to call into our hook-point procedure, and the publish procedure:</p>

<pre><code class="language-sql">ALTER PROCEDURE dbo.pr_LogWager ...
AS
BEGIN
  ...

  BEGIN TRY
    BEGIN TRAN
      ...
    COMMIT TRAN;

    EXEC dbo.pr_GenerateAndPublishWagerEvent 
                              @UserID = @UserID,
                              @GameID = @GameID,
                              @WagerAmount = @WagerAmount,
                              @PayoutAmount = @PayoutAmount,
                              @EventTime = @EventTime;
  ...

END  
GO

ALTER PROCEDURE dbo.pr_GenerateAndPublishWagerEvent ... 
                                
AS
BEGIN
  DECLARE @msg nvarchar(max);

  --generate the event
  SET @msg =  ...;
  
  EXEC dbo.pr_PublishToKafka @EventID = 1500,
                             @PartitionValue = @UserID,
                             @EventMessage = @msg; 
END
GO
</code></pre>

<p><strong>Code Snippet 17:</strong> <em>Altering Procedures</em></p>

<p>In the procedures in <em>Code Snippet 17</em> I have taken out the parameter definitions, and also left out some of the code for brevity. We see how <code>dbo.pr_LogWager</code> calls into <code>dbo.pr_GenerateAndPublishWagerEvent</code>, which calls into <code>dbo.pr_PublishToKafka</code>.</p>

<h2 id="deploy">Deploy</h2>

<p>We are now ready to deploy our Java code into the database. However, if this is the first time we deploy Java code to the database, we need to create Java as an external language in the database. I covered this in my post: <a href="/2019/06/06/sql-server-2019-extensibility-framework--external-languages/">SQL Server 2019 Extensibility Framework &amp; External Languages</a>. From reading that post, we see we need to:</p>

<ul>
<li>Create an archive file (<code>.zip</code>) of the Java language extension file <code>javaextension.dll</code>, which is located at <code>..\&lt;path_to_sql_instance&gt;\MSSQL\Binn\javaextension.dll</code>.</li>
<li>Deploy the zip file to the database using the <code>CREATE EXTERNAL LANGUAGE</code> syntax:</li>
</ul>

<pre><code class="language-sql">CREATE EXTERNAL LANGUAGE Java
FROM (CONTENT = 'W:\javaextension.zip'
      , FILE_NAME = 'javaextension.dll');
GO
</code></pre>

<p><strong>Code Snippet 18:</strong> <em>Creating External Language</em></p>

<p>I zipped the extension dll and placed it in the root of my <code>W:\</code> drive, and then called <code>CREATE EXTERNAL LANGUAGE</code>. We can check that it worked by calling <code>SELECT * FROM sys.external_languages</code>. If all is well we can go ahead.</p>

<blockquote>
<p><strong>NOTE:</strong> The name we assign to the language is as such of no importance, except that we use it in <code>sp_execute_external_script</code>, and as well as when we create external libraries, which we see below.</p>
</blockquote>

<p>Once again, if this is the first time we deploy Java to a database we also need to deploy the Java language SDK (<code>mssql-java-lang-extension.jar</code>), the one we used in our Java code above. For this, we use the <code>CREATE EXTERNAL LIBRARY</code>:</p>

<pre><code class="language-sql">CREATE EXTERNAL LIBRARY javaSDK 
FROM (CONTENT = 'W:\mssql-java-lang-extension.jar')
WITH (LANGUAGE = 'Java');
GO 
</code></pre>

<p><strong>Code Snippet 19:</strong> <em>Creating Java SDK Library</em></p>

<p>As in <em>Code Snippet 18</em>, I copied the SDK file to <code>W:\</code>, and then ran what we see in <em>Code Snippet 19</em>. The name we give the library does not matter, but it is a good idea to keep it somewhat descriptive. Use <code>SELECT * FROM sys.external_libraries</code> to ensure it worked.</p>

<p>Right, so finally we can deploy our application, and what we deploy is the <code>.jar</code> file we created just after <em>Code Snippet 13</em>:</p>

<pre><code class="language-sql">CREATE EXTERNAL LIBRARY javePublishToKafka 
FROM (CONTENT = 'W:\SqlToKafka-1.0.jar')
WITH (LANGUAGE = 'Java');
GO 
</code></pre>

<p><strong>Code Snippet 20:</strong> <em>Deploying the Application</em></p>

<p>We deploy the application by creating an external library, as we see in <em>Code Snippet 20</em>, and yes, I copied the <code>.jar</code> file to <code>W:\</code> this time as well.</p>

<h2 id="run-the-application">Run the Application</h2>

<p>So finally, it is time to see what we have done works. Start with &ldquo;spinning up&rdquo; a Kafka consumer against the <code>wagers</code> topic. Use similar code to what we see in <em>Code Snippet 7</em>, but change <code>--topics testTopic</code> to <code>--topics wagers</code>. When the Kafka consumer is up and listening, it is time to see if it works:</p>

<pre><code class="language-sql">dbo.pr_EmulateGamePlay @Loops = 5,
                       @MinDelay = 50,
                       @MaxDelay = 500;
</code></pre>

<p><strong>Code Snippet 21:</strong> <em>Emulate Game Play</em></p>

<p>In <em>Code Snippet 21</em> we see a procedure which emulates gameplay, and it expects some parameters:</p>

<ul>
<li><code>@Loops</code> - In the procedure, we loop around <code>dbo.pr_LogWager</code> and passes in random values for the required parameters in <code>dbo.pr_LogWager</code>.</li>
<li><code>@MinDelay</code>, <code>@MaxDelay</code> - in the procedure we do a random wait between each loop and these two parameters define min, and max values, (milliseconds).</li>
</ul>

<p>Let us see if we get any output from the Kafka consumer when we execute the code in <em>Code Snippet 21</em>:</p>

<p><img src="/images/posts/sql_kafka_extlang_publish_output.png" alt="" /></p>

<p><strong>Figure 4:</strong> <em>Kafka Output</em></p>

<p>Yes! From what we see in <em>Figure 4</em> it works, as we see 5 events as output. We have succeeded in streaming data out from the database into Kafka by using SQL Server Extensibility Framework and the Java external language.</p>

<blockquote>
<p><strong>NOTE:</strong> You may ask what the <code>dbo.pr_EmulateGamePlay</code> looks like. I have included the source for that procedure at the very end of this post as an Appendix.</p>
</blockquote>

<h2 id="summary">Summary</h2>

<p>There are various ways one can get data out of SQL Server and into the streaming world. In this blog post, we looked at using SQL Server&rsquo;s Extensibility framework and Java as an external language.</p>

<p>We looked at:</p>

<ul>
<li>Creating hook-point procedures and injecting them into the procedures whose data we want to capture and create events from.</li>
<li>Using the Java Kafka client to publish data.</li>
<li>Adding a dependency in the Java project against the Microsoft Java SDK.</li>
<li>Creating an &ldquo;uber jar&rdquo; containing all Java dependencies, except for the Java SDK. The actual application should also be in that &ldquo;uber jar&rdquo;.</li>
<li>Creating the Java language in the database.</li>
<li>Deploying the Java SDK to the database.</li>
<li>Deploying the &ldquo;uber jar&rdquo; to the database.</li>
<li>Using <code>sp_execute_external_script</code> to call into the Java code from SQL Server.</li>
</ul>

<p>As I mentioned above, there are various ways to &ldquo;free&rdquo; your data, and we look at other ways in future posts.</p>

<h2 id="finally">~ Finally</h2>

<p>If you have comments, questions etc., please comment on this post or <a href="mailto:niels.it.berglund@gmail.com">ping</a> me.</p>

<h2 id="appendix-i">Appendix - I</h2>

<p>Below is the code for the <code>dbo.pr_EmulateGamePlay</code> procedure:</p>

<pre><code class="language-sql">CREATE PROCEDURE dbo.pr_EmulateGamePlay @Loops int = 10,
                                        @MinDelay int = 50,
                                        @MaxDelay int = 500
AS
BEGIN                                        
  DECLARE @delay int;
  DECLARE @waitfor varchar(25);
  DECLARE @winMultiplier int;
  DECLARE @wager decimal(10, 2);
  DECLARE @userId int;
  DECLARE @gameId int;
  DECLARE @payout decimal(10, 2) = 0.00;
  DECLARE @minUserId int = 100;
  DECLARE @maxUserId int = 125;
  DECLARE @minwager int = 25;
  DECLARE @maxwager int = 5000;
  DECLARE @minWinMult int = 1;
  DECLARE @maxWinMult int = 10;
  DECLARE @minGameId int = 1000;
  DECLARE @maxGameId int = 1050;
  DECLARE @minWinIndicator int = 1;
  DECLARE @maxWinIndicator int = 12;
  DECLARE @noLoops int = 0;
  DECLARE @eventTime datetime2;
  
  WHILE(@noLoops &lt; @Loops)
  BEGIN

    SET @eventTime = SYSUTCDATETIME();
  -- get random values for delay between games, 
  -- wager size, user id, game id,
  -- win multiplier
    SELECT @delay = @MinDelay + ROUND(RAND() * 
                   (@MaxDelay + 1 - @MinDelay), 0),
            @wager = (@minwager + ROUND(RAND() * 
                     (@maxwager + 1 - @minwager), 0)) / 100,
            @userId = @minUserId + ROUND(RAND() * 
                      (@maxUserId + 1 - @minUserId), 0),
            @gameId = @minGameId + ROUND(RAND() * 
                      (@maxGameId + 1 - @minGameId), 0),
            @winMultiplier = @minWinMult + ROUND(RAND() * 
                            (@maxWinMult + 1 - @minWinMult), 0)
    -- set up the waitfor variable
    SELECT @waitfor = FORMATMESSAGE('00:00:00.%i', @delay);
    --check if win
    IF(CAST((@minWinIndicator + ROUND(RAND() * 
             (@maxWinIndicator + 1 - @minWinIndicator), 
                 0)) AS int) % 3) = 0
    BEGIN
        SET @payout =   @wager * @winMultiplier;
    END

    EXEC dbo.pr_LogWager    @UserID = @userId,
                            @GameID = @gameId,
                            @WagerAmount = @wager,
                            @PayoutAmount = @payout,
                            @EventTime = @eventTime;
  
    SELECT @noLoops += 1, @delay = null, @wager = null, 
           @userId = null, @gameId = null, @winMultiplier = null,  
         @waitfor = '', @payout = 0;

    WAITFOR DELAY @waitfor;

  END
END  
GO
</code></pre>

<p><strong>Code Snippet 22:</strong> <em>Procedure to Generate Game Play</em></p>

<p>As we see in <em>Code Snippet 22</em>, the procedure is looping, and in each loop, it generates some random values based on min and max setting variables.</p>]]></content>
    <author>
      <name>Niels Berglund</name>

    </author>
  </entry>
  
</feed>

